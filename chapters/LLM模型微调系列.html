<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.md Â· æ·±åº¦å­¦ä¹ ç›¸å…³å­¦ä¹ è®°å½•</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="LLMæ¨¡å‹å¾®è°ƒç³»åˆ—" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.html" rel="next"/>
<link href="../" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"narutohyc","repo":"bk_python","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="è¾“å…¥å¹¶æœç´¢" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">ä¹¦ç±ä¸»é¡µ</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter active" data-level="1.2" data-path="LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.html" id="chapter_id_1">
<a href="LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.html">
<b>1.2.</b>
                    
                    LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.md
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.html" id="chapter_id_2">
<a href="LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.html">
<b>1.3.</b>
                    
                    LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="dl_in_vision_field.html" id="chapter_id_3">
<a href="dl_in_vision_field.html">
<b>1.4.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.html" id="chapter_id_4">
<a href="huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.html">
<b>1.5.</b>
                    
                    huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.md
            
                </a>
</li>
<li class="chapter" data-level="1.6" data-path="nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.html" id="chapter_id_5">
<a href="nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.html">
<b>1.6.</b>
                    
                    nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.md
            
                </a>
</li>
<li class="chapter" data-level="1.7" data-path="pytorchå­¦ä¹ .html" id="chapter_id_6">
<a href="pytorchå­¦ä¹ .html">
<b>1.7.</b>
                    
                    pytorchå­¦ä¹ .md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="transformer.html" id="chapter_id_7">
<a href="transformer.html">
<b>1.8.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="å›¾åƒåˆ†å‰²ç®—æ³•.html" id="chapter_id_8">
<a href="å›¾åƒåˆ†å‰²ç®—æ³•.html">
<b>1.9.</b>
                    
                    å›¾åƒåˆ†å‰²ç®—æ³•.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="å›¾åƒåˆ†ç±»ç®—æ³•.html" id="chapter_id_9">
<a href="å›¾åƒåˆ†ç±»ç®—æ³•.html">
<b>1.10.</b>
                    
                    å›¾åƒåˆ†ç±»ç®—æ³•.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="æ•°æ®æ ‡æ³¨å·¥å…·.html" id="chapter_id_10">
<a href="æ•°æ®æ ‡æ³¨å·¥å…·.html">
<b>1.11.</b>
                    
                    æ•°æ®æ ‡æ³¨å·¥å…·.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹ä¼˜åŒ–å™¨.html" id="chapter_id_11">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹ä¼˜åŒ–å™¨.html">
<b>1.12.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹ä¼˜åŒ–å™¨.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æŸå¤±å‡½æ•°.html" id="chapter_id_12">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æŸå¤±å‡½æ•°.html">
<b>1.13.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æŸå¤±å‡½æ•°.md
            
                </a>
</li>
<li class="chapter" data-level="1.14" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æ¿€æ´»å‡½æ•°.html" id="chapter_id_13">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æ¿€æ´»å‡½æ•°.html">
<b>1.14.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æ¿€æ´»å‡½æ•°.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸºç¡€çŸ¥è¯†ç‚¹.html" id="chapter_id_14">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸºç¡€çŸ¥è¯†ç‚¹.html">
<b>1.15.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸºç¡€çŸ¥è¯†ç‚¹.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æŠ€æœ¯.html" id="chapter_id_15">
<a href="æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æŠ€æœ¯.html">
<b>1.16.</b>
                    
                    æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æŠ€æœ¯.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªç®—æ³•.html" id="chapter_id_16">
<a href="ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªç®—æ³•.html">
<b>1.17.</b>
                    
                    ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªç®—æ³•.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            æœ¬ä¹¦ä½¿ç”¨ GitBook å‘å¸ƒ
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#llmæ¨¡å‹">1 LLMæ¨¡å‹</a></li><ul><li><span class="title-icon"></span><a href="#æ¦‚è¿°">1.1 æ¦‚è¿°</a></li><li><span class="title-icon"></span><a href="#å¾®è°ƒ">1.2 å¾®è°ƒ</a></li><li><span class="title-icon"></span><a href="#å‘å±•è„‰ç»œ">1.3 å‘å±•è„‰ç»œ</a></li></ul><li><span class="title-icon"></span><a href="#prefixprompt-tuning">2 Prefix/Prompt-Tuning</a></li><ul><li><span class="title-icon"></span><a href="#prefix-tuning">2.1 Prefix-Tuning</a></li><li><span class="title-icon"></span><a href="#prompt-tuning">2.2 Prompt-Tuning</a></li><li><span class="title-icon"></span><a href="#p-tuning">2.3 P-Tuning</a></li><ul><li><span class="title-icon"></span><a href="#p-tuning-v1">2.3.1 P-Tuning V1</a></li><li><span class="title-icon"></span><a href="#p-tuning-v2">2.3.2 P-Tuning V2</a></li></ul></ul><li><span class="title-icon"></span><a href="#loraç³»åˆ—">3 LORAç³»åˆ—</a></li><ul><li><span class="title-icon"></span><a href="#loraè½¬">3.1 LORA(è½¬)</a></li><li><span class="title-icon"></span><a href="#adaloraè½¬">3.2 AdaLoRA(è½¬)</a></li><li><span class="title-icon"></span><a href="#qlora">3.3 QLORA</a></li></ul><li><span class="title-icon"></span><a href="#rlhf">4 RLHF</a></li><li><span class="title-icon"></span><a href="#flashattenè½¬">5 Flash_Atten(è½¬)</a></li><ul><li><span class="title-icon"></span><a href="#æ¦‚è¿°_2">5.1 æ¦‚è¿°</a></li><li><span class="title-icon"></span><a href="#æ ¸å¿ƒè¦ç‚¹">5.2 æ ¸å¿ƒè¦ç‚¹</a></li><li><span class="title-icon"></span><a href="#æå‡ºé—®é¢˜">5.3 æå‡ºé—®é¢˜</a></li><li><span class="title-icon"></span><a href="#è§£å†³æ–¹æ¡ˆ">5.4 è§£å†³æ–¹æ¡ˆ</a></li><li><span class="title-icon"></span><a href="#forward">5.5 Forward</a></li><ul><li><span class="title-icon"></span><a href="#standard-attention">5.5.1 Standard Attention</a></li><li><span class="title-icon"></span><a href="#flashattentiontiling">5.5.2 FlashAttention(Tiling)</a></li></ul><li><span class="title-icon"></span><a href="#ioå¤æ‚åº¦åˆ†æ">5.6 IOå¤æ‚åº¦åˆ†æ</a></li><ul><li><span class="title-icon"></span><a href="#standard-attention_1">5.6.1 Standard Attention</a></li><li><span class="title-icon"></span><a href="#flashattention">5.6.2 FlashAttention</a></li></ul><li><span class="title-icon"></span><a href="#backward">5.7 Backward</a></li><ul><li><span class="title-icon"></span><a href="#ç†è®ºåŸºç¡€">5.7.1 ç†è®ºåŸºç¡€</a></li><li><span class="title-icon"></span><a href="#ä»£ç å®ç°">5.7.2 ä»£ç å®ç°</a></li><li><span class="title-icon"></span><a href="#block-sparse">5.7.3 Block-Sparse</a></li></ul><li><span class="title-icon"></span><a href="#å®éªŒéªŒè¯">5.8 å®éªŒéªŒè¯</a></li></ul></ul></div><a href="#llmæ¨¡å‹" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><h1 id="llmæ¨¡å‹">1 LLMæ¨¡å‹</h1>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/626377667" target="_blank">ä»€ä¹ˆæ˜¯LLMï¼ˆå¤§è¯­éŸ³æ¨¡å‹ï¼‰</a></p>
</blockquote>
<h2 id="æ¦‚è¿°">1.1 æ¦‚è¿°</h2>
<p><code>Large Language Model(LLM)</code>ï¼Œä¹Ÿç§°ä¸º<code>å¤§å‹è¯­è¨€æ¨¡å‹</code>ï¼Œæ˜¯ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡å¯¹å¤§é‡çš„æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ¥å­¦ä¹ æœåŠ¡äººç±»è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›</p>
<p>LLMçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤§è§„æ¨¡çš„æ— ç›‘ç£è®­ç»ƒæ¥å­¦ä¹ è‡ªç„¶è¯­è¨€çš„æ¨¡å¼å’Œè¯­è¨€ç»“æ„ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„è¯­è¨€è®¤çŸ¥å’Œç”Ÿæˆè¿‡ç¨‹</p>
<p>ä¸ä¼ ç»Ÿçš„NLPæ¨¡å‹ç›¸æ¯”ï¼ŒLLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆè‡ªç„¶æ–‡æœ¬ï¼ŒåŒæ—¶è¿˜èƒ½å¤Ÿè¡¨ç°å‡ºä¸€å®šçš„é€»è¾‘æ€ç»´å’Œæ¨ç†èƒ½åŠ›</p>
<p>è¿‘å¹´æ¥ï¼ŒLLMå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œå…¶ä¸­æœ€å…·ä»£è¡¨æ€§çš„æ˜¯è°·æ­Œçš„BERTå’ŒOpenAIçš„GPTç³»åˆ—ã€‚è¿™äº›æ¨¡å‹åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ã€è‡ªåŠ¨é—®ç­”ç­‰</p>
<p>ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒLLMé¢ä¸´ç€æ›´å¤šçš„æŒ‘æˆ˜</p>
<ol>
<li>é¦–å…ˆï¼ŒLLMéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡çš„æ•°æ®é›†æ¥è®­ç»ƒï¼Œè¿™å¯¹äºä¸€èˆ¬çš„ä¼ä¸šå’Œä¸ªäººæ¥è¯´ååˆ†å›°éš¾</li>
<li>å…¶æ¬¡ï¼Œç”±äºLLMæ¨¡å‹çš„å¤æ‚æ€§å’Œè®¡ç®—é‡è¾ƒå¤§ï¼Œå¯¹äºå®æ—¶çš„è¯­è¨€å¤„ç†åº”ç”¨æ¥è¯´ï¼ŒLLMåœ¨åº”ç”¨æ•ˆç‡å’Œå“åº”é€Ÿåº¦ä¸Šè¿˜å­˜åœ¨ä¸€å®šçš„å±€é™æ€§</li>
</ol>
<p>å› æ­¤ï¼Œå¦‚ä½•è§£å†³æ¨¡å‹è®­ç»ƒå’Œåº”ç”¨è¿‡ç¨‹ä¸­çš„è®¡ç®—æ€§èƒ½å’Œæ•ˆç‡é—®é¢˜ï¼Œæ˜¯LLMé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€</p>
<h2 id="å¾®è°ƒ">1.2 å¾®è°ƒ</h2>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/622810394" target="_blank">LLMå¤§æ¨¡å‹ä½èµ„æºå¾®è°ƒp tuning v2å’ŒloraåŒºåˆ«</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/627921175" target="_blank">prefix, p-tuningv2, lora finetuneè¯¥æ€ä¹ˆé€‰æ‹©?</a></p>
<p><a href="http://news.sohu.com/a/670981907_121119001" target="_blank">è®©å¤©ä¸‹æ²¡æœ‰éš¾Tuningçš„å¤§æ¨¡å‹ï¼šPEFTæŠ€æœ¯ç®€ä»‹ 2023-04</a></p>
</blockquote>
<p><code>å¾®è°ƒ(Fine-tuning)</code>æ˜¯ä¸€ç§å¸¸ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹é€‚åº”äºç‰¹å®šçš„ä»»åŠ¡æˆ–é¢†åŸŸã€‚å¾®è°ƒçš„ç›®çš„æ˜¯é€šè¿‡åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œæœ‰ç›‘ç£çš„è®­ç»ƒï¼Œè°ƒæ•´æ¨¡å‹å‚æ•°ä»¥æé«˜å…¶æ€§èƒ½å’Œé€‚åº”æ€§</p>
<p>ä»¥ä¸‹æ˜¯å¾®è°ƒåœ¨é€‚åº”è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§çš„å‡ ä¸ªåŸå› ï¼š</p>
<ol>
<li><strong>è¿ç§»å­¦ä¹ </strong>ï¼šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†æ— ç›‘ç£çš„å­¦ä¹ ï¼Œä»ä¸­å­¦ä¹ åˆ°äº†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºã€‚é€šè¿‡å¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›é€šç”¨çš„è¯­è¨€è¡¨ç¤ºè¿ç§»åˆ°ç‰¹å®šä»»åŠ¡æˆ–é¢†åŸŸä¸Šï¼Œå› æ­¤å¯ä»¥åˆ©ç”¨æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦åˆ°çš„çŸ¥è¯†</li>
<li><strong>å°‘æ ·æœ¬å­¦ä¹ </strong>ï¼šå¾®è°ƒé€šå¸¸åªéœ€è¦åœ¨ç‰¹å®šä»»åŠ¡çš„ç›¸å¯¹è¾ƒå°çš„æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ã€‚è¿™å¯¹äºè®¸å¤šä»»åŠ¡æ¥è¯´æ˜¯éå¸¸æœ‰ç›Šçš„ï¼Œå› ä¸ºè·å¾—å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®å¯èƒ½æ˜¯æ˜‚è´µæˆ–å›°éš¾çš„ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¾®è°ƒå¯ä»¥åœ¨å°‘é‡æ ‡æ³¨æ ·æœ¬ä¸Šå®ç°è¾ƒå¥½çš„æ€§èƒ½</li>
<li><strong>é¢†åŸŸè‡ªé€‚åº”</strong>ï¼šé€šè¿‡å¾®è°ƒï¼Œå¯ä»¥å°†è¯­è¨€æ¨¡å‹ä»é€šç”¨é¢†åŸŸé€‚åº”åˆ°ç‰¹å®šé¢†åŸŸã€‚é€šè¿‡åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®ä¸Šå¾®è°ƒï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°è¯¥é¢†åŸŸçš„ç‰¹å®šè¯­è¨€æ¨¡å¼ã€è¯æ±‡å’Œä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜åœ¨è¯¥é¢†åŸŸä»»åŠ¡ä¸Šçš„æ€§èƒ½</li>
<li><strong>æ¨¡å‹ä¸ªæ€§åŒ–</strong>ï¼šå¾®è°ƒè¿˜å¯ä»¥ç”¨äºä¸ªæ€§åŒ–æ¨¡å‹ï¼Œä»¥é€‚åº”ç‰¹å®šç”¨æˆ·æˆ–ç‰¹å®šåº”ç”¨åœºæ™¯çš„éœ€æ±‚ã€‚é€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®ä¸ªä½“ç”¨æˆ·çš„åå¥½ã€è¡Œä¸ºæˆ–æ•°æ®ç‰¹ç‚¹è¿›è¡Œå®šåˆ¶ï¼Œæä¾›æ›´å‡†ç¡®å’Œä¸ªæ€§åŒ–çš„é¢„æµ‹å’Œæ¨è</li>
</ol>
<p>å¾®è°ƒè¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡è¿ç§»å­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ ã€é¢†åŸŸè‡ªé€‚åº”å’Œæ¨¡å‹ä¸ªæ€§åŒ–ç­‰æ–¹å¼ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡æˆ–é¢†åŸŸä¸Šçš„æ€§èƒ½å’Œé€‚åº”æ€§</p>
<blockquote>
<p>ä¸ºä»€ä¹ˆéœ€è¦å¾®è°ƒ</p>
</blockquote>
<ol>
<li>é«˜æ•ˆè®­ç»ƒï¼Œå‡å°‘è®­ç»ƒæˆæœ¬</li>
<li>å…±äº«åŸºç¡€å¤§æ¨¡å‹ï¼Œåœ¨ä¸Šé¢å åŠ è‡ªå·±çš„æ–°æ¨¡å‹</li>
</ol>
<h2 id="å‘å±•è„‰ç»œ">1.3 å‘å±•è„‰ç»œ</h2>
<p><a data-lightbox="5eb77559-2afb-41cb-89be-98533d2320bc" data-title="LLMå¾®è°ƒæŠ€æœ¯å‘å±•è„‰ç»œ" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/LLMå¾®è°ƒæŠ€æœ¯å‘å±•è„‰ç»œ.svg" target="_blank"><img alt="LLMå¾®è°ƒæŠ€æœ¯å‘å±•è„‰ç»œ" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/LLMå¾®è°ƒæŠ€æœ¯å‘å±•è„‰ç»œ.svg"/></a></p>
<blockquote>
<p>Adapterç³»åˆ—</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2005.00247.pdf" target="_blank">AdapterFusion: Non-Destructive Task Composition for Transfer Learning 2021</a></p>
<p><a href="https://arxiv.org/pdf/2105.07148.pdf" target="_blank">Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter 2021</a></p>
<p><a href="https://arxiv.org/pdf/2303.16199.pdf" target="_blank">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention 2023</a></p>
<p><a href="https://arxiv.org/pdf/2304.15010.pdf" target="_blank">LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model 2023</a></p>
<p><a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">github LLaMA-Adapter: Efficient Fine-tuning of LLaMA</a></p>
<blockquote>
<p>p-tunningç³»åˆ—</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf" target="_blank">Prefix-Tuning: Optimizing Continuous Prompts for Generation 2021</a></p>
<p><a href="https://aclanthology.org/2021.emnlp-main.243.pdf" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning 2021</a></p>
<p><a href="https://arxiv.org/pdf/2103.10385.pdf" target="_blank">P-Tuning - GPT Understands, Too 2021</a></p>
<p><a href="https://arxiv.org/pdf/2110.07602.pdf" target="_blank">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks 2022</a></p>
<blockquote>
<p>loraç³»åˆ—</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 2021</a></p>
<p><a href="https://arxiv.org/pdf/2303.10512.pdf" target="_blank">AdaLoRA Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning 2023</a></p>
<p><a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank">QLORA: Efficient Finetuning of Quantized LLM 2023</a></p>
<p>å¦å¤–huggingfaceå¾ˆè´´å¿ƒçš„æŠŠå¸¸è§çš„fine-Tuningæ–¹æ³•éƒ½åšäº†é›†æˆï¼Œåªç”¨å‡ è¡Œä»£ç å°±å¯æ·»åŠ å’Œä¿®æ”¹ï¼Œååˆ†æ–¹ä¾¿ï¼Œè¿˜æœ‰å¾®è½¯æä¾›çš„åŠ é€Ÿåº“</p>
<ul>
<li><a href="https://huggingface.co/docs/peft/index" target="_blank">huggingfaceå®˜ç½‘å®ç°çš„fine-Tuningæ–¹æ³•</a></li>
<li><a href="https://github.com/microsoft/DeepSpeed/tree/master" target="_blank">microsoft/DeepSpeed åŠ é€Ÿ</a></li>
</ul>
<blockquote>
<p><a href="https://www.cnblogs.com/gogoSandy/p/17202169.html" target="_blank">è§£å¯†Promptç³»åˆ—3. å†»ç»“LMå¾®è°ƒPrompt: Prefix-Tuning &amp; Prompt-Tuning &amp; P-Tuning</a></p>
</blockquote>
<p>å¾®è°ƒLMå’Œå…¨éƒ¨å†»ç»“çš„promptæ¨¡æ¿ç›¸æ¯”ï¼Œå¾®è°ƒPromptèŒƒå¼æœ€å¤§çš„åŒºåˆ«å°±æ˜¯promptæ¨¡æ¿éƒ½æ˜¯è¿ç»­å‹(Embedding)ï¼Œè€Œéå’ŒTokenå¯¹åº”çš„ç¦»æ•£å‹æ¨¡æ¿</p>
<p>æ ¸å¿ƒåœ¨äºæˆ‘ä»¬å¹¶ä¸å…³å¿ƒpromptæœ¬èº«æ˜¯å¦æ˜¯è‡ªç„¶è¯­è¨€ï¼Œåªå…³å¿ƒpromptä½œä¸ºæ¢é’ˆèƒ½å¦å¼•å¯¼å‡ºé¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ç‰¹å®šèƒ½åŠ›</p>
<p>å›ºå®šLMå¾®è°ƒPromptçš„èŒƒå¼æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜ç‚¹</p>
<ul>
<li><strong>æ€§ä»·æ¯”é«˜</strong>: å¾®è°ƒå‚æ•°å°‘ï¼Œå†»ç»“LMåªå¾®è°ƒpromptéƒ¨åˆ†çš„å‚æ•°</li>
<li><strong>æ— äººå·¥å‚ä¸</strong>: æ— éœ€äººå·¥è®¾è®¡promptæ¨¡æ¿ï¼Œä¾èµ–æ¨¡å‹å¾®è°ƒå³å¯</li>
<li><strong>å¤šä»»åŠ¡å…±äº«æ¨¡å‹</strong>: å› ä¸ºLMè¢«å†»ç»“ï¼Œåªéœ€è®­ç»ƒé’ˆå¯¹ä¸åŒä»»åŠ¡çš„promptå³å¯ã€‚å› æ­¤å¯ä»¥å›ºå®šé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‹”æ’å¼åŠ å…¥Promptç”¨äºä¸åŒä¸‹æ¸¸ä»»åŠ¡</li>
</ul>
<h1 id="prefixprompt-tuning">2 Prefix/Prompt-Tuning</h1>
<h2 id="prefix-tuning">2.1 Prefix-Tuning</h2>
<p>ç­‰å¾…...</p>
<p>Prefix-Tuningå¯ä»¥ç†è§£æ˜¯CTRL[1]æ¨¡å‹çš„è¿ç»­åŒ–å‡çº§ç‰ˆï¼Œä¸ºäº†ç”Ÿæˆä¸åŒé¢†åŸŸå’Œè¯é¢˜çš„æ–‡æœ¬ï¼ŒCTRLæ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µåœ¨è¾“å…¥æ–‡æœ¬å‰åŠ å…¥äº†control codeï¼Œä¾‹å¦‚å¥½è¯„å‰é¢åŠ 'Reviews Rating:5.0',å·®è¯„å‰é¢åŠ 'Reviews Rating:1.0', æ”¿æ²»è¯„è®ºå‰é¢åŠ â€˜Politics Title:â€™ï¼ŒæŠŠè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆæ¦‚ç‡ï¼Œä¼˜åŒ–æˆäº†åŸºäºæ–‡æœ¬ä¸»é¢˜çš„æ¡ä»¶æ¦‚ç‡</p>
<p>Prefix-Tuningè¿›ä¸€æ­¥æŠŠcontrol codeä¼˜åŒ–æˆäº†è™šæ‹ŸTokenï¼Œæ¯ä¸ªNLPä»»åŠ¡å¯¹åº”å¤šä¸ªè™šæ‹ŸTokençš„Embeddingï¼ˆprefixï¼‰ï¼Œå¯¹äºDecoder-Onlyçš„GPTï¼ŒprefixåªåŠ åœ¨å¥é¦–ï¼Œå¯¹äºEncoder-Decoderçš„BARTï¼Œä¸åŒçš„prefixåŒæ—¶åŠ åœ¨ç¼–ç å™¨å’Œè§£ç å™¨çš„å¼€å¤´ã€‚åœ¨ä¸‹æ¸¸å¾®è°ƒæ—¶ï¼ŒLMçš„å‚æ•°è¢«å†»ç»“ï¼Œåªæœ‰prefixéƒ¨åˆ†çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚ä¸è¿‡è¿™é‡Œçš„prefixå‚æ•°ä¸åªåŒ…æ‹¬embeddingå±‚è€Œæ˜¯è™šæ‹Ÿtokenä½ç½®å¯¹åº”çš„æ¯ä¸€å±‚çš„activationéƒ½è¿›è¡Œæ›´æ–°</p>
<h2 id="prompt-tuning">2.2 Prompt-Tuning</h2>
<blockquote>
<p><a href="https://github.com/google-research/prompt-tuning" target="_blank">https://github.com/google-research/prompt-tuning</a></p>
</blockquote>
<p>ç­‰å¾…...</p>
<p>Prompt-Tunningæ˜¯ä»¥ä¸Šprefix-Tunningçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œé¢å‘NLUä»»åŠ¡ï¼Œè¿›è¡Œäº†æ›´å…¨é¢çš„æ•ˆæœå¯¹æ¯”ï¼Œå¹¶ä¸”åœ¨å¤§æ¨¡å‹ä¸ŠæˆåŠŸæ‰“å¹³äº†LMå¾®è°ƒçš„æ•ˆæœ</p>
<p>å¯¹æ¯”Prefix-Tunningï¼Œprompt-tuningçš„ä¸»è¦å·®å¼‚å¦‚ä¸‹:</p>
<p>è®ºæ–‡ä½¿ç”¨100ä¸ªprefix tokenä½œä¸ºé»˜è®¤å‚æ•°ï¼Œå¤§äºä»¥ä¸Šprefix-tuningé»˜è®¤çš„10ä¸ªtokenï¼Œä¸è¿‡å·®å¼‚åœ¨äºprompt-Tunningåªå¯¹è¾“å…¥å±‚(Embedding)è¿›è¡Œå¾®è°ƒï¼Œè€ŒPrefixæ˜¯å¯¹è™šæ‹ŸTokenå¯¹åº”çš„ä¸Šæ¸¸layerå…¨éƒ¨è¿›è¡Œå¾®è°ƒã€‚å› æ­¤Prompt-Tunningçš„å¾®è°ƒå‚æ•°é‡çº§è¦æ›´å°ï¼Œä¸”ä¸éœ€è¦ä¿®æ”¹åŸå§‹æ¨¡å‹ç»“æ„ï¼Œè¿™æ˜¯â€œç®€åŒ–â€çš„æ¥æºã€‚ç›¸åŒçš„prefixé•¿åº¦ï¼ŒPrompt-Tunning(&lt;0.01%)å¾®è°ƒçš„å‚æ•°é‡çº§è¦æ¯”Prefix-Tunning(0.1%~1%)å°10å€ä»¥ä¸Š</p>
<h2 id="p-tuning">2.3 P-Tuning</h2>
<h3 id="p-tuning-v1">2.3.1 P-Tuning V1</h3>
<blockquote>
<p><a href="https://github.com/THUDM/P-tuning" target="_blank">github THUDM/P-tuning</a></p>
</blockquote>
<p>æ‰‹åŠ¨å°è¯•æœ€ä¼˜çš„æç¤ºæ— å¼‚äºå¤§æµ·æé’ˆï¼Œäºæ˜¯ä¾¿æœ‰äº†è‡ªåŠ¨ç¦»æ•£æç¤ºæœç´¢çš„æ–¹æ³•(å·¦å›¾)ï¼Œä½†æç¤ºæ˜¯ç¦»æ•£çš„ï¼Œç¥ç»ç½‘ç»œæ˜¯è¿ç»­çš„ï¼Œæ‰€ä»¥å¯»æ‰¾çš„æœ€ä¼˜æç¤ºå¯èƒ½æ˜¯æ¬¡ä¼˜çš„ã€‚p-tuningä¾ç„¶æ˜¯å›ºå®šLLMå‚æ•°ï¼Œåˆ©ç”¨å¤šå±‚æ„ŸçŸ¥æœºå’ŒLSTMå¯¹promptè¿›è¡Œç¼–ç ï¼Œç¼–ç ä¹‹åä¸å…¶ä»–å‘é‡è¿›è¡Œæ‹¼æ¥ä¹‹åæ­£å¸¸è¾“å…¥LLMã€‚æ³¨æ„ï¼Œè®­ç»ƒä¹‹ååªä¿ç•™promptç¼–ç ä¹‹åçš„å‘é‡å³å¯ï¼Œæ— éœ€ä¿ç•™ç¼–ç å™¨</p>
<p><a data-lightbox="73244259-6916-493e-af57-508d2950b0c9" data-title="p_tunningæ¶æ„v1 vs ç¦»æ•£å‹promote" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/p_tunningæ¶æ„v1 vs ç¦»æ•£å‹promote.webp" target="_blank"><img alt="p_tunningæ¶æ„v1 vs ç¦»æ•£å‹promote" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/p_tunningæ¶æ„v1 vs ç¦»æ•£å‹promote.webp"/></a></p>
<blockquote>
<p>åŠ¨æœº</p>
</blockquote>
<ul>
<li>ä¸€ä¸ªåˆ»æ¿å°è±¡æ˜¯GPTä¸é€‚åˆç†è§£ç±»ä»»åŠ¡ï¼Œè¿™ç¯‡å°±æ˜¯å»æ€è€ƒè¿™ç§åˆ»æ¿å°è±¡æ˜¯å¦æ­£ç¡®</li>
<li>GPT-3é‡‡ç”¨äººå·¥æ„é€ çš„æ¨¡ç‰ˆæ¥åšin context learningï¼Œäººå·¥è®¾è®¡çš„æ¨¡ç‰ˆçš„å˜åŒ–ç‰¹åˆ«æ•æ„Ÿï¼ŒåŠ ä¸€ä¸ªè¯æˆ–è€…å°‘ä¸€ä¸ªè¯ï¼Œæˆ–è€…å˜åŠ¨ä½ç½®å•¥çš„éƒ½ä¼šé€ æˆæ¯”è¾ƒå¤§çš„å˜åŒ–ï¼ˆè¿™é‡Œä½œè€…åšäº†ä¸€ä¸ªç®€å•çš„éªŒè¯å®éªŒï¼Œå…·ä½“çœ‹è®ºæ–‡ï¼‰ã€‚è¿‘æ¥çš„è‡ªåŠ¨åŒ–æœç´¢æ¨¡ç‰ˆå·¥ä½œæˆæœ¬ä¹Ÿæ¯”è¾ƒé«˜ï¼ŒåŒæ—¶ä»¥å‰è¿™ç§ç¦»æ•£åŒ–çš„tokençš„æœç´¢å‡ºæ¥çš„ç»“æœå¯èƒ½å¹¶ä¸æ˜¯æœ€ä¼˜çš„</li>
</ul>
<p>å’Œprefix-tuningå·®ä¸å¤šï¼Œåæ­£æ˜¯åŸºäºè¿™ä¸¤ç‚¹å»è®¾è®¡äº†ä¸€ç§è¿ç»­å¯å¾®çš„æ¨¡ç‰ˆ</p>
<p>ç›¸æ¯”prefix-tuningï¼Œè¿™é‡ŒåŠ äº†å¯å¾®çš„virtual tokenï¼Œä½†æ˜¯ä»…é™äºè¾“å…¥ï¼Œæ²¡æœ‰åœ¨æ¯å±‚åŠ ï¼›å¦å¤–virtual tokençš„ä½ç½®ä¹Ÿä¸ä¸€å®šæ˜¯å‰ç¼€ï¼Œæ’å…¥çš„ä½ç½®æ˜¯å¯é€‰çš„ã€‚è¿™é‡Œçš„å‡ºå‘ç‚¹å®é™…æ˜¯æŠŠä¼ ç»Ÿäººå·¥è®¾è®¡æ¨¡ç‰ˆä¸­çš„çœŸå®tokenæ›¿æ¢æˆå¯å¾®çš„virtual token</p>
<h3 id="p-tuning-v2">2.3.2 P-Tuning V2</h3>
<blockquote>
<p><a href="https://github.com/THUDM/P-tuning-v2" target="_blank">github THUDM/P-tuning-v2</a></p>
<p><a href="https://blog.csdn.net/as949179700/article/details/130900814" target="_blank">P-tuning V2è®ºæ–‡å’Œä»£ç å®ç°è¯¦è§£</a></p>
</blockquote>
<h4 id="æ¦‚è¿°_1"><a class="anchor-navigation-ex-anchor" href="#æ¦‚è¿°_1" name="æ¦‚è¿°_1"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#æ¦‚è¿°" name="æ¦‚è¿°"><i aria-hidden="true" class="fa fa-link"></i></a>æ¦‚è¿°</h4>
<p><a data-lightbox="c0eecf7b-d030-408a-b3e7-b24f37b03155" data-title="p_tunningæ¶æ„v1 vs v2" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/p_tunningæ¶æ„v1 vs v2.webp" target="_blank"><img alt="p_tunningæ¶æ„v1 vs v2" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/p_tunningæ¶æ„v1 vs v2.webp"/></a></p>
<h4 id="ä»£ç ç¤ºä¾‹"><a class="anchor-navigation-ex-anchor" href="#ä»£ç ç¤ºä¾‹" name="ä»£ç ç¤ºä¾‹"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#ä»£ç ç¤ºä¾‹" name="ä»£ç ç¤ºä¾‹"><i aria-hidden="true" class="fa fa-link"></i></a>ä»£ç ç¤ºä¾‹</h4>
<blockquote>
<p>PrefixEncoderç±»ï¼Œä¸ºäº†è·å¾—è¿ç»­promptï¼Œè®¾è®¡çš„æ¨¡å—</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PrefixEncoder</span><span class="hljs-params">(torch.nn.Module)</span>:</span>
    <span class="hljs-string">r'''
    The torch.nn model to encode the prefix

    Input shape: (batch-size, prefix-length)

    Output shape: (batch-size, prefix-length, 2*layers*hidden)
    '''</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__()
        self.prefix_projection = config.prefix_projection
        <span class="hljs-keyword">if</span> self.prefix_projection:
            <span class="hljs-comment"># Use a two-layer MLP to encode the prefix</span>
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)
            self.trans = torch.nn.Sequential(
                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),
                torch.nn.Tanh(),
                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * <span class="hljs-number">2</span> * config.hidden_size)
            )
        <span class="hljs-keyword">else</span>:
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * <span class="hljs-number">2</span> * config.hidden_size)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, prefix: torch.Tensor)</span>:</span>
        <span class="hljs-keyword">if</span> self.prefix_projection:
            prefix_tokens = self.embedding(prefix)
            past_key_values = self.trans(prefix_tokens)
        <span class="hljs-keyword">else</span>:
            past_key_values = self.embedding(prefix)
        <span class="hljs-keyword">return</span> past_key_values
</code></pre>
<p>&gt;</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BertPrefixForTokenClassification</span><span class="hljs-params">(BertPreTrainedModel)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__(config)
        self.num_labels = config.num_labels
        self.bert = BertModel(config, add_pooling_layer=<span class="hljs-keyword">False</span>)
        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)
        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)

        from_pretrained = <span class="hljs-keyword">False</span>
        <span class="hljs-keyword">if</span> from_pretrained:
            self.classifier.load_state_dict(torch.load(<span class="hljs-string">'model/checkpoint.pkl'</span>))

        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.bert.parameters():
            param.requires_grad = <span class="hljs-keyword">False</span>

        self.pre_seq_len = config.pre_seq_len
        self.n_layer = config.num_hidden_layers
        self.n_head = config.num_attention_heads
        self.n_embd = config.hidden_size // config.num_attention_heads

        self.prefix_tokens = torch.arange(self.pre_seq_len).long()
        self.prefix_encoder = PrefixEncoder(config)

        bert_param = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.bert.named_parameters():
            bert_param += param.numel()
        all_param = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.named_parameters():
            all_param += param.numel()
        total_param = all_param - bert_param
        print(<span class="hljs-string">'total param is {}'</span>.format(total_param))  <span class="hljs-comment"># 9860105</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_prompt</span><span class="hljs-params">(self, batch_size)</span>:</span>
        prefix_tokens = self.prefix_tokens.unsqueeze(<span class="hljs-number">0</span>).expand(batch_size, <span class="hljs-number">-1</span>).to(self.bert.device)
        <span class="hljs-comment"># å¾—åˆ°è¿ç»­Prompt</span>
        past_key_values = self.prefix_encoder(prefix_tokens)
        <span class="hljs-comment"># bsz, seqlen, _ = past_key_values.shape</span>
        <span class="hljs-comment"># æ”¹å˜å½¢çŠ¶</span>
        past_key_values = past_key_values.view(
            batch_size,
            self.pre_seq_len,
            self.n_layer * <span class="hljs-number">2</span>,
            self.n_head,
            self.n_embd
        )
        past_key_values = self.dropout(past_key_values)
        <span class="hljs-comment"># æ”¹å˜å½¢çŠ¶ï¼Œåˆ’åˆ†æˆæ•°ç»„ã€‚æ¯ä¸€ä¸ªæ•°ç»„å…ƒç´ å½¢çŠ¶ä¸ºï¼š(2,batch_size,n_head,seq_len,head_dim)</span>
        past_key_values = past_key_values.permute([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>]).split(<span class="hljs-number">2</span>)
        <span class="hljs-keyword">return</span> past_key_values

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            labels=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
    )</span>:</span>
        return_dict = return_dict <span class="hljs-keyword">if</span> return_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">else</span> self.config.use_return_dict

        batch_size = input_ids.shape[<span class="hljs-number">0</span>]
        past_key_values = self.get_prompt(batch_size=batch_size)
        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)
        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=<span class="hljs-number">1</span>)

        <span class="hljs-comment"># å¼€å§‹ä¼ é€’past_key_values</span>
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            past_key_values=past_key_values,
        )

        ...

        <span class="hljs-keyword">return</span> TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
</code></pre>
<p>ä¸€æ¬¡å‰å‘è®¡ç®—ä¸­ï¼ŒP-tuning v2ä¼šé€šè¿‡<code>self.get_prompt(batch_size=batch_size)</code>å¾—åˆ°è¦è¿ç»­Prompt</p>
<p>BertEncoderä¼šæ‰§è¡Œforå¾ªç¯ï¼ŒæŠŠpast_key_valuesæ‹†åˆ†åˆ°ä¸€ä¸ªä¸ªBertLayer</p>
<pre><code>self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])

...

for i, layer_module in enumerate(self.layer):
    if output_hidden_states:
        all_hidden_states = all_hidden_states + (hidden_states,)

    layer_head_mask = head_mask[i] if head_mask is not None else None
    past_key_value = past_key_values[i] if past_key_values is not None else None
    ...
    # BertLayer
    layer_module(..., past_key_value, ...)
</code></pre><p>å·§å¦™çš„åˆ©ç”¨past_key_valueså‚æ•°ï¼Œå°†past_key_valuesæ•°ç»„ä¸­æ¯ä¸€ä¸ªå…ƒç´ ï¼Œæ‹¼æ¥åˆ°BertSelfAttentionä¸­Keyå’ŒValue</p>
<p>ä»£ç è·Ÿè¸ªé“¾è·¯BertModel -&gt; BertEncoder -&gt; BertLayer -&gt; BertAttention -&gt; BertSelfAttention</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BertSelfAttention</span><span class="hljs-params">(nn.Module)</span>:</span>
    ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transpose_for_scores</span><span class="hljs-params">(self, x: torch.Tensor)</span> -&gt; torch.Tensor:</span>
        <span class="hljs-comment"># å°†å¼ é‡è½¬æ¢å½¢çŠ¶ï¼Œè°ƒæ¢ç»´åº¦ã€‚è¿™ä¸ªä»£ç ä¼šåœ¨seq_lengthç»´åº¦è¿›è¡Œæ‹¼æ¥ï¼Œå…¶ä»–ç»´åº¦ä¸å¯åŠ¨</span>
        new_x_shape = x.size()[:<span class="hljs-number">-1</span>] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.FloatTensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        output_attentions: Optional[bool] = False,
    )</span> -&gt; Tuple[torch.Tensor]:</span>
        mixed_query_layer = self.query(hidden_states)

        <span class="hljs-comment"># If this is instantiated as a cross-attention module, the keys</span>
        <span class="hljs-comment"># and values come from an encoder; the attention mask needs to be</span>
        <span class="hljs-comment"># such that the encoder's padding tokens are not attended to.</span>
        is_cross_attention = encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>

        <span class="hljs-keyword">if</span> is_cross_attention <span class="hljs-keyword">and</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            <span class="hljs-comment"># reuse k,v, cross_attentions</span>
            key_layer = past_key_value[<span class="hljs-number">0</span>]
            value_layer = past_key_value[<span class="hljs-number">1</span>]
            attention_mask = encoder_attention_mask
        <span class="hljs-keyword">elif</span> is_cross_attention:
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask
        <span class="hljs-keyword">elif</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[<span class="hljs-number">0</span>], key_layer], dim=<span class="hljs-number">2</span>)
            value_layer = torch.cat([past_key_value[<span class="hljs-number">1</span>], value_layer], dim=<span class="hljs-number">2</span>)
        <span class="hljs-keyword">else</span>:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))

        query_layer = self.transpose_for_scores(mixed_query_layer)

        ...
</code></pre>
<p>è¿™é‡Œå°±ä¼šæŠŠpast_key_valueæ‹¼æ¥åˆ°äº†åŸå§‹çš„kã€vä¸Šé¢ï¼Œè¿™æ ·å­å°±ç›¸å½“äºç»™kã€væ·»åŠ äº†é¢å¤–éœ€è¦å­¦ä¹ çš„å‚æ•°äº†ï¼Œå†å¾®è°ƒæ—¶åªæ›´æ–°è¿™éƒ¨åˆ†æ–°çš„å‚æ•°å³å¯</p>
<blockquote>
<p>P-tuning V2è¿ç»­Promptä»£ç å®ç°ä»¿çœŸä»£ç </p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment">#!/usr/bin/env Python</span>
<span class="hljs-comment"># -- coding: utf-8 --</span>

<span class="hljs-string">"""
@version: v1.0
@author: huangyc
@file: p_tuning_test.py
@Description: 
@time: 2023/6/6 15:31
"""</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transpose_for_scores</span><span class="hljs-params">(x: torch.Tensor)</span> -&gt; torch.Tensor:</span>
        new_x_shape = x.size()[:<span class="hljs-number">-1</span>] + (<span class="hljs-number">12</span>, <span class="hljs-number">64</span>)
        x = x.view(new_x_shape)
        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)

    prompt = torch.rand(<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">48</span>, <span class="hljs-number">12</span>, <span class="hljs-number">64</span>)  <span class="hljs-comment"># batch_size, seq_len, num_layer*2, num_head, head_size</span>
    prompt = prompt.permute([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>])
    print(f<span class="hljs-string">"P-tuningV2æ„é€ çš„trainable continuous embeddingså½¢çŠ¶ï¼š{prompt.shape}"</span>)
    past_key_values = prompt.split(<span class="hljs-number">2</span>)
    num_layers = <span class="hljs-number">24</span>
    hidden_dim = <span class="hljs-number">768</span>
    n_head = <span class="hljs-number">12</span>
    head_dim = hidden_dim // n_head
    all_head_size = n_head * head_dim
    hidden_states = torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>)  <span class="hljs-comment"># batch_size, seq_len, hidden_size</span>
    print(f<span class="hljs-string">"è¾“å…¥çš„å‘é‡å½¢çŠ¶ï¼š{hidden_states.shape}"</span>)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_layers):
        past_key_value = past_key_values[i]
        print(f<span class="hljs-string">"æ¯ä¸€å±‚BertLayeréœ€è¦åŠ å…¥çš„promptå½¢çŠ¶: {past_key_value.shape}"</span>)
        self_attn_past_key_value = past_key_value[:<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">else</span> <span class="hljs-keyword">None</span>
        <span class="hljs-comment"># BertSelfAttention</span>
        query = nn.Linear(hidden_dim, all_head_size)
        key = nn.Linear(hidden_dim, all_head_size)
        value = nn.Linear(hidden_dim, all_head_size)

        <span class="hljs-comment"># åŸå§‹kvçš„å¤§å°</span>
        key_layer = transpose_for_scores(key(hidden_states))
        old_key_layer_shape = key_layer.shape
        print(f<span class="hljs-string">"ç»è¿‡transpose_for_scoresåçš„keyå½¢çŠ¶ï¼š{old_key_layer_shape}"</span>)
        value_layer = transpose_for_scores(value(hidden_states))
        old_value_layer_shape = value_layer.shape
        print(f<span class="hljs-string">"ç»è¿‡transpose_for_scoresåçš„valueå½¢çŠ¶ï¼š{old_value_layer_shape}\n"</span>)

        <span class="hljs-comment"># æ‹¼æ¥åkvçš„å¤§å°</span>
        key_layer = torch.cat([past_key_value[<span class="hljs-number">0</span>], key_layer], dim=<span class="hljs-number">2</span>)
        print(
            f<span class="hljs-string">"past_key_value[0]çš„å½¢çŠ¶ï¼š{past_key_value[0].shape} åŸå§‹key_layerçš„å½¢çŠ¶ï¼š{old_key_layer_shape} ç»è¿‡catåçš„key_layerå½¢çŠ¶ï¼š{key_layer.shape}"</span>)
        value_layer = torch.cat([past_key_value[<span class="hljs-number">1</span>], value_layer], dim=<span class="hljs-number">2</span>)
        print(
            f<span class="hljs-string">"past_key_value[1]çš„å½¢çŠ¶ï¼š{past_key_value[1].shape} åŸå§‹value_layerçš„å½¢çŠ¶ï¼š{old_value_layer_shape} ç»è¿‡catåçš„value_layerå½¢çŠ¶ï¼š{value_layer.shape}\n"</span>)

        mixed_query_layer = query(hidden_states)
        print(f<span class="hljs-string">"hidden_statesç»è¿‡queryå±‚åè¾“å‡ºçš„å½¢çŠ¶ï¼š{mixed_query_layer.size()}"</span>)  <span class="hljs-comment"># batch seq len embed</span>
        query_layer = transpose_for_scores(mixed_query_layer)
        print(f<span class="hljs-string">"ç»è¿‡transpose_for_scoresåçš„queryå½¢çŠ¶{query_layer.size()}"</span>)  <span class="hljs-comment"># batch</span>

        print(<span class="hljs-string">"æ³¨æ„åŠ›åˆ†æ•°å¼€å§‹è®¡ç®—"</span>)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="hljs-number">-1</span>, <span class="hljs-number">-2</span>))
        print(f<span class="hljs-string">"attention_scoresçš„å½¢çŠ¶ï¼š{attention_scores.size()}"</span>)  <span class="hljs-comment"># batch head seq_len seq_len</span>
        print(<span class="hljs-string">"å¼€å§‹æ³¨æ„åŠ›æ±‡èšè®¡ç®—"</span>)
        context_layer = torch.matmul(attention_scores, value_layer)
        print(f<span class="hljs-string">"æ³¨æ„åŠ›æ±‡èšåè¾“å‡ºçŸ©é˜µcontext_layerçš„å½¢çŠ¶ï¼š{context_layer.size()}"</span>)  <span class="hljs-comment"># batch head seq_len embed/12</span>
        print(<span class="hljs-string">"æœ€åï¼Œå°†context_layerçš„å½¢çŠ¶æ¢å¤æˆè¾“å…¥hidden_statesçš„å½¢çŠ¶"</span>)
        context_layer = context_layer.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()
        new_context_layer_shape = context_layer.size()[:<span class="hljs-number">-2</span>] + (<span class="hljs-number">768</span>,)
        context_layer = context_layer.view(new_context_layer_shape)
        print(f<span class="hljs-string">"context_layerçš„å½¢çŠ¶æ¢å¤å®Œæˆï¼Œå…¶å½¢çŠ¶ä¸ºï¼š{context_layer.size()}"</span>)
        print(<span class="hljs-string">"ä¸€æ¬¡P-tuningV2çš„BertLayerè®¡ç®—ä»¿çœŸç»“æŸ"</span>)
        <span class="hljs-keyword">break</span>


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    run()
</code></pre>
<p>æµ‹è¯•è¾“å‡º</p>
<pre><code class="lang-python">S:\Anaconda3\envs\torch38\python.exe Q:\pyCharmWS\chatgpts\P-tuning-v2\tests\p_tuning_test.py 
P-tuningV2æ„é€ çš„trainable continuous embeddingså½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">48</span>, <span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
è¾“å…¥çš„å‘é‡å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>])
æ¯ä¸€å±‚BertLayeréœ€è¦åŠ å…¥çš„promptå½¢çŠ¶: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
ç»è¿‡transpose_for_scoresåçš„keyå½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
ç»è¿‡transpose_for_scoresåçš„valueå½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])

====================&gt; æ ¸å¿ƒ
past_key_value[<span class="hljs-number">0</span>]çš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) åŸå§‹key_layerçš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) ç»è¿‡catåçš„key_layerå½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">256</span>, <span class="hljs-number">64</span>])
past_key_value[<span class="hljs-number">1</span>]çš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) åŸå§‹value_layerçš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) ç»è¿‡catåçš„value_layerå½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">256</span>, <span class="hljs-number">64</span>])
====================&gt; æ ¸å¿ƒ

hidden_statesç»è¿‡queryå±‚åè¾“å‡ºçš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>])
ç»è¿‡transpose_for_scoresåçš„queryå½¢çŠ¶torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
æ³¨æ„åŠ›åˆ†æ•°å¼€å§‹è®¡ç®—
attention_scoresçš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>])
å¼€å§‹æ³¨æ„åŠ›æ±‡èšè®¡ç®—
æ³¨æ„åŠ›æ±‡èšåè¾“å‡ºçŸ©é˜µcontext_layerçš„å½¢çŠ¶ï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
æœ€åï¼Œå°†context_layerçš„å½¢çŠ¶æ¢å¤æˆè¾“å…¥hidden_statesçš„å½¢çŠ¶
context_layerçš„å½¢çŠ¶æ¢å¤å®Œæˆï¼Œå…¶å½¢çŠ¶ä¸ºï¼štorch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>])
ä¸€æ¬¡P-tuningV2çš„BertLayerè®¡ç®—ä»¿çœŸç»“æŸ
</code></pre>
<h1 id="loraç³»åˆ—">3 LORAç³»åˆ—</h1>
<blockquote>
<p>ç‚¹èµğŸ‘<a href="https://space.bilibili.com/1564408396" target="_blank">Bç«™åšä¸» å°æ¨ä¸åŠªåŠ›0v0</a> + <a href="https://bytedance.feishu.cn/docx/doxcn3zm448MK9sK6pHuPsqtH8f" target="_blank">åšä¸»ç›¸å…³çš„æ–‡ç« é“¾æ¥</a></p>
</blockquote>
<h2 id="loraè½¬">3.1 LORA(è½¬)</h2>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV17g4y1g7S6/?spm_id_from=333.788&amp;vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">LoRAï¼šè®­ç»ƒä½ çš„GPTã€è®ºæ–‡ç²—è¯»Â·1ã€‘</a></p>
</blockquote>
<p>ä¸€ç§é€šè¿‡<strong>ä½ç§©è¿‘ä¼¼å¢é‡çŸ©é˜µ</strong>çš„ï¼Œç»è¿‡å¹¿æ³›éªŒè¯<strong>è¶³å¤ŸRobust</strong>çš„å¾®è°ƒæ–¹æ³•</p>
<blockquote>
<p>æ‘˜è¦</p>
</blockquote>
<p>éšç€è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ¨¡å‹è§„æ¨¡çš„ä¸æ–­å¢é•¿ï¼Œç”±äºæˆæœ¬å’Œèµ„æºé™åˆ¶ï¼Œå¯¹å…¶è¿›è¡Œå®Œå…¨å¾®è°ƒä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„æŒ‘æˆ˜æ—¥ç›Šå¢åŠ </p>
<p>ä»‹ç»<code>ä½ç§©é€‚åº”(Low-Rank Adaptation)</code>ã€‚<code>LoRA</code>é€šè¿‡å¼•å…¥å‚æ•°çŸ©é˜µæ¥å‡å°‘å‚æ•°ï¼Œå¹¶å°†GPUå†…å­˜éœ€æ±‚é™ä½äº†3å€ã€‚ç›¸æ¯”äºä½¿ç”¨GPT-3è¿›è¡Œå¾®è°ƒï¼Œå®ƒå°†å‚æ•°å‡å°‘äº†10,000å€</p>
<p>å°½ç®¡å¯è®­ç»ƒå‚æ•°æ›´å°‘ï¼Œä½†LoRAåœ¨å¤§å¤šæ•°è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜äºå¾®è°ƒï¼Œå…·æœ‰æ›´é«˜çš„è®­ç»ƒååé‡å’Œæ— æ¨ç†å»¶è¿Ÿ</p>
<p>å¯¹è¯­è¨€æ¨¡å‹é€‚åº”ä¸­çš„ç§©ç¼ºå¤±è¿›è¡Œçš„å®è¯ç ”ç©¶ä¸ºLoRAçš„æœ‰æ•ˆæ€§æä¾›äº†è¯æ®ï¼ŒLoRAæ˜¯å¼€æºçš„</p>
<blockquote>
<p>ä»‹ç»</p>
</blockquote>
<p>å¯¹äºä¸‹æ¸¸ä»»åŠ¡è€Œè¨€ï¼Œå®Œå…¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ å—åˆ°[å†…åœ¨ç»´åº¦]çš„ç ”ç©¶å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†LoRAï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š</p>
<ul>
<li><strong>ä½ä»»åŠ¡åˆ‡æ¢å¼€é”€</strong>: ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥è¢«å…±äº«å¹¶ç”¨äºæ„å»ºå¤šä¸ªé’ˆå¯¹ä¸åŒä»»åŠ¡çš„å°å‹LoRAæ¨¡å—</li>
<li><strong>å‚æ•°é«˜æ•ˆ</strong>: LoRAé€šè¿‡ä½¿ç”¨è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿å¾—è®­ç»ƒæ›´é«˜æ•ˆï¼Œå¹¶å°†ç¡¬ä»¶é—¨æ§›é™ä½äº†æœ€å¤š3å€</li>
<li><strong>æ— æ¨ç†å»¶è¿Ÿ</strong>: LoRAçš„ç®€å•çº¿æ€§è®¾è®¡ä½¿å¾—å¯è®­ç»ƒçŸ©é˜µåœ¨éƒ¨ç½²æ—¶å¯ä»¥ä¸å†»ç»“æƒé‡åˆå¹¶</li>
<li><strong>æ­£äº¤æ€§</strong>: LoRAä¸è®¸å¤šå…ˆå‰çš„æ–¹æ³•æ˜¯æ­£äº¤çš„ï¼Œå¹¶ä¸”å¯ä»¥ä¸å®ƒä»¬ç»“åˆä½¿ç”¨ï¼Œæ¯”å¦‚å‰ç¼€å¾®è°ƒ(prefix-tuning)</li>
</ul>
<blockquote>
<p>é—®é¢˜æè¿°</p>
</blockquote>
<p>æ¨¡å‹è®­ç»ƒæ—¶å‚æ•°é‡è¯„ä¼°ï¼Œå¯¹äºLLMï¼Œå¦‚æœæ¨¡å‹çš„å‚æ•°æ—¶<script type="math/tex; ">\Phi</script>çš„è¯</p>
<ul>
<li>å…¨é‡å¾®è°ƒ</li>
</ul>
<p><script type="math/tex; mode=display">
\max _{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(P_{\Phi}\left(y_{t} \mid x, y_{<t}\right)\right)
</script></p>
<p>ä½¿ç”¨Adamä¼˜åŒ–å™¨ä¸‹çš„ï¼Œ<script type="math/tex; ">\mathrm{n}=8</script>ï¼Œå¹¶ä¸”ä½¿ç”¨æ··åˆç²¾åº¦ï¼Œä¸€ä¸ªå‚æ•°éœ€è¦16ä¸ªbytesæ¥å­˜å‚¨ï¼Œè¿™16ä¸ªbytesåˆ†åˆ«ä¸º</p>
<p>æƒé‡<script type="math/tex; ">W</script>éœ€è¦<script type="math/tex; ">fp16</script>æ¥å­˜å‚¨ï¼Œæ¿€æ´»å€¼<script type="math/tex; ">A</script>éœ€è¦<script type="math/tex; ">fp16</script>ï¼Œä¸ºäº†æ›´æ–°æƒé‡è¿˜éœ€å­˜ä¸€ä¸ªå¤åˆ¶<script type="math/tex; ">W_c</script>éœ€è¦<script type="math/tex; ">fp32</script>ï¼Œä¼˜åŒ–å™¨éœ€è¦å­˜ä¸¤ä¸ªå€¼ï¼Œåˆ†åˆ«æ˜¯<script type="math/tex; ">M</script>å’Œ<script type="math/tex; ">V</script>(æ–¹å·®)ï¼Œåˆ†åˆ«éœ€è¦<script type="math/tex; ">fp32</script></p>
<p>å…¶ä¸­<script type="math/tex; ">fp16</script>å 2bytesï¼Œ<script type="math/tex; ">fp32</script>å bytesï¼Œæ€»å…±ä¸º<script type="math/tex; ">2+2+4+4+4 = 16</script>bytes</p>
<p>å› æ­¤ï¼Œ<script type="math/tex; ">\Phi</script>ä¸ªå‚æ•°å°±éœ€è¦<script type="math/tex; "> \Phi * 2n</script>bytes</p>
<ul>
<li>Lora</li>
</ul>
<p><script type="math/tex; mode=display">
\max _{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(p_{\Phi_{0}+\Delta \Phi(\Theta)}\left(y_{t} \mid x, y_{<t}\right)\right)
</script></p>
<p>Loraåœ¨è®­ç»ƒæ—¶ï¼Œå°†åŸæ¥çš„å‚æ•°å›ºå®šä¸‹æ¥ï¼Œåªæ›´æ–°æ–°å¢åŠ çš„å‚æ•°ï¼Œå› æ­¤Mem Required: <script type="math/tex; ">(4 \Phi+\theta * 2 n)</script> bytes</p>
<p>åœ¨GPT-3çš„175Bå‚æ•°ä¸‹ï¼Œè¿™é‡Œçš„<script type="math/tex; ">\Theta</script>å¯ä»¥è¾¾åˆ°åŸæ¥çš„<script type="math/tex; ">0.01 \%</script></p>
<blockquote>
<p><strong>ä¸ºä»€ä¹ˆä¼šæå‡ºLoraå‘¢</strong></p>
</blockquote>
<p>åŸæ¥çš„Adapteræ–¹æ³•ä¹Ÿæ˜¯å›ºå®šæ¨¡å‹çš„å‚æ•°ï¼Œåªè®­ç»ƒMLPå‚æ•°ï¼Œä½†æ˜¯è¿™æ ·å­æœ‰å‡ ä¸ªå¼Šç«¯</p>
<ol>
<li>å¢åŠ äº†ç½‘ç»œæ·±åº¦ï¼Œ<strong>å¢åŠ äº†æ¨ç†çš„æ—¶é—´</strong></li>
<li>æ·»åŠ MLPä¹‹åï¼Œè®­ç»ƒå‡ºæ¥çš„æœ€ä¼˜æ–¹æ¡ˆä¹Ÿåªèƒ½æ”¶æ•›åˆ°MLPå±‚ï¼Œä¸ä¸€å®šæ˜¯å…¨å±€æœ€å¥½çš„</li>
<li>ç›´æ¥å»ä¼˜åŒ–è¿™ä¸ªPromoteå¹¶ä¸èƒ½ä¿è¯ä¼˜åŒ–æ˜¯å•è°ƒçš„ï¼Œä¹Ÿå°±æ˜¯<strong>ä¸æ˜¯å…¨å±€æœ€ä¼˜</strong>ï¼Œå¾ˆéš¾ä¼˜åŒ–å¥½</li>
<li>å‡å°‘äº†å¯ç”¨äºå¤„ç†ä¸‹æ¸¸ä»»åŠ¡çš„åºåˆ—é•¿åº¦ï¼Œå› ä¸ºæ–°åŠ å…¥çš„Promoteä¼šå ç”¨è¾“å…¥çš„tokené•¿åº¦</li>
</ol>
<blockquote>
<p>å…·ä½“æ–¹æ³•</p>
</blockquote>
<p>æœ€æ ¸å¿ƒçš„æ€è·¯å¦‚ä¸‹å…¬å¼æ‰€ç¤ºï¼Œç ”ç©¶è¡¨æ˜<script type="math/tex; ">\Delta W</script>é€šå¸¸æ˜¯ä¸€ä¸ªæ¬ ç§©çš„çŸ©é˜µ
<script type="math/tex; mode=display">
W \Leftarrow W_{0}+\Delta W \\

W_{0}+\Delta W=W_{0}+B A \\

h=W_{0} x+\Delta W x=W_{0} x+B A x
</script>
å› æ­¤ï¼Œ<script type="math/tex; ">\Delta W</script>å¯ä»¥è¿›è¡Œä½ç§©åˆ†è§£</p>
<p><a data-lightbox="e9879f28-a333-439b-99ba-b298aa46c889" data-title="Loraæ€è·¯" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraæ€è·¯.webp" target="_blank"><img alt="Loraæ€è·¯" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraæ€è·¯.webp"/></a></p>
<p>è®­ç»ƒæ—¶ï¼Œ<script type="math/tex; ">B</script>åˆå§‹åŒ–ä¸ºå…¨é›¶çŸ©é˜µï¼Œè¿™æ ·å­å‚æ•°é‡å°±ä»<script type="math/tex; ">d*d</script>å˜æˆäº†<script type="math/tex; ">d*2*r</script>ï¼Œè¿™é‡Œçš„<script type="math/tex; ">r</script>ä¸€èˆ¬æ˜¯è¿œå°äº<script type="math/tex; ">d</script>çš„</p>
<blockquote>
<p>ä¼˜ç‚¹</p>
</blockquote>
<ul>
<li><p>LoRAæ˜¯å¯¹å®Œå…¨å¾®è°ƒçš„ä¸€ç§æ¨å¹¿æ–¹æ³•</p>
<ul>
<li>åœ¨é€‚åº”è¿‡ç¨‹ä¸­ï¼ŒLoRAä¸éœ€è¦å¯¹æƒé‡çŸ©é˜µè¿›è¡Œå®Œå…¨ç§©çš„ç´¯ç§¯æ¢¯åº¦æ›´æ–°ï¼Œè€Œæ˜¯å¯ä»¥åŸºäºé¢„è®­ç»ƒçš„æƒé‡çŸ©é˜µè®¾ç½®ç§©</li>
<li>å½“LoRAåº”ç”¨äºæ‰€æœ‰æƒé‡çŸ©é˜µå¹¶ä¸”åç½®è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†ç±»ä¼¼äºå®Œå…¨å¾®è°ƒçš„è¡¨ç°èƒ½åŠ›</li>
</ul>
</li>
<li><p>æ²¡æœ‰é¢å¤–çš„æ¨ç†å»¶è¿Ÿ</p>
<p>åœ¨ç”Ÿäº§éƒ¨ç½²ä¸­ï¼ŒLoRAå¯ä»¥è®¡ç®—å’Œå­˜å‚¨<script type="math/tex; ">W=W_0+BA</script>ï¼Œå…¶ä¸­<script type="math/tex; ">W_0</script>å’Œ<script type="math/tex; ">ğµğ´</script>å±äº<script type="math/tex; ">â„^{ğ‘‘Ã—ğ‘˜}</script>ï¼Œå½“åˆ‡æ¢åˆ°å¦ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œå¯ä»¥é€šè¿‡å‡å»<script type="math/tex; ">ğµğ´</script>å¹¶åŠ ä¸Šä¸åŒçš„<script type="math/tex; ">ğµ'ğ´'</script>æ¥æ¢å¤<script type="math/tex; ">ğ‘Š_0</script>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿæ“ä½œï¼Œå‡ ä¹æ²¡æœ‰é¢å¤–çš„å†…å­˜å¼€é”€(<code>æ½®æ±GPU</code>)</p>
</li>
</ul>
<blockquote>
<p>ä¸ºä»€ä¹ˆä½ç§©çŸ©é˜µæœ‰æ•ˆ</p>
</blockquote>
<ol>
<li><p>å½“ç»™å®šå‚æ•°æ•°é‡æ—¶ï¼Œåº”è¯¥è°ƒæ•´é¢„è®­ç»ƒTransformerä¸­çš„å“ªäº›å…·ä½“æƒé‡çŸ©é˜µå­é›†ä»¥å®ç°æœ€ä½³çš„ä¸‹æ¸¸æ€§èƒ½ï¼Ÿ</p>
<ul>
<li><p>åœ¨ç»™å®šå‚æ•°é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œç¡®å®šè¦è°ƒæ•´çš„æƒé‡çŸ©é˜µå­é›†ä»¥å®ç°æœ€ä½³ä¸‹æ¸¸æ€§èƒ½æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå¹¶ä¸”æ²¡æœ‰å›ºå®šçš„ç­”æ¡ˆ</p>
<p>é€šå¸¸ï¼Œå¯ä»¥è€ƒè™‘æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„ç‰¹ç‚¹å’Œéœ€æ±‚è¿›è¡Œæƒè¡¡å’Œé€‰æ‹©</p>
<p>ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡å¯¹ä¸åŒæƒé‡çŸ©é˜µè¿›è¡Œå®éªŒæ€§å¾®è°ƒï¼Œå¹¶æ ¹æ®æ€§èƒ½è¯„ä¼°æ¥ç¡®å®šé€‚åˆç‰¹å®šä»»åŠ¡çš„æƒé‡çŸ©é˜µå­é›†</p>
</li>
</ul>
</li>
<li><p><strong>æœ€ä¼˜</strong>çš„é€‚åº”çŸ©é˜µ<script type="math/tex; ">\Delta W</script>æ˜¯å¦çœŸçš„æ˜¯æ¬ ç§©çš„å—ï¼Ÿå¦‚æœæ˜¯ï¼Œé‚£ä¹ˆåœ¨å®é™…æƒ…å†µä¸‹æ¨èçš„ç§©æ˜¯å¤šå°‘ï¼Ÿ</p>
<ul>
<li><strong>æœ€ä¼˜</strong>çš„é€‚åº”çŸ©é˜µ<script type="math/tex; ">\Delta W</script>æ˜¯å¦çœŸçš„æ˜¯æ¬ ç§©çš„ï¼Œè¿™å–å†³äºå…·ä½“æƒ…å†µã€‚ç§©ç¼ºå¤±æ„å‘³ç€çŸ©é˜µçš„ç§©(çŸ©é˜µçš„çº¿æ€§ç‹¬ç«‹åˆ—æ•°æˆ–è¡Œæ•°çš„æœ€å¤§æ•°é‡)è¾ƒä½</li>
<li>å¯¹äºå®é™…ç›®çš„ï¼Œå»ºè®®é€‰æ‹©é€‚å½“çš„ç§©ä»¥å¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ã€‚å…·ä½“æ¨èçš„ç§©å–å†³äºä»»åŠ¡çš„å¤æ‚æ€§ã€æ•°æ®é›†çš„è§„æ¨¡ä»¥åŠå¯ç”¨çš„è®¡ç®—èµ„æºç­‰å› ç´ </li>
</ul>
</li>
<li><p><script type="math/tex; ">\Delta W</script>å’Œ<script type="math/tex; ">W</script>ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ<script type="math/tex; ">\Delta W</script>å’Œ<script type="math/tex; ">W</script>ä¹‹é—´æ˜¯å¦å­˜åœ¨é«˜ç›¸å…³æ€§ï¼Ÿ<script type="math/tex; ">\Delta W</script>çš„å¤§å°ä¸<script type="math/tex; ">W</script>ç›¸æ¯”å¦‚ä½•ï¼Ÿ</p>
<ul>
<li><script type="math/tex; ">\Delta W</script>è¡¨ç¤ºé€‚åº”çŸ©é˜µï¼Œç”¨äºè°ƒæ•´é¢„è®­ç»ƒæƒé‡çŸ©é˜µ<script type="math/tex; ">W</script>ï¼Œ<script type="math/tex; ">\Delta W</script>å’Œ<script type="math/tex; ">W</script>ä¹‹é—´çš„å…³ç³»å–å†³äºå…·ä½“çš„é€‚åº”æ–¹æ³•å’Œä¼˜åŒ–ç®—æ³•ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ<script type="math/tex; ">\Delta W</script>å¯ä»¥é€šè¿‡å¯¹<script type="math/tex; ">W</script>çš„å¾®å°è°ƒæ•´æ¥è·å¾—ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œ<script type="math/tex; ">\Delta W</script>å¯èƒ½åŒ…å«æ›´å¤§çš„å˜åŒ–</li>
<li><script type="math/tex; ">\Delta W</script>å’Œ<script type="math/tex; ">W</script>ä¹‹é—´çš„ç›¸å…³æ€§å–å†³äºé€‚åº”æ–¹æ³•çš„è®¾è®¡å’Œä¼˜åŒ–è¿‡ç¨‹çš„ç»†èŠ‚ã€‚å®ƒä»¬å¯ä»¥å­˜åœ¨ä¸€å®šçš„ç›¸å…³æ€§ï¼Œä½†å…·ä½“æƒ…å†µå¯èƒ½å› æ¨¡å‹æ¶æ„ã€ä»»åŠ¡è¦æ±‚å’Œæ•°æ®é›†ç‰¹å¾è€Œå¼‚</li>
<li><script type="math/tex; ">\Delta W</script>çš„å¤§å°ä¸<script type="math/tex; ">W</script>çš„å¤§å°ä¹‹é—´æ²¡æœ‰å›ºå®šçš„æ¯”è¾ƒå…³ç³»ï¼Œå› ä¸ºå®ƒä»¬çš„å°ºåº¦å–å†³äºå…·ä½“çš„æ•°å€¼èŒƒå›´å’Œè°ƒæ•´æ–¹æ³•</li>
</ul>
</li>
</ol>
<blockquote>
<p>å¯¹äºattentionå‚æ•°é™„åŠ åˆ°å“ªä¸ªä¸Šæ›´æœ‰æ•ˆ</p>
</blockquote>
<p>å®éªŒåœ¨GPT-3 175Bæ¨¡å‹ä¸Šè®¾ç½®äº†ä¸€ä¸ªå‚æ•°é¢„ç®—ä¸º18M(å¦‚æœä»¥FP16å­˜å‚¨ï¼Œå¤§çº¦ä¸º35MB)ã€‚è¿™å¯¹åº”äºå½“æˆ‘ä»¬é€‚åº”ä¸€ç§æ³¨æ„åŠ›æƒé‡æ—¶r=8ï¼Œæˆ–è€…å½“æˆ‘ä»¬é€‚åº”ä¸¤ç§ç±»å‹æ—¶r=4ï¼Œé€‚ç”¨äºæ‰€æœ‰96å±‚</p>
<p><a data-lightbox="464de704-14d8-4bda-864a-204934b732a6" data-title="Loraå…³äºrå‚æ•°é€‰æ‹©çš„å®éªŒ" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraå…³äºrå‚æ•°é€‰æ‹©çš„å®éªŒ.webp" target="_blank"><img alt="Loraå…³äºrå‚æ•°é€‰æ‹©çš„å®éªŒ" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraå…³äºrå‚æ•°é€‰æ‹©çš„å®éªŒ.webp"/></a></p>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå°†æ‰€æœ‰å‚æ•°æ”¾åœ¨<script type="math/tex; ">\Delta ğ‘Š_ğ‘</script>æˆ–<script type="math/tex; ">\Delta ğ‘Š_k</script>ä¸­ä¼šå¯¼è‡´æ˜¾è‘—é™ä½æ€§èƒ½ï¼Œè€Œæ³¨å…¥åˆ°<script type="math/tex; ">ğ‘Š_ğ‘</script>å’Œ<script type="math/tex; ">ğ‘Š_v</script>åˆ™äº§ç”Ÿæœ€ä½³ç»“æœã€‚è¿™è¡¨æ˜ï¼Œå³ä½¿ç§©ä¸º4ï¼Œ<script type="math/tex; ">\Delta ğ‘Š</script>ä¸­åŒ…å«äº†è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä½¿å¾—ä¸ä½¿ç”¨å…·æœ‰è¾ƒå¤§ç§©çš„å•ä¸€ç±»å‹çš„æƒé‡ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å¤šçš„æƒé‡çŸ©é˜µæ›´å¯å–</p>
<blockquote>
<p>å›ç­”ï¼šå°†å‚æ•°è®¾ç½®åˆ°qã€vä¸Šæ—¶ï¼Œrå–å¤šå°‘åˆé€‚</p>
</blockquote>
<p><a data-lightbox="166ce514-c9e4-48ad-b7d5-c16b73b705d4" data-title="Loraå…³äºrå‚æ•°å–å¤šå°‘çš„å®éªŒ" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraå…³äºrå‚æ•°å–å¤šå°‘çš„å®éªŒ.webp" target="_blank"><img alt="Loraå…³äºrå‚æ•°å–å¤šå°‘çš„å®éªŒ" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraå…³äºrå‚æ•°å–å¤šå°‘çš„å®éªŒ.webp"/></a></p>
<p>LoRAåœ¨éå¸¸å°çš„ç§©(ç‰¹åˆ«æ˜¯å¯¹äº<script type="math/tex; ">ğ‘Š_ğ‘</script>ï¼Œ<script type="math/tex; ">ğ‘Š_ğ‘£</script>è€Œè¨€)ä¸‹å·²ç»å–å¾—äº†ç›¸å½“çš„ç«äº‰åŠ›ï¼Œè¿™è¡¨æ˜æ›´æ–°çŸ©é˜µ<script type="math/tex; ">\Delta W</script>å¯èƒ½å…·æœ‰<strong>éå¸¸å°çš„å†…åœ¨ç§©</strong>ï¼Œä½¿ç”¨ä½ç§©çŸ©é˜µå¯¹LLMè¿›è¡Œfine tuneçš„æ—¶å€™ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªéå¸¸å°çš„ä½ç§©çŸ©é˜µï¼Œå°±å¯ä»¥æ•æ‰åˆ°å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„ä¸€äº›ç‰¹å¾ä¿¡æ¯ï¼Œè¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªéå¸¸é«˜æ•ˆçš„LLMçš„fine tuneçš„æ–¹å¼ï¼ŒåŒæ—¶æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½</p>
<blockquote>
<p>å›ç­”ï¼š<script type="math/tex; ">\Delta W</script>å’Œ<script type="math/tex; ">W</script>ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆ</p>
</blockquote>
<p>é¦–å…ˆå¯¹<script type="math/tex; ">\Delta W</script>è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼Œå¹¶ä¸”æŠŠå®ƒå·¦å¥‡å¼‚å€¼å‘é‡å’Œå³å¥‡å¼‚å€¼å‘é‡ä¹˜åˆ°<script type="math/tex; ">W_q</script>ä¸Šï¼ŒæŠŠ<script type="math/tex; ">W_q</script>æ˜ å°„åˆ°<script type="math/tex; ">\Delta W_q</script>çš„å­ç©ºé—´ï¼Œå¹¶è®¡ç®—<script type="math/tex; ">F</script>èŒƒæ•°ï¼ŒåŒæ—¶è¿˜æŠŠ<script type="math/tex; ">W_q</script>æ˜ å°„åˆ°æ˜ å°„åˆ°éšæœºçŸ©é˜µä¸Š</p>
<p>ä»¥æ­¤æ¥è¯æ˜<script type="math/tex; ">\Delta W_q</script>ä¸<script type="math/tex; ">W_q</script>æœ‰æ›´å¼ºçš„ç›¸å…³æ€§</p>
<p><a data-lightbox="433c4fde-22cb-4ada-8732-a47b477ddd70" data-title="Loraä¸‹wå’Œdelta wçš„å…³ç³»" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraä¸‹wå’Œdelta wçš„å…³ç³».webp" target="_blank"><img alt="Loraä¸‹wå’Œdelta wçš„å…³ç³»" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/Loraä¸‹wå’Œdelta wçš„å…³ç³».webp"/></a></p>
<p>é¦–å…ˆï¼Œä¸éšæœºçŸ©é˜µç›¸æ¯”ï¼Œ<script type="math/tex; ">\Delta W_q</script>ä¸<script type="math/tex; ">W_q</script>å…·æœ‰<strong>æ›´å¼ºçš„ç›¸å…³æ€§</strong>ï¼Œè¡¨æ˜<script type="math/tex; ">\Delta W</script>æ”¾å¤§äº†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„<script type="math/tex; ">W</script>ä¸­å·²ç»å­˜åœ¨çš„æŸäº›ç‰¹å¾</p>
<p>å…¶æ¬¡ï¼Œ<script type="math/tex; ">\Delta W</script>ä¸æ˜¯é‡å¤<script type="math/tex; ">W</script>çš„å¥‡å¼‚å€¼æ–¹å‘ï¼Œè€Œæ˜¯åªæ”¾å¤§åœ¨<script type="math/tex; ">W</script>ä¸­æ²¡æœ‰å¼ºè°ƒçš„æ–¹å‘</p>
<p>ç¬¬ä¸‰ï¼Œæ”¾å¤§å› å­ç›¸å½“å·¨å¤§ï¼šå½“r=4æ—¶ï¼Œ21.5â‰ˆ6.91/0.32ï¼Œè¡¨æ˜<script type="math/tex; ">\Delta W</script>åªæ˜¯æ”¾å¤§äº†<script type="math/tex; ">W</script>ä¸­çš„ä¸€äº›ç‰¹å¾ï¼Œä¸”æ”¾å¤§å€æ•°æ˜¯å¾ˆå¤§çš„ï¼Œç›¸å½“äºæ˜¯æŠŠä¸‹æ¸¸ä»»åŠ¡éœ€è¦çš„ç‰¹å¾æå–å‡ºæ¥å¹¶è¿›è¡Œæ”¾å¤§</p>
<h2 id="adaloraè½¬">3.2 AdaLoRA(è½¬)</h2>
<blockquote>
<p><a href="https://readpaper.feishu.cn/docx/KWTkdA6EmonokyxdCmocdbXmnwc" target="_blank">AdaLoRAï¼šæ›´å¼ºå¤§çš„LoRA</a></p>
<p><a href="https://github.com/cauyxy/YourGPT" target="_blank">github cauyxy/YourGPT</a></p>
</blockquote>
<p>Loraä¸­çš„ræ˜¯ä¸€ä¸ªç¡®å®šå€¼ï¼Œä½†ä¸æ˜¯å¯¹äºæ‰€æœ‰çš„å±‚ï¼Œåƒqã€kè¿™æ ·çš„å±‚ï¼Œqå†…åœ¨ç§©æ¯”è¾ƒå¤§ï¼Œvå†…åœ¨ç§©æ¯”è¾ƒå°ï¼Œå¯¹äºä¸åŒçš„çŸ©é˜µåº”è¯¥ä½¿ç”¨ä¸åŒçš„å†…åœ¨ç§©</p>
<p>AdaLoRAæ˜¯åœ¨å¯¹åŒæ ·çš„å‚æ•°é‡ä¸‹ï¼Œå¯¹ä¸åŒçš„çŸ©é˜µä½¿ç”¨ä¸åŒçš„rï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼Œåˆ¤æ–­rçš„å¤§å°ï¼Œæ¥å–å¾—æ›´å¥½çš„æ•ˆæœ</p>
<blockquote>
<p>æå‡ºé—®é¢˜</p>
</blockquote>
<p>åœ¨NLPé¢†åŸŸï¼Œå¯¹äºä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¾®è°ƒå·²ç»æˆä¸ºä¸€ç§é‡è¦çš„åšæ³•ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œæˆ‘ä»¬ä¼šé‡‡ç”¨å¯¹åŸæœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå…¨é‡å¾®è°ƒçš„æ–¹æ³•æ¥é€‚é…ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªé—®é¢˜</p>
<ol>
<li><strong>è®­ç»ƒé˜¶æ®µ</strong>: å¯¹äºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ—¶å€™ï¼Œä¸ºäº†æ›´æ–°æƒé‡å‚æ•°ï¼Œéœ€è¦å¤§é‡çš„æ˜¾å­˜æ¥å­˜å‚¨å‚æ•°çš„æ¢¯åº¦å’Œä¼˜åŒ–å™¨ä¿¡æ¯ï¼Œåœ¨å½“ä»Šé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å˜å¾—è¶Šæ¥è¶Šå¤§çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒé—¨æ§›å˜å¾—è¶Šæ¥è¶Šé«˜</li>
<li><strong>æ¨ç†é˜¶æ®µ</strong>: ç”±äºæˆ‘ä»¬è®­ç»ƒçš„æ—¶å€™æ˜¯å¯¹äºæ¨¡å‹å‚æ•°è¿›è¡Œå…¨é‡çš„æ›´æ–°ï¼Œæ‰€ä»¥<strong>å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡éœ€è¦ä¸ºæ¯ä¸ªä»»åŠ¡ç»´æŠ¤ä¸€ä¸ªå¤§å‹æ¨¡å‹çš„ç‹¬ç«‹å‰¯æœ¬</strong>ï¼Œè¿™æ ·å°±å¯¼è‡´æˆ‘ä»¬åœ¨å®é™…åº”ç”¨çš„æ—¶å€™æµªè´¹äº†ä¸å¿…è¦çš„å­˜å‚¨</li>
</ol>
<p>ç°æœ‰æ–¹æ³•: ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸¤ä¸ªä¸»è¦ç ”ç©¶æ–¹å‘ï¼Œä»¥å‡å°‘å¾®è°ƒå‚æ•°çš„æ•°é‡ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æé«˜é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ€§èƒ½</p>
<blockquote>
<p>æ·»åŠ å°å‹ç½‘ç»œæ¨¡å—</p>
</blockquote>
<p>å°†å°å‹ç½‘ç»œæ¨¡å—æ·»åŠ åˆ°PLMsä¸­ï¼Œä¿æŒåŸºç¡€æ¨¡å‹ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ä»…é’ˆå¯¹æ¯ä¸ªä»»åŠ¡å¾®è°ƒè¿™äº›æ¨¡å—ï¼Œå¯ä»¥ç”¨äºæ‰€æœ‰ä»»åŠ¡ã€‚è¿™æ ·ï¼Œåªéœ€å¼•å…¥å’Œæ›´æ–°å°‘é‡ä»»åŠ¡ç‰¹å®šçš„å‚æ•°ï¼Œå°±å¯ä»¥é€‚é…ä¸‹æ¸¸çš„ä»»åŠ¡ï¼Œå¤§å¤§æé«˜äº†é¢„è®­ç»ƒæ¨¡å‹çš„å®ç”¨æ€§ï¼Œæ–¹æ³•ç¤ºä¾‹</p>
<p><a data-lightbox="8f318a4f-be8a-4e2c-822e-1c5d2d2bcdb1" data-title="æ·»åŠ å°å‹ç½‘ç»œæ¨¡å—" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/æ·»åŠ å°å‹ç½‘ç»œæ¨¡å—.webp" target="_blank"><img alt="æ·»åŠ å°å‹ç½‘ç»œæ¨¡å—" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/æ·»åŠ å°å‹ç½‘ç»œæ¨¡å—.webp"/></a></p>
<ul>
<li><p><strong>Adapter tuning</strong>ï¼šæ˜¯åœ¨åŸºç¡€æ¨¡å‹çš„å„å±‚ä¹‹é—´æ’å…¥å°å‹ç¥ç»æ¨¡å—</p>
</li>
<li><p><strong>Prefix tuning</strong>ï¼šå°†å¯è®­ç»ƒçš„å‰ç¼€æ ‡è®°é™„åŠ åˆ°åŸºç¡€æ¨¡å‹çš„è¾“å…¥æˆ–éšè—å±‚ä¸Š</p>
</li>
<li><p><strong>Prompt Tuning</strong>: ä¿®æ”¹æ¨¡å‹çš„è¾“å…¥ï¼Œåœ¨æ¨¡å‹è¾“å…¥çš„å‰é¢åŠ ä¸€äº›ç‰¹å®šçš„å‰ç¼€</p>
</li>
</ul>
<p><strong>å¯è¡Œä¹‹å¤„</strong>ï¼šå¯ä»¥è¾¾åˆ°ä¸å®Œå…¨å¾®è°ƒå‡ ä¹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä»…æ›´æ–°ä¸åˆ°åŸå§‹æ¨¡å‹å‚æ•°çš„1ï¼…ï¼Œå¤§å¤§å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚</p>
<p><strong>å­˜åœ¨é—®é¢˜</strong>ï¼š</p>
<ul>
<li><p><strong>Adapter tuning</strong>ï¼šå¼•å…¥äº†æ¨ç†å»¶è¿Ÿï¼Œæœ€ç»ˆæ”¶æ•›åˆ°é€‚é…å™¨å±‚</p>
</li>
<li><p><strong>Prefix or Prompt tuning</strong>ï¼šç›´æ¥ä¼˜åŒ–Prefixå’ŒPromptæ˜¯éå•è°ƒçš„ï¼Œæ¯”è¾ƒéš¾æ”¶æ•›ï¼Œå¹¶ä¸”æ¶ˆè€—äº†è¾“å…¥çš„token</p>
</li>
</ul>
<blockquote>
<p>ä¸‹æ¸¸ä»»åŠ¡å¢é‡æ›´æ–°</p>
</blockquote>
<p>å¯¹é¢„è®­ç»ƒæƒé‡çš„å¢é‡æ›´æ–°è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„
<script type="math/tex; mode=display">
W=W^{(0)}+\Delta
</script>
<strong>æ–¹æ³•ç¤ºä¾‹</strong>ï¼š</p>
<ul>
<li><p><strong>Diff pruning</strong>ï¼šå°†<script type="math/tex; ">\Delta</script>åˆå§‹åŒ–ä¸ºä¸<script type="math/tex; ">W</script>ç›¸åŒçš„ç»´åº¦ï¼Œç„¶åæ ¹æ®å‚æ•°çš„å¤§å°æŒ‰å…ƒç´ å¯¹<script type="math/tex; ">\Delta</script>è¿›è¡Œå‰ªæ</p>
</li>
<li><p><strong>LoRA</strong>ï¼šé€šè¿‡ä¸¤ä¸ªå°å¾—å¤šçš„çŸ©é˜µçš„ä¹˜ç§¯å°†<script type="math/tex; ">\Delta</script>å‚æ•°åŒ–ä¸ºä½é˜¶çŸ©é˜µ</p>
</li>
</ul>
<p><script type="math/tex; mode=display">
W=W^{(0)}+\Delta=W^{(0)}+B A
</script></p>
<p><strong>å¯è¡Œä¹‹å¤„</strong>ï¼šå¯ä»¥è¾¾åˆ°ä¸å®Œå…¨å¾®è°ƒå‡ ä¹ç›¸å½“çš„æ€§èƒ½</p>
<p><strong>å­˜åœ¨é—®é¢˜</strong>:</p>
<ul>
<li><strong>Diff pruning</strong>ï¼š<ul>
<li>éœ€è¦åº•å±‚å®ç°æ¥åŠ é€Ÿéç»“æ„åŒ–ç¨€ç–çŸ©é˜µçš„è®¡ç®—ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨ç°æœ‰çš„æ¡†æ¶</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å­˜å‚¨å®Œæ•´çš„<script type="math/tex; ">\Delta</script>çŸ©é˜µï¼Œç›¸æ¯”äºFull finetuneå¹¶æ²¡æœ‰é™ä½è®¡ç®—æˆæœ¬</li>
</ul>
</li>
<li><strong>LoRA</strong>ï¼š<ul>
<li>é¢„å…ˆæŒ‡å®šæ¯ä¸ªå¢é‡çŸ©é˜µçš„å†…åœ¨ç§©rç›¸åŒï¼Œå¿½ç•¥äº†åœ¨å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæƒé‡çŸ©é˜µçš„é‡è¦æ€§åœ¨ä¸åŒæ¨¡å—å’Œå±‚ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚</li>
<li>åªè®­ç»ƒäº†self-attentionï¼Œæ²¡æœ‰è®­ç»ƒfeed-forward networksï¼Œäº‹å®ä¸ŠFFNæ›´é‡è¦</li>
</ul>
</li>
</ul>
<blockquote>
<p>é—®é¢˜æ€»ç»“</p>
</blockquote>
<p>ä¸èƒ½é¢„å…ˆæŒ‡å®šçŸ©é˜µçš„ç§©ï¼Œéœ€è¦åŠ¨æ€æ›´æ–°å¢é‡çŸ©é˜µçš„R</p>
<ul>
<li>æƒé‡çŸ©é˜µçš„é‡è¦æ€§åœ¨ä¸åŒæ¨¡å—å’Œå±‚ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚</li>
</ul>
<p>éœ€è¦æ‰¾åˆ°æ›´åŠ é‡è¦çš„çŸ©é˜µï¼Œåˆ†é…æ›´å¤šçš„å‚æ•°ï¼Œè£å‰ªä¸é‡è¦çš„çŸ©é˜µ</p>
<ul>
<li>æ‰¾åˆ°é‡è¦çš„çŸ©é˜µï¼Œæå‡æ¨¡å‹æ•ˆæœ</li>
<li>è£å‰ªä¸é‡è¦çš„çŸ©é˜µï¼Œé™ä½å‚æ•°è®¡ç®—é‡ï¼Œé™ä½æ¨¡å‹æ•ˆæœå·®çš„é£é™©</li>
</ul>
<blockquote>
<p>è§£å†³æ–¹æ¡ˆ</p>
</blockquote>
<p>ç›®æ ‡ï¼šåœ¨ç±»ä¼¼LoRAçš„å¾®è°ƒè¿‡ç¨‹ä¸­åŠ¨æ€åˆ†é…å‚æ•°é¢„ç®—ç»™æƒé‡çŸ©é˜µ</p>
<ol>
<li><strong>è°ƒæ•´å¢é‡çŸ©é˜µçš„ç§©</strong>æ¥æ§åˆ¶é¢„ç®—åˆ†é…ã€‚AdaLoRAå°†å…³é”®çš„å¢é‡çŸ©é˜µåˆ†é…é«˜ç§©ä»¥æ•æ‰æ›´ç²¾ç»†å’Œä»»åŠ¡ç‰¹å®šçš„ä¿¡æ¯ï¼Œè€Œå°†è¾ƒä¸é‡è¦çš„çŸ©é˜µçš„ç§©é™ä½ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶èŠ‚çœè®¡ç®—é¢„ç®—</li>
<li>é‡‡ç”¨å‚æ•°åŒ–çŸ©é˜µæ¥æ¨¡æ‹ŸSVDï¼Œå¹¶èˆå¼ƒä¸é‡è¦çš„å¥‡å¼‚å€¼ï¼ŒåŒæ—¶ä¿ç•™å¥‡å¼‚å‘é‡ã€‚ç”±äºå¯¹ä¸€ä¸ªå¤§çŸ©é˜µè¿›è¡Œç²¾ç¡®SVDåˆ†è§£çš„è®¡ç®—æ¶ˆè€—éå¸¸å¤§ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥åŠ é€Ÿè®¡ç®—ï¼ŒåŒæ—¶ä¿ç•™æœªæ¥æ¢å¤çš„å¯èƒ½æ€§å¹¶ç¨³å®šè®­ç»ƒ</li>
<li>åœ¨è®­ç»ƒæŸå¤±ä¸­æ·»åŠ äº†é¢å¤–çš„æƒ©ç½šé¡¹ï¼Œä»¥è§„èŒƒå¥‡å¼‚çŸ©é˜µPå’ŒQçš„æ­£äº¤æ€§ï¼Œä»è€Œé¿å…SVDçš„å¤§é‡è®¡ç®—å¹¶ç¨³å®šè®­ç»ƒ</li>
</ol>
<blockquote>
<p>SVD-BASED ADAPTATION</p>
</blockquote>
<p>å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬æŠŠå¢é‡çŸ©é˜µ<script type="math/tex; ">\Delta</script>åšä¸€ä¸ªå¥‡å¼‚å€¼åˆ†è§£çš„è¿‘ä¼¼ï¼Œå³<script type="math/tex; ">\Delta=P \Lambda Q</script>ï¼Œå¯¹çŸ©é˜µæ›´æ–°çš„æè¿°åˆ™æœ‰å¦‚ä¸‹è¡¨ç¤º
<script type="math/tex; mode=display">
W=W^{(0)}+\Delta=W^{(0)}+P \Lambda Q
</script>
ä¸ºäº†ä¿è¯<script type="math/tex; ">\mathrm{P}</script>å’Œ<script type="math/tex; ">\mathrm{Q}</script>çš„æ­£äº¤æ€§ï¼Œå³<script type="math/tex; ">P^{\top} P=Q Q^{\top}=I</script>ï¼Œæˆ‘ä»¬æå‡ºå¦‚ä¸‹æ‰€ç¤ºçš„æ­£åˆ™æŸå¤±
<script type="math/tex; mode=display">
R(P, Q)=\left\|P^{\top} P-I\right\|_{\mathrm{F}}^{2}+\left\|Q Q^{\top}-I\right\|_{\mathrm{F}}^{2}
</script>
<strong>ä¸ºä»€ä¹ˆä¸ç›´æ¥åœ¨åŸæ¥çš„BAä¸Šè¿›è¡Œä¿®å‰ªï¼Ÿ</strong></p>
<ol>
<li><p>å½“ä¸€å¯¹å¥‡å¼‚å‘é‡è¢«è®¤ä¸ºä¸ºä¸é‡è¦æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»ä¿®å‰ªå®ƒçš„æ‰€æœ‰å…ƒç´ ã€‚è¿™å°±å¯¼è‡´å‡ ä¹ä¸å¯èƒ½é‡æ–°æ¿€æ´»ä¿®å‰ªè¿‡çš„å¥‡å¼‚å‘é‡ï¼Œå› ä¸ºå®ƒä»¬çš„å…ƒç´ éƒ½è¢«æ¸…é›¶å¹¶ä¸”ä¸å†è®­ç»ƒ</p>
<p>ä¸ä¹‹å¯¹æ¯”ï¼ŒAdaLoRAåªæ˜¯Maskäº†å¥‡å¼‚å€¼</p>
</li>
<li><p>LoRAçš„Aå’ŒBä¸æ˜¯æ­£äº¤çš„ï¼Œè¿™æ„å‘³ç€å¥‡å¼‚å‘é‡å¯ä»¥ç›¸äº’ä¾èµ–ã€‚ ä¸æˆªæ–­æœ€å°çš„å¥‡å¼‚å€¼ç›¸æ¯”ï¼Œä¸¢å¼ƒå¥‡å¼‚å‘é‡å¯èƒ½ä¼šå¯¼è‡´åŸå§‹çŸ©é˜µå‘ç”Ÿæ›´å¤§çš„å˜åŒ–ã€‚</p>
<p>å› æ­¤ï¼Œåœ¨åˆ†é…å®Œç§©çš„æ¯ä¸€æ­¥ä¹‹åï¼Œå¢é‡çŸ©é˜µé€šå¸¸ä¼šå‘ç”Ÿæ›´å¤šä¸å¯é¢„æµ‹çš„æ˜¾è‘—å˜åŒ–ï¼Œè¿™å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œç”šè‡³æŸå®³æ¨¡å‹çš„æ•ˆæœ</p>
</li>
</ol>
<blockquote>
<p>IMPORTANCE-AWARE RANK ALLOCATION</p>
</blockquote>
<p>æˆ‘ä»¬å°†åŸºäºSVDçš„ç§©è°ƒæ•´åº”ç”¨äºæ¯ä¸ªæƒé‡çŸ©é˜µï¼ŒåŒ…æ‹¬æ¯ä¸ªtransformerå±‚çš„<script type="math/tex; ">W_{q}, W_{k}, W_{v}, W_{f 1}</script>å’Œ<script type="math/tex; ">W_{f 2}</script>ã€‚ä¸ºäº†æ§åˆ¶å‚æ•°é¢„ç®—ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´æ ¹æ®é‡è¦æ€§å¾—åˆ†è¿­ä»£ä¿®å‰ªå¥‡å¼‚å€¼</p>
<p>ä¸ºäº†æ›´å¥½åœ°è¡¨ç¤ºï¼Œæˆ‘ä»¬ç”¨<script type="math/tex; ">k</script>æ¥ç´¢å¼•å¢é‡çŸ©é˜µ<script type="math/tex; ">\Delta_{k}=P_{k} \Lambda_{k} Q_{k}</script> for <script type="math/tex; ">k=1, \ldots, n</script>ï¼Œç”¨<script type="math/tex; ">\mathcal{G}_{k, i}=\left\{P_{k, * i}, \lambda_{k, i}, Q_{k, i *}\right\}</script>æ¥è¡¨ç¤ºç¬¬<script type="math/tex; ">k</script>ä¸ªçŸ©é˜µçš„å¥‡å¼‚å€¼ï¼Œå¥‡å¼‚å‘é‡ä¸‰å…ƒç»„ï¼Œ<script type="math/tex; ">\mathcal{S}_{k, i}</script> æ¥è¡¨ç¤ºè¿™ä¸ªä¸‰å…ƒç»„çš„é‡è¦æ€§</p>
<p><script type="math/tex; ">\mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q})</script>ä½œä¸ºå‚æ•°è®­ç»ƒçš„ä»£ä»·ï¼ŒåŒæ—¶åŠ ä¸Šæ­£åˆ™åŒ–é¡¹ï¼Œå°±å¾—å‡ºäº†å¦‚ä¸‹çš„ç›®æ ‡å‡½æ•°: (<script type="math/tex; ">\gamma</script>æ˜¯æ­£åˆ™åŒ–ç³»æ•°)
<script type="math/tex; mode=display">
\mathcal{L}(\mathcal{P}, \mathcal{E}, \mathcal{Q})=\mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q})+\gamma \sum_{k=1}^{n} R\left(P_{k}, Q_{k}\right)
</script>
æˆ‘ä»¬åœ¨è®­ç»ƒçš„æ—¶å€™å°±å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™çš„æ–¹å¼å¯¹<script type="math/tex; ">P, \Lambda, Q</script>è¿›è¡Œæ›´æ–°ï¼Œä¸‹é¢æ˜¯<script type="math/tex; ">\Lambda</script>çš„ä¾‹å­
<script type="math/tex; mode=display">
\tilde{\Lambda}_{k}^{(t)}=\Lambda_{k}^{(t)}-\eta \nabla_{\Lambda_{k}} \mathcal{L}\left(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)}\right)
</script>
ç„¶åæˆ‘ä»¬å†åŸºäº<script type="math/tex; ">\mathcal{S}_{k, i}</script>å¯¹<script type="math/tex; ">\Lambda</script>è¿›è¡Œè£å‰ª
<script type="math/tex; mode=display">
\Lambda_{k}^{(t+1)}=\mathcal{T}\left(\tilde{\Lambda}_{k}^{(t)}, S_{k}^{(t)}\right), \text { with } \mathcal{T}\left(\tilde{\Lambda}_{k}^{(t)}, S_{k}^{(t)}\right)_{i i}=\left\{\begin{array}{ll}
\tilde{\Lambda}_{k, i i}^{(t)} & S_{k, i}^{(t)} \text { is in the top- } b^{(t)} \text { of } S^{(t)} \\
0 & \text { otherwise }
\end{array}\right.
</script>
å…¶ä¸­<script type="math/tex; ">S^{(t)}=\left\{S_{k, i}^{(t)}\right\}_{1 \leq k \leq n, 1 \leq i \leq r}</script>åŒ…å«æ‰€æœ‰ä¸‰å…ƒç»„çš„é‡è¦æ€§åˆ†æ•°ã€‚<script type="math/tex; ">b^{(t)}</script>æ˜¯ç¬¬<script type="math/tex; ">\mathrm{t}</script>æ­¥å‰©ä½™å¥‡å¼‚å€¼çš„é¢„ç®—</p>
<p>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬é€šè¿‡ä¿®å‰ªä¸å¤ªé‡è¦çš„å¥‡å¼‚å€¼ï¼Œå°†æ›´å¤šé¢„ç®—ç•™ç»™ä¼˜å…ˆçº§è¾ƒé«˜çš„å¢é‡çŸ©é˜µ</p>
<blockquote>
<p>Magnitude of singular values</p>
</blockquote>
<p>è¿™æ ·çš„è¯åªæœ‰æœ€å°çš„å¥‡å¼‚å€¼ä»¥åŠæœ€ä¸é‡è¦çš„å¥‡å¼‚å‘é‡è¢«å»å¼ƒã€‚å®ƒæœ€å¤§é™åº¦åœ°å‡å°äº†ä¸åŸå§‹çŸ©é˜µçš„åå·®ï¼Œè¿›ä¸€æ­¥ç¨³å®šäº†è®­ç»ƒã€‚ä½†æ˜¯è¿™ä¸ªåº¦é‡ä¸èƒ½æ­£ç¡®é‡åŒ–å‚æ•°(ä¸‰å…ƒç»„)å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®
<script type="math/tex; mode=display">
S_{k, i}=\left|\lambda_{k, i}\right|
</script></p>
<blockquote>
<p>Sensitivity-based importance</p>
</blockquote>
<p>ä¹‹å‰çš„å·¥ä½œåˆ©ç”¨çµæ•åº¦æ¥é‡åŒ–å•ä¸ªå‚æ•°çš„é‡è¦æ€§ï¼Œå¹¶æ®æ­¤å¯¹å‚æ•°è¿›è¡Œéç»“æ„åŒ–ä¿®å‰ªã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸Šï¼Œæˆ‘ä»¬å¿…é¡»è®¾è®¡ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†ï¼Œå› ä¸ºä¸‰å…ƒç»„è¦è¢«æŒ‰ç»„ä¸¢å¼ƒäº†ï¼Œæ‰€ä»¥æ¯ä¸€é¡¹çš„æ•æ„Ÿæ€§éƒ½åº”è¯¥è¢«è€ƒè™‘ï¼Œå¹¶é€‚å½“åœ°ç»„åˆèµ·æ¥ï¼Œä»¥é‡åŒ–ä¸‰å…ƒç»„å¯¹æ¨¡å‹æ€§èƒ½çš„æ•´ä½“è´¡çŒ®</p>
<p>æˆ‘ä»¬è®¾è®¡äº†å¦‚ä¸‹æ‰€ç¤ºçš„å‡½æ•°æ¥è®¡ç®—importance score
<script type="math/tex; mode=display">
S_{k, i}=s\left(\lambda_{k, i}\right)+\frac{1}{d_{1}} \sum_{j=1}^{d_{1}} s\left(P_{k, j i}\right)+\frac{1}{d_{2}} \sum_{j=1}^{d_{2}} s\left(Q_{k, i j}\right)
</script>
æˆ‘ä»¬å¯ä»¥é‡‡ç”¨<script type="math/tex; ">s(\cdot)</script>çš„çµæ•åº¦ï¼Œ<script type="math/tex; ">s(\cdot)</script>å®šä¹‰ä¸ºæ¢¯åº¦æƒé‡ä¹˜ç§¯çš„å¤§å°:
<script type="math/tex; mode=display">
I\left(w_{i j}\right)=\left|w_{i j} \nabla_{w_{i j}} \mathcal{L}\right|
</script>
æœ¬è´¨ä¸Šè¿‘ä¼¼äºå‚æ•°å½’é›¶æ—¶çš„æŸå¤±å˜åŒ–ã€‚å¦‚æœå»é™¤ä¸€ä¸ªå‚æ•°å½±å“è¾ƒå¤§ï¼Œåˆ™æ¨¡å‹å¯¹è¯¥å‚æ•°æ•æ„Ÿï¼Œæˆ‘ä»¬åº”è¯¥ä¿ç•™å®ƒ</p>
<p>ä½†ä¹‹å‰çš„å·¥ä½œæŒ‡å‡ºï¼Œç›´æ¥è®¡ç®—çš„æ•æ„Ÿæ€§è¿˜ä¸æ˜¯ä¸€ä¸ªå¯é çš„é‡è¦æŒ‡æ ‡ã€‚è¿™æ ·çš„åˆ†æ•°æ˜¯åœ¨æŠ½æ ·çš„minibatchä¸Šä¼°è®¡çš„ã€‚éšæœºé‡‡æ ·å’Œå¤æ‚çš„è®­ç»ƒåŠ¨æ€å¯¼è‡´çµæ•åº¦ä¼°è®¡çš„å˜å¼‚æ€§å¤§ï¼Œä¸ç¡®å®šæ€§å¤§ï¼Œè¿™æ ·å¯èƒ½ä¼šå¯¼è‡´å¯¹äºå‚æ•°çš„é‡è¦æ€§çš„é”™è¯¯ä¼°è®¡ã€‚æå‡ºé€šè¿‡çµæ•åº¦å¹³æ»‘å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼ŒåŠ å…¥ç´¯è®¡çµæ•åº¦çš„å½±å“æ¥è§£å†³è¿™ä¸€é—®é¢˜:
<script type="math/tex; mode=display">
\begin{array}{l}
\bar{I}^{(t)}\left(w_{i j}\right)=\beta_{1} \bar{I}^{(t-1)}\left(w_{i j}\right)+\left(1-\beta_{1}\right) I^{(t)}\left(w_{i j}\right) \\
\bar{U}^{(t)}\left(w_{i j}\right)=\beta_{2} \bar{U}^{(t-1)}\left(w_{i j}\right)+\left(1-\beta_{2}\right)\left|I^{(t)}\left(w_{i j}\right)-\bar{I}^{(t)}\left(w_{i j}\right)\right|
\end{array}
</script>
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠ<script type="math/tex; ">s(\cdot)</script>å®šä¹‰ä¸º<script type="math/tex; ">\bar{I}^{(t)}</script>å’Œ<script type="math/tex; ">\bar{U}^{(t)}</script>çš„ä¹˜ç§¯
<script type="math/tex; mode=display">
s^{(t)}\left(w_{i j}\right)=\bar{I}^{(t)}\left(w_{i j}\right) \cdot \bar{U}^{(t)}\left(w_{i j}\right)
</script>
è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªæ—¢è€ƒè™‘äº†ä¸‰å…ƒç»„æ‰€æœ‰å…ƒç´ ï¼Œåˆè€ƒè™‘äº†ç´¯è®¡çµæ•åº¦è¶³å¤Ÿå¹³æ»‘çš„ä¸€ä¸ªé‡è¦æ€§å‡½æ•°</p>
<blockquote>
<p>GLOBAL BUDGET SCHEDULER</p>
</blockquote>
<p>åœ¨ä½ç§©è‡ªé€‚åº”çš„æƒ…å†µä¸‹ï¼Œè°ƒæ•´ç§©è‡ªç„¶æ˜¯ä¸ºäº†æ§åˆ¶å‚æ•°é¢„ç®—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é¢„ç®—<script type="math/tex; ">b^{(t)}</script>å®šä¹‰ä¸ºæ‰€æœ‰å¢é‡çŸ©é˜µçš„æ€»ç§©ï¼Œå³æ€»å¥‡å¼‚å€¼çš„æ•°é‡</p>
<p>å›æƒ³ä¸€ä¸‹ï¼Œé¢„ç®—åˆ†é…æ˜¯åœ¨å¾®è°ƒæœŸé—´è¿­ä»£æ‰§è¡Œçš„ã€‚ä¸ºäº†ä¾¿äºè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨å±€é¢„ç®—è°ƒåº¦å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»ç•¥é«˜äºç›®æ ‡é¢„ç®—<script type="math/tex; ">b^{(T)}</script>çš„åˆå§‹é¢„ç®—ç®—<script type="math/tex; ">b^{(0)}</script>å¼€å§‹(ä¾‹å¦‚ï¼Œ<script type="math/tex; ">b^{(T)}</script>çš„1.5å€)</p>
<p>æˆ‘ä»¬å°†æ¯ä¸ªå¢é‡çŸ©é˜µçš„åˆå§‹ç§©è®¾ä¸º<script type="math/tex; ">r=\frac{b^{(0)}}{n}</script>ã€‚æˆ‘ä»¬å¯¹<script type="math/tex; ">t_{\text {init }}</script>æ­¥è¿›è¡Œwarmupï¼Œç„¶åæŒ‰ç…§ä¸‰æ¬¡è®¡åˆ’å‡å°‘é¢„ç®—<script type="math/tex; ">b^{(t)}</script>ï¼Œç›´åˆ°è¾¾åˆ°<script type="math/tex; ">b^{(t)}</script></p>
<p>æœ€åï¼Œæˆ‘ä»¬å¾—åˆ°çš„ä¿®æ­£å®Œé¢„ç®—åˆ†å¸ƒï¼Œå¹¶å¯¹<script type="math/tex; ">t_{\text {final }}</script>æ­¥éª¤çš„æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒ</p>
<p>è¿™ä½¿å¾—AdaLoRAå¯ä»¥å…ˆæ¢ç´¢å‚æ•°ç©ºé—´ï¼Œç„¶åå†å…³æ³¨æœ€é‡è¦çš„æƒé‡</p>
<blockquote>
<p>å®éªŒéªŒè¯</p>
</blockquote>
<h2 id="qlora">3.3 QLORA</h2>
<p>FineTune -&gt; P_tuning -&gt; P_tuning V2 -&gt; LoRA -&gt; QLoRA</p>
<p>BERT Adapter</p>
<h1 id="rlhf">4 RLHF</h1>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV1Yc411g78a/?spm_id_from=333.1007.0.0&amp;vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">å…¥é—¨ã€‘å¤§è¯­è¨€æ¨¡å‹å¸¸ç”¨å¾®è°ƒæ¡†æ¶ä»‹ç»ï½œLoRA&amp;Prefix-Tuning&amp;Prompt-Tuning&amp;P-Tuning v2&amp;RLHFå¾®è°ƒåŸç†ç®€ä»‹</a></p>
</blockquote>
<p>RLHF: Reinforcement Learning from Humanã€‚Feedbackï¼Œå³åŸºäºäººå·¥åé¦ˆæœºåˆ¶çš„å¼ºåŒ–å­¦ä¹ ã€‚æœ€æ—©ä¸2022å¹´4æœˆï¼Œç”±OpenAIç ”ç©¶å›¢é˜Ÿç³»ç»Ÿæ€»ç»“å¹¶æå‡º.å¹¶åœ¨GPTæ¨¡å‹çš„å¯¹è¯ç±»ä»»åŠ¡å¾®è°ƒä¸­å¤§æ”¾å¼‚å½©ï¼Œè¢«ç§°ä¸ºChatGPT<strong>èƒŒåçš„åŠŸè‡£</strong></p>
<p>RLHFä¹Ÿæ˜¯ç›®å‰ä¸ºæ­¢å¸¸ç”¨çš„ã€æœ€ä¸ºå¤æ‚çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œç›®å‰æœ€å¥½çš„ç«¯åˆ°ç«¯RLHFå®ç°æ˜¯DeepSpeedChatåº“ï¼Œç”±å¾®è½¯å¼€æºå¹¶ç»´æŠ¤</p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„è¿›é˜¶å¾®è°ƒæ–¹æ³•RLHFæ–¹æ³•</p>
<p>è®ºæ–‡åœ°å€: <a href="https://arxiv.org/abs/2203.02155" target="_blank">https://arxiv.org/abs/2203.02155</a></p>
<p>æ­¥éª¤1: ç›‘ç£å¾®è°ƒ (SFT)-ä¸€ ä½¿ç”¨ç²¾é€‰çš„äººç±»å›ç­”æ¥å¾®è°ƒé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä»¥åº”å¯¹å„ç§æŸ¥è¯¢</p>
<p>æ­¥éª¤2:å¥–åŠ±æ¨¡å‹å¾®è°ƒ -- ä½¿ç”¨ä¸€ä¸ªåŒ…å«äººç±»å¯¹åŒä¸€æŸ¥è¯¢çš„å¤šä¸ªç­”æ¡ˆæ‰“åˆ†çš„æ•°æ®é›†æ¥è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„ (é€šå¸¸æ¯” SFT å°çš„) å¥–åŠ±æ¨¡å‹ (RW) </p>
<p>æ­¥éª¤3: RLHF è®­ç»ƒ --åˆ©ç”¨ Proximal Policy Optimization (PPO) ç®—æ³•æ ¹æ® RW æ¨¡å‹çš„å¥–åŠ±Dä¹å¤©Hectoråé¦ˆè¿›ä¸€æ­¥å¾®è°ƒ SFT æ¨¡å‹ã€‚</p>
<h1 id="flashattenè½¬">5 Flash_Atten(è½¬)</h1>
<blockquote>
<p><a href="https://readpaper.feishu.cn/docx/UwT2dQsiko6u0RxoiXRcBtwfnAf" target="_blank">å‰ç½®çŸ¥è¯† GPU Arch:è‡ªé¡¶å‘ä¸‹åˆ†æ</a> + <a href="https://www.bilibili.com/video/BV1Az4y1B7Da/?vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">Bç«™ GPU Archï¼šè‡ªé¡¶å‘ä¸‹åˆ†æã€æµ…è°ˆåº•å±‚Â·1ã€‘</a></p>
</blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ç‰¹åˆ«æ˜¯ä»¥GPTä¸ºä»£è¡¨çš„ç”Ÿæˆå¼AIçš„è¿…çŒ›å‘å±•ï¼ŒGPUå·²ç»æˆä¸ºäº†ä¸€ç§ä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œç”šè‡³ä¼ä¸šéƒ½ä»¥æ‹¥æœ‰å¤šå°‘é«˜ç«¯GPUä½œä¸ºæŠ“ä½é£å£èƒ½åŠ›çš„è¡¡é‡æ ‡å‡†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCPUè™½ç„¶åœ¨ä¼ ç»Ÿè®¡ç®—é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œä½†åœ¨å¤„ç†AIä»»åŠ¡æ—¶å´ä¸åŠGPUå‡ºè‰²</p>
<p>ä¸ºä»€ä¹ˆAIè®¡ç®—é€šå¸¸é€‰æ‹©GPUè€Œä¸æ˜¯CPUï¼Œåˆ†æGPUåœ¨AIè®¡ç®—ä¸­çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ï¼Œä»åº•å±‚åŸç†æ¢è®¨ä»Voltaåˆ°æœ€æ–°çš„Hopperå››ä»£NVIDIA GPUæ¶æ„çš„æ¼”è¿›ï¼Œå±•ç¤ºå…¶ä¸æ–­æå‡çš„æ€§èƒ½å’ŒåŠŸèƒ½</p>
<p>GPUä¸»è¦ç”±è®¡ç®—å•å…ƒALUç»„æˆã€‚CPUä¸ä»…è¢«Cacheå æ®äº†å¤§é‡ç©ºé—´ï¼Œè€Œä¸”è¿˜æœ‰æœ‰å¤æ‚çš„æ§åˆ¶é€»è¾‘å’Œè¯¸å¤šä¼˜åŒ–ç”µè·¯ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œè®¡ç®—èƒ½åŠ›åªæ˜¯CPUå¾ˆå°çš„ä¸€éƒ¨åˆ†</p>
<p><a data-lightbox="3167a3f4-6e2e-4ea9-906e-3611cc68c2c4" data-title="GPUå’ŒCPUæ¯”è¾ƒ" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/GPUå’ŒCPUæ¯”è¾ƒ.webp" target="_blank"><img alt="GPUå’ŒCPUæ¯”è¾ƒ" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/GPUå’ŒCPUæ¯”è¾ƒ.webp"/></a></p>
<p>é€šè¿‡ä¸Šé¢è‡ªé¡¶å‘ä¸‹çš„åˆ†æï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œå¯¹äºGPUä¸­çš„å­˜å‚¨éƒ¨åˆ†è®¿é—®é€Ÿåº¦ç”±å¿«åˆ°æ…¢ï¼Œè®¡ç®—éƒ¨åˆ†ä»å¤§åˆ°å°æ’åˆ—ä¸º
<script type="math/tex; mode=display">
\begin{array}{c}
\text{Mem Speed:(L1 Cache/SMEM)>L2 Cache>HBM}
\\
\text{Compute Unit:GPC>TPC>SM>(TensorCore, SFU, INT32, FP32..)}
\end{array}
</script>
<a data-lightbox="17aef84e-1d18-4a3f-aa74-932d3479c8ed" data-title="GPUæ¶æ„å‘å±•å‚æ•°" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/GPUæ¶æ„å‘å±•å‚æ•°.webp" target="_blank"><img alt="GPUæ¶æ„å‘å±•å‚æ•°" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/GPUæ¶æ„å‘å±•å‚æ•°.webp"/></a></p>
<p>NVLinkæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦ä»–ï¼Ÿ</p>
<p>å¤§æ¨¡å‹é€šå¸¸å…·æœ‰å·¨å¤§çš„å‚æ•°æ•°é‡å’Œå¤æ‚çš„ç»“æ„ï¼Œéœ€è¦å¤„ç†å¤§é‡çš„æ•°æ®ã€‚åˆ†å¸ƒå¼è®­ç»ƒå°†è¿™äº›å¤§å‹æ¨¡å‹åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œç”±å¤šä¸ªGPUæˆ–è®¡ç®—èŠ‚ç‚¹å¹¶è¡Œå¤„ç†ï¼Œæ¯ä¸ªéƒ¨åˆ†å¤„ç†è‡ªå·±çš„æ•°æ®å­é›†ã€‚ç„¶åé€šè¿‡å…¨å±€é€šä¿¡ï¼Œå‚æ•°åŒæ­¥ç­‰æ–¹å¼è¿›è¡Œæ¢¯åº¦ä¼ æ’­ï¼Œæ­¤æ—¶GPUä¹‹é—´çš„é€šä¿¡å¸¦å®½å°±å˜çš„è¶Šæ¥è¶Šé‡è¦</p>
<p>åœ¨NVLinkå‡ºç°ä¹‹å‰ï¼ŒGPUä¸GPUä¹‹é—´çš„æ•°æ®äº¤äº’é€šè¿‡PCIeï¼ˆPeripheral Component Interconnect Expressï¼‰æ€»çº¿è¿›è¡Œã€‚ä½†PCIeå­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯PCIeæ€»çº¿çš„å¸¦å®½ç›¸å¯¹æœ‰é™ï¼Œå…¶ä¸­PCIe 4.0x16çš„æœ€å¤§å¸¦å®½ä¹Ÿå°±64GB/sï¼ŒäºŒæ˜¯PCIeæ€»çº¿çš„å»¶è¿Ÿç›¸å¯¹è¾ƒé«˜ï¼Œåœ¨GPUä¹‹é—´ä¼ è¾“æ•°æ®æ—¶ï¼Œæ¯æ¬¡æ•°æ®ä¼ è¾“éƒ½éœ€è¦é€šè¿‡CPUå’Œä¸»æœºå†…å­˜æ¥å®Œæˆã€‚è¿™ç§ä¼ è¾“è·¯å¾„ä¼šå¯¼è‡´é¢å¤–çš„å»¶è¿Ÿï¼Œå¹¶é™ä½æ•°æ®ä¼ è¾“çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ åº”ç”¨ä¸­éœ€è¦æ›´é«˜çš„å¸¦å®½å’Œæ›´ä½çš„å»¶è¿Ÿï¼ŒPCIeæ˜¾ç„¶æ˜¯æ— æ³•æ»¡è¶³å½“ä¸‹çš„ç¥ç»ç½‘ç»œè®­ç»ƒéœ€æ±‚</p>
<p><a data-lightbox="f714304e-d65e-421c-a281-07c8964414bb" data-title="å¼•å…¥NVLink" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/å¼•å…¥NVLink.webp" target="_blank"><img alt="å¼•å…¥NVLink" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/å¼•å…¥NVLink.webp"/></a></p>
<p>NVLinkåˆ©ç”¨é«˜å¸¦å®½ã€ä½å»¶è¿Ÿçš„é€šä¿¡é€šé“ï¼Œç›´æ¥å°†å¤šä¸ªGPUè¿æ¥åœ¨ä¸€èµ·ï¼Œå®ç°å¿«é€Ÿã€é«˜æ•ˆçš„æ•°æ®ä¼ è¾“å’Œå…±äº«ã€‚é€šè¿‡NVLinkï¼ŒGPUä¹‹é—´çš„æ•°æ®äº¤äº’å¯ä»¥ç›´æ¥åœ¨GPUä¹‹é—´è¿›è¡Œï¼Œè€Œæ— éœ€é€šè¿‡CPUå’Œä¸»æœºå†…å­˜ã€‚è¿™ç§ç›´æ¥å†…å­˜è®¿é—®ï¼ˆDMAï¼‰çš„æ–¹å¼å¤§å¤§å‡å°‘äº†æ•°æ®ä¼ è¾“çš„å¤åˆ¶å’Œå»¶è¿Ÿï¼Œæé«˜äº†æ•°æ®å…±äº«çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒNVLinkè¿˜æä¾›äº†ä¸€è‡´çš„å†…å­˜ç©ºé—´ï¼Œä½¿å¾—å¤šä¸ªGPUèƒ½å¤Ÿå…±äº«åŒä¸€ä»½å†…å­˜ï¼Œç®€åŒ–äº†ç¨‹åºè®¾è®¡å’Œæ•°æ®ç®¡ç†çš„å¤æ‚æ€§</p>
<h2 id="æ¦‚è¿°_2">5.1 æ¦‚è¿°</h2>
<blockquote>
<p><a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Paper 2022</a></p>
<p><a href="https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh" target="_blank">FlashAttention: æ›´å¿«è®­ç»ƒæ›´é•¿ä¸Šä¸‹æ–‡çš„GPT</a></p>
</blockquote>
<p>Transformerä½œä¸ºGPTç±»æ¨¡å‹çš„åŸºç¡€æ¶æ„æä¾›äº†å¼ºå¤§çš„ç‰¹å¾å¤„ç†èƒ½åŠ›ï¼Œä½†æ˜¯å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ ¸å¿ƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—åœ¨åºåˆ—é•¿åº¦ä¸Šå…·æœ‰O(N^2)çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ğŸ˜“</p>
<p>è¿™ç¯‡Flash Attentionçš„å·¥ä½œæ·±å…¥ç¡¬ä»¶ï¼Œæ–°æå‡ºäº†ä¸€ç§å…·æœ‰<strong>IOæ„ŸçŸ¥çš„</strong>ï¼Œ<strong>å¿«é€Ÿçš„</strong>âš¡ï¸ï¼Œ<strong>èŠ‚çœå†…å­˜çš„</strong>ğŸ§ ï¼Œ<strong>ç²¾ç¡®çš„</strong>ğŸ¯æ³¨æ„åŠ›ç®—æ³•ã€‚ç›®å‰ï¼ŒFlash Attentionå·²ç»<strong>é›†æˆè‡³torch2.0</strong>ï¼Œå¹¶ä¸”ç¤¾åŒºä¹Ÿæä¾›äº†å¤šç§å®ç°</p>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV1Zz4y1q7FX/?vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">78sçœ‹æ‡‚FlashAttentionã€æœ‰ç‚¹æ„æ€Â·1ã€‘</a></p>
</blockquote>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe allowfullscreen="true" border="0" frameborder="no" framespacing="0" scrolling="no" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/78sçœ‹æ‡‚FlashAttention.mp4" style="position: absolute; width: 100%; 
height: 100%; left: 0; top: 0;"> </iframe></div>
<h2 id="æ ¸å¿ƒè¦ç‚¹">5.2 æ ¸å¿ƒè¦ç‚¹</h2>
<blockquote>
<p>âš¡ï¸<strong>ä¸ºä»€ä¹ˆåŠ å¿«äº†è®¡ç®—ï¼ŸFast</strong></p>
</blockquote>
<p>é™ä½äº†è€—æ—¶çš„HBMè®¿é—®æ¬¡æ•°ã€‚é‡‡ç”¨TilingæŠ€æœ¯åˆ†å—ä»HBMåŠ è½½æ•°æ®åˆ°SRAMè¿›è¡Œèåˆè®¡ç®—</p>
<blockquote>
<p><strong>ğŸ§ ä¸ºä»€ä¹ˆèŠ‚çœäº†å†…å­˜ï¼ŸMemory-Efficient</strong></p>
</blockquote>
<p>ä¸å†å¯¹ä¸­é—´çŸ©é˜µSï¼ŒPè¿›è¡Œå­˜å‚¨ã€‚åœ¨åå‘çš„æ—¶å€™é€šè¿‡Recomputationé‡æ–°è®¡ç®—æ¥è®¡ç®—æ¢¯åº¦</p>
<blockquote>
<p><strong>ğŸ¯ä¸ºä»€ä¹ˆæ˜¯ç²¾å‡†æ³¨æ„åŠ›ï¼ŸExact Attention</strong></p>
</blockquote>
<p>ç®—æ³•æµç¨‹åªæ˜¯åˆ†å—è®¡ç®—ï¼Œ<strong>æ— è¿‘ä¼¼æ“ä½œ</strong></p>
<h2 id="æå‡ºé—®é¢˜">5.3 æå‡ºé—®é¢˜</h2>
<p>Transformerç»“æ„å·²æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ç±»ç­‰åº”ç”¨ä¸­æœ€å¸¸ç”¨çš„æ¶æ„ã€‚å°½ç®¡Transformeråœ¨è§„æ¨¡ä¸Šä¸æ–­å¢å¤§å’ŒåŠ æ·±ï¼Œä½†å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæ ¸å¿ƒçš„è‡ªæ³¨æ„åŠ›æ¨¡å—åœ¨åºåˆ—é•¿åº¦ä¸Šå…·æœ‰äºŒæ¬¡æ–¹çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ã€‚è¿™å¯¼è‡´åœ¨å¤„ç†é•¿åºåˆ—æ—¶é€Ÿåº¦å˜æ…¢ä¸”å†…å­˜éœ€æ±‚å·¨å¤§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›ä¼˜åŒ–ç®—æ³•æ¥æé«˜æ³¨æ„åŠ›æ¨¡å—çš„è®¡ç®—é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ç‡</p>
<h2 id="è§£å†³æ–¹æ¡ˆ">5.4 è§£å†³æ–¹æ¡ˆ</h2>
<p><a data-lightbox="c2143130-d93b-464a-a3a8-63cc8dbe85ee" data-title="flash_Attenæ¶æ„å›¾" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenæ¶æ„å›¾.webp" target="_blank"><img alt="flash_Attenæ¶æ„å›¾" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenæ¶æ„å›¾.webp"/></a></p>
<h2 id="forward">5.5 Forward</h2>
<h3 id="standard-attention">5.5.1 Standard Attention</h3>
<p>åœ¨æ³¨æ„åŠ›çš„ä¸€èˆ¬å®ç°ä¸­ï¼Œå¯¹<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{N \times d}</script> ä¸‰ä¸ªè¾“å…¥æ‰§è¡Œä»¥ä¸‹ç®—æ³•å¾—åˆ°è¾“å‡º<script type="math/tex; ">\mathbf{O}</script>ï¼Œå…¶ä¸­softmaxè¡Œçº§åˆ«æ‰§è¡Œ
<script type="math/tex; mode=display">
\mathbf{S}=\mathbf{Q} \mathbf{K}^{\top} \in \mathbb{R}^{N \times N}, \quad \mathbf{P}=\operatorname{softmax}(\mathbf{S}) \in \mathbb{R}^{N \times N}, \quad \mathbf{O}=\mathbf{P V} \in \mathbb{R}^{N \times d}
</script>
åœ¨è¿™ä¸ªç®—æ³•ä¸­ï¼Œ<script type="math/tex; ">\mathbf{S}</script>ï¼Œ<script type="math/tex; ">\mathbf{P}</script>çŸ©é˜µéƒ½æ˜¯å¾ˆå¤§ï¼Œéœ€è¦åœ¨HBMä¸­å®ä¾‹åŒ–æ¥è¿›è¡Œå­˜å‚¨ï¼Œè¿™æ ·å°±ä¼šå¸¦æ¥å¾ˆå¤šHBMçš„è®¿é—®æ¬¡æ•°ï¼Œ æœ€ç»ˆä½“ç°åˆ°ç®—æ³•æ—¶é—´ç«¯åˆ°ç«¯è¾ƒé•¿çš„å»¶è¿Ÿ</p>
<p><a data-lightbox="b2bf78bb-01ee-4347-bdbc-8b94ec578e33" data-title="flash_Attenæµç¨‹" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenæµç¨‹.webp" target="_blank"><img alt="flash_Attenæµç¨‹" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenæµç¨‹.webp"/></a></p>
<h3 id="flashattentiontiling">5.5.2 FlashAttention(Tiling)</h3>
<blockquote>
<p>ç†è®ºåŸºç¡€</p>
</blockquote>
<p>åœ¨ä¼ ç»Ÿç®—æ³•ä¸­ï¼Œä¸€ç§æ–¹å¼æ˜¯å°†Maskå’ŒSoftMaxéƒ¨åˆ†èåˆï¼Œä»¥å‡å°‘è®¿å­˜æ¬¡æ•°ã€‚ç„¶è€Œï¼ŒFlashAttentionåˆ™æ›´åŠ æ¿€è¿›ï¼Œå®ƒå°†ä»è¾“å…¥<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}</script>åˆ°è¾“å‡º<script type="math/tex; ">\mathbf{O}</script>çš„æ•´ä¸ªè¿‡ç¨‹è¿›è¡Œèåˆï¼Œä»¥é¿å…<script type="math/tex; ">\mathbf{S}</script>ï¼Œ<script type="math/tex; "> \mathbf{P}</script>çŸ©é˜µçš„å­˜å‚¨å¼€é”€ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å»¶è¿Ÿç¼©å‡ã€‚ç„¶è€Œï¼Œç”±äºè¾“å…¥çš„é•¿åº¦<script type="math/tex; ">N</script>é€šå¸¸å¾ˆé•¿ï¼Œæ— æ³•å®Œå…¨å°†å®Œæ•´çš„<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}</script>åŠä¸­é—´è®¡ç®—ç»“æœå­˜å‚¨åœ¨SRAMä¸­ã€‚å› æ­¤ï¼Œéœ€è¦ä¾ èµ–HBMè¿›è¡Œè®¿å­˜æ“ä½œï¼Œä¸åŸå§‹è®¡ç®—å»¶è¿Ÿç›¸æ¯”æ²¡æœ‰å¤ªå¤§å·®å¼‚ï¼Œç”šè‡³ä¼šå˜æ…¢(æ²¡å…·ä½“æµ‹)</p>
<p>ä¸ºäº†è®©è®¡ç®—è¿‡ç¨‹çš„ç»“æœå®Œå…¨åœ¨SRAMä¸­ï¼Œæ‘†è„±å¯¹HBMçš„ä¾èµ–ï¼Œå¯ä»¥é‡‡ç”¨åˆ†ç‰‡æ“ä½œï¼Œæ¯æ¬¡è¿›è¡Œéƒ¨åˆ†è®¡ç®—ï¼Œç¡®ä¿è¿™äº›è®¡ç®—ç»“æœèƒ½åœ¨SRAMå†…è¿›è¡Œäº¤äº’ï¼Œå¾…å¾—åˆ°å¯¹åº”çš„ç»“æœåå†è¿›è¡Œè¾“å‡º</p>
<p>è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¹‹å‰å¯¹äºsoftmaxçš„è®¡ç®—æ˜¯ä»¥è¡Œä¸ºå•ä½çš„ï¼Œå¦‚ä¸‹æ‰€ç¤º:
<script type="math/tex; mode=display">
m(x):=\max _{i} x_{i}, \quad f(x):=\left[\begin{array}{lll}
e^{x_{1}-m(x)} & \ldots & e^{x_{B}-m(x)}
\end{array}\right], \quad \ell(x):=\sum_{i} f(x)_{i}, \quad \operatorname{softmax}(x):=\frac{f(x)}{\ell(x)}
</script>
å½“æˆ‘ä»¬å°†è¾“å…¥è¿›è¡Œåˆ†ç‰‡åï¼Œæ— æ³•å¯¹å®Œæ•´çš„è¡Œæ•°æ®æ‰§è¡ŒSoftmaxæ“ä½œã€‚è¿™æ˜¯å› ä¸ºSoftmaxå‡½æ•°åœ¨è®¡ç®—æ—¶éœ€è¦è€ƒè™‘æ•´ä¸ªè¡Œçš„æ•°æ®</p>
<p><strong>ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¦‚ä¸‹æ‰€ç¤ºæ–¹æ³•æ¥è·å¾—ä¸å®Œæ•´è¡ŒSoftmaxç›¸åŒçš„ç»“æœï¼Œè€Œæ— éœ€ä½¿ç”¨è¿‘ä¼¼æ“ä½œ</strong>
<script type="math/tex; mode=display">
\begin{array}{l}
m(x)=m\left(\left[x^{(1)} x^{(2)}\right]\right)=\max \left(m\left(x^{(1)}\right), m\left(x^{(2)}\right)\right), \quad f(x)=\left[\begin{array}{ll}
e^{m\left(x^{(1)}\right)-m(x)} f\left(x^{(1)}\right) & \left.e^{m\left(x^{(2)}\right)-m(x)} f\left(x^{(2)}\right)\right]
\end{array}\right. \\
\ell(x)=\ell\left(\left[x^{(1)} x^{(2)}\right]\right)=e^{m\left(x^{(1)}\right)-m(x)} \ell\left(x^{(1)}\right)+e^{m\left(x^{(2)}\right)-m(x)} \ell\left(x^{(2)}\right), \quad \operatorname{softmax}(x)=\frac{f(x)}{\ell(x)}
\end{array}
</script>
å…·ä½“çš„åˆ†å—softmaxä»£ç æ¼”ç¤º</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch


q = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]).float()
v = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]).float()
q_sm = torch.softmax(q, <span class="hljs-number">0</span>)
print(q_sm)   <span class="hljs-comment"># tensor([0.2689, 0.7311])</span>

torch.dot(q_sm, v)  <span class="hljs-comment"># tensor(1.7311)</span>

m_pre = float(<span class="hljs-string">"-inf"</span>)
l_pre = <span class="hljs-number">0</span>
cur_sum = <span class="hljs-number">0</span>

block1 = torch.tensor([<span class="hljs-number">1</span>]).float()
<span class="hljs-comment"># get cur max value</span>
m_cur = max(torch.max(block1), m_pre)
<span class="hljs-comment"># scale pre log value by max exp</span>
l_pre *= torch.exp(m_pre - m_cur)
<span class="hljs-comment"># calculate current log sum</span>
p = torch.exp(block1 - m_cur)
l_cur = torch.sum(p) + l_pre
<span class="hljs-comment"># scale pre result by log sum</span>
cur_sum = cur_sum * l_pre / l_cur
p = p / l_cur
cur_sum = <span class="hljs-number">1</span> * p[<span class="hljs-number">0</span>]

l_pre = l_cur
m_pre = m_cur
print(cur_sum)   <span class="hljs-comment"># tensor(1.)</span>

block2 = torch.tensor([<span class="hljs-number">2</span>]).float()
m_cur = max(torch.max(block2), m_pre)
l_pre *= torch.exp(m_pre - m_cur)
p = torch.exp(block2 - m_cur)
l_cur = torch.sum(p) + l_pre
cur_sum = cur_sum * l_pre / l_cur
p = p / l_cur
cur_sum += <span class="hljs-number">2</span> * p[<span class="hljs-number">0</span>]
print(cur_sum)   <span class="hljs-comment"># tensor(1.7311)</span>
</code></pre>
<blockquote>
<p>ä»£ç å®ç°</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_fwd_kernel</span><span class="hljs-params">(
    Q, K, V, sm_scale,
    L, M,
    Out,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vk, stride_vn,
    stride_oz, stride_oh, stride_om, stride_on,
    Z, H, N_CTX,
    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,
    BLOCK_N: tl.constexpr,
)</span>:</span>
    start_m = tl.program_id(<span class="hljs-number">0</span>)
    off_hz = tl.program_id(<span class="hljs-number">1</span>)
    <span class="hljs-comment"># initialize offsets</span>
    offs_m = start_m * BLOCK_M + tl.arange(<span class="hljs-number">0</span>, BLOCK_M)
    offs_n = tl.arange(<span class="hljs-number">0</span>, BLOCK_N)
    offs_d = tl.arange(<span class="hljs-number">0</span>, BLOCK_DMODEL)
    off_q = off_hz * stride_qh + offs_m[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_d[<span class="hljs-keyword">None</span>, :] * stride_qk
    off_k = off_hz * stride_qh + offs_n[<span class="hljs-keyword">None</span>, :] * stride_kn + offs_d[:, <span class="hljs-keyword">None</span>] * stride_kk
    off_v = off_hz * stride_qh + offs_n[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_d[<span class="hljs-keyword">None</span>, :] * stride_qk
    <span class="hljs-comment"># Initialize pointers to Q, K, V</span>
    q_ptrs = Q + off_q
    k_ptrs = K + off_k
    v_ptrs = V + off_v
    <span class="hljs-comment"># initialize pointer to m and l</span>
    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(<span class="hljs-string">"inf"</span>)
    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)
    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)
    <span class="hljs-comment"># load q: it will stay in SRAM throughout</span>
    q = tl.load(q_ptrs)
    <span class="hljs-comment"># loop over k, v and update accumulator</span>
    <span class="hljs-keyword">for</span> start_n <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, (start_m + <span class="hljs-number">1</span>) * BLOCK_M, BLOCK_N):
        <span class="hljs-comment"># -- compute qk ----</span>
        k = tl.load(k_ptrs)
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk += tl.dot(q, k)
        qk *= sm_scale
        qk = tl.where(offs_m[:, <span class="hljs-keyword">None</span>] &gt;= (start_n + offs_n[<span class="hljs-keyword">None</span>, :]), qk, float(<span class="hljs-string">"-inf"</span>))
        <span class="hljs-comment"># compute new m</span>
        m_curr = tl.maximum(tl.max(qk, <span class="hljs-number">1</span>), m_prev)
        <span class="hljs-comment"># correct old l</span>
        l_prev *= tl.exp(m_prev - m_curr)
        <span class="hljs-comment"># attention weights</span>
        p = tl.exp(qk - m_curr[:, <span class="hljs-keyword">None</span>])
        l_curr = tl.sum(p, <span class="hljs-number">1</span>) + l_prev
        <span class="hljs-comment"># rescale operands of matmuls</span>
        l_rcp = <span class="hljs-number">1.</span> / l_curr
        p *= l_rcp[:, <span class="hljs-keyword">None</span>]
        acc *= (l_prev * l_rcp)[:, <span class="hljs-keyword">None</span>]
        <span class="hljs-comment"># update acc</span>
        p = p.to(Q.dtype.element_ty)
        v = tl.load(v_ptrs)
        acc += tl.dot(p, v)
        <span class="hljs-comment"># update m_i and l_i</span>
        l_prev = l_curr
        m_prev = m_curr
        <span class="hljs-comment"># update pointers</span>
        k_ptrs += BLOCK_N * stride_kn
        v_ptrs += BLOCK_N * stride_vk
    <span class="hljs-comment"># rematerialize offsets to save registers</span>
    start_m = tl.program_id(<span class="hljs-number">0</span>)
    offs_m = start_m * BLOCK_M + tl.arange(<span class="hljs-number">0</span>, BLOCK_M)
    <span class="hljs-comment"># write back l and m</span>
    l_ptrs = L + off_hz * N_CTX + offs_m
    m_ptrs = M + off_hz * N_CTX + offs_m
    tl.store(l_ptrs, l_prev)
    tl.store(m_ptrs, m_prev)
    <span class="hljs-comment"># initialize pointers to output</span>
    offs_n = tl.arange(<span class="hljs-number">0</span>, BLOCK_DMODEL)
    off_o = off_hz * stride_oh + offs_m[:, <span class="hljs-keyword">None</span>] * stride_om + offs_n[<span class="hljs-keyword">None</span>, :] * stride_on
    out_ptrs = Out + off_o
    tl.store(out_ptrs, acc)
</code></pre>
<h2 id="ioå¤æ‚åº¦åˆ†æ">5.6 IOå¤æ‚åº¦åˆ†æ</h2>
<h3 id="standard-attention_1">5.6.1 Standard Attention</h3>
<p>å¯¹äºæ ‡å‡†æ³¨æ„åŠ›å®ç°ï¼ŒåˆæœŸæˆ‘ä»¬éœ€è¦æŠŠè¾“å…¥ <script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}</script>ä»HBMä¸­è¯»å–ï¼Œå¹¶è®¡ç®—å®Œæ¯•åæŠŠè¾“å‡º<script type="math/tex; ">\mathbf{O}</script>å†™å…¥åˆ°HBMä¸­</p>
<ol>
<li>ç¬¬ä¸€æ­¥æŠŠ<script type="math/tex; ">\mathbf{Q}, \mathbf{K}</script>è¯»å–å‡ºæ¥è®¡ç®—å‡º<script type="math/tex; ">\mathbf{S}=\mathbf{Q} \mathbf{K}^{\top}</script>ï¼Œç„¶åæŠŠ<script type="math/tex; ">\mathbf{S}</script>å­˜å›å»ï¼Œå†…å­˜è®¿é—®å¤æ‚åº¦<script type="math/tex; ">\Theta\left(N d+N^{2}\right)</script></li>
<li>ç¬¬äºŒæ­¥æŠŠ<script type="math/tex; ">\mathbf{S}</script>è¯»å–å‡ºæ¥è®¡ç®—å‡º<script type="math/tex; ">\mathbf{P}=\operatorname{softmax}(\mathbf{S})</script>ï¼Œç„¶åæŠŠ<script type="math/tex; ">\mathbf{P}</script>å­˜å›å»ï¼Œå†…å­˜è®¿é—®å¤æ‚åº¦<script type="math/tex; ">\Theta\left(N^{2}\right)</script></li>
<li>ç¬¬ä¸‰æ­¥æŠŠ<script type="math/tex; ">\mathbf{V}, \mathbf{P}</script>è¯»å–å‡ºæ¥è®¡ç®—å‡º<script type="math/tex; ">\mathbf{O}=\mathbf{P V}</script>ï¼Œç„¶åè®¡ç®—å‡ºç»“æœ<script type="math/tex; ">\mathbf{O}</script>ï¼Œå†…å­˜è®¿é—®å¤æ‚åº¦<script type="math/tex; ">\Theta\left(N d+N^{2}\right)</script></li>
</ol>
<p>ç»¼ä¸Šæ‰€è¿°ï¼Œæ•´ä½“çš„å†…å­˜è®¿é—®å¤æ‚åº¦ä¸º<script type="math/tex; ">\Theta\left(N d+N^{2}\right)</script></p>
<h3 id="flashattention">5.6.2 FlashAttention</h3>
<p>å¯¹äºFlashAttentionï¼Œæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªåˆ†å—å¤§å°<script type="math/tex; ">B_{c}</script>æ¥æŠŠ<script type="math/tex; ">\mathbf{K}, \mathbf{V}</script>åˆ†æˆ<script type="math/tex; ">T_{c}</script>å—ï¼Œå¯¹äº<script type="math/tex; ">\mathbf{Q}, \mathbf{O}</script>çš„æ¯ä¸€å—éƒ½è¦æŠŠ<script type="math/tex; ">\mathbf{K}, \mathbf{V}</script>éƒ¨åˆ†çš„å…¨éƒ¨å…ƒç´ Loadä¸€éï¼Œè¿™æ ·åˆ™æœ‰FlashAttentionçš„å†…å­˜è®¿é—®å¤æ‚åº¦ä¸º<script type="math/tex; ">\Theta\left(N d+N d T_{c}\right)=\Theta\left(N d T_{c}\right)</script></p>
<p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªåˆ†å—å¤§å°ï¼Œ<script type="math/tex; ">\mathbf{Q}, \mathbf{O}</script>çš„åˆ†å—å¤§å°<script type="math/tex; ">B_{r}</script>ï¼Œ<script type="math/tex; ">\mathbf{K}, \mathbf{V}</script>çš„åˆ†å—å¤§å°<script type="math/tex; ">B_{c}</script>ï¼Œæˆ‘ä»¬è®¾å®šSRAMçš„å¤§å°ä¸º<script type="math/tex; ">M</script>ï¼Œä¸ºäº†èƒ½æŠŠåˆ†å—åçš„<script type="math/tex; ">\mathbf{K}, \mathbf{V} \in \mathbb{R}^{B_{c} \times d}</script>æ”¾è¿›SRAMï¼Œé‚£ä¹ˆåˆ™æœ‰ä¸€ä¸‹é™åˆ¶:
<script type="math/tex; mode=display">
B_{c} d=O(M) \Leftrightarrow B_{c}=O\left(\frac{M}{d}\right)
</script>
ç›¸åº”çš„ï¼Œ<script type="math/tex; ">\mathbf{Q}, \mathbf{O} \in \mathbb{R}^{B_{r} \times d}</script>æœ‰å¦‚ä¸‹é™åˆ¶:
<script type="math/tex; mode=display">
B_{r} d=O(M) \Leftrightarrow B_{r}=O\left(\frac{M}{d}\right)
</script>
æœ€ç»ˆï¼Œè¿˜æœ‰ä¸€ä¸ªä¸­é—´æ€<script type="math/tex; ">\mathbf{S}=\mathbf{Q K}^{\top} \in \mathbb{R}^{B_{r} \times B_{c}}</script>éœ€è¦å­˜å‚¨ï¼Œåˆ™æœ‰å¦‚ä¸‹é™åˆ¶:
<script type="math/tex; mode=display">
B_{r} B_{c}=O(M)
</script>
ç»¼ä¸Šï¼Œé™åˆ¶å¦‚ä¸‹
<script type="math/tex; mode=display">
B_{c}=\Theta\left(\frac{M}{d}\right), \quad B_{r}=\Theta\left(\min \left(\frac{M}{d}, \frac{M}{B_{c}}\right)\right)=\Theta\left(\min \left(\frac{M}{d}, d\right)\right)
</script>
è¿›è€Œæ¨å‡º
<script type="math/tex; mode=display">
T_{c}=\frac{N}{B_{c}}=\Theta\left(\frac{N d}{M}\right)
</script>
é‚£ä¹ˆåœ¨<script type="math/tex; ">M=\Theta(N d)</script> çš„å‰æä¸‹ï¼Œåˆ™æœ‰FlashAttentionçš„HBMå†…å­˜è®¿é—®å¤æ‚åº¦ä¸ºï¼š
<script type="math/tex; mode=display">
\Theta\left(N d T_{c}\right)=\Theta\left(\frac{N^{2} d^{2}}{M}\right)=\Theta(N d)
</script>
åœ¨è¯­è¨€å»ºæ¨¡ä¸­ï¼Œé€šå¸¸æœ‰<script type="math/tex; ">d \lll N</script>ï¼Œåˆ™æœ‰<script type="math/tex; ">\Theta_{\text {stand }}\left(N d+N^{2}\right)>\Theta_{f l a s h}(N d)</script>ã€‚è¿™æ ·ï¼Œåœ¨å‰å‘çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†å—è®¡ç®—çš„æ–¹å¼ï¼Œé¿å…äº†<script type="math/tex; ">\mathbf{S}, \mathbf{P}</script>çŸ©é˜µçš„å­˜å‚¨å¼€é”€ï¼Œæ•´ä½“çš„è¿ç®—éƒ½åœ¨SRAMå†…è¿›è¡Œï¼Œé™ä½äº†HBMè®¿é—®æ¬¡æ•°ï¼Œå¤§å¤§æå‡äº†è®¡ç®—çš„é€Ÿåº¦ï¼Œå‡å°‘äº†å¯¹å­˜å‚¨çš„æ¶ˆè€—</p>
<h2 id="backward">5.7 Backward</h2>
<h3 id="ç†è®ºåŸºç¡€">5.7.1 ç†è®ºåŸºç¡€</h3>
<p>åœ¨ä¸Šé¢å‰å‘çš„æ—¶å€™æˆ‘ä»¬ä¸ºäº†å‡å°‘HBMè®¿å­˜æ¬¡æ•°ï¼Œé™ä½å†…å­˜æ¶ˆè€—é‡ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å¯¹<script type="math/tex; ">\mathbf{S}, \mathbf{P}</script>çŸ©é˜µè¿›è¡Œå­˜å‚¨ï¼Œè€Œè¿™ä¸ªåœ¨åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦çš„æ—¶å€™ç¡®å®éœ€è¦çš„ä¸€ä¸ªä¿¡æ¯</p>
<p>ä¹‹å‰æœ‰é€šè¿‡Gradient checkpointingçš„æ–¹å¼æ¥å®ç°æ¢¯åº¦å®ç°åœ¨å‰å‘çš„æ—¶å€™æ›´åŠ èŠ‚çœå†…å­˜</p>
<p>æˆ‘ä»¬è¿™é‡Œåˆ™é‡‡ç”¨<strong>é‡æ–°è®¡ç®—çš„æ–¹å¼æ¥è®¡ç®—å¯¹åº”çš„æ¢¯åº¦</strong>ã€‚åœ¨ä¸Šé¢å‰å‘è®¡ç®—çš„æ—¶å€™æˆ‘ä»¬ä¸ä¼šå­˜å‚¨<script type="math/tex; ">\mathbf{S}, \mathbf{P}</script>çŸ©é˜µï¼Œä½†æ˜¯æˆ‘ä»¬ä¼šå­˜å‚¨å¯¹åº”çš„æŒ‡æ•°é¡¹ä¹‹å’Œ<script type="math/tex; ">L</script>æ¥è¿›è¡Œæ¢¯åº¦çš„è®¡ç®—</p>
<p>æˆ‘ä»¬åœ¨åå‘çš„è¿‡ç¨‹ä¸­æœ€é‡è¦çš„äº‹æƒ…å°±æ˜¯å°±æ˜¯Losså‡½æ•°<script type="math/tex; ">\phi</script>å¯¹<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}</script>å¯¹åº”çš„æ¢¯åº¦</p>
<p><script type="math/tex; ">\mathbf{O}</script>å¯¹åº”çš„æ¢¯åº¦æœ€å¥½è®¡ç®—<script type="math/tex; ">\mathbf{d} \mathbf{O}=\frac{\partial \phi}{\partial \mathbf{O}}</script>ï¼Œå…¶ä¸­<script type="math/tex; ">\mathbf{O}</script>æ˜¯ç°æˆçš„</p>
<p><script type="math/tex; ">\mathbf{V}</script>å¯¹åº”çš„æ¢¯åº¦ä¹Ÿå¾ˆå¥½è®¡ç®—ï¼Œç”±äº<script type="math/tex; ">\mathbf{O}=\mathbf{P V}</script>ï¼Œæ ¹æ®é“¾å¼æ±‚å¯¼æ³•åˆ™å’ŒçŸ©é˜µæ±‚å¯¼æ³•åˆ™åˆ™æœ‰<script type="math/tex; ">\mathbf{d V}=\mathbf{P}^{T} \mathbf{d} \mathbf{O}</script>ï¼Œæ›´è¯¦ç»†å¦‚ä¸‹æ‰€ç¤º:
<script type="math/tex; mode=display">
d v_{j}=\sum_{i} P_{i j} d o_{i}=\sum_{i} \frac{e^{q_{i}^{T}} k_{j}}{L_{i}} d o_{i}
</script>
<script type="math/tex; ">\mathbf{Q}, \mathbf{K}</script>å¯¹åº”çš„æ¢¯åº¦ç®—èµ·æ¥å°±æ¯”è¾ƒå¤æ‚ä¸€ç‚¹ã€‚è¿™ä¸¤ä¸ªç»è¿‡çš„è®¡ç®—é€»è¾‘æ­¥éª¤æ›´å¤šï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ­¥ä¸€æ­¥çš„æ¥è¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥å…ˆè®¡ç®—<script type="math/tex; ">\mathbf{d P}</script>ï¼Œ<script type="math/tex; ">\mathbf{d S}</script>ã€‚ç”±äº<script type="math/tex; ">\mathbf{O}=\mathbf{P V}</script> ï¼Œåˆ™æœ‰<script type="math/tex; ">\mathbf{d P}</script>å¦‚ä¸‹è¡¨ç¤º
<script type="math/tex; mode=display">
d P_{i j}=d o_{i}^{T} v_{j}
</script></p>
<p>Fact: <script type="math/tex; ">y=\operatorname{softmax}(x)</script>çš„é›…å„æ¯”çŸ©é˜µä¸º<script type="math/tex; ">\operatorname{diag}(y)-y y^{T}</script>ï¼Œå…·ä½“æ¨å¯¼è§</p>
<p><a href="https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1" target="_blank">Derivative of the Softmax Function and the Categorical Cross-Entropy Loss</a></p>
<p>ç”±äº<script type="math/tex; ">P_{i:}=\operatorname{softmax}\left(S_{i:}\right)</script>ï¼Œ æ ¹æ®ä¸Šè¿°å®šç†åˆ™æœ‰:
<script type="math/tex; mode=display">
d S_{i:}=\left(\operatorname{diag}\left(P_{i:}\right)-P_{i:} P_{i:}^{T}\right) d P_{i:}=P_{i:} \circ d P_{i:}-\left(P_{i:}^{T} d P_{i:}\right) P_{i:}
</script>
æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰å¦‚ä¸‹è¡¨ç¤º:
<script type="math/tex; mode=display">
D_{i}=P_{i:}^{T} d P_{i:}=\sum \frac{e^{q_{i} \kappa_{j}}}{L_{i}} d o_{i}^{T} v_{j}=d o_{i}^{T} \sum \frac{e^{q_{i} \kappa_{j}}}{L_{i}} v_{j}=d o_{i}^{T} o_{i}
</script>
æ ¹æ®ä¸Šè¿°å®šä¹‰ç®€åŒ–ä¸Šä¸Šå¼åˆ™æœ‰å¦‚ä¸‹è¡¨ç¤º:
<script type="math/tex; mode=display">
d S_{i:}=P_{i:} \circ d P_{i:}-D_{i} P_{i:}
</script>
ç›¸åº”çš„<script type="math/tex; ">d \mathbf{S}</script>å¯è¡¨ç¤ºä¸ºå¦‚ä¸‹å½¢å¼:
<script type="math/tex; mode=display">
d S_{i j}=P_{i j} d P_{i j}-D_{i} P_{i j}=P_{i j}\left(d P_{i j}-D_{i}\right)
</script>
åˆå› ä¸º<script type="math/tex; ">S_{i j}=q_{i}^{T} k_{j}</script>ï¼Œç»“åˆä¸Šè¿°æ¨å¯¼åˆ©ç”¨é“¾å¼æ±‚å¯¼æ³•åˆ™<script type="math/tex; ">\mathbf{Q}, \mathbf{K}</script>å¯¹åº”çš„æ¢¯åº¦æœ‰å¦‚ä¸‹è¡¨ç¤ºï¼š
<script type="math/tex; mode=display">
\begin{array}{l}
d q_{i}=\sum_{j} d S_{i j} k_{j}=\sum_{j} P_{i j}\left(d P_{i j}-D_{i}\right) k_{j}=\sum_{j} \frac{e^{q_{i}^{T} k_{j}}}{L_{i}}\left(d o_{i}^{T} v_{j}-D_{i}\right) k_{j} \\
d k_{j}=\sum_{i} d S_{i j} q_{i}=\sum_{i} P_{i j}\left(d P_{i j}-D_{i}\right) q_{i}=\sum_{i} \frac{e^{q_{i}^{T} k_{j}}}{L_{i}}\left(d o_{i}^{T} v_{j}-D_{i}\right) q_{i}
\end{array}
</script>
è‡³æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„åŒ…å«å‰å‘å’Œåå‘çš„ï¼Œé™ä½äº†HBMè®¿é—®æ¬¡æ•°çš„ï¼Œæ–°çš„Attentionç®—å­</p>
<h3 id="ä»£ç å®ç°">5.7.2 ä»£ç å®ç°</h3>
<pre><code class="lang-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_bwd_kernel</span><span class="hljs-params">(
    Q, K, V, sm_scale, Out, DO,
    DQ, DK, DV,
    L, M,
    D,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vk, stride_vn,
    Z, H, N_CTX,
    num_block,
    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,
    BLOCK_N: tl.constexpr,
)</span>:</span>
    off_hz = tl.program_id(<span class="hljs-number">0</span>)
    off_z = off_hz // H
    off_h = off_hz % H
    <span class="hljs-comment"># offset pointers for batch/head</span>
    Q += off_z * stride_qz + off_h * stride_qh
    K += off_z * stride_qz + off_h * stride_qh
    V += off_z * stride_qz + off_h * stride_qh
    DO += off_z * stride_qz + off_h * stride_qh
    DQ += off_z * stride_qz + off_h * stride_qh
    DK += off_z * stride_qz + off_h * stride_qh
    DV += off_z * stride_qz + off_h * stride_qh
    <span class="hljs-keyword">for</span> start_n <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, num_block):
        lo = start_n * BLOCK_M
        <span class="hljs-comment"># initialize row/col offsets</span>
        offs_qm = lo + tl.arange(<span class="hljs-number">0</span>, BLOCK_M)
        offs_n = start_n * BLOCK_M + tl.arange(<span class="hljs-number">0</span>, BLOCK_M)
        offs_m = tl.arange(<span class="hljs-number">0</span>, BLOCK_N)
        offs_k = tl.arange(<span class="hljs-number">0</span>, BLOCK_DMODEL)
        <span class="hljs-comment"># initialize pointers to value-like data</span>
        q_ptrs = Q + (offs_qm[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_k[<span class="hljs-keyword">None</span>, :] * stride_qk)
        k_ptrs = K + (offs_n[:, <span class="hljs-keyword">None</span>] * stride_kn + offs_k[<span class="hljs-keyword">None</span>, :] * stride_kk)
        v_ptrs = V + (offs_n[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_k[<span class="hljs-keyword">None</span>, :] * stride_qk)
        do_ptrs = DO + (offs_qm[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_k[<span class="hljs-keyword">None</span>, :] * stride_qk)
        dq_ptrs = DQ + (offs_qm[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_k[<span class="hljs-keyword">None</span>, :] * stride_qk)
        <span class="hljs-comment"># pointer to row-wise quantities in value-like data</span>
        D_ptrs = D + off_hz * N_CTX
        m_ptrs = M + off_hz * N_CTX
        <span class="hljs-comment"># initialize dv amd dk</span>
        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)
        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)
        <span class="hljs-comment"># k and v stay in SRAM throughout</span>
        k = tl.load(k_ptrs)
        v = tl.load(v_ptrs)
        <span class="hljs-comment"># loop over rows</span>
        <span class="hljs-keyword">for</span> start_m <span class="hljs-keyword">in</span> range(lo, num_block * BLOCK_M, BLOCK_M):
            offs_m_curr = start_m + offs_m
            <span class="hljs-comment"># load q, k, v, do on-chip</span>
            q = tl.load(q_ptrs)
            <span class="hljs-comment"># recompute p = softmax(qk, dim=-1).T</span>
            <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> `do` is pre-divided by `l`; no normalization here</span>
            qk = tl.dot(q, tl.trans(k))
            qk = tl.where(offs_m_curr[:, <span class="hljs-keyword">None</span>] &gt;= (offs_n[<span class="hljs-keyword">None</span>, :]), qk, float(<span class="hljs-string">"-inf"</span>))
            m = tl.load(m_ptrs + offs_m_curr)
            p = tl.exp(qk * sm_scale - m[:, <span class="hljs-keyword">None</span>])
            <span class="hljs-comment"># compute dv</span>
            do = tl.load(do_ptrs)
            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)
            <span class="hljs-comment"># compute dp = dot(v, do)</span>
            Di = tl.load(D_ptrs + offs_m_curr)
            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, <span class="hljs-keyword">None</span>]
            dp += tl.dot(do, tl.trans(v))
            <span class="hljs-comment"># compute ds = p * (dp - delta[:, None])</span>
            ds = p * dp * sm_scale
            <span class="hljs-comment"># compute dk = dot(ds.T, q)</span>
            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)
            <span class="hljs-comment"># compute dq</span>
            dq = tl.load(dq_ptrs)
            dq += tl.dot(ds.to(Q.dtype.element_ty), k)
            tl.store(dq_ptrs, dq)
            <span class="hljs-comment"># increment pointers</span>
            dq_ptrs += BLOCK_M * stride_qm
            q_ptrs += BLOCK_M * stride_qm
            do_ptrs += BLOCK_M * stride_qm
        <span class="hljs-comment"># write-back</span>
        dv_ptrs = DV + (offs_n[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_k[<span class="hljs-keyword">None</span>, :] * stride_qk)
        dk_ptrs = DK + (offs_n[:, <span class="hljs-keyword">None</span>] * stride_kn + offs_k[<span class="hljs-keyword">None</span>, :] * stride_kk)
        tl.store(dv_ptrs, dv)
        tl.store(dk_ptrs, dk)
</code></pre>
<h3 id="block-sparse">5.7.3 Block-Sparse</h3>
<p>ç›¸æ¯”äºä¸Šé¢çš„å…¨é‡è®¡ç®—ï¼Œå—ç¨€ç–çš„FlashAttentionéœ€è¦é¢å¤–æä¾›ä¸€ä¸ªMaskçŸ©é˜µ<script type="math/tex; ">\tilde{\mathbf{M}} \in\{0,1\}^{N \times N}</script>ç”¨äºå°†ä¸€äº›å…ƒ ç´ ç½®é›¶æ¥ä¿è¯å—ç¨€ç–åŠ é€Ÿè®¡ç®—</p>
<p>æœ¬ç« å¯¹äºå—ç¨€ç–çš„ä¸€ä¸ªè®¡ç®—åªæ˜¯ä¸€ä¸ªç®€å•çš„å°è¯•ï¼Œæ²¡æœ‰è¿›è¡Œå¤ªæ·±å…¥çš„æ¢ç´¢ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬å…ˆä¸€ç¬”å¸¦è¿‡ï¼Œåé¢æˆ‘ä»¬å¯ä»¥è®²ä¸€ç¯‡å¯¹FlashAttentionè¿›è¡Œå—ç¨€ç–ä¼˜åŒ–çš„å·¥ä½œSCFA
<script type="math/tex; mode=display">
\mathbf{S}=\mathbf{Q K} \mathbf{K}^{\top} \in \mathbb{R}^{N \times N}, \quad \mathbf{P}=\operatorname{softmax}\left(\mathbf{S} \odot 1_{\overline{\mathbf{M}}}\right) \in \mathbb{R}^{N \times N}, \quad \mathbf{O}=\mathbf{P V} \in \mathbb{R}^{N \times d}
</script></p>
<h2 id="å®éªŒéªŒè¯">5.8 å®éªŒéªŒè¯</h2>
<p>é€šè¿‡å®éªŒéªŒè¯å‘ç°ï¼ŒFlashAttentionåœ¨é€Ÿåº¦å’Œå†…å­˜å ç”¨æ–¹é¢éƒ½è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ï¼Œå¹¶å–å¾—äº†è‰¯å¥½çš„æ•ˆæœ</p>
<p><a data-lightbox="c444f5c7-2ba3-4b01-a28f-6771fab2a66f" data-title="flash_Attenå®éªŒéªŒè¯1" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenå®éªŒéªŒè¯1.webp" target="_blank"><img alt="flash_Attenå®éªŒéªŒè¯1" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenå®éªŒéªŒè¯1.webp"/></a></p>
<p><a data-lightbox="345063d1-30b6-4f62-bb81-98f1118492f8" data-title="flash_Attenå®éªŒéªŒè¯2" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenå®éªŒéªŒè¯2.webp" target="_blank"><img alt="flash_Attenå®éªŒéªŒè¯2" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—/flash_Attenå®éªŒéªŒè¯2.webp"/></a></p>
<p>ç›®å‰ï¼ŒFlashAttentionå·²ç»ç»è¿‡å¹¿æ³›éªŒè¯, torch2.0ä¸­å·²æä¾›flashattentionçš„å®ç°</p>
<p>æ­£å¦‚æ ‡é¢˜ã€ŠFast and Memory-Efficient Exact Attention with IO-Awarenessã€‹æ‰€ç¤ºï¼ŒFlashAttentionçš„ä¼˜ç‚¹åœ¨äºå……åˆ†è€ƒè™‘äº†åœ¨è®¡ç®—ä»»åŠ¡ä¸­IOçš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡åˆ†å—è®¡ç®—çš„æ–¹å¼å¼€å‘äº†ä¸€ç§å¿«é€Ÿã€èŠ‚çœæ˜¾å­˜ã€ç²¾ç¡®æ— è¿‘ä¼¼çš„æ³¨æ„åŠ›å®ç°æ–¹æ³•ã€‚è¿™ä½¿å¾—æˆ‘ä»¬æ›´ä¾¿äºè®­ç»ƒå…·æœ‰æ›´é•¿ä¸Šä¸‹æ–‡çš„Transformeræ¨¡å‹ï¼Œå¹¶ä¸”ä¸ºåç»­æ³¨æ„åŠ›ç®—æ³•çš„ä¼˜åŒ–æä¾›äº†ä¸€ä¸ªåŸºå‡†</p>
<p></p><footer class="page-footer"><span class="copyright">Copyright Â© narutohyc.com 2021 all right reservedï¼Œpowered by Gitbook</span><span class="footer-modification">è¯¥æ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š
2023-08-15 00:51:56
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: 'æ¬¢è¿ç•™ä¸‹è¯„è®ºäº¤æµ~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: Introduction" class="navigation navigation-prev" href="../">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.md" class="navigation navigation-next" href="LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":59381,"date":"2023/06/15 12:46:10","cover":"https://pic.hycbook.com/i/hexo/post_cover/è•¾å§†0.webp","title":"LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.md","tags":["æ·±åº¦å­¦ä¹ ","LLMæ¨¡å‹","å¾®è°ƒ","p tuning v2","lora"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/è•¾å§†0.webp","mathjax":true,"categories":["deep_learning"],"description":"LLMæ¨¡å‹å¾®è°ƒç³»åˆ—","level":"1.2","depth":1,"next":{"title":"LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.md","level":"1.3","depth":1,"path":"chapters/LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.md","ref":"chapters/LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.md","articles":[]},"previous":{"title":"Introduction","level":"1.1","depth":1,"path":"README.md","ref":"README.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright Â© narutohyc.com 2021","modify_label":"è¯¥æ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipayæ‰“èµ","button":"æ¬¢è¿æ‰“èµ","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechatæ‰“èµ"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"æ·±åº¦å­¦ä¹ çŸ¥è¯†é©¿ç«™","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"narutohyc","repo":"bk_python","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"æ¬¢è¿ç•™ä¸‹è¯„è®ºäº¤æµ~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"æ·±åº¦å­¦ä¹ ç›¸å…³å­¦ä¹ è®°å½•","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"ä¹¦ç±ä¸»é¡µ":"https://study.hycbook.com"}},"gitbook":"*","description":"è®°å½• æ·±åº¦å­¦ä¹  çš„å­¦ä¹ å’Œä¸€äº›æŠ€å·§çš„ä½¿ç”¨"},"file":{"path":"chapters/LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.md","mtime":"2023-08-15T00:51:56.021Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-08-15T00:52:58.704Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
