<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>LLM模型微调系列.md · 深度学习相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="LLM模型微调系列" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="LLM模型部署调试推理.html" rel="next"/>
<link href="LLM Tokenizer分词系列.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">书籍主页</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="LLM Tokenizer分词系列.html" id="chapter_id_1">
<a href="LLM Tokenizer分词系列.html">
<b>1.2.</b>
                    
                    LLM Tokenizer分词系列.md
            
                </a>
</li>
<li class="chapter active" data-level="1.3" data-path="LLM模型微调系列.html" id="chapter_id_2">
<a href="LLM模型微调系列.html">
<b>1.3.</b>
                    
                    LLM模型微调系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="LLM模型部署调试推理.html" id="chapter_id_3">
<a href="LLM模型部署调试推理.html">
<b>1.4.</b>
                    
                    LLM模型部署调试推理.md
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="dl_in_vision_field.html" id="chapter_id_4">
<a href="dl_in_vision_field.html">
<b>1.5.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter" data-level="1.6" data-path="huggingface基本使用教程.html" id="chapter_id_5">
<a href="huggingface基本使用教程.html">
<b>1.6.</b>
                    
                    huggingface基本使用教程.md
            
                </a>
</li>
<li class="chapter" data-level="1.7" data-path="nlp关键词和摘要提取技术整理.html" id="chapter_id_6">
<a href="nlp关键词和摘要提取技术整理.html">
<b>1.7.</b>
                    
                    nlp关键词和摘要提取技术整理.md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="pytorch学习_基础知识.html" id="chapter_id_7">
<a href="pytorch学习_基础知识.html">
<b>1.8.</b>
                    
                    pytorch学习_基础知识.md
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="pytorch学习_进阶知识.html" id="chapter_id_8">
<a href="pytorch学习_进阶知识.html">
<b>1.9.</b>
                    
                    pytorch学习_进阶知识.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="transformer.html" id="chapter_id_9">
<a href="transformer.html">
<b>1.10.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="图像分割算法.html" id="chapter_id_10">
<a href="图像分割算法.html">
<b>1.11.</b>
                    
                    图像分割算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="图像分类算法.html" id="chapter_id_11">
<a href="图像分类算法.html">
<b>1.12.</b>
                    
                    图像分类算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="图神经网络.html" id="chapter_id_12">
<a href="图神经网络.html">
<b>1.13.</b>
                    
                    图神经网络.md
            
                </a>
</li>
<li class="chapter" data-level="1.14" data-path="数据标注工具.html" id="chapter_id_13">
<a href="数据标注工具.html">
<b>1.14.</b>
                    
                    数据标注工具.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="深度学习核心之优化器.html" id="chapter_id_14">
<a href="深度学习核心之优化器.html">
<b>1.15.</b>
                    
                    深度学习核心之优化器.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="深度学习核心之损失函数.html" id="chapter_id_15">
<a href="深度学习核心之损失函数.html">
<b>1.16.</b>
                    
                    深度学习核心之损失函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="深度学习核心之激活函数.html" id="chapter_id_16">
<a href="深度学习核心之激活函数.html">
<b>1.17.</b>
                    
                    深度学习核心之激活函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.18" data-path="深度学习核心基础知识点.html" id="chapter_id_17">
<a href="深度学习核心基础知识点.html">
<b>1.18.</b>
                    
                    深度学习核心基础知识点.md
            
                </a>
</li>
<li class="chapter" data-level="1.19" data-path="深度学习模型压缩技术.html" id="chapter_id_18">
<a href="深度学习模型压缩技术.html">
<b>1.19.</b>
                    
                    深度学习模型压缩技术.md
            
                </a>
</li>
<li class="chapter" data-level="1.20" data-path="目标检测与跟踪算法.html" id="chapter_id_19">
<a href="目标检测与跟踪算法.html">
<b>1.20.</b>
                    
                    目标检测与跟踪算法.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">LLM模型微调系列.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#llm模型">1 LLM模型</a></li><ul><li><span class="title-icon"></span><a href="#概述">1.1 概述</a></li><li><span class="title-icon"></span><a href="#微调">1.2 微调</a></li><li><span class="title-icon"></span><a href="#发展脉络">1.3 发展脉络</a></li></ul><li><span class="title-icon"></span><a href="#adapter-tuning">2 Adapter Tuning</a></li><li><span class="title-icon"></span><a href="#prefixprompt-tuning">3 Prefix/Prompt-Tuning</a></li><ul><li><span class="title-icon"></span><a href="#prefix-tuning">3.1 Prefix-Tuning</a></li><li><span class="title-icon"></span><a href="#prompt-tuning">3.2 Prompt-Tuning</a></li><li><span class="title-icon"></span><a href="#p-tuning">3.3 P-Tuning</a></li><ul><li><span class="title-icon"></span><a href="#p-tuning-v1">3.3.1 P-Tuning V1</a></li><li><span class="title-icon"></span><a href="#p-tuning-v2">3.3.2 P-Tuning V2</a></li></ul></ul><li><span class="title-icon"></span><a href="#lora系列">4 LORA系列</a></li><ul><li><span class="title-icon"></span><a href="#lora转">4.1 LORA(转)</a></li><li><span class="title-icon"></span><a href="#adalora转">4.2 AdaLoRA(转)</a></li><li><span class="title-icon"></span><a href="#qlora">4.3 QLORA</a></li></ul><li><span class="title-icon"></span><a href="#rlhf">5 RLHF</a></li><li><span class="title-icon"></span><a href="#flashatten转">6 Flash_Atten(转)</a></li><ul><li><span class="title-icon"></span><a href="#概述_2">6.1 概述</a></li><li><span class="title-icon"></span><a href="#核心要点">6.2 核心要点</a></li><li><span class="title-icon"></span><a href="#提出问题">6.3 提出问题</a></li><li><span class="title-icon"></span><a href="#解决方案">6.4 解决方案</a></li><li><span class="title-icon"></span><a href="#forward">6.5 Forward</a></li><ul><li><span class="title-icon"></span><a href="#standard-attention">6.5.1 Standard Attention</a></li><li><span class="title-icon"></span><a href="#flashattentiontiling">6.5.2 FlashAttention(Tiling)</a></li></ul><li><span class="title-icon"></span><a href="#io复杂度分析">6.6 IO复杂度分析</a></li><ul><li><span class="title-icon"></span><a href="#standard-attention_1">6.6.1 Standard Attention</a></li><li><span class="title-icon"></span><a href="#flashattention">6.6.2 FlashAttention</a></li></ul><li><span class="title-icon"></span><a href="#backward">6.7 Backward</a></li><ul><li><span class="title-icon"></span><a href="#理论基础">6.7.1 理论基础</a></li></ul><li><span class="title-icon"></span><a href="#实验验证">6.8 实验验证</a></li></ul></ul></div><a href="#llm模型" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><h1 id="llm模型">1 LLM模型</h1>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/626377667" target="_blank">什么是LLM（大语音模型）</a></p>
</blockquote>
<h2 id="概述">1.1 概述</h2>
<p><code>Large Language Model(LLM)</code>，也称为<code>大型语言模型</code>，是一种基于机器学习和自然语言处理技术的模型，它通过对大量的文本数据进行训练，来学习服务人类语言理解和生成的能力</p>
<p>LLM的核心思想是通过大规模的无监督训练来学习自然语言的模式和语言结构，这在一定程度上能够模拟人类的语言认知和生成过程</p>
<p>与传统的NLP模型相比，LLM能够更好地理解和生成自然文本，同时还能够表现出一定的逻辑思维和推理能力</p>
<p>近年来，LLM得到了广泛的应用，其中最具代表性的是谷歌的BERT和OpenAI的GPT系列。这些模型在多个自然语言处理领域已经取得了显著的成果，包括文本分类、命名实体识别、情感分析、机器翻译、自动问答等</p>
<p>然而，在实际应用中，LLM面临着更多的挑战</p>
<ol>
<li>首先，LLM需要大量的计算资源和大规模的数据集来训练，这对于一般的企业和个人来说十分困难</li>
<li>其次，由于LLM模型的复杂性和计算量较大，对于实时的语言处理应用来说，LLM在应用效率和响应速度上还存在一定的局限性</li>
</ol>
<p>因此，如何解决模型训练和应用过程中的计算性能和效率问题，是LLM面临的主要挑战之一</p>
<h2 id="微调">1.2 微调</h2>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/622810394" target="_blank">LLM大模型低资源微调p tuning v2和lora区别</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/627921175" target="_blank">prefix, p-tuningv2, lora finetune该怎么选择?</a></p>
<p><a href="http://news.sohu.com/a/670981907_121119001" target="_blank">让天下没有难Tuning的大模型：PEFT技术简介 2023-04</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/638809556" target="_blank">大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/639068809" target="_blank">大模型高效微调综述下： DiffPruning、BitFit、LoRa、AdaLoRA、MAM Adapters、UniPELT</a></p>
</blockquote>
<p><code>微调(Fine-tuning)</code>是一种常用的技术，用于将预训练的语言模型适应于特定的任务或领域。微调的目的是通过在特定任务上进行有监督的训练，调整模型参数以提高其性能和适应性</p>
<p>以下是微调在适应语言模型中的有效性的几个原因：</p>
<ol>
<li><strong>迁移学习</strong>：预训练的语言模型在大规模文本数据上进行了无监督的学习，从中学习到了通用的语言表示。通过微调，我们可以将这些通用的语言表示迁移到特定任务或领域上，因此可以利用模型在预训练阶段学到的知识</li>
<li><strong>少样本学习</strong>：微调通常只需要在特定任务的相对较小的标注数据集上进行训练，而不是从头开始训练一个全新的模型。这对于许多任务来说是非常有益的，因为获得大规模标注数据可能是昂贵或困难的。通过利用预训练模型的泛化能力，微调可以在少量标注样本上实现较好的性能</li>
<li><strong>领域自适应</strong>：通过微调，可以将语言模型从通用领域适应到特定领域。通过在特定领域的数据上微调，模型可以学习到该领域的特定语言模式、词汇和上下文，从而提高在该领域任务上的性能</li>
<li><strong>模型个性化</strong>：微调还可以用于个性化模型，以适应特定用户或特定应用场景的需求。通过微调模型，可以根据个体用户的偏好、行为或数据特点进行定制，提供更准确和个性化的预测和推荐</li>
</ol>
<p>微调语言模型是一种有效的方法，可以通过迁移学习、少样本学习、领域自适应和模型个性化等方式，利用预训练模型的优势和泛化能力，提高模型在特定任务或领域上的性能和适应性</p>
<blockquote>
<p>为什么需要微调</p>
</blockquote>
<ol>
<li>高效训练，减少训练成本</li>
<li>共享基础大模型，在上面叠加自己的新模型</li>
</ol>
<h2 id="发展脉络">1.3 发展脉络</h2>
<p><a data-lightbox="a6841b59-dd7c-4a11-a960-da94d5180fa8" data-title="LLM微调技术发展脉络" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/LLM微调技术发展脉络.svg" target="_blank"><img alt="LLM微调技术发展脉络" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/LLM微调技术发展脉络.svg"/></a></p>
<blockquote>
<p>Adapter系列</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2005.00247.pdf" target="_blank">AdapterFusion: Non-Destructive Task Composition for Transfer Learning 2021</a></p>
<p><a href="https://arxiv.org/pdf/2105.07148.pdf" target="_blank">Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter 2021</a></p>
<p><a href="https://arxiv.org/pdf/2303.16199.pdf" target="_blank">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention 2023</a></p>
<p><a href="https://arxiv.org/pdf/2304.15010.pdf" target="_blank">LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model 2023</a></p>
<p><a href="https://github.com/OpenGVLab/LLaMA-Adapter" target="_blank">github LLaMA-Adapter: Efficient Fine-tuning of LLaMA</a></p>
<blockquote>
<p>p-tunning系列</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf" target="_blank">Prefix-Tuning: Optimizing Continuous Prompts for Generation 2021</a></p>
<p><a href="https://aclanthology.org/2021.emnlp-main.243.pdf" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning 2021</a></p>
<p><a href="https://arxiv.org/pdf/2103.10385.pdf" target="_blank">P-Tuning - GPT Understands, Too 2021</a></p>
<p><a href="https://arxiv.org/pdf/2110.07602.pdf" target="_blank">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks 2022</a></p>
<blockquote>
<p>lora系列</p>
</blockquote>
<p><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 2021</a></p>
<p><a href="https://arxiv.org/pdf/2303.10512.pdf" target="_blank">AdaLoRA Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning 2023</a></p>
<p><a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank">QLORA: Efficient Finetuning of Quantized LLM 2023</a></p>
<p>另外huggingface很贴心的把常见的fine-Tuning方法都做了集成，只用几行代码就可添加和修改，十分方便，还有微软提供的加速库</p>
<ul>
<li><a href="https://huggingface.co/docs/peft/index" target="_blank">huggingface官网实现的fine-Tuning方法</a></li>
<li><a href="https://github.com/microsoft/DeepSpeed/tree/master" target="_blank">microsoft/DeepSpeed 加速</a></li>
</ul>
<blockquote>
<p><a href="https://www.cnblogs.com/gogoSandy/p/17202169.html" target="_blank">解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning &amp; Prompt-Tuning &amp; P-Tuning</a></p>
</blockquote>
<p>微调LM和全部冻结的prompt模板相比，微调Prompt范式最大的区别就是prompt模板都是连续型(Embedding)，而非和Token对应的离散型模板</p>
<p>核心在于我们并不关心prompt本身是否是自然语言，只关心prompt作为探针能否引导出预训练模型在下游任务上的特定能力</p>
<p>固定LM微调Prompt的范式有以下几个优点</p>
<ul>
<li><strong>性价比高</strong>: 微调参数少，冻结LM只微调prompt部分的参数</li>
<li><strong>无人工参与</strong>: 无需人工设计prompt模板，依赖模型微调即可</li>
<li><strong>多任务共享模型</strong>: 因为LM被冻结，只需训练针对不同任务的prompt即可。因此可以固定预训练模型，拔插式加入Prompt用于不同下游任务</li>
</ul>
<h1 id="adapter-tuning">2 Adapter Tuning</h1>
<blockquote>
<p><a href="https://cloud.tencent.com/developer/article/2186743" target="_blank">预训练模型微调 | 一文带你了解Adapter Tuning</a></p>
</blockquote>
<p><strong>Adapter Tuning</strong></p>
<p>随着计算机硬件性能的提高，预训练模型参数量越来越多，在训练下游任务时进行全模型微调变得昂贵且耗时，Adapter 的出现缓解了这个问题。Adapter在预训练模型每层中插入用于下游任务的参数，在微调时将模型主体冻结，仅训练特定于任务的参数，减少训练时算力开销</p>
<p><strong>Adapter模块设计方法</strong></p>
<p>2019年，Houlsby N等人将Adapter引入NLP领域，作为全模型微调的一种替代方案。Adapter主体架构下图所示</p>
<p><a data-lightbox="0b130d9f-0e1b-4769-a130-6837f5570a8a" data-title="img" href="https://ask.qcloudimg.com/http-save/yehe-5990800/f112c68b91090f20df55116a20f64903.png" target="_blank"><img alt="img" src="https://ask.qcloudimg.com/http-save/yehe-5990800/f112c68b91090f20df55116a20f64903.png"/></a></p>
<h1 id="prefixprompt-tuning">3 Prefix/Prompt-Tuning</h1>
<blockquote>
<p><a href="https://blog.csdn.net/cxx654/article/details/131620130" target="_blank">hugging face参数高效微调peft源码解析</a></p>
</blockquote>
<p><code>P系列</code>关系：</p>
<ul>
<li><strong>Prefix-Tuning</strong>(软提示/连续提示)</li>
<li><strong>Prompt-Tuning</strong>(软提示/连续提示)(可看做是<strong>Prefix-Tuning</strong>的简化版本)</li>
<li><strong>P-Tuning</strong>(软提示/连续提示)</li>
<li><strong>P-Tuning V2</strong>(软提示/连续提示)(可看做是<strong>Prefix-Tuning</strong>的优化版本)</li>
</ul>
<p>Prefix Tuning和PTuning V2在实现上基本上是一样的，其实就是一样的</p>
<p>下面是peft作者回复的关于Prefix Tuning和PTuning V2在实现上的关系(<a href="https://github.com/huggingface/peft/issues/630" target="_blank">How to switch to P-Tuning v2</a>)</p>
<pre><code class="lang-cmd">Hello, those are implemented together. P-Tuning v2 introduced optional parameterization of prompt tokens which you can specify via prefix_projection of PrefixTuningConfig. The other contribution was the ability of work without verbalizers using the linear classification head <span class="hljs-keyword">for</span> NLU tasks whereas Prefix-Tuning paper which focused on NLG didn't focus on this.

So, they are supported via the same PrefixEncoder PEFT method
</code></pre>
<p>另外在<a href="https://github.com/huggingface/peft/blob/main/src/peft/peft_model.py" target="_blank">peft/peft_model.p的代码</a>中有这样一段(大概1106行)</p>
<pre><code class="lang-python"><span class="hljs-keyword">if</span> peft_config.peft_type == PeftType.PREFIX_TUNING:
    <span class="hljs-comment"># PREFIX_TUNING、P_TUNING_V2</span>
    past_key_values = self.get_prompt(batch_size)
    <span class="hljs-keyword">return</span> self.base_model(
        input_ids=input_ids, inputs_embeds=inputs_embeds, past_key_values=past_key_values, **kwargs
    )
<span class="hljs-keyword">else</span>:
    <span class="hljs-comment"># PROMPT_TUNING、P_TUNING</span>
    <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
        inputs_embeds = self.word_embeddings(input_ids)
    <span class="hljs-comment"># concat prompt labels</span>
    <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
        prefix_labels = torch.full((batch_size, peft_config.num_virtual_tokens), <span class="hljs-number">-100</span>).to(labels.device)
        kwargs[<span class="hljs-string">"labels"</span>] = torch.cat((prefix_labels, labels), dim=<span class="hljs-number">1</span>)
    prompts = self.get_prompt(batch_size=batch_size, task_ids=task_ids)
    prompts = prompts.to(inputs_embeds.dtype)
    inputs_embeds = torch.cat((prompts, inputs_embeds), dim=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> self.base_model(inputs_embeds=inputs_embeds, **kwargs)
</code></pre>
<p>可以看出<code>PREFIX_TUNING</code>生效是通过<code>past_key_values</code>传播的，下面是通过拼接到<code>inputs_embeds</code>上实现的</p>
<h2 id="prefix-tuning">3.1 Prefix-Tuning</h2>
<p>等待...</p>
<p>Prefix-Tuning可以理解是CTRL[1]模型的连续化升级版，为了生成不同领域和话题的文本，CTRL是在预训练阶段在输入文本前加入了control code，例如好评前面加'Reviews Rating:5.0',差评前面加'Reviews Rating:1.0', 政治评论前面加‘Politics Title:’，把语言模型的生成概率，优化成了基于文本主题的条件概率</p>
<p>Prefix-Tuning进一步把control code优化成了虚拟Token，每个NLP任务对应多个虚拟Token的Embedding（prefix），对于Decoder-Only的GPT，prefix只加在句首，对于Encoder-Decoder的BART，不同的prefix同时加在编码器和解码器的开头。在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新。不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新</p>
<h2 id="prompt-tuning">3.2 Prompt-Tuning</h2>
<blockquote>
<p><a href="https://github.com/google-research/prompt-tuning" target="_blank">https://github.com/google-research/prompt-tuning</a></p>
</blockquote>
<p>等待...</p>
<p>Prompt-Tunning是以上prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果</p>
<p>对比Prefix-Tunning，prompt-tuning的主要差异如下:</p>
<p>论文使用100个prefix token作为默认参数，大于以上prefix-tuning默认的10个token，不过差异在于prompt-Tunning只对输入层(Embedding)进行微调，而Prefix是对虚拟Token对应的上游layer全部进行微调。因此Prompt-Tunning的微调参数量级要更小，且不需要修改原始模型结构，这是“简化”的来源。相同的prefix长度，Prompt-Tunning(&lt;0.01%)微调的参数量级要比Prefix-Tunning(0.1%~1%)小10倍以上</p>
<h2 id="p-tuning">3.3 P-Tuning</h2>
<h3 id="p-tuning-v1">3.3.1 P-Tuning V1</h3>
<blockquote>
<p><a href="https://github.com/THUDM/P-tuning" target="_blank">github THUDM/P-tuning</a></p>
</blockquote>
<p>手动尝试最优的提示无异于大海捞针，于是便有了自动离散提示搜索的方法(左图)，但提示是离散的，神经网络是连续的，所以寻找的最优提示可能是次优的。p-tuning依然是固定LLM参数，利用多层感知机和LSTM对prompt进行编码，编码之后与其他向量进行拼接之后正常输入LLM。注意，训练之后只保留prompt编码之后的向量即可，无需保留编码器</p>
<p><a data-lightbox="50e6c2bc-dbe2-492c-8ed6-7d21de1196dc" data-title="p_tunning架构v1 vs 离散型promote" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/p_tunning架构v1 vs 离散型promote.webp" target="_blank"><img alt="p_tunning架构v1 vs 离散型promote" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/p_tunning架构v1 vs 离散型promote.webp"/></a></p>
<blockquote>
<p>动机</p>
</blockquote>
<ul>
<li>一个刻板印象是GPT不适合理解类任务，这篇就是去思考这种刻板印象是否正确</li>
<li>GPT-3采用人工构造的模版来做in context learning，人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置啥的都会造成比较大的变化（这里作者做了一个简单的验证实验，具体看论文）。近来的自动化搜索模版工作成本也比较高，同时以前这种离散化的token的搜索出来的结果可能并不是最优的</li>
</ul>
<p>和prefix-tuning差不多，反正是基于这两点去设计了一种连续可微的模版</p>
<p>相比prefix-tuning，这里加了可微的virtual token，但是仅限于输入，没有在每层加；另外virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token</p>
<h3 id="p-tuning-v2">3.3.2 P-Tuning V2</h3>
<blockquote>
<p><a href="https://github.com/THUDM/P-tuning-v2" target="_blank">github THUDM/P-tuning-v2</a></p>
<p><a href="https://blog.csdn.net/as949179700/article/details/130900814" target="_blank">P-tuning V2论文和代码实现详解</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/652635676" target="_blank">chatGLM的浅薄解析 P-tuning V2</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/635848732?utm_id=0" target="_blank">大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2</a></p>
</blockquote>
<p>可以简单的将P-Tuning认为是针对Prompt Tuning的改进，P-Tuning v2认为是针对Prefix Tuning的改进</p>
<h4 id="概述_1"><a class="anchor-navigation-ex-anchor" href="#概述_1" name="概述_1"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#概述" name="概述"><i aria-hidden="true" class="fa fa-link"></i></a>概述</h4>
<p><a data-lightbox="135e07d8-f82b-4e91-8a89-3634c9552784" data-title="p_tunning架构v1 vs v2" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/p_tunning架构v1 vs v2.webp" target="_blank"><img alt="p_tunning架构v1 vs v2" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/p_tunning架构v1 vs v2.webp"/></a></p>
<h4 id="代码示例"><a class="anchor-navigation-ex-anchor" href="#代码示例" name="代码示例"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#代码示例" name="代码示例"><i aria-hidden="true" class="fa fa-link"></i></a>代码示例</h4>
<blockquote>
<p>PrefixEncoder类，为了获得连续prompt，设计的模块</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PrefixEncoder</span><span class="hljs-params">(torch.nn.Module)</span>:</span>
    <span class="hljs-string">r'''
    The torch.nn model to encode the prefix

    Input shape: (batch-size, prefix-length)

    Output shape: (batch-size, prefix-length, 2*layers*hidden)
    '''</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__()
        self.prefix_projection = config.prefix_projection
        <span class="hljs-keyword">if</span> self.prefix_projection:
            <span class="hljs-comment"># Use a two-layer MLP to encode the prefix</span>
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)

            <span class="hljs-comment"># 初始化重参数化的编码器</span>
            self.trans = torch.nn.Sequential(
                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),
                torch.nn.Tanh(),
                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * <span class="hljs-number">2</span> * config.hidden_size)
            )
        <span class="hljs-keyword">else</span>:
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * <span class="hljs-number">2</span> * config.hidden_size)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, prefix: torch.Tensor)</span>:</span>
        <span class="hljs-keyword">if</span> self.prefix_projection:
            prefix_tokens = self.embedding(prefix)
            past_key_values = self.trans(prefix_tokens)
        <span class="hljs-keyword">else</span>:
            past_key_values = self.embedding(prefix)
        <span class="hljs-keyword">return</span> past_key_values
</code></pre>
<p><strong>源码也可以看到 Prefix Tuning 与 P-Tuning v2 最主要的差别就是是否进行重新参数化编码</strong></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BertPrefixForTokenClassification</span><span class="hljs-params">(BertPreTrainedModel)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__(config)
        self.num_labels = config.num_labels
        self.bert = BertModel(config, add_pooling_layer=<span class="hljs-keyword">False</span>)
        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)
        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)

        from_pretrained = <span class="hljs-keyword">False</span>
        <span class="hljs-keyword">if</span> from_pretrained:
            self.classifier.load_state_dict(torch.load(<span class="hljs-string">'model/checkpoint.pkl'</span>))

        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.bert.parameters():
            param.requires_grad = <span class="hljs-keyword">False</span>

        self.pre_seq_len = config.pre_seq_len
        self.n_layer = config.num_hidden_layers
        self.n_head = config.num_attention_heads
        self.n_embd = config.hidden_size // config.num_attention_heads

        self.prefix_tokens = torch.arange(self.pre_seq_len).long()
        self.prefix_encoder = PrefixEncoder(config)

        bert_param = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.bert.named_parameters():
            bert_param += param.numel()
        all_param = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> self.named_parameters():
            all_param += param.numel()
        total_param = all_param - bert_param
        print(<span class="hljs-string">'total param is {}'</span>.format(total_param))  <span class="hljs-comment"># 9860105</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_prompt</span><span class="hljs-params">(self, batch_size)</span>:</span>
        prefix_tokens = self.prefix_tokens.unsqueeze(<span class="hljs-number">0</span>).expand(batch_size, <span class="hljs-number">-1</span>).to(self.bert.device)
        <span class="hljs-comment"># 得到连续Prompt</span>
        past_key_values = self.prefix_encoder(prefix_tokens)
        <span class="hljs-comment"># bsz, seqlen, _ = past_key_values.shape</span>
        <span class="hljs-comment"># 改变形状</span>
        past_key_values = past_key_values.view(
            batch_size,
            self.pre_seq_len,
            self.n_layer * <span class="hljs-number">2</span>,
            self.n_head,
            self.n_embd
        )
        past_key_values = self.dropout(past_key_values)
        <span class="hljs-comment"># 改变形状，划分成数组。每一个数组元素形状为：(2,batch_size,n_head,seq_len,head_dim)</span>
        past_key_values = past_key_values.permute([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>]).split(<span class="hljs-number">2</span>)
        <span class="hljs-keyword">return</span> past_key_values

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            labels=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
    )</span>:</span>
        return_dict = return_dict <span class="hljs-keyword">if</span> return_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">else</span> self.config.use_return_dict

        batch_size = input_ids.shape[<span class="hljs-number">0</span>]
        past_key_values = self.get_prompt(batch_size=batch_size)
        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)
        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=<span class="hljs-number">1</span>)

        <span class="hljs-comment"># 开始传递past_key_values</span>
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            past_key_values=past_key_values,
        )

        ...

        <span class="hljs-keyword">return</span> TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
</code></pre>
<p>一次前向计算中，P-tuning v2会通过<code>self.get_prompt(batch_size=batch_size)</code>得到要连续Prompt</p>
<p>BertEncoder会执行for循环，把past_key_values拆分到一个个BertLayer</p>
<pre><code>self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])

...

for i, layer_module in enumerate(self.layer):
    if output_hidden_states:
        all_hidden_states = all_hidden_states + (hidden_states,)

    layer_head_mask = head_mask[i] if head_mask is not None else None
    past_key_value = past_key_values[i] if past_key_values is not None else None
    ...
    # BertLayer
    layer_module(..., past_key_value, ...)
</code></pre><p>巧妙的利用past_key_values参数，将past_key_values数组中每一个元素，拼接到BertSelfAttention中Key和Value</p>
<p>代码跟踪链路BertModel -&gt; BertEncoder -&gt; BertLayer -&gt; BertAttention -&gt; BertSelfAttention</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BertSelfAttention</span><span class="hljs-params">(nn.Module)</span>:</span>
    ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transpose_for_scores</span><span class="hljs-params">(self, x: torch.Tensor)</span> -&gt; torch.Tensor:</span>
        <span class="hljs-comment"># 将张量转换形状，调换维度。这个代码会在seq_length维度进行拼接，其他维度不可动</span>
        new_x_shape = x.size()[:<span class="hljs-number">-1</span>] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.FloatTensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        output_attentions: Optional[bool] = False,
    )</span> -&gt; Tuple[torch.Tensor]:</span>
        mixed_query_layer = self.query(hidden_states)

        <span class="hljs-comment"># If this is instantiated as a cross-attention module, the keys</span>
        <span class="hljs-comment"># and values come from an encoder; the attention mask needs to be</span>
        <span class="hljs-comment"># such that the encoder's padding tokens are not attended to.</span>
        is_cross_attention = encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>

        <span class="hljs-keyword">if</span> is_cross_attention <span class="hljs-keyword">and</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            <span class="hljs-comment"># reuse k,v, cross_attentions</span>
            key_layer = past_key_value[<span class="hljs-number">0</span>]
            value_layer = past_key_value[<span class="hljs-number">1</span>]
            attention_mask = encoder_attention_mask
        <span class="hljs-keyword">elif</span> is_cross_attention:
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask
        <span class="hljs-keyword">elif</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[<span class="hljs-number">0</span>], key_layer], dim=<span class="hljs-number">2</span>)
            value_layer = torch.cat([past_key_value[<span class="hljs-number">1</span>], value_layer], dim=<span class="hljs-number">2</span>)
        <span class="hljs-keyword">else</span>:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))

        query_layer = self.transpose_for_scores(mixed_query_layer)

        ...
</code></pre>
<p>这里就会把past_key_value拼接到了原始的k、v上面，这样子就相当于给k、v添加了额外需要学习的参数了，再微调时只更新这部分新的参数即可</p>
<blockquote>
<p>P-tuning V2连续Prompt代码实现仿真代码</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment">#!/usr/bin/env Python</span>
<span class="hljs-comment"># -- coding: utf-8 --</span>

<span class="hljs-string">"""
@version: v1.0
@author: huangyc
@file: p_tuning_test.py
@Description: 
@time: 2023/6/6 15:31
"""</span>
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transpose_for_scores</span><span class="hljs-params">(x: torch.Tensor)</span> -&gt; torch.Tensor:</span>
        new_x_shape = x.size()[:<span class="hljs-number">-1</span>] + (<span class="hljs-number">12</span>, <span class="hljs-number">64</span>)
        x = x.view(new_x_shape)
        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)

    prompt = torch.rand(<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">48</span>, <span class="hljs-number">12</span>, <span class="hljs-number">64</span>)  <span class="hljs-comment"># batch_size, seq_len, num_layer*2, num_head, head_size</span>
    prompt = prompt.permute([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>])
    print(f<span class="hljs-string">"P-tuningV2构造的trainable continuous embeddings形状：{prompt.shape}"</span>)
    past_key_values = prompt.split(<span class="hljs-number">2</span>)
    num_layers = <span class="hljs-number">24</span>
    hidden_dim = <span class="hljs-number">768</span>
    n_head = <span class="hljs-number">12</span>
    head_dim = hidden_dim // n_head
    all_head_size = n_head * head_dim
    hidden_states = torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>)  <span class="hljs-comment"># batch_size, seq_len, hidden_size</span>
    print(f<span class="hljs-string">"输入的向量形状：{hidden_states.shape}"</span>)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_layers):
        past_key_value = past_key_values[i]
        print(f<span class="hljs-string">"每一层BertLayer需要加入的prompt形状: {past_key_value.shape}"</span>)
        self_attn_past_key_value = past_key_value[:<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">else</span> <span class="hljs-keyword">None</span>
        <span class="hljs-comment"># BertSelfAttention</span>
        query = nn.Linear(hidden_dim, all_head_size)
        key = nn.Linear(hidden_dim, all_head_size)
        value = nn.Linear(hidden_dim, all_head_size)

        <span class="hljs-comment"># 原始kv的大小</span>
        key_layer = transpose_for_scores(key(hidden_states))
        old_key_layer_shape = key_layer.shape
        print(f<span class="hljs-string">"经过transpose_for_scores后的key形状：{old_key_layer_shape}"</span>)
        value_layer = transpose_for_scores(value(hidden_states))
        old_value_layer_shape = value_layer.shape
        print(f<span class="hljs-string">"经过transpose_for_scores后的value形状：{old_value_layer_shape}\n"</span>)

        <span class="hljs-comment"># 拼接后kv的大小</span>
        key_layer = torch.cat([past_key_value[<span class="hljs-number">0</span>], key_layer], dim=<span class="hljs-number">2</span>)
        print(
            f<span class="hljs-string">"past_key_value[0]的形状：{past_key_value[0].shape} 原始key_layer的形状：{old_key_layer_shape} 经过cat后的key_layer形状：{key_layer.shape}"</span>)
        value_layer = torch.cat([past_key_value[<span class="hljs-number">1</span>], value_layer], dim=<span class="hljs-number">2</span>)
        print(
            f<span class="hljs-string">"past_key_value[1]的形状：{past_key_value[1].shape} 原始value_layer的形状：{old_value_layer_shape} 经过cat后的value_layer形状：{value_layer.shape}\n"</span>)

        mixed_query_layer = query(hidden_states)
        print(f<span class="hljs-string">"hidden_states经过query层后输出的形状：{mixed_query_layer.size()}"</span>)  <span class="hljs-comment"># batch seq len embed</span>
        query_layer = transpose_for_scores(mixed_query_layer)
        print(f<span class="hljs-string">"经过transpose_for_scores后的query形状{query_layer.size()}"</span>)  <span class="hljs-comment"># batch</span>

        print(<span class="hljs-string">"注意力分数开始计算"</span>)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="hljs-number">-1</span>, <span class="hljs-number">-2</span>))
        print(f<span class="hljs-string">"attention_scores的形状：{attention_scores.size()}"</span>)  <span class="hljs-comment"># batch head seq_len seq_len</span>
        print(<span class="hljs-string">"开始注意力汇聚计算"</span>)
        context_layer = torch.matmul(attention_scores, value_layer)
        print(f<span class="hljs-string">"注意力汇聚后输出矩阵context_layer的形状：{context_layer.size()}"</span>)  <span class="hljs-comment"># batch head seq_len embed/12</span>
        print(<span class="hljs-string">"最后，将context_layer的形状恢复成输入hidden_states的形状"</span>)
        context_layer = context_layer.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()
        new_context_layer_shape = context_layer.size()[:<span class="hljs-number">-2</span>] + (<span class="hljs-number">768</span>,)
        context_layer = context_layer.view(new_context_layer_shape)
        print(f<span class="hljs-string">"context_layer的形状恢复完成，其形状为：{context_layer.size()}"</span>)
        print(<span class="hljs-string">"一次P-tuningV2的BertLayer计算仿真结束"</span>)
        <span class="hljs-keyword">break</span>


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    run()
</code></pre>
<p>测试输出</p>
<pre><code class="lang-python">S:\Anaconda3\envs\torch38\python.exe Q:\pyCharmWS\chatgpts\P-tuning-v2\tests\p_tuning_test.py 
P-tuningV2构造的trainable continuous embeddings形状：torch.Size([<span class="hljs-number">48</span>, <span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
输入的向量形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>])
每一层BertLayer需要加入的prompt形状: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
经过transpose_for_scores后的key形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
经过transpose_for_scores后的value形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])

====================&gt; 核心
past_key_value[<span class="hljs-number">0</span>]的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) 原始key_layer的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) 经过cat后的key_layer形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">256</span>, <span class="hljs-number">64</span>])
past_key_value[<span class="hljs-number">1</span>]的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) 原始value_layer的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>]) 经过cat后的value_layer形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">256</span>, <span class="hljs-number">64</span>])
====================&gt; 核心

hidden_states经过query层后输出的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>])
经过transpose_for_scores后的query形状torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
注意力分数开始计算
attention_scores的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>])
开始注意力汇聚计算
注意力汇聚后输出矩阵context_layer的形状：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">12</span>, <span class="hljs-number">128</span>, <span class="hljs-number">64</span>])
最后，将context_layer的形状恢复成输入hidden_states的形状
context_layer的形状恢复完成，其形状为：torch.Size([<span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">768</span>])
一次P-tuningV2的BertLayer计算仿真结束
</code></pre>
<h1 id="lora系列">4 LORA系列</h1>
<blockquote>
<p>点赞👍<a href="https://space.bilibili.com/1564408396" target="_blank">B站博主 小杨不努力0v0</a> + <a href="https://bytedance.feishu.cn/docx/doxcn3zm448MK9sK6pHuPsqtH8f" target="_blank">博主相关的文章链接</a></p>
</blockquote>
<h2 id="lora转">4.1 LORA(转)</h2>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV17g4y1g7S6/?spm_id_from=333.788&amp;vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">LoRA：训练你的GPT【论文粗读·1】</a></p>
</blockquote>
<p>一种通过<strong>低秩近似增量矩阵</strong>的，经过广泛验证<strong>足够Robust</strong>的微调方法</p>
<blockquote>
<p>摘要</p>
</blockquote>
<p>随着自然语言处理(NLP)模型规模的不断增长，由于成本和资源限制，对其进行完全微调以用于下游任务的挑战日益增加</p>
<p>介绍<code>低秩适应(Low-Rank Adaptation)</code>。<code>LoRA</code>通过引入参数矩阵来减少参数，并将GPU内存需求降低了3倍。相比于使用GPT-3进行微调，它将参数减少了10,000倍</p>
<p>尽管可训练参数更少，但LoRA在大多数语言模型上表现优于微调，具有更高的训练吞吐量和无推理延迟</p>
<p>对语言模型适应中的秩缺失进行的实证研究为LoRA的有效性提供了证据，LoRA是开源的</p>
<blockquote>
<p>介绍</p>
</blockquote>
<p>对于下游任务而言，完全微调大型语言模型是具有挑战性的。 受到[内在维度]的研究启发，我们提出了LoRA，具有以下优势：</p>
<ul>
<li><strong>低任务切换开销</strong>: 一个预训练模型可以被共享并用于构建多个针对不同任务的小型LoRA模块</li>
<li><strong>参数高效</strong>: LoRA通过使用自适应优化器，在训练过程中使得训练更高效，并将硬件门槛降低了最多3倍</li>
<li><strong>无推理延迟</strong>: LoRA的简单线性设计使得可训练矩阵在部署时可以与冻结权重合并</li>
<li><strong>正交性</strong>: LoRA与许多先前的方法是正交的，并且可以与它们结合使用，比如前缀微调(prefix-tuning)</li>
</ul>
<blockquote>
<p>问题描述</p>
</blockquote>
<p>模型训练时参数量评估，对于LLM，如果模型的参数时<script type="math/tex; ">\Phi</script>的话</p>
<ul>
<li>全量微调</li>
</ul>
<p><script type="math/tex; mode=display">
\max _{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(P_{\Phi}\left(y_{t} \mid x, y_{<t}\right)\right)
</script></p>
<p>使用Adam优化器下的，<script type="math/tex; ">\mathrm{n}=8</script>，并且使用混合精度，一个参数需要16个bytes来存储，这16个bytes分别为</p>
<p>权重<script type="math/tex; ">W</script>需要<script type="math/tex; ">fp16</script>来存储，激活值<script type="math/tex; ">A</script>需要<script type="math/tex; ">fp16</script>，为了更新权重还需存一个复制<script type="math/tex; ">W_c</script>需要<script type="math/tex; ">fp32</script>，优化器需要存两个值，分别是<script type="math/tex; ">M</script>和<script type="math/tex; ">V</script>(方差)，分别需要<script type="math/tex; ">fp32</script></p>
<p>其中<script type="math/tex; ">fp16</script>占2bytes，<script type="math/tex; ">fp32</script>占bytes，总共为<script type="math/tex; ">2+2+4+4+4 = 16</script>bytes</p>
<p>因此，<script type="math/tex; ">\Phi</script>个参数就需要<script type="math/tex; "> \Phi * 2n</script>bytes</p>
<ul>
<li>Lora</li>
</ul>
<p><script type="math/tex; mode=display">
\max _{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(p_{\Phi_{0}+\Delta \Phi(\Theta)}\left(y_{t} \mid x, y_{<t}\right)\right)
</script></p>
<p>Lora在训练时，将原来的参数固定下来，只更新新增加的参数，因此Mem Required: <script type="math/tex; ">(4 \Phi+\theta * 2 n)</script> bytes</p>
<p>在GPT-3的175B参数下，这里的<script type="math/tex; ">\Theta</script>可以达到原来的<script type="math/tex; ">0.01 \%</script></p>
<blockquote>
<p><strong>为什么会提出Lora呢</strong></p>
</blockquote>
<p>原来的Adapter方法也是固定模型的参数，只训练MLP参数，但是这样子有几个弊端</p>
<ol>
<li>增加了网络深度，<strong>增加了推理的时间</strong></li>
<li>添加MLP之后，训练出来的最优方案也只能收敛到MLP层，不一定是全局最好的</li>
<li>直接去优化这个Promote并不能保证优化是单调的，也就是<strong>不是全局最优</strong>，很难优化好</li>
<li>减少了可用于处理下游任务的序列长度，因为新加入的Promote会占用输入的token长度</li>
</ol>
<blockquote>
<p>具体方法</p>
</blockquote>
<p>最核心的思路如下公式所示，研究表明<script type="math/tex; ">\Delta W</script>通常是一个欠秩的矩阵
<script type="math/tex; mode=display">
W \Leftarrow W_{0}+\Delta W \\

W_{0}+\Delta W=W_{0}+B A \\

h=W_{0} x+\Delta W x=W_{0} x+B A x
</script>
因此，<script type="math/tex; ">\Delta W</script>可以进行低秩分解</p>
<p><a data-lightbox="cb842c28-014c-476e-a837-92791ecd1a67" data-title="Lora思路" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora思路.webp" target="_blank"><img alt="Lora思路" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora思路.webp"/></a></p>
<p>训练时，<script type="math/tex; ">B</script>初始化为全零矩阵，这样子参数量就从<script type="math/tex; ">d*d</script>变成了<script type="math/tex; ">d*2*r</script>，这里的<script type="math/tex; ">r</script>一般是远小于<script type="math/tex; ">d</script>的</p>
<blockquote>
<p>优点</p>
</blockquote>
<ul>
<li><p>LoRA是对完全微调的一种推广方法</p>
<ul>
<li>在适应过程中，LoRA不需要对权重矩阵进行完全秩的累积梯度更新，而是可以基于预训练的权重矩阵设置秩</li>
<li>当LoRA应用于所有权重矩阵并且偏置进行训练时，这种方法提供了类似于完全微调的表现能力</li>
</ul>
</li>
<li><p>没有额外的推理延迟</p>
<p>在生产部署中，LoRA可以计算和存储<script type="math/tex; ">W=W_0+BA</script>，其中<script type="math/tex; ">W_0</script>和<script type="math/tex; ">𝐵𝐴</script>属于<script type="math/tex; ">ℝ^{𝑑×𝑘}</script>，当切换到另一个下游任务时，可以通过减去<script type="math/tex; ">𝐵𝐴</script>并加上不同的<script type="math/tex; ">𝐵'𝐴'</script>来恢复<script type="math/tex; ">𝑊_0</script>，这是一个快速操作，几乎没有额外的内存开销(<code>潮汐GPU</code>)</p>
</li>
</ul>
<blockquote>
<p>为什么低秩矩阵有效</p>
</blockquote>
<ol>
<li><p>当给定参数数量时，应该调整预训练Transformer中的哪些具体权重矩阵子集以实现最佳的下游性能？</p>
<ul>
<li><p>在给定参数预算的情况下，确定要调整的权重矩阵子集以实现最佳下游性能是一个复杂的问题，并且没有固定的答案</p>
<p>通常，可以考虑根据下游任务的特点和需求进行权衡和选择</p>
<p>一种常见的方法是通过对不同权重矩阵进行实验性微调，并根据性能评估来确定适合特定任务的权重矩阵子集</p>
</li>
</ul>
</li>
<li><p><strong>最优</strong>的适应矩阵<script type="math/tex; ">\Delta W</script>是否真的是欠秩的吗？如果是，那么在实际情况下推荐的秩是多少？</p>
<ul>
<li><strong>最优</strong>的适应矩阵<script type="math/tex; ">\Delta W</script>是否真的是欠秩的，这取决于具体情况。秩缺失意味着矩阵的秩(矩阵的线性独立列数或行数的最大数量)较低</li>
<li>对于实际目的，建议选择适当的秩以平衡模型性能和计算成本。具体推荐的秩取决于任务的复杂性、数据集的规模以及可用的计算资源等因素</li>
</ul>
</li>
<li><p><script type="math/tex; ">\Delta W</script>和<script type="math/tex; ">W</script>之间的关系是什么？<script type="math/tex; ">\Delta W</script>和<script type="math/tex; ">W</script>之间是否存在高相关性？<script type="math/tex; ">\Delta W</script>的大小与<script type="math/tex; ">W</script>相比如何？</p>
<ul>
<li><script type="math/tex; ">\Delta W</script>表示适应矩阵，用于调整预训练权重矩阵<script type="math/tex; ">W</script>，<script type="math/tex; ">\Delta W</script>和<script type="math/tex; ">W</script>之间的关系取决于具体的适应方法和优化算法。在某些情况下，<script type="math/tex; ">\Delta W</script>可以通过对<script type="math/tex; ">W</script>的微小调整来获得，而在其他情况下，<script type="math/tex; ">\Delta W</script>可能包含更大的变化</li>
<li><script type="math/tex; ">\Delta W</script>和<script type="math/tex; ">W</script>之间的相关性取决于适应方法的设计和优化过程的细节。它们可以存在一定的相关性，但具体情况可能因模型架构、任务要求和数据集特征而异</li>
<li><script type="math/tex; ">\Delta W</script>的大小与<script type="math/tex; ">W</script>的大小之间没有固定的比较关系，因为它们的尺度取决于具体的数值范围和调整方法</li>
</ul>
</li>
</ol>
<blockquote>
<p>对于attention参数附加到哪个上更有效</p>
</blockquote>
<p>实验在GPT-3 175B模型上设置了一个参数预算为18M(如果以FP16存储，大约为35MB)。这对应于当我们适应一种注意力权重时r=8，或者当我们适应两种类型时r=4，适用于所有96层</p>
<p><a data-lightbox="0d1d3b3a-26f3-4f82-a8aa-559b3c87597b" data-title="Lora关于r参数选择的实验" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora关于r参数选择的实验.webp" target="_blank"><img alt="Lora关于r参数选择的实验" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora关于r参数选择的实验.webp"/></a></p>
<p>需要注意的是，将所有参数放在<script type="math/tex; ">\Delta 𝑊_𝑞</script>或<script type="math/tex; ">\Delta 𝑊_k</script>中会导致显著降低性能，而注入到<script type="math/tex; ">𝑊_𝑞</script>和<script type="math/tex; ">𝑊_v</script>则产生最佳结果。这表明，即使秩为4，<script type="math/tex; ">\Delta 𝑊</script>中包含了足够的信息，使得与使用具有较大秩的单一类型的权重相比，使用更多的权重矩阵更可取</p>
<blockquote>
<p>回答：将参数设置到q、v上时，r取多少合适</p>
</blockquote>
<p><a data-lightbox="8f4d72bb-8f66-421a-a9e3-9cd265dbc965" data-title="Lora关于r参数取多少的实验" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora关于r参数取多少的实验.webp" target="_blank"><img alt="Lora关于r参数取多少的实验" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora关于r参数取多少的实验.webp"/></a></p>
<p>LoRA在非常小的秩(特别是对于<script type="math/tex; ">𝑊_𝑞</script>，<script type="math/tex; ">𝑊_𝑣</script>而言)下已经取得了相当的竞争力，这表明更新矩阵<script type="math/tex; ">\Delta W</script>可能具有<strong>非常小的内在秩</strong>，使用低秩矩阵对LLM进行fine tune的时候，可以用一个非常小的低秩矩阵，就可以捕捉到对下游任务的一些特征信息，这为我们提供了一个非常高效的LLM的fine tune的方式，同时提高了下游任务的性能</p>
<blockquote>
<p>回答：<script type="math/tex; ">\Delta W</script>和<script type="math/tex; ">W</script>之间的关系是什么</p>
</blockquote>
<p>首先对<script type="math/tex; ">\Delta W</script>进行奇异值分解，并且把它左奇异值向量和右奇异值向量乘到<script type="math/tex; ">W_q</script>上，把<script type="math/tex; ">W_q</script>映射到<script type="math/tex; ">\Delta W_q</script>的子空间，并计算<script type="math/tex; ">F</script>范数，同时还把<script type="math/tex; ">W_q</script>映射到映射到随机矩阵上</p>
<p>以此来证明<script type="math/tex; ">\Delta W_q</script>与<script type="math/tex; ">W_q</script>有更强的相关性</p>
<p><a data-lightbox="7eb1e60f-a138-4fef-a5b5-509ade63b4a4" data-title="Lora下w和delta w的关系" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora下w和delta w的关系.webp" target="_blank"><img alt="Lora下w和delta w的关系" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/Lora下w和delta w的关系.webp"/></a></p>
<p>首先，与随机矩阵相比，<script type="math/tex; ">\Delta W_q</script>与<script type="math/tex; ">W_q</script>具有<strong>更强的相关性</strong>，表明<script type="math/tex; ">\Delta W</script>放大了预训练模型中的<script type="math/tex; ">W</script>中已经存在的某些特征</p>
<p>其次，<script type="math/tex; ">\Delta W</script>不是重复<script type="math/tex; ">W</script>的奇异值方向，而是只放大在<script type="math/tex; ">W</script>中没有强调的方向</p>
<p>第三，放大因子相当巨大：当r=4时，21.5≈6.91/0.32，表明<script type="math/tex; ">\Delta W</script>只是放大了<script type="math/tex; ">W</script>中的一些特征，且放大倍数是很大的，相当于是把下游任务需要的特征提取出来并进行放大</p>
<h2 id="adalora转">4.2 AdaLoRA(转)</h2>
<blockquote>
<p><a href="https://readpaper.feishu.cn/docx/KWTkdA6EmonokyxdCmocdbXmnwc" target="_blank">AdaLoRA：更强大的LoRA</a></p>
<p><a href="https://github.com/cauyxy/YourGPT" target="_blank">github cauyxy/YourGPT</a></p>
</blockquote>
<p>Lora中的r是一个确定值，但不是对于所有的层，像q、k这样的层，q内在秩比较大，v内在秩比较小，对于不同的矩阵应该使用不同的内在秩</p>
<p>AdaLoRA是在对同样的参数量下，对不同的矩阵使用不同的r，通过奇异值分解，判断r的大小，来取得更好的效果</p>
<blockquote>
<p>提出问题</p>
</blockquote>
<p>在NLP领域，对于下游任务进行大型预训练语言模型的微调已经成为一种重要的做法。一般而言，我们会采用对原有的预训练模型进行全量微调的方法来适配下游任务，但这种方法存在两个问题</p>
<ol>
<li><strong>训练阶段</strong>: 对于预训练模型进行微调的时候，为了更新权重参数，需要大量的显存来存储参数的梯度和优化器信息，在当今预训练模型的参数变得越来越大的情况下，针对下游任务微调门槛变得越来越高</li>
<li><strong>推理阶段</strong>: 由于我们训练的时候是对于模型参数进行全量的更新，所以<strong>多个下游任务需要为每个任务维护一个大型模型的独立副本</strong>，这样就导致我们在实际应用的时候浪费了不必要的存储</li>
</ol>
<p>现有方法: 为了解决这些问题，研究者提出了两个主要研究方向，以减少微调参数的数量，同时保持甚至提高预训练语言模型的性能</p>
<blockquote>
<p>添加小型网络模块</p>
</blockquote>
<p>将小型网络模块添加到PLMs中，保持基础模型保持不变的情况下仅针对每个任务微调这些模块，可以用于所有任务。这样，只需引入和更新少量任务特定的参数，就可以适配下游的任务，大大提高了预训练模型的实用性，方法示例</p>
<p><a data-lightbox="a305bfff-e973-45a4-9b50-8b48db00bfeb" data-title="添加小型网络模块" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/添加小型网络模块.webp" target="_blank"><img alt="添加小型网络模块" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/添加小型网络模块.webp"/></a></p>
<ul>
<li><p><strong>Adapter tuning</strong>：是在基础模型的各层之间插入小型神经模块</p>
</li>
<li><p><strong>Prefix tuning</strong>：将可训练的前缀标记附加到基础模型的输入或隐藏层上</p>
</li>
<li><p><strong>Prompt Tuning</strong>: 修改模型的输入，在模型输入的前面加一些特定的前缀</p>
</li>
</ul>
<p><strong>可行之处</strong>：可以达到与完全微调几乎相当的性能，同时仅更新不到原始模型参数的1％，大大减少了内存消耗。</p>
<p><strong>存在问题</strong>：</p>
<ul>
<li><p><strong>Adapter tuning</strong>：引入了推理延迟，最终收敛到适配器层</p>
</li>
<li><p><strong>Prefix or Prompt tuning</strong>：直接优化Prefix和Prompt是非单调的，比较难收敛，并且消耗了输入的token</p>
</li>
</ul>
<blockquote>
<p>下游任务增量更新</p>
</blockquote>
<p>对预训练权重的增量更新进行建模，而无需修改模型架构
<script type="math/tex; mode=display">
W=W^{(0)}+\Delta
</script>
<strong>方法示例</strong>：</p>
<ul>
<li><p><strong>Diff pruning</strong>：将<script type="math/tex; ">\Delta</script>初始化为与<script type="math/tex; ">W</script>相同的维度，然后根据参数的大小按元素对<script type="math/tex; ">\Delta</script>进行剪枝</p>
</li>
<li><p><strong>LoRA</strong>：通过两个小得多的矩阵的乘积将<script type="math/tex; ">\Delta</script>参数化为低阶矩阵</p>
</li>
</ul>
<p><script type="math/tex; mode=display">
W=W^{(0)}+\Delta=W^{(0)}+B A
</script></p>
<p><strong>可行之处</strong>：可以达到与完全微调几乎相当的性能</p>
<p><strong>存在问题</strong>:</p>
<ul>
<li><strong>Diff pruning</strong>：<ul>
<li>需要底层实现来加速非结构化稀疏矩阵的计算，不能直接使用现有的框架</li>
<li>训练过程中需要存储完整的<script type="math/tex; ">\Delta</script>矩阵，相比于Full finetune并没有降低计算成本</li>
</ul>
</li>
<li><strong>LoRA</strong>：<ul>
<li>预先指定每个增量矩阵的内在秩r相同，忽略了在微调预训练模型时，权重矩阵的重要性在不同模块和层之间存在显著差异</li>
<li>只训练了self-attention，没有训练feed-forward networks，事实上FFN更重要</li>
</ul>
</li>
</ul>
<blockquote>
<p>问题总结</p>
</blockquote>
<p>不能预先指定矩阵的秩，需要动态更新增量矩阵的R</p>
<ul>
<li>权重矩阵的重要性在不同模块和层之间存在显著差异</li>
</ul>
<p>需要找到更加重要的矩阵，分配更多的参数，裁剪不重要的矩阵</p>
<ul>
<li>找到重要的矩阵，提升模型效果</li>
<li>裁剪不重要的矩阵，降低参数计算量，降低模型效果差的风险</li>
</ul>
<blockquote>
<p>解决方案</p>
</blockquote>
<p>目标：在类似LoRA的微调过程中动态分配参数预算给权重矩阵</p>
<ol>
<li><strong>调整增量矩阵的秩</strong>来控制预算分配。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低以防止过拟合并节省计算预算</li>
<li>采用参数化矩阵来模拟SVD，并舍弃不重要的奇异值，同时保留奇异向量。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法可以加速计算，同时保留未来恢复的可能性并稳定训练</li>
<li>在训练损失中添加了额外的惩罚项，以规范奇异矩阵P和Q的正交性，从而避免SVD的大量计算并稳定训练</li>
</ol>
<blockquote>
<p>SVD-BASED ADAPTATION</p>
</blockquote>
<p>如上所述，我们把增量矩阵<script type="math/tex; ">\Delta</script>做一个奇异值分解的近似，即<script type="math/tex; ">\Delta=P \Lambda Q</script>，对矩阵更新的描述则有如下表示
<script type="math/tex; mode=display">
W=W^{(0)}+\Delta=W^{(0)}+P \Lambda Q
</script>
为了保证<script type="math/tex; ">\mathrm{P}</script>和<script type="math/tex; ">\mathrm{Q}</script>的正交性，即<script type="math/tex; ">P^{\top} P=Q Q^{\top}=I</script>，我们提出如下所示的正则损失
<script type="math/tex; mode=display">
R(P, Q)=\left\|P^{\top} P-I\right\|_{\mathrm{F}}^{2}+\left\|Q Q^{\top}-I\right\|_{\mathrm{F}}^{2}
</script>
<strong>为什么不直接在原来的BA上进行修剪？</strong></p>
<ol>
<li><p>当一对奇异向量被认为为不重要时，我们必须修剪它的所有元素。这就导致几乎不可能重新激活修剪过的奇异向量，因为它们的元素都被清零并且不再训练</p>
<p>与之对比，AdaLoRA只是Mask了奇异值</p>
</li>
<li><p>LoRA的A和B不是正交的，这意味着奇异向量可以相互依赖。 与截断最小的奇异值相比，丢弃奇异向量可能会导致原始矩阵发生更大的变化。</p>
<p>因此，在分配完秩的每一步之后，增量矩阵通常会发生更多不可预测的显著变化，这导致训练不稳定，甚至损害模型的效果</p>
</li>
</ol>
<blockquote>
<p>IMPORTANCE-AWARE RANK ALLOCATION</p>
</blockquote>
<p>我们将基于SVD的秩调整应用于每个权重矩阵，包括每个transformer层的<script type="math/tex; ">W_{q}, W_{k}, W_{v}, W_{f 1}</script>和<script type="math/tex; ">W_{f 2}</script>。为了控制参数预算，我们在训练期间根据重要性得分迭代修剪奇异值</p>
<p>为了更好地表示，我们用<script type="math/tex; ">k</script>来索引增量矩阵<script type="math/tex; ">\Delta_{k}=P_{k} \Lambda_{k} Q_{k}</script> for <script type="math/tex; ">k=1, \ldots, n</script>，用<script type="math/tex; ">\mathcal{G}_{k, i}=\left\{P_{k, * i}, \lambda_{k, i}, Q_{k, i *}\right\}</script>来表示第<script type="math/tex; ">k</script>个矩阵的奇异值，奇异向量三元组，<script type="math/tex; ">\mathcal{S}_{k, i}</script> 来表示这个三元组的重要性</p>
<p>而<script type="math/tex; ">\mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q})</script>作为参数训练的代价，同时加上正则化项，就得出了如下的目标函数: (<script type="math/tex; ">\gamma</script>是正则化系数)
<script type="math/tex; mode=display">
\mathcal{L}(\mathcal{P}, \mathcal{E}, \mathcal{Q})=\mathcal{C}(\mathcal{P}, \mathcal{E}, \mathcal{Q})+\gamma \sum_{k=1}^{n} R\left(P_{k}, Q_{k}\right)
</script>
我们在训练的时候就可以通过梯度下降的方式对<script type="math/tex; ">P, \Lambda, Q</script>进行更新，下面是<script type="math/tex; ">\Lambda</script>的例子
<script type="math/tex; mode=display">
\tilde{\Lambda}_{k}^{(t)}=\Lambda_{k}^{(t)}-\eta \nabla_{\Lambda_{k}} \mathcal{L}\left(\mathcal{P}^{(t)}, \mathcal{E}^{(t)}, \mathcal{Q}^{(t)}\right)
</script>
然后我们再基于<script type="math/tex; ">\mathcal{S}_{k, i}</script>对<script type="math/tex; ">\Lambda</script>进行裁剪
<script type="math/tex; mode=display">
\Lambda_{k}^{(t+1)}=\mathcal{T}\left(\tilde{\Lambda}_{k}^{(t)}, S_{k}^{(t)}\right), \text { with } \mathcal{T}\left(\tilde{\Lambda}_{k}^{(t)}, S_{k}^{(t)}\right)_{i i}=\left\{\begin{array}{ll}
\tilde{\Lambda}_{k, i i}^{(t)} & S_{k, i}^{(t)} \text { is in the top- } b^{(t)} \text { of } S^{(t)} \\
0 & \text { otherwise }
\end{array}\right.
</script>
其中<script type="math/tex; ">S^{(t)}=\left\{S_{k, i}^{(t)}\right\}_{1 \leq k \leq n, 1 \leq i \leq r}</script>包含所有三元组的重要性分数。<script type="math/tex; ">b^{(t)}</script>是第<script type="math/tex; ">\mathrm{t}</script>步剩余奇异值的预算</p>
<p>通过这种方式，我们通过修剪不太重要的奇异值，将更多预算留给优先级较高的增量矩阵</p>
<blockquote>
<p>Magnitude of singular values</p>
</blockquote>
<p>这样的话只有最小的奇异值以及最不重要的奇异向量被去弃。它最大限度地减小了与原始矩阵的偏差，进一步稳定了训练。但是这个度量不能正确量化参数(三元组)对模型性能的贡献
<script type="math/tex; mode=display">
S_{k, i}=\left|\lambda_{k, i}\right|
</script></p>
<blockquote>
<p>Sensitivity-based importance</p>
</blockquote>
<p>之前的工作利用灵敏度来量化单个参数的重要性，并据此对参数进行非结构化修剪。在我们的例子上，我们必须设计一个新的度量标准，因为三元组要被按组丢弃了，所以每一项的敏感性都应该被考虑，并适当地组合起来，以量化三元组对模型性能的整体贡献</p>
<p>我们设计了如下所示的函数来计算importance score
<script type="math/tex; mode=display">
S_{k, i}=s\left(\lambda_{k, i}\right)+\frac{1}{d_{1}} \sum_{j=1}^{d_{1}} s\left(P_{k, j i}\right)+\frac{1}{d_{2}} \sum_{j=1}^{d_{2}} s\left(Q_{k, i j}\right)
</script>
我们可以采用<script type="math/tex; ">s(\cdot)</script>的灵敏度，<script type="math/tex; ">s(\cdot)</script>定义为梯度权重乘积的大小:
<script type="math/tex; mode=display">
I\left(w_{i j}\right)=\left|w_{i j} \nabla_{w_{i j}} \mathcal{L}\right|
</script>
本质上近似于参数归零时的损失变化。如果去除一个参数影响较大，则模型对该参数敏感，我们应该保留它</p>
<p>但之前的工作指出，直接计算的敏感性还不是一个可靠的重要指标。这样的分数是在抽样的minibatch上估计的。随机采样和复杂的训练动态导致灵敏度估计的变异性大，不确定性大，这样可能会导致对于参数的重要性的错误估计。提出通过灵敏度平滑和不确定性量化，加入累计灵敏度的影响来解决这一问题:
<script type="math/tex; mode=display">
\begin{array}{l}
\bar{I}^{(t)}\left(w_{i j}\right)=\beta_{1} \bar{I}^{(t-1)}\left(w_{i j}\right)+\left(1-\beta_{1}\right) I^{(t)}\left(w_{i j}\right) \\
\bar{U}^{(t)}\left(w_{i j}\right)=\beta_{2} \bar{U}^{(t-1)}\left(w_{i j}\right)+\left(1-\beta_{2}\right)\left|I^{(t)}\left(w_{i j}\right)-\bar{I}^{(t)}\left(w_{i j}\right)\right|
\end{array}
</script>
接下来，我们把<script type="math/tex; ">s(\cdot)</script>定义为<script type="math/tex; ">\bar{I}^{(t)}</script>和<script type="math/tex; ">\bar{U}^{(t)}</script>的乘积
<script type="math/tex; mode=display">
s^{(t)}\left(w_{i j}\right)=\bar{I}^{(t)}\left(w_{i j}\right) \cdot \bar{U}^{(t)}\left(w_{i j}\right)
</script>
这样，我们就得到了一个既考虑了三元组所有元素，又考虑了累计灵敏度足够平滑的一个重要性函数</p>
<blockquote>
<p>GLOBAL BUDGET SCHEDULER</p>
</blockquote>
<p>在低秩自适应的情况下，调整秩自然是为了控制参数预算。因此，我们将预算<script type="math/tex; ">b^{(t)}</script>定义为所有增量矩阵的总秩，即总奇异值的数量</p>
<p>回想一下，预算分配是在微调期间迭代执行的。为了便于训练，我们提出了一个全局预算调度器。具体来说，我们从略高于目标预算<script type="math/tex; ">b^{(T)}</script>的初始预算算<script type="math/tex; ">b^{(0)}</script>开始(例如，<script type="math/tex; ">b^{(T)}</script>的1.5倍)</p>
<p>我们将每个增量矩阵的初始秩设为<script type="math/tex; ">r=\frac{b^{(0)}}{n}</script>。我们对<script type="math/tex; ">t_{\text {init }}</script>步进行warmup，然后按照三次计划减少预算<script type="math/tex; ">b^{(t)}</script>，直到达到<script type="math/tex; ">b^{(t)}</script></p>
<p>最后，我们得到的修正完预算分布，并对<script type="math/tex; ">t_{\text {final }}</script>步骤的模型进行了微调</p>
<p>这使得AdaLoRA可以先探索参数空间，然后再关注最重要的权重</p>
<blockquote>
<p>实验验证</p>
</blockquote>
<h2 id="qlora">4.3 QLORA</h2>
<p>FineTune -&gt; P_tuning -&gt; P_tuning V2 -&gt; LoRA -&gt; QLoRA</p>
<p>BERT Adapter</p>
<h1 id="rlhf">5 RLHF</h1>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV1Yc411g78a/?spm_id_from=333.1007.0.0&amp;vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">入门】大语言模型常用微调框架介绍｜LoRA&amp;Prefix-Tuning&amp;Prompt-Tuning&amp;P-Tuning v2&amp;RLHF微调原理简介</a></p>
</blockquote>
<p>RLHF: Reinforcement Learning from Human。Feedback，即基于人工反馈机制的强化学习。最早与2022年4月，由OpenAI研究团队系统总结并提出.并在GPT模型的对话类任务微调中大放异彩，被称为ChatGPT<strong>背后的功臣</strong></p>
<p>RLHF也是目前为止常用的、最为复杂的基于强化学习的大语言模型微调方法，目前最好的端到端RLHF实现是DeepSpeedChat库，由微软开源并维护</p>
<p>基于强化学习的进阶微调方法RLHF方法</p>
<p>论文地址: <a href="https://arxiv.org/abs/2203.02155" target="_blank">https://arxiv.org/abs/2203.02155</a></p>
<p>步骤1: 监督微调 (SFT)-一 使用精选的人类回答来微调预训练的语言模型以应对各种查询</p>
<p>步骤2:奖励模型微调 -- 使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立的 (通常比 SFT 小的) 奖励模型 (RW) </p>
<p>步骤3: RLHF 训练 --利用 Proximal Policy Optimization (PPO) 算法根据 RW 模型的奖励D九天Hector反馈进一步微调 SFT 模型。</p>
<h1 id="flashatten转">6 Flash_Atten(转)</h1>
<blockquote>
<p><a href="https://readpaper.feishu.cn/docx/UwT2dQsiko6u0RxoiXRcBtwfnAf" target="_blank">前置知识 GPU Arch:自顶向下分析</a> + <a href="https://www.bilibili.com/video/BV1Az4y1B7Da/?vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">B站 GPU Arch：自顶向下分析【浅谈底层·1】</a></p>
</blockquote>
<p>随着人工智能特别是以GPT为代表的生成式AI的迅猛发展，GPU已经成为了一种不可或缺的工具，甚至企业都以拥有多少高端GPU作为抓住风口能力的衡量标准。相比之下，CPU虽然在传统计算领域占据主导地位，但在处理AI任务时却不及GPU出色</p>
<p>为什么AI计算通常选择GPU而不是CPU，分析GPU在AI计算中的优势，同时，从底层原理探讨从Volta到最新的Hopper四代NVIDIA GPU架构的演进，展示其不断提升的性能和功能</p>
<p>GPU主要由计算单元ALU组成。CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下，计算能力只是CPU很小的一部分</p>
<p><a data-lightbox="243dcec3-f40a-4444-aeac-20b112a531c4" data-title="GPU和CPU比较" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/GPU和CPU比较.webp" target="_blank"><img alt="GPU和CPU比较" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/GPU和CPU比较.webp"/></a></p>
<p>通过上面自顶向下的分析，我们知道，对于GPU中的存储部分访问速度由快到慢，计算部分从大到小排列为
<script type="math/tex; mode=display">
\begin{array}{c}
\text{Mem Speed:(L1 Cache/SMEM)>L2 Cache>HBM}
\\
\text{Compute Unit:GPC>TPC>SM>(TensorCore, SFU, INT32, FP32..)}
\end{array}
</script>
<a data-lightbox="e4760adc-9ef3-4c1d-b78d-c0424e09c64b" data-title="GPU架构发展参数" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/GPU架构发展参数.webp" target="_blank"><img alt="GPU架构发展参数" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/GPU架构发展参数.webp"/></a></p>
<p>NVLink是什么？为什么需要他？</p>
<p>大模型通常具有巨大的参数数量和复杂的结构，需要处理大量的数据。分布式训练将这些大型模型分割成多个部分，由多个GPU或计算节点并行处理，每个部分处理自己的数据子集。然后通过全局通信，参数同步等方式进行梯度传播，此时GPU之间的通信带宽就变的越来越重要</p>
<p>在NVLink出现之前，GPU与GPU之间的数据交互通过PCIe（Peripheral Component Interconnect Express）总线进行。但PCIe存在两个问题，一是PCIe总线的带宽相对有限，其中PCIe 4.0x16的最大带宽也就64GB/s，二是PCIe总线的延迟相对较高，在GPU之间传输数据时，每次数据传输都需要通过CPU和主机内存来完成。这种传输路径会导致额外的延迟，并降低数据传输的效率。然而，深度学习应用中需要更高的带宽和更低的延迟，PCIe显然是无法满足当下的神经网络训练需求</p>
<p><a data-lightbox="e4092b31-e2f3-43aa-9d3b-37ae1b5ba073" data-title="引入NVLink" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/引入NVLink.webp" target="_blank"><img alt="引入NVLink" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/引入NVLink.webp"/></a></p>
<p>NVLink利用高带宽、低延迟的通信通道，直接将多个GPU连接在一起，实现快速、高效的数据传输和共享。通过NVLink，GPU之间的数据交互可以直接在GPU之间进行，而无需通过CPU和主机内存。这种直接内存访问（DMA）的方式大大减少了数据传输的复制和延迟，提高了数据共享的效率。此外，NVLink还提供了一致的内存空间，使得多个GPU能够共享同一份内存，简化了程序设计和数据管理的复杂性</p>
<h2 id="概述_2">6.1 概述</h2>
<blockquote>
<p><a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Paper 2022</a></p>
<p><a href="https://readpaper.feishu.cn/docx/AC7JdtLrhoKpgxxSRM8cfUounsh" target="_blank">FlashAttention: 更快训练更长上下文的GPT</a></p>
</blockquote>
<p>Transformer作为GPT类模型的基础架构提供了强大的特征处理能力，但是处理更长上下文仍然是一个挑战，因为核心的自注意力模块在序列长度上具有O(N^2)的时间和内存复杂度😓</p>
<p>这篇Flash Attention的工作深入硬件，新提出了一种具有<strong>IO感知的</strong>，<strong>快速的</strong>⚡️，<strong>节省内存的</strong>🧠，<strong>精确的</strong>🎯注意力算法。目前，Flash Attention已经<strong>集成至torch2.0</strong>，并且社区也提供了多种实现</p>
<blockquote>
<p><a href="https://www.bilibili.com/video/BV1Zz4y1q7FX/?vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">78s看懂FlashAttention【有点意思·1】</a></p>
</blockquote>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe allowfullscreen="true" border="0" frameborder="no" framespacing="0" scrolling="no" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/78s看懂FlashAttention.mp4" style="position: absolute; width: 100%; 
height: 100%; left: 0; top: 0;"> </iframe></div>
<h2 id="核心要点">6.2 核心要点</h2>
<blockquote>
<p>⚡️<strong>为什么加快了计算？Fast</strong></p>
</blockquote>
<p>降低了耗时的HBM访问次数。采用Tiling技术分块从HBM加载数据到SRAM进行融合计算</p>
<blockquote>
<p><strong>🧠为什么节省了内存？Memory-Efficient</strong></p>
</blockquote>
<p>不再对中间矩阵S，P进行存储。在反向的时候通过Recomputation重新计算来计算梯度</p>
<blockquote>
<p><strong>🎯为什么是精准注意力？Exact Attention</strong></p>
</blockquote>
<p>算法流程只是分块计算，<strong>无近似操作</strong></p>
<h2 id="提出问题">6.3 提出问题</h2>
<p>Transformer结构已成为自然语言处理和图像分类等应用中最常用的架构。尽管Transformer在规模上不断增大和加深，但处理更长上下文仍然是一个挑战，因为核心的自注意力模块在序列长度上具有二次方的时间和内存复杂度。这导致在处理长序列时速度变慢且内存需求巨大。因此，我们需要一些优化算法来提高注意力模块的计算速度和内存利用率</p>
<h2 id="解决方案">6.4 解决方案</h2>
<p><a data-lightbox="a0377e63-b973-4062-ba4e-74b38d6a784e" data-title="flash_Atten架构图" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten架构图.webp" target="_blank"><img alt="flash_Atten架构图" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten架构图.webp"/></a></p>
<h2 id="forward">6.5 Forward</h2>
<h3 id="standard-attention">6.5.1 Standard Attention</h3>
<p>在注意力的一般实现中，对<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{N \times d}</script> 三个输入执行以下算法得到输出<script type="math/tex; ">\mathbf{O}</script>，其中softmax行级别执行
<script type="math/tex; mode=display">
\mathbf{S}=\mathbf{Q} \mathbf{K}^{\top} \in \mathbb{R}^{N \times N}, \quad \mathbf{P}=\operatorname{softmax}(\mathbf{S}) \in \mathbb{R}^{N \times N}, \quad \mathbf{O}=\mathbf{P V} \in \mathbb{R}^{N \times d}
</script>
在这个算法中，<script type="math/tex; ">\mathbf{S}</script>，<script type="math/tex; ">\mathbf{P}</script>矩阵都是很大，需要在HBM中实例化来进行存储，这样就会带来很多HBM的访问次数， 最终体现到算法时间端到端较长的延迟</p>
<p><a data-lightbox="ea536724-35b0-41e9-a60d-218132f50f99" data-title="flash_Atten流程" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten流程.webp" target="_blank"><img alt="flash_Atten流程" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten流程.webp"/></a></p>
<h3 id="flashattentiontiling">6.5.2 FlashAttention(Tiling)</h3>
<blockquote>
<p>理论基础</p>
</blockquote>
<p>在传统算法中，一种方式是将Mask和SoftMax部分融合，以减少访存次数。然而，FlashAttention则更加激进，它将从输入<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}</script>到输出<script type="math/tex; ">\mathbf{O}</script>的整个过程进行融合，以避免<script type="math/tex; ">\mathbf{S}</script>，<script type="math/tex; "> \mathbf{P}</script>矩阵的存储开销，实现端到端的延迟缩减。然而，由于输入的长度<script type="math/tex; ">N</script>通常很长，无法完全将完整的<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}</script>及中间计算结果存储在SRAM中。因此，需要依 赖HBM进行访存操作，与原始计算延迟相比没有太大差异，甚至会变慢(没具体测)</p>
<p>为了让计算过程的结果完全在SRAM中，摆脱对HBM的依赖，可以采用分片操作，每次进行部分计算，确保这些计算结果能在SRAM内进行交互，待得到对应的结果后再进行输出</p>
<p>这个过程中，有一点需要注意的是，之前对于softmax的计算是以行为单位的，如下所示:
<script type="math/tex; mode=display">
m(x):=\max _{i} x_{i}, \quad f(x):=\left[\begin{array}{lll}
e^{x_{1}-m(x)} & \ldots & e^{x_{B}-m(x)}
\end{array}\right], \quad \ell(x):=\sum_{i} f(x)_{i}, \quad \operatorname{softmax}(x):=\frac{f(x)}{\ell(x)}
</script>
当我们将输入进行分片后，无法对完整的行数据执行Softmax操作。这是因为Softmax函数在计算时需要考虑整个行的数据</p>
<p><strong>然而，我们可以通过如下所示方法来获得与完整行Softmax相同的结果，而无需使用近似操作</strong>
<script type="math/tex; mode=display">
\begin{array}{l}
m(x)=m\left(\left[x^{(1)} x^{(2)}\right]\right)=\max \left(m\left(x^{(1)}\right), m\left(x^{(2)}\right)\right), \quad f(x)=\left[\begin{array}{ll}
e^{m\left(x^{(1)}\right)-m(x)} f\left(x^{(1)}\right) & \left.e^{m\left(x^{(2)}\right)-m(x)} f\left(x^{(2)}\right)\right]
\end{array}\right. \\
\ell(x)=\ell\left(\left[x^{(1)} x^{(2)}\right]\right)=e^{m\left(x^{(1)}\right)-m(x)} \ell\left(x^{(1)}\right)+e^{m\left(x^{(2)}\right)-m(x)} \ell\left(x^{(2)}\right), \quad \operatorname{softmax}(x)=\frac{f(x)}{\ell(x)}
\end{array}
</script>
具体的分块softmax代码演示</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch


q = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]).float()
v = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]).float()
q_sm = torch.softmax(q, <span class="hljs-number">0</span>)
print(q_sm)   <span class="hljs-comment"># tensor([0.2689, 0.7311])</span>

torch.dot(q_sm, v)  <span class="hljs-comment"># tensor(1.7311)</span>

m_pre = float(<span class="hljs-string">"-inf"</span>)
l_pre = <span class="hljs-number">0</span>
cur_sum = <span class="hljs-number">0</span>

block1 = torch.tensor([<span class="hljs-number">1</span>]).float()
<span class="hljs-comment"># get cur max value</span>
m_cur = max(torch.max(block1), m_pre)
<span class="hljs-comment"># scale pre log value by max exp</span>
l_pre *= torch.exp(m_pre - m_cur)
<span class="hljs-comment"># calculate current log sum</span>
p = torch.exp(block1 - m_cur)
l_cur = torch.sum(p) + l_pre
<span class="hljs-comment"># scale pre result by log sum</span>
cur_sum = cur_sum * l_pre / l_cur
p = p / l_cur
cur_sum = <span class="hljs-number">1</span> * p[<span class="hljs-number">0</span>]

l_pre = l_cur
m_pre = m_cur
print(cur_sum)   <span class="hljs-comment"># tensor(1.)</span>

block2 = torch.tensor([<span class="hljs-number">2</span>]).float()
m_cur = max(torch.max(block2), m_pre)
l_pre *= torch.exp(m_pre - m_cur)
p = torch.exp(block2 - m_cur)
l_cur = torch.sum(p) + l_pre
cur_sum = cur_sum * l_pre / l_cur
p = p / l_cur
cur_sum += <span class="hljs-number">2</span> * p[<span class="hljs-number">0</span>]
print(cur_sum)   <span class="hljs-comment"># tensor(1.7311)</span>
</code></pre>
<blockquote>
<p>代码实现</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_fwd_kernel</span><span class="hljs-params">(
    Q, K, V, sm_scale,
    L, M,
    Out,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vk, stride_vn,
    stride_oz, stride_oh, stride_om, stride_on,
    Z, H, N_CTX,
    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,
    BLOCK_N: tl.constexpr,
)</span>:</span>
    start_m = tl.program_id(<span class="hljs-number">0</span>)
    off_hz = tl.program_id(<span class="hljs-number">1</span>)
    <span class="hljs-comment"># initialize offsets</span>
    offs_m = start_m * BLOCK_M + tl.arange(<span class="hljs-number">0</span>, BLOCK_M)
    offs_n = tl.arange(<span class="hljs-number">0</span>, BLOCK_N)
    offs_d = tl.arange(<span class="hljs-number">0</span>, BLOCK_DMODEL)
    off_q = off_hz * stride_qh + offs_m[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_d[<span class="hljs-keyword">None</span>, :] * stride_qk
    off_k = off_hz * stride_qh + offs_n[<span class="hljs-keyword">None</span>, :] * stride_kn + offs_d[:, <span class="hljs-keyword">None</span>] * stride_kk
    off_v = off_hz * stride_qh + offs_n[:, <span class="hljs-keyword">None</span>] * stride_qm + offs_d[<span class="hljs-keyword">None</span>, :] * stride_qk
    <span class="hljs-comment"># Initialize pointers to Q, K, V</span>
    q_ptrs = Q + off_q
    k_ptrs = K + off_k
    v_ptrs = V + off_v
    <span class="hljs-comment"># initialize pointer to m and l</span>
    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(<span class="hljs-string">"inf"</span>)
    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)
    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)
    <span class="hljs-comment"># load q: it will stay in SRAM throughout</span>
    q = tl.load(q_ptrs)
    <span class="hljs-comment"># loop over k, v and update accumulator</span>
    <span class="hljs-keyword">for</span> start_n <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, (start_m + <span class="hljs-number">1</span>) * BLOCK_M, BLOCK_N):
        <span class="hljs-comment"># -- compute qk ----</span>
        k = tl.load(k_ptrs)
        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
        qk += tl.dot(q, k)
        qk *= sm_scale
        qk = tl.where(offs_m[:, <span class="hljs-keyword">None</span>] &gt;= (start_n + offs_n[<span class="hljs-keyword">None</span>, :]), qk, float(<span class="hljs-string">"-inf"</span>))
        <span class="hljs-comment"># compute new m</span>
        m_curr = tl.maximum(tl.max(qk, <span class="hljs-number">1</span>), m_prev)
        <span class="hljs-comment"># correct old l</span>
        l_prev *= tl.exp(m_prev - m_curr)
        <span class="hljs-comment"># attention weights</span>
        p = tl.exp(qk - m_curr[:, <span class="hljs-keyword">None</span>])
        l_curr = tl.sum(p, <span class="hljs-number">1</span>) + l_prev
        <span class="hljs-comment"># rescale operands of matmuls</span>
        l_rcp = <span class="hljs-number">1.</span> / l_curr
        p *= l_rcp[:, <span class="hljs-keyword">None</span>]
        acc *= (l_prev * l_rcp)[:, <span class="hljs-keyword">None</span>]
        <span class="hljs-comment"># update acc</span>
        p = p.to(Q.dtype.element_ty)
        v = tl.load(v_ptrs)
        acc += tl.dot(p, v)
        <span class="hljs-comment"># update m_i and l_i</span>
        l_prev = l_curr
        m_prev = m_curr
        <span class="hljs-comment"># update pointers</span>
        k_ptrs += BLOCK_N * stride_kn
        v_ptrs += BLOCK_N * stride_vk
    <span class="hljs-comment"># rematerialize offsets to save registers</span>
    start_m = tl.program_id(<span class="hljs-number">0</span>)
    offs_m = start_m * BLOCK_M + tl.arange(<span class="hljs-number">0</span>, BLOCK_M)
    <span class="hljs-comment"># write back l and m</span>
    l_ptrs = L + off_hz * N_CTX + offs_m
    m_ptrs = M + off_hz * N_CTX + offs_m
    tl.store(l_ptrs, l_prev)
    tl.store(m_ptrs, m_prev)
    <span class="hljs-comment"># initialize pointers to output</span>
    offs_n = tl.arange(<span class="hljs-number">0</span>, BLOCK_DMODEL)
    off_o = off_hz * stride_oh + offs_m[:, <span class="hljs-keyword">None</span>] * stride_om + offs_n[<span class="hljs-keyword">None</span>, :] * stride_on
    out_ptrs = Out + off_o
    tl.store(out_ptrs, acc)
</code></pre>
<h2 id="io复杂度分析">6.6 IO复杂度分析</h2>
<h3 id="standard-attention_1">6.6.1 Standard Attention</h3>
<p>对于标准注意力实现，初期我们需要把输入 <script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}</script>从HBM中读取，并计算完毕后把输出<script type="math/tex; ">\mathbf{O}</script>写入到HBM中</p>
<ol>
<li>第一步把<script type="math/tex; ">\mathbf{Q}, \mathbf{K}</script>读取出来计算出<script type="math/tex; ">\mathbf{S}=\mathbf{Q} \mathbf{K}^{\top}</script>，然后把<script type="math/tex; ">\mathbf{S}</script>存回去，内存访问复杂度<script type="math/tex; ">\Theta\left(N d+N^{2}\right)</script></li>
<li>第二步把<script type="math/tex; ">\mathbf{S}</script>读取出来计算出<script type="math/tex; ">\mathbf{P}=\operatorname{softmax}(\mathbf{S})</script>，然后把<script type="math/tex; ">\mathbf{P}</script>存回去，内存访问复杂度<script type="math/tex; ">\Theta\left(N^{2}\right)</script></li>
<li>第三步把<script type="math/tex; ">\mathbf{V}, \mathbf{P}</script>读取出来计算出<script type="math/tex; ">\mathbf{O}=\mathbf{P V}</script>，然后计算出结果<script type="math/tex; ">\mathbf{O}</script>，内存访问复杂度<script type="math/tex; ">\Theta\left(N d+N^{2}\right)</script></li>
</ol>
<p>综上所述，整体的内存访问复杂度为<script type="math/tex; ">\Theta\left(N d+N^{2}\right)</script></p>
<h3 id="flashattention">6.6.2 FlashAttention</h3>
<p>对于FlashAttention，我们设置一个分块大小<script type="math/tex; ">B_{c}</script>来把<script type="math/tex; ">\mathbf{K}, \mathbf{V}</script>分成<script type="math/tex; ">T_{c}</script>块，对于<script type="math/tex; ">\mathbf{Q}, \mathbf{O}</script>的每一块都要把<script type="math/tex; ">\mathbf{K}, \mathbf{V}</script>部分的全部元素Load一遍，这样则有FlashAttention的内存访问复杂度为<script type="math/tex; ">\Theta\left(N d+N d T_{c}\right)=\Theta\left(N d T_{c}\right)</script></p>
<p>在这里，我们需要两个分块大小，<script type="math/tex; ">\mathbf{Q}, \mathbf{O}</script>的分块大小<script type="math/tex; ">B_{r}</script>，<script type="math/tex; ">\mathbf{K}, \mathbf{V}</script>的分块大小<script type="math/tex; ">B_{c}</script>，我们设定SRAM的大小为<script type="math/tex; ">M</script>，为了能把分块后的<script type="math/tex; ">\mathbf{K}, \mathbf{V} \in \mathbb{R}^{B_{c} \times d}</script>放进SRAM，那么则有一下限制:
<script type="math/tex; mode=display">
B_{c} d=O(M) \Leftrightarrow B_{c}=O\left(\frac{M}{d}\right)
</script>
相应的，<script type="math/tex; ">\mathbf{Q}, \mathbf{O} \in \mathbb{R}^{B_{r} \times d}</script>有如下限制:
<script type="math/tex; mode=display">
B_{r} d=O(M) \Leftrightarrow B_{r}=O\left(\frac{M}{d}\right)
</script>
最终，还有一个中间态<script type="math/tex; ">\mathbf{S}=\mathbf{Q K}^{\top} \in \mathbb{R}^{B_{r} \times B_{c}}</script>需要存储，则有如下限制:
<script type="math/tex; mode=display">
B_{r} B_{c}=O(M)
</script>
综上，限制如下
<script type="math/tex; mode=display">
B_{c}=\Theta\left(\frac{M}{d}\right), \quad B_{r}=\Theta\left(\min \left(\frac{M}{d}, \frac{M}{B_{c}}\right)\right)=\Theta\left(\min \left(\frac{M}{d}, d\right)\right)
</script>
进而推出
<script type="math/tex; mode=display">
T_{c}=\frac{N}{B_{c}}=\Theta\left(\frac{N d}{M}\right)
</script>
那么在<script type="math/tex; ">M=\Theta(N d)</script> 的前提下，则有FlashAttention的HBM内存访问复杂度为：
<script type="math/tex; mode=display">
\Theta\left(N d T_{c}\right)=\Theta\left(\frac{N^{2} d^{2}}{M}\right)=\Theta(N d)
</script>
在语言建模中，通常有<script type="math/tex; ">d \lll N</script>，则有<script type="math/tex; ">\Theta_{\text {stand }}\left(N d+N^{2}\right)>\Theta_{f l a s h}(N d)</script>。这样，在前向的过程中，我们采用分块计算的方式，避免了<script type="math/tex; ">\mathbf{S}, \mathbf{P}</script>矩阵的存储开销，整体的运算都在SRAM内进行，降低了HBM访问次数，大大提升了计算的速度，减少了对存储的消耗</p>
<h2 id="backward">6.7 Backward</h2>
<h3 id="理论基础">6.7.1 理论基础</h3>
<p>在上面前向的时候我们为了减少HBM访存次数，降低内存消耗量，我们并没有对<script type="math/tex; ">\mathbf{S}, \mathbf{P}</script>矩阵进行存储，而这个在反向传播计算梯度的时候确实需要的一个信息</p>
<p>之前有通过Gradient checkpointing的方式来实现梯度实现在前向的时候更加节省内存</p>
<p>我们这里则采用<strong>重新计算的方式来计算对应的梯度</strong>。在上面前向计算的时候我们不会存储<script type="math/tex; ">\mathbf{S}, \mathbf{P}</script>矩阵，但是我们会存储对应的指数项之和<script type="math/tex; ">L</script>来进行梯度的计算</p>
<p>我们在反向的过程中最重要的事情就是就是Loss函数<script type="math/tex; ">\phi</script>对<script type="math/tex; ">\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O}</script>对应的梯度</p>
<p>其中$\mathbf{O}<script type="math/tex; ">对应的梯度最好计算</script>\mathbf{d} \mathbf{O}=\frac{\partial \phi}{\partial \mathbf{O}}<script type="math/tex; ">，其中</script>\mathbf{O}<script type="math/tex; ">是现成的

而</script>\mathbf{V}<script type="math/tex; ">对应的梯度也很好计算，由于</script>\mathbf{O}=\mathbf{P V}<script type="math/tex; ">，根据链式求导法则和矩阵求导法则则有</script>\mathbf{d V}=\mathbf{P}^{T} \mathbf{d} \mathbf{O}<script type="math/tex; ">，更详细如下所示:
</script>
d v<em>{j}=\sum</em>{i} P<em>{i j} d o</em>{i}=\sum<em>{i} \frac{e^{q</em>{i}^{T}} k<em>{j}}{L</em>{i}} d o<em>{i}
<script type="math/tex; mode=display">
而</script>\mathbf{Q}, \mathbf{K}<script type="math/tex; ">对应的梯度算起来就比较复杂一点。这两个经过的计算逻辑步骤更多，我们可以一步一步的来进行计算。我们可以先计算</script>\mathbf{d P}<script type="math/tex; ">，</script>\mathbf{d S}<script type="math/tex; ">。由于</script>\mathbf{O}=\mathbf{P V}<script type="math/tex; "> ，则有</script>\mathbf{d P}<script type="math/tex; ">如下表示
</script>
d P</em>{i j}=d o<em>{i}^{T} v</em>{j}
<script type="math/tex; mode=display">


Fact: </script>y=\operatorname{softmax}(x)<script type="math/tex; ">的雅各比矩阵为</script>\operatorname{diag}(y)-y y^{T}<script type="math/tex; ">，具体推导见

[Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)



由于</script>P<em>{i:}=\operatorname{softmax}\left(S</em>{i:}\right)<script type="math/tex; ">， 根据上述定理则有:
</script>
d S<em>{i:}=\left(\operatorname{diag}\left(P</em>{i:}\right)-P<em>{i:} P</em>{i:}^{T}\right) d P<em>{i:}=P</em>{i:} \circ d P<em>{i:}-\left(P</em>{i:}^{T} d P<em>{i:}\right) P</em>{i:}
<script type="math/tex; mode=display">
接下来我们定义如下表示:
</script>
D<em>{i}=P</em>{i:}^{T} d P<em>{i:}=\sum \frac{e^{q</em>{i} \kappa<em>{j}}}{L</em>{i}} d o<em>{i}^{T} v</em>{j}=d o<em>{i}^{T} \sum \frac{e^{q</em>{i} \kappa<em>{j}}}{L</em>{i}} v<em>{j}=d o</em>{i}^{T} o<em>{i}
<script type="math/tex; mode=display">
根据上述定义简化上上式则有如下表示:
</script>
d S</em>{i:}=P<em>{i:} \circ d P</em>{i:}-D<em>{i} P</em>{i:}
<script type="math/tex; mode=display">
相应的</script>d \mathbf{S}<script type="math/tex; ">可表示为如下形式:
</script>
d S<em>{i j}=P</em>{i j} d P<em>{i j}-D</em>{i} P<em>{i j}=P</em>{i j}\left(d P<em>{i j}-D</em>{i}\right)
<script type="math/tex; mode=display">
又因为</script>S<em>{i j}=q</em>{i}^{T} k<em>{j}<script type="math/tex; ">，结合上述推导利用链式求导法则</script>\mathbf{Q}, \mathbf{K}<script type="math/tex; ">对应的梯度有如下表示：
</script>
\begin{array}{l}
d q</em>{i}=\sum<em>{j} d S</em>{i j} k<em>{j}=\sum</em>{j} P<em>{i j}\left(d P</em>{i j}-D<em>{i}\right) k</em>{j}=\sum<em>{j} \frac{e^{q</em>{i}^{T} k<em>{j}}}{L</em>{i}}\left(d o<em>{i}^{T} v</em>{j}-D<em>{i}\right) k</em>{j} \
d k<em>{j}=\sum</em>{i} d S<em>{i j} q</em>{i}=\sum<em>{i} P</em>{i j}\left(d P<em>{i j}-D</em>{i}\right) q<em>{i}=\sum</em>{i} \frac{e^{q<em>{i}^{T} k</em>{j}}}{L<em>{i}}\left(d o</em>{i}^{T} v<em>{j}-D</em>{i}\right) q<em>{i}
\end{array}
<script type="math/tex; mode=display">
至此，我们得到了一个完整的包含前向和反向的，降低了HBM访问次数的，新的Attention算子

### 代码实现

```python
@triton.jit
def _bwd_kernel(
    Q, K, V, sm_scale, Out, DO,
    DQ, DK, DV,
    L, M,
    D,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vk, stride_vn,
    Z, H, N_CTX,
    num_block,
    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    off_hz = tl.program_id(0)
    off_z = off_hz // H
    off_h = off_hz % H
    # offset pointers for batch/head
    Q += off_z * stride_qz + off_h * stride_qh
    K += off_z * stride_qz + off_h * stride_qh
    V += off_z * stride_qz + off_h * stride_qh
    DO += off_z * stride_qz + off_h * stride_qh
    DQ += off_z * stride_qz + off_h * stride_qh
    DK += off_z * stride_qz + off_h * stride_qh
    DV += off_z * stride_qz + off_h * stride_qh
    for start_n in range(0, num_block):
        lo = start_n * BLOCK_M
        # initialize row/col offsets
        offs_qm = lo + tl.arange(0, BLOCK_M)
        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)
        offs_m = tl.arange(0, BLOCK_N)
        offs_k = tl.arange(0, BLOCK_DMODEL)
        # initialize pointers to value-like data
        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)
        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)
        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)
        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)
        # pointer to row-wise quantities in value-like data
        D_ptrs = D + off_hz * N_CTX
        m_ptrs = M + off_hz * N_CTX
        # initialize dv amd dk
        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)
        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)
        # k and v stay in SRAM throughout
        k = tl.load(k_ptrs)
        v = tl.load(v_ptrs)
        # loop over rows
        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):
            offs_m_curr = start_m + offs_m
            # load q, k, v, do on-chip
            q = tl.load(q_ptrs)
            # recompute p = softmax(qk, dim=-1).T
            # NOTE: `do` is pre-divided by `l`; no normalization here
            qk = tl.dot(q, tl.trans(k))
            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float("-inf"))
            m = tl.load(m_ptrs + offs_m_curr)
            p = tl.exp(qk * sm_scale - m[:, None])
            # compute dv
            do = tl.load(do_ptrs)
            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)
            # compute dp = dot(v, do)
            Di = tl.load(D_ptrs + offs_m_curr)
            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]
            dp += tl.dot(do, tl.trans(v))
            # compute ds = p * (dp - delta[:, None])
            ds = p * dp * sm_scale
            # compute dk = dot(ds.T, q)
            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)
            # compute dq
            dq = tl.load(dq_ptrs)
            dq += tl.dot(ds.to(Q.dtype.element_ty), k)
            tl.store(dq_ptrs, dq)
            # increment pointers
            dq_ptrs += BLOCK_M * stride_qm
            q_ptrs += BLOCK_M * stride_qm
            do_ptrs += BLOCK_M * stride_qm
        # write-back
        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)
        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)
        tl.store(dv_ptrs, dv)
        tl.store(dk_ptrs, dk)
```

### Block-Sparse

相比于上面的全量计算，块稀疏的FlashAttention需要额外提供一个Mask矩阵</script>\tilde{\mathbf{M}} \in{0,1}^{N \times N}<script type="math/tex; ">用于将一些元 素置零来保证块稀疏加速计算

本章对于块稀疏的一个计算只是一个简单的尝试，没有进行太深入的探索，所以这里我们先一笔带过，后面我们可以讲一篇对FlashAttention进行块稀疏优化的工作SCFA
</script>
\mathbf{S}=\mathbf{Q K} \mathbf{K}^{\top} \in \mathbb{R}^{N \times N}, \quad \mathbf{P}=\operatorname{softmax}\left(\mathbf{S} \odot 1</em>{\overline{\mathbf{M}}}\right) \in \mathbb{R}^{N \times N}, \quad \mathbf{O}=\mathbf{P V} \in \mathbb{R}^{N \times d}</p>
<p>$$</p>
<h2 id="实验验证">6.8 实验验证</h2>
<p>通过实验验证发现，FlashAttention在速度和内存占用方面都表现出明显的优势，并取得了良好的效果</p>
<p><a data-lightbox="289bfaaa-30f7-4bda-ae91-ee11bc5ca28b" data-title="flash_Atten实验验证1" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten实验验证1.webp" target="_blank"><img alt="flash_Atten实验验证1" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten实验验证1.webp"/></a></p>
<p><a data-lightbox="041fd0d4-a193-413d-901c-59f90ce7c2ea" data-title="flash_Atten实验验证2" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten实验验证2.webp" target="_blank"><img alt="flash_Atten实验验证2" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/LLM模型微调系列/flash_Atten实验验证2.webp"/></a></p>
<p>目前，FlashAttention已经经过广泛验证, torch2.0中已提供flashattention的实现</p>
<p>正如标题《Fast and Memory-Efficient Exact Attention with IO-Awareness》所示，FlashAttention的优点在于充分考虑了在计算任务中IO的重要性，并通过分块计算的方式开发了一种快速、节省显存、精确无近似的注意力实现方法。这使得我们更便于训练具有更长上下文的Transformer模型，并且为后续注意力算法的优化提供了一个基准</p>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2024-02-26 02:37:03
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: LLM Tokenizer分词系列.md" class="navigation navigation-prev" href="LLM Tokenizer分词系列.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: LLM模型部署调试推理.md" class="navigation navigation-next" href="LLM模型部署调试推理.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":59381,"date":"2023/06/15 12:46:10","cover":"https://pic.hycbook.com/i/hexo/post_cover/蕾姆1.webp","title":"LLM模型微调系列.md","tags":["深度学习","LLM模型","微调","p tuning v2","lora"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/蕾姆1.webp","mathjax":true,"categories":["deep-learning"],"description":"LLM模型微调系列","level":"1.3","depth":1,"next":{"title":"LLM模型部署调试推理.md","level":"1.4","depth":1,"path":"chapters/LLM模型部署调试推理.md","ref":"chapters/LLM模型部署调试推理.md","articles":[]},"previous":{"title":"LLM Tokenizer分词系列.md","level":"1.2","depth":1,"path":"chapters/LLM Tokenizer分词系列.md","ref":"chapters/LLM Tokenizer分词系列.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2021","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipay打赏","button":"欢迎打赏","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechat打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"深度学习知识驿站","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"欢迎留下评论交流~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"深度学习相关学习记录","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"书籍主页":"https://study.hycbook.com"}},"gitbook":"*","description":"记录 深度学习 的学习和一些技巧的使用"},"file":{"path":"chapters/LLM模型微调系列.md","mtime":"2024-02-26T02:37:03.394Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2024-02-26T02:37:25.600Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
