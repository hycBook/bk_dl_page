<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>pytorch学习_进阶知识.md · 深度学习相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="pytorch学习_进阶知识" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="transformer.html" rel="next"/>
<link href="pytorch学习_基础知识.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">书籍主页</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="LLM Tokenizer分词系列.html" id="chapter_id_1">
<a href="LLM Tokenizer分词系列.html">
<b>1.2.</b>
                    
                    LLM Tokenizer分词系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="LLM模型微调系列.html" id="chapter_id_2">
<a href="LLM模型微调系列.html">
<b>1.3.</b>
                    
                    LLM模型微调系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="LLM模型部署调试推理.html" id="chapter_id_3">
<a href="LLM模型部署调试推理.html">
<b>1.4.</b>
                    
                    LLM模型部署调试推理.md
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="dl_in_vision_field.html" id="chapter_id_4">
<a href="dl_in_vision_field.html">
<b>1.5.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter" data-level="1.6" data-path="huggingface基本使用教程.html" id="chapter_id_5">
<a href="huggingface基本使用教程.html">
<b>1.6.</b>
                    
                    huggingface基本使用教程.md
            
                </a>
</li>
<li class="chapter" data-level="1.7" data-path="nlp关键词和摘要提取技术整理.html" id="chapter_id_6">
<a href="nlp关键词和摘要提取技术整理.html">
<b>1.7.</b>
                    
                    nlp关键词和摘要提取技术整理.md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="pytorch学习_基础知识.html" id="chapter_id_7">
<a href="pytorch学习_基础知识.html">
<b>1.8.</b>
                    
                    pytorch学习_基础知识.md
            
                </a>
</li>
<li class="chapter active" data-level="1.9" data-path="pytorch学习_进阶知识.html" id="chapter_id_8">
<a href="pytorch学习_进阶知识.html">
<b>1.9.</b>
                    
                    pytorch学习_进阶知识.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="transformer.html" id="chapter_id_9">
<a href="transformer.html">
<b>1.10.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="图像分割算法.html" id="chapter_id_10">
<a href="图像分割算法.html">
<b>1.11.</b>
                    
                    图像分割算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="图像分类算法.html" id="chapter_id_11">
<a href="图像分类算法.html">
<b>1.12.</b>
                    
                    图像分类算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="图神经网络.html" id="chapter_id_12">
<a href="图神经网络.html">
<b>1.13.</b>
                    
                    图神经网络.md
            
                </a>
</li>
<li class="chapter" data-level="1.14" data-path="数据标注工具.html" id="chapter_id_13">
<a href="数据标注工具.html">
<b>1.14.</b>
                    
                    数据标注工具.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="深度学习核心之优化器.html" id="chapter_id_14">
<a href="深度学习核心之优化器.html">
<b>1.15.</b>
                    
                    深度学习核心之优化器.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="深度学习核心之损失函数.html" id="chapter_id_15">
<a href="深度学习核心之损失函数.html">
<b>1.16.</b>
                    
                    深度学习核心之损失函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="深度学习核心之激活函数.html" id="chapter_id_16">
<a href="深度学习核心之激活函数.html">
<b>1.17.</b>
                    
                    深度学习核心之激活函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.18" data-path="深度学习核心基础知识点.html" id="chapter_id_17">
<a href="深度学习核心基础知识点.html">
<b>1.18.</b>
                    
                    深度学习核心基础知识点.md
            
                </a>
</li>
<li class="chapter" data-level="1.19" data-path="深度学习模型压缩技术.html" id="chapter_id_18">
<a href="深度学习模型压缩技术.html">
<b>1.19.</b>
                    
                    深度学习模型压缩技术.md
            
                </a>
</li>
<li class="chapter" data-level="1.20" data-path="目标检测与跟踪算法.html" id="chapter_id_19">
<a href="目标检测与跟踪算法.html">
<b>1.20.</b>
                    
                    目标检测与跟踪算法.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">pytorch学习_进阶知识.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#tensor">1 Tensor</a></li><ul><li><span class="title-icon"></span><a href="#storage">1.1 storage</a></li></ul><li><span class="title-icon"></span><a href="#实例">2 实例</a></li><ul><li><span class="title-icon"></span><a href="#pytorch加载数据">2.1 Pytorch加载数据</a></li><li><span class="title-icon"></span><a href="#tensorboard">2.2 Tensorboard</a></li><li><span class="title-icon"></span><a href="#transforms">2.3 Transforms</a></li><li><span class="title-icon"></span><a href="#torchvision数据集">2.4 torchvision数据集</a></li><li><span class="title-icon"></span><a href="#损失函数">2.5 损失函数</a></li><li><span class="title-icon"></span><a href="#优化器">2.6 优化器</a></li><li><span class="title-icon"></span><a href="#网络模型使用及修改">2.7 网络模型使用及修改</a></li><li><span class="title-icon"></span><a href="#网络模型保存与读取">2.8 网络模型保存与读取</a></li><li><span class="title-icon"></span><a href="#固定模型参数">2.9 固定模型参数</a></li><li><span class="title-icon"></span><a href="#训练流程">2.10 训练流程</a></li></ul></ul></div><a href="#tensor" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><hr/>
<blockquote>
<p><a href="https://www.pytorchtutorial.com/docs/package_references/torch/" target="_blank">pytorch中文文档</a></p>
</blockquote>
<h1 id="tensor">1 Tensor</h1>
<p><code>torch.Tensor</code>是一种包含单一数据类型元素的多维矩阵</p>
<p>Torch定义了10种CPU tensor类型和GPU tensor类型：</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.float32</code> or <code>torch.float</code></td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.float64</code> or <code>torch.double</code></td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a href="https://pytorch.org/docs/stable/tensors.html#id4" target="_blank">[1]</a></td>
<td><code>torch.float16</code> or <code>torch.half</code></td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a href="https://pytorch.org/docs/stable/tensors.html#id5" target="_blank">[2]</a></td>
<td><code>torch.bfloat16</code></td>
<td><code>torch.BFloat16Tensor</code></td>
<td><code>torch.cuda.BFloat16Tensor</code></td>
</tr>
<tr>
<td>32-bit complex</td>
<td><code>torch.complex32</code> or <code>torch.chalf</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>64-bit complex</td>
<td><code>torch.complex64</code> or <code>torch.cfloat</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>128-bit complex</td>
<td><code>torch.complex128</code> or <code>torch.cdouble</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.int8</code></td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.int16</code> or <code>torch.short</code></td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.int32</code> or <code>torch.int</code></td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.int64</code> or <code>torch.long</code></td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
<tr>
<td>Boolean</td>
<td><code>torch.bool</code></td>
<td><code>torch.BoolTensor</code></td>
<td><code>torch.cuda.BoolTensor</code></td>
</tr>
<tr>
<td>quantized 8-bit integer (unsigned)</td>
<td><code>torch.quint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 8-bit integer (signed)</td>
<td><code>torch.qint8</code></td>
<td><code>torch.CharTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 32-bit integer (signed)</td>
<td><code>torch.qint32</code></td>
<td><code>torch.IntTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 4-bit integer (unsigned) <a href="https://pytorch.org/docs/stable/tensors.html#id6" target="_blank">[3]</a></td>
<td><code>torch.quint4x2</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
</tbody>
</table>
<blockquote>
<p>创建</p>
</blockquote>
<p>一个张量tensor可以从Python的<code>list</code>或序列构建</p>
<pre><code class="lang-python">torch.FloatTensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])

Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])
</code></pre>
<p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<code>numpy.ndarray</code>,<code>torch.Tensor</code>或<code>torch.Storage</code>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor</p>
<pre><code class="lang-python"># 接口 一个空张量tensor可以通过规定其大小来构建
class torch.Tensor
class torch.Tensor(*sizes)
class torch.Tensor(size)
class torch.Tensor(sequence)
class torch.Tensor(ndarray)
class torch.Tensor(tensor)
class torch.Tensor(storage)

# 实例化
torch.IntTensor(2, 4).zero_()
</code></pre>
<p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p>
<pre><code class="lang-python">x = torch.FloatTensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])
x[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>]
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">6.</span>)

x[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = <span class="hljs-number">8</span>
x
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])
</code></pre>
<p>每一个张量tensor都有一个相应的<code>torch.Storage</code>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算</p>
<p><strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，<code>torch.FloatTensor.abs_()</code>会在原地计算绝对值，并返回改变后的tensor，而<code>tensor.FloatTensor.abs()</code>将会在一个新的tensor中计算结果</p>
<blockquote>
<p>关键属性和方法</p>
</blockquote>
<table>
<thead>
<tr>
<th><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor" target="_blank"><code>Tensor.new_tensor</code></a></th>
<th>Returns a new Tensor with <code>data</code> as the tensor data.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_full.html#torch.Tensor.new_full" target="_blank"><code>Tensor.new_full</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>fill_value</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty" target="_blank"><code>Tensor.new_empty</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with uninitialized data.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones" target="_blank"><code>Tensor.new_ones</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>1</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros" target="_blank"><code>Tensor.new_zeros</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>0</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda" target="_blank"><code>Tensor.is_cuda</code></a></td>
<td>Is <code>True</code> if the Tensor is stored on the GPU, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized" target="_blank"><code>Tensor.is_quantized</code></a></td>
<td>Is <code>True</code> if the Tensor is quantized, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta" target="_blank"><code>Tensor.is_meta</code></a></td>
<td>Is <code>True</code> if the Tensor is a meta tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.device.html#torch.Tensor.device" target="_blank"><code>Tensor.device</code></a></td>
<td>Is the <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" target="_blank"><code>torch.device</code></a> where this Tensor is.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html#torch.Tensor.grad" target="_blank"><code>Tensor.grad</code></a></td>
<td>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <code>backward()</code> computes gradients for <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html#torch.Tensor.ndim" target="_blank"><code>Tensor.ndim</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim" target="_blank"><code>dim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.real.html#torch.Tensor.real" target="_blank"><code>Tensor.real</code></a></td>
<td>Returns a new tensor containing real values of the <code>self</code> tensor for a complex-valued input tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.imag.html#torch.Tensor.imag" target="_blank"><code>Tensor.imag</code></a></td>
<td>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs" target="_blank"><code>Tensor.abs</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs" target="_blank"><code>torch.abs()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs_.html#torch.Tensor.abs_" target="_blank"><code>Tensor.abs_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs" target="_blank"><code>abs()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute" target="_blank"><code>Tensor.absolute</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs" target="_blank"><code>abs()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_" target="_blank"><code>Tensor.absolute_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute" target="_blank"><code>absolute()</code></a> Alias for <code>abs_()</code></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos" target="_blank"><code>Tensor.acos</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos" target="_blank"><code>torch.acos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos_.html#torch.Tensor.acos_" target="_blank"><code>Tensor.acos_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos" target="_blank"><code>acos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos" target="_blank"><code>Tensor.arccos</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos" target="_blank"><code>torch.arccos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_" target="_blank"><code>Tensor.arccos_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos" target="_blank"><code>arccos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add" target="_blank"><code>Tensor.add</code></a></td>
<td>Add a scalar or tensor to <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.add_.html#torch.Tensor.add_" target="_blank"><code>Tensor.add_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add" target="_blank"><code>add()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" target="_blank"><code>Tensor.addbmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm" target="_blank"><code>torch.addbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_" target="_blank"><code>Tensor.addbmm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" target="_blank"><code>addbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" target="_blank"><code>Tensor.addcdiv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv" target="_blank"><code>torch.addcdiv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_" target="_blank"><code>Tensor.addcdiv_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" target="_blank"><code>addcdiv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" target="_blank"><code>Tensor.addcmul</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul" target="_blank"><code>torch.addcmul()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_" target="_blank"><code>Tensor.addcmul_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" target="_blank"><code>addcmul()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm" target="_blank"><code>Tensor.addmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm" target="_blank"><code>torch.addmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_" target="_blank"><code>Tensor.addmm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm" target="_blank"><code>addmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm" target="_blank"><code>Tensor.sspaddmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sspaddmm.html#torch.sspaddmm" target="_blank"><code>torch.sspaddmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv" target="_blank"><code>Tensor.addmv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv" target="_blank"><code>torch.addmv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_" target="_blank"><code>Tensor.addmv_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv" target="_blank"><code>addmv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr" target="_blank"><code>Tensor.addr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr" target="_blank"><code>torch.addr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr_.html#torch.Tensor.addr_" target="_blank"><code>Tensor.addr_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr" target="_blank"><code>addr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint" target="_blank"><code>Tensor.adjoint</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.adjoint.html#torch.adjoint" target="_blank"><code>adjoint()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.allclose.html#torch.Tensor.allclose" target="_blank"><code>Tensor.allclose</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose" target="_blank"><code>torch.allclose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.amax.html#torch.Tensor.amax" target="_blank"><code>Tensor.amax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax" target="_blank"><code>torch.amax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.amin.html#torch.Tensor.amin" target="_blank"><code>Tensor.amin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin" target="_blank"><code>torch.amin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax" target="_blank"><code>Tensor.aminmax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.aminmax.html#torch.aminmax" target="_blank"><code>torch.aminmax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.angle.html#torch.Tensor.angle" target="_blank"><code>Tensor.angle</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle" target="_blank"><code>torch.angle()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.apply_.html#torch.Tensor.apply_" target="_blank"><code>Tensor.apply_</code></a></td>
<td>Applies the function <code>callable</code> to each element in the tensor, replacing each element with the value returned by <code>callable</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html#torch.Tensor.argmax" target="_blank"><code>Tensor.argmax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax" target="_blank"><code>torch.argmax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmin.html#torch.Tensor.argmin" target="_blank"><code>Tensor.argmin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin" target="_blank"><code>torch.argmin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argsort.html#torch.Tensor.argsort" target="_blank"><code>Tensor.argsort</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort" target="_blank"><code>torch.argsort()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere" target="_blank"><code>Tensor.argwhere</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argwhere.html#torch.argwhere" target="_blank"><code>torch.argwhere()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin" target="_blank"><code>Tensor.asin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin" target="_blank"><code>torch.asin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin_.html#torch.Tensor.asin_" target="_blank"><code>Tensor.asin_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin" target="_blank"><code>asin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" target="_blank"><code>Tensor.arcsin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin" target="_blank"><code>torch.arcsin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_" target="_blank"><code>Tensor.arcsin_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" target="_blank"><code>arcsin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" target="_blank"><code>Tensor.as_strided</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided" target="_blank"><code>torch.as_strided()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan" target="_blank"><code>Tensor.atan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan" target="_blank"><code>torch.atan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan_.html#torch.Tensor.atan_" target="_blank"><code>Tensor.atan_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan" target="_blank"><code>atan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan" target="_blank"><code>Tensor.arctan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan" target="_blank"><code>torch.arctan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_" target="_blank"><code>Tensor.arctan_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan" target="_blank"><code>arctan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2" target="_blank"><code>Tensor.atan2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2" target="_blank"><code>torch.atan2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_" target="_blank"><code>Tensor.atan2_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2" target="_blank"><code>atan2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2" target="_blank"><code>Tensor.arctan2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arctan2.html#torch.arctan2" target="_blank"><code>torch.arctan2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_" target="_blank"><code>Tensor.arctan2_</code></a></td>
<td>atan2_(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.all.html#torch.Tensor.all" target="_blank"><code>Tensor.all</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.all.html#torch.all" target="_blank"><code>torch.all()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.any.html#torch.Tensor.any" target="_blank"><code>Tensor.any</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.any.html#torch.any" target="_blank"><code>torch.any()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" target="_blank"><code>Tensor.backward</code></a></td>
<td>Computes the gradient of current tensor w.r.t.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" target="_blank"><code>Tensor.baddbmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm" target="_blank"><code>torch.baddbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_" target="_blank"><code>Tensor.baddbmm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" target="_blank"><code>baddbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli" target="_blank"><code>Tensor.bernoulli</code></a></td>
<td>Returns a result tensor where each \texttt{result[i]}result[i] is independently sampled from \text{Bernoulli}(\texttt{self[i]})Bernoulli(self[i]).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_" target="_blank"><code>Tensor.bernoulli_</code></a></td>
<td>Fills each location of <code>self</code> with an independent sample from \text{Bernoulli}(\texttt{p})Bernoulli(p).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16" target="_blank"><code>Tensor.bfloat16</code></a></td>
<td><code>self.bfloat16()</code> is equivalent to <code>self.to(torch.bfloat16)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bincount.html#torch.Tensor.bincount" target="_blank"><code>Tensor.bincount</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount" target="_blank"><code>torch.bincount()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" target="_blank"><code>Tensor.bitwise_not</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not" target="_blank"><code>torch.bitwise_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_" target="_blank"><code>Tensor.bitwise_not_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" target="_blank"><code>bitwise_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" target="_blank"><code>Tensor.bitwise_and</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and" target="_blank"><code>torch.bitwise_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_" target="_blank"><code>Tensor.bitwise_and_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" target="_blank"><code>bitwise_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" target="_blank"><code>Tensor.bitwise_or</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or" target="_blank"><code>torch.bitwise_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_" target="_blank"><code>Tensor.bitwise_or_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" target="_blank"><code>bitwise_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" target="_blank"><code>Tensor.bitwise_xor</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor" target="_blank"><code>torch.bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_" target="_blank"><code>Tensor.bitwise_xor_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" target="_blank"><code>bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" target="_blank"><code>Tensor.bitwise_left_shift</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" target="_blank"><code>torch.bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_" target="_blank"><code>Tensor.bitwise_left_shift_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" target="_blank"><code>bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" target="_blank"><code>Tensor.bitwise_right_shift</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" target="_blank"><code>torch.bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_" target="_blank"><code>Tensor.bitwise_right_shift_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" target="_blank"><code>bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bmm.html#torch.Tensor.bmm" target="_blank"><code>Tensor.bmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm" target="_blank"><code>torch.bmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bool.html#torch.Tensor.bool" target="_blank"><code>Tensor.bool</code></a></td>
<td><code>self.bool()</code> is equivalent to <code>self.to(torch.bool)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.byte.html#torch.Tensor.byte" target="_blank"><code>Tensor.byte</code></a></td>
<td><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to" target="_blank"><code>Tensor.broadcast_to</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to" target="_blank"><code>torch.broadcast_to()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_" target="_blank"><code>Tensor.cauchy_</code></a></td>
<td>Fills the tensor with numbers drawn from the Cauchy distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil" target="_blank"><code>Tensor.ceil</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil" target="_blank"><code>torch.ceil()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_" target="_blank"><code>Tensor.ceil_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil" target="_blank"><code>ceil()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.char.html#torch.Tensor.char" target="_blank"><code>Tensor.char</code></a></td>
<td><code>self.char()</code> is equivalent to <code>self.to(torch.int8)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky" target="_blank"><code>Tensor.cholesky</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky" target="_blank"><code>torch.cholesky()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse" target="_blank"><code>Tensor.cholesky_inverse</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse" target="_blank"><code>torch.cholesky_inverse()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve" target="_blank"><code>Tensor.cholesky_solve</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve" target="_blank"><code>torch.cholesky_solve()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.chunk.html#torch.Tensor.chunk" target="_blank"><code>Tensor.chunk</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" target="_blank"><code>torch.chunk()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp" target="_blank"><code>Tensor.clamp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp" target="_blank"><code>torch.clamp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" target="_blank"><code>Tensor.clamp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp" target="_blank"><code>clamp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip.html#torch.Tensor.clip" target="_blank"><code>Tensor.clip</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp" target="_blank"><code>clamp()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip_.html#torch.Tensor.clip_" target="_blank"><code>Tensor.clip_</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" target="_blank"><code>clamp_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html#torch.Tensor.clone" target="_blank"><code>Tensor.clone</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone" target="_blank"><code>torch.clone()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous" target="_blank"><code>Tensor.contiguous</code></a></td>
<td>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html#torch.Tensor.copy_" target="_blank"><code>Tensor.copy_</code></a></td>
<td>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj.html#torch.Tensor.conj" target="_blank"><code>Tensor.conj</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj" target="_blank"><code>torch.conj()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" target="_blank"><code>Tensor.conj_physical</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.conj_physical.html#torch.conj_physical" target="_blank"><code>torch.conj_physical()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_" target="_blank"><code>Tensor.conj_physical_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" target="_blank"><code>conj_physical()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj" target="_blank"><code>Tensor.resolve_conj</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.resolve_conj.html#torch.resolve_conj" target="_blank"><code>torch.resolve_conj()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg" target="_blank"><code>Tensor.resolve_neg</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.resolve_neg.html#torch.resolve_neg" target="_blank"><code>torch.resolve_neg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign" target="_blank"><code>Tensor.copysign</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign" target="_blank"><code>torch.copysign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_" target="_blank"><code>Tensor.copysign_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign" target="_blank"><code>copysign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos" target="_blank"><code>Tensor.cos</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" target="_blank"><code>torch.cos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos_.html#torch.Tensor.cos_" target="_blank"><code>Tensor.cos_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos" target="_blank"><code>cos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh" target="_blank"><code>Tensor.cosh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh" target="_blank"><code>torch.cosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_" target="_blank"><code>Tensor.cosh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh" target="_blank"><code>cosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef" target="_blank"><code>Tensor.corrcoef</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.corrcoef.html#torch.corrcoef" target="_blank"><code>torch.corrcoef()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero" target="_blank"><code>Tensor.count_nonzero</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero" target="_blank"><code>torch.count_nonzero()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cov.html#torch.Tensor.cov" target="_blank"><code>Tensor.cov</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cov.html#torch.cov" target="_blank"><code>torch.cov()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh" target="_blank"><code>Tensor.acosh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh" target="_blank"><code>torch.acosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_" target="_blank"><code>Tensor.acosh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh" target="_blank"><code>acosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh" target="_blank"><code>Tensor.arccosh</code></a></td>
<td>acosh() -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_" target="_blank"><code>Tensor.arccosh_</code></a></td>
<td>acosh_() -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html#torch.Tensor.cpu" target="_blank"><code>Tensor.cpu</code></a></td>
<td>Returns a copy of this object in CPU memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cross.html#torch.Tensor.cross" target="_blank"><code>Tensor.cross</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross" target="_blank"><code>torch.cross()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda" target="_blank"><code>Tensor.cuda</code></a></td>
<td>Returns a copy of this object in CUDA memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp" target="_blank"><code>Tensor.logcumsumexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp" target="_blank"><code>torch.logcumsumexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummax.html#torch.Tensor.cummax" target="_blank"><code>Tensor.cummax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax" target="_blank"><code>torch.cummax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummin.html#torch.Tensor.cummin" target="_blank"><code>Tensor.cummin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin" target="_blank"><code>torch.cummin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" target="_blank"><code>Tensor.cumprod</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod" target="_blank"><code>torch.cumprod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_" target="_blank"><code>Tensor.cumprod_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" target="_blank"><code>cumprod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" target="_blank"><code>Tensor.cumsum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum" target="_blank"><code>torch.cumsum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_" target="_blank"><code>Tensor.cumsum_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" target="_blank"><code>cumsum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.chalf.html#torch.Tensor.chalf" target="_blank"><code>Tensor.chalf</code></a></td>
<td><code>self.chalf()</code> is equivalent to <code>self.to(torch.complex32)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat" target="_blank"><code>Tensor.cfloat</code></a></td>
<td><code>self.cfloat()</code> is equivalent to <code>self.to(torch.complex64)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble" target="_blank"><code>Tensor.cdouble</code></a></td>
<td><code>self.cdouble()</code> is equivalent to <code>self.to(torch.complex128)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr" target="_blank"><code>Tensor.data_ptr</code></a></td>
<td>Returns the address of the first element of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad" target="_blank"><code>Tensor.deg2rad</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad" target="_blank"><code>torch.deg2rad()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize" target="_blank"><code>Tensor.dequantize</code></a></td>
<td>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.det.html#torch.Tensor.det" target="_blank"><code>Tensor.det</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.det.html#torch.det" target="_blank"><code>torch.det()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" target="_blank"><code>Tensor.dense_dim</code></a></td>
<td>Return the number of dense dimensions in a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" target="_blank">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach" target="_blank"><code>Tensor.detach</code></a></td>
<td>Returns a new Tensor, detached from the current graph.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach_.html#torch.Tensor.detach_" target="_blank"><code>Tensor.detach_</code></a></td>
<td>Detaches the Tensor from the graph that created it, making it a leaf.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag.html#torch.Tensor.diag" target="_blank"><code>Tensor.diag</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag" target="_blank"><code>torch.diag()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed" target="_blank"><code>Tensor.diag_embed</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed" target="_blank"><code>torch.diag_embed()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat" target="_blank"><code>Tensor.diagflat</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat" target="_blank"><code>torch.diagflat()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal" target="_blank"><code>Tensor.diagonal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal" target="_blank"><code>torch.diagonal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter" target="_blank"><code>Tensor.diagonal_scatter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter" target="_blank"><code>torch.diagonal_scatter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_" target="_blank"><code>Tensor.fill_diagonal_</code></a></td>
<td>Fill the main diagonal of a tensor that has at least 2-dimensions.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmax.html#torch.Tensor.fmax" target="_blank"><code>Tensor.fmax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax" target="_blank"><code>torch.fmax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmin.html#torch.Tensor.fmin" target="_blank"><code>Tensor.fmin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin" target="_blank"><code>torch.fmin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diff.html#torch.Tensor.diff" target="_blank"><code>Tensor.diff</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff" target="_blank"><code>torch.diff()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma" target="_blank"><code>Tensor.digamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma" target="_blank"><code>torch.digamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_" target="_blank"><code>Tensor.digamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma" target="_blank"><code>digamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim" target="_blank"><code>Tensor.dim</code></a></td>
<td>Returns the number of dimensions of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dist.html#torch.Tensor.dist" target="_blank"><code>Tensor.dist</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist" target="_blank"><code>torch.dist()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div" target="_blank"><code>Tensor.div</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.div.html#torch.div" target="_blank"><code>torch.div()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.div_.html#torch.Tensor.div_" target="_blank"><code>Tensor.div_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div" target="_blank"><code>div()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide" target="_blank"><code>Tensor.divide</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide" target="_blank"><code>torch.divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide_.html#torch.Tensor.divide_" target="_blank"><code>Tensor.divide_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide" target="_blank"><code>divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html#torch.Tensor.dot" target="_blank"><code>Tensor.dot</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot" target="_blank"><code>torch.dot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.double.html#torch.Tensor.double" target="_blank"><code>Tensor.double</code></a></td>
<td><code>self.double()</code> is equivalent to <code>self.to(torch.float64)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit" target="_blank"><code>Tensor.dsplit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit" target="_blank"><code>torch.dsplit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.element_size.html#torch.Tensor.element_size" target="_blank"><code>Tensor.element_size</code></a></td>
<td>Returns the size in bytes of an individual element.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq" target="_blank"><code>Tensor.eq</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq" target="_blank"><code>torch.eq()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq_.html#torch.Tensor.eq_" target="_blank"><code>Tensor.eq_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq" target="_blank"><code>eq()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.equal.html#torch.Tensor.equal" target="_blank"><code>Tensor.equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal" target="_blank"><code>torch.equal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf" target="_blank"><code>Tensor.erf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf" target="_blank"><code>torch.erf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf_.html#torch.Tensor.erf_" target="_blank"><code>Tensor.erf_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf" target="_blank"><code>erf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc" target="_blank"><code>Tensor.erfc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc" target="_blank"><code>torch.erfc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_" target="_blank"><code>Tensor.erfc_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc" target="_blank"><code>erfc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" target="_blank"><code>Tensor.erfinv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv" target="_blank"><code>torch.erfinv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_" target="_blank"><code>Tensor.erfinv_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" target="_blank"><code>erfinv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp" target="_blank"><code>Tensor.exp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp" target="_blank"><code>torch.exp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp_.html#torch.Tensor.exp_" target="_blank"><code>Tensor.exp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp" target="_blank"><code>exp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1" target="_blank"><code>Tensor.expm1</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1" target="_blank"><code>torch.expm1()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_" target="_blank"><code>Tensor.expm1_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1" target="_blank"><code>expm1()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand" target="_blank"><code>Tensor.expand</code></a></td>
<td>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as" target="_blank"><code>Tensor.expand_as</code></a></td>
<td>Expand this tensor to the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_" target="_blank"><code>Tensor.exponential_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the exponential distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix" target="_blank"><code>Tensor.fix</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix" target="_blank"><code>torch.fix()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix_.html#torch.Tensor.fix_" target="_blank"><code>Tensor.fix_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix" target="_blank"><code>fix()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_.html#torch.Tensor.fill_" target="_blank"><code>Tensor.fill_</code></a></td>
<td>Fills <code>self</code> tensor with the specified value.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.flatten.html#torch.Tensor.flatten" target="_blank"><code>Tensor.flatten</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten" target="_blank"><code>torch.flatten()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.flip.html#torch.Tensor.flip" target="_blank"><code>Tensor.flip</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip" target="_blank"><code>torch.flip()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr" target="_blank"><code>Tensor.fliplr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr" target="_blank"><code>torch.fliplr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.flipud.html#torch.Tensor.flipud" target="_blank"><code>Tensor.flipud</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud" target="_blank"><code>torch.flipud()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float" target="_blank"><code>Tensor.float</code></a></td>
<td><code>self.float()</code> is equivalent to <code>self.to(torch.float32)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power" target="_blank"><code>Tensor.float_power</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power" target="_blank"><code>torch.float_power()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_" target="_blank"><code>Tensor.float_power_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power" target="_blank"><code>float_power()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor" target="_blank"><code>Tensor.floor</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor" target="_blank"><code>torch.floor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_.html#torch.Tensor.floor_" target="_blank"><code>Tensor.floor_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor" target="_blank"><code>floor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" target="_blank"><code>Tensor.floor_divide</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide" target="_blank"><code>torch.floor_divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_" target="_blank"><code>Tensor.floor_divide_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" target="_blank"><code>floor_divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod" target="_blank"><code>Tensor.fmod</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod" target="_blank"><code>torch.fmod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_" target="_blank"><code>Tensor.fmod_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod" target="_blank"><code>fmod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac" target="_blank"><code>Tensor.frac</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac" target="_blank"><code>torch.frac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac_.html#torch.Tensor.frac_" target="_blank"><code>Tensor.frac_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac" target="_blank"><code>frac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frexp.html#torch.Tensor.frexp" target="_blank"><code>Tensor.frexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp" target="_blank"><code>torch.frexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html#torch.Tensor.gather" target="_blank"><code>Tensor.gather</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather" target="_blank"><code>torch.gather()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd" target="_blank"><code>Tensor.gcd</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd" target="_blank"><code>torch.gcd()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_" target="_blank"><code>Tensor.gcd_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd" target="_blank"><code>gcd()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge" target="_blank"><code>Tensor.ge</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge" target="_blank"><code>torch.ge()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge_.html#torch.Tensor.ge_" target="_blank"><code>Tensor.ge_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge" target="_blank"><code>ge()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" target="_blank"><code>Tensor.greater_equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal" target="_blank"><code>torch.greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_" target="_blank"><code>Tensor.greater_equal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" target="_blank"><code>greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" target="_blank"><code>Tensor.geometric_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the geometric distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf" target="_blank"><code>Tensor.geqrf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf" target="_blank"><code>torch.geqrf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ger.html#torch.Tensor.ger" target="_blank"><code>Tensor.ger</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger" target="_blank"><code>torch.ger()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html#torch.Tensor.get_device" target="_blank"><code>Tensor.get_device</code></a></td>
<td>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt" target="_blank"><code>Tensor.gt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt" target="_blank"><code>torch.gt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt_.html#torch.Tensor.gt_" target="_blank"><code>Tensor.gt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt" target="_blank"><code>gt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater" target="_blank"><code>Tensor.greater</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater" target="_blank"><code>torch.greater()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_.html#torch.Tensor.greater_" target="_blank"><code>Tensor.greater_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater" target="_blank"><code>greater()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html#torch.Tensor.half" target="_blank"><code>Tensor.half</code></a></td>
<td><code>self.half()</code> is equivalent to <code>self.to(torch.float16)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink" target="_blank"><code>Tensor.hardshrink</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink" target="_blank"><code>torch.nn.functional.hardshrink()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside" target="_blank"><code>Tensor.heaviside</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside" target="_blank"><code>torch.heaviside()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.histc.html#torch.Tensor.histc" target="_blank"><code>Tensor.histc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc" target="_blank"><code>torch.histc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.histogram.html#torch.Tensor.histogram" target="_blank"><code>Tensor.histogram</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.histogram.html#torch.histogram" target="_blank"><code>torch.histogram()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit" target="_blank"><code>Tensor.hsplit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit" target="_blank"><code>torch.hsplit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot" target="_blank"><code>Tensor.hypot</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot" target="_blank"><code>torch.hypot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_" target="_blank"><code>Tensor.hypot_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot" target="_blank"><code>hypot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0" target="_blank"><code>Tensor.i0</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0" target="_blank"><code>torch.i0()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0_.html#torch.Tensor.i0_" target="_blank"><code>Tensor.i0_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0" target="_blank"><code>i0()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma" target="_blank"><code>Tensor.igamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma" target="_blank"><code>torch.igamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_" target="_blank"><code>Tensor.igamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma" target="_blank"><code>igamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac" target="_blank"><code>Tensor.igammac</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac" target="_blank"><code>torch.igammac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_" target="_blank"><code>Tensor.igammac_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac" target="_blank"><code>igammac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" target="_blank"><code>Tensor.index_add_</code></a></td>
<td>Accumulate the elements of <code>alpha</code> times <code>source</code> into the <code>self</code> tensor by adding to the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add.html#torch.Tensor.index_add" target="_blank"><code>Tensor.index_add</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" target="_blank"><code>torch.Tensor.index_add_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" target="_blank"><code>Tensor.index_copy_</code></a></td>
<td>Copies the elements of <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank"><code>tensor</code></a> into the <code>self</code> tensor by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy" target="_blank"><code>Tensor.index_copy</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" target="_blank"><code>torch.Tensor.index_copy_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" target="_blank"><code>Tensor.index_fill_</code></a></td>
<td>Fills the elements of the <code>self</code> tensor with value <code>value</code> by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill" target="_blank"><code>Tensor.index_fill</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" target="_blank"><code>torch.Tensor.index_fill_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" target="_blank"><code>Tensor.index_put_</code></a></td>
<td>Puts values from the tensor <code>values</code> into the tensor <code>self</code> using the indices specified in <code>indices</code> (which is a tuple of Tensors).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put.html#torch.Tensor.index_put" target="_blank"><code>Tensor.index_put</code></a></td>
<td>Out-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" target="_blank"><code>index_put_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_" target="_blank"><code>Tensor.index_reduce_</code></a></td>
<td>Accumulate the elements of <code>source</code> into the <code>self</code> tensor by accumulating to the indices in the order given in <code>index</code> using the reduction given by the <code>reduce</code> argument.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce" target="_blank"><code>Tensor.index_reduce</code></a></td>
<td></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_select.html#torch.Tensor.index_select" target="_blank"><code>Tensor.index_select</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select" target="_blank"><code>torch.index_select()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.indices.html#torch.Tensor.indices" target="_blank"><code>Tensor.indices</code></a></td>
<td>Return the indices tensor of a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" target="_blank">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.inner.html#torch.Tensor.inner" target="_blank"><code>Tensor.inner</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner" target="_blank"><code>torch.inner()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.int.html#torch.Tensor.int" target="_blank"><code>Tensor.int</code></a></td>
<td><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr" target="_blank"><code>Tensor.int_repr</code></a></td>
<td>Given a quantized Tensor, <code>self.int_repr()</code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.inverse.html#torch.Tensor.inverse" target="_blank"><code>Tensor.inverse</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse" target="_blank"><code>torch.inverse()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isclose.html#torch.Tensor.isclose" target="_blank"><code>Tensor.isclose</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose" target="_blank"><code>torch.isclose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite" target="_blank"><code>Tensor.isfinite</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite" target="_blank"><code>torch.isfinite()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isinf.html#torch.Tensor.isinf" target="_blank"><code>Tensor.isinf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf" target="_blank"><code>torch.isinf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf" target="_blank"><code>Tensor.isposinf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf" target="_blank"><code>torch.isposinf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf" target="_blank"><code>Tensor.isneginf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf" target="_blank"><code>torch.isneginf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isnan.html#torch.Tensor.isnan" target="_blank"><code>Tensor.isnan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan" target="_blank"><code>torch.isnan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous" target="_blank"><code>Tensor.is_contiguous</code></a></td>
<td>Returns True if <code>self</code> tensor is contiguous in memory in the order specified by memory format.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex" target="_blank"><code>Tensor.is_complex</code></a></td>
<td>Returns True if the data type of <code>self</code> is a complex data type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj" target="_blank"><code>Tensor.is_conj</code></a></td>
<td>Returns True if the conjugate bit of <code>self</code> is set to true.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point" target="_blank"><code>Tensor.is_floating_point</code></a></td>
<td>Returns True if the data type of <code>self</code> is a floating point data type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference" target="_blank"><code>Tensor.is_inference</code></a></td>
<td>See <code>torch.is_inference()</code></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf" target="_blank"><code>Tensor.is_leaf</code></a></td>
<td>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned" target="_blank"><code>Tensor.is_pinned</code></a></td>
<td>Returns true if this tensor resides in pinned memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to" target="_blank"><code>Tensor.is_set_to</code></a></td>
<td>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared" target="_blank"><code>Tensor.is_shared</code></a></td>
<td>Checks if tensor is in shared memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed" target="_blank"><code>Tensor.is_signed</code></a></td>
<td>Returns True if the data type of <code>self</code> is a signed data type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" target="_blank"><code>Tensor.is_sparse</code></a></td>
<td>Is <code>True</code> if the Tensor uses sparse storage layout, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.istft.html#torch.Tensor.istft" target="_blank"><code>Tensor.istft</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft" target="_blank"><code>torch.istft()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isreal.html#torch.Tensor.isreal" target="_blank"><code>Tensor.isreal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal" target="_blank"><code>torch.isreal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item" target="_blank"><code>Tensor.item</code></a></td>
<td>Returns the value of this tensor as a standard Python number.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue" target="_blank"><code>Tensor.kthvalue</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue" target="_blank"><code>torch.kthvalue()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm" target="_blank"><code>Tensor.lcm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm" target="_blank"><code>torch.lcm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_" target="_blank"><code>Tensor.lcm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm" target="_blank"><code>lcm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" target="_blank"><code>Tensor.ldexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp" target="_blank"><code>torch.ldexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_" target="_blank"><code>Tensor.ldexp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" target="_blank"><code>ldexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le" target="_blank"><code>Tensor.le</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.le.html#torch.le" target="_blank"><code>torch.le()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.le_.html#torch.Tensor.le_" target="_blank"><code>Tensor.le_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le" target="_blank"><code>le()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" target="_blank"><code>Tensor.less_equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal" target="_blank"><code>torch.less_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_" target="_blank"><code>Tensor.less_equal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" target="_blank"><code>less_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp" target="_blank"><code>Tensor.lerp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp" target="_blank"><code>torch.lerp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_" target="_blank"><code>Tensor.lerp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp" target="_blank"><code>lerp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" target="_blank"><code>Tensor.lgamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma" target="_blank"><code>torch.lgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_" target="_blank"><code>Tensor.lgamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" target="_blank"><code>lgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log" target="_blank"><code>Tensor.log</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log.html#torch.log" target="_blank"><code>torch.log()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_.html#torch.Tensor.log_" target="_blank"><code>Tensor.log_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log" target="_blank"><code>log()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logdet.html#torch.Tensor.logdet" target="_blank"><code>Tensor.logdet</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet" target="_blank"><code>torch.logdet()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10" target="_blank"><code>Tensor.log10</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10" target="_blank"><code>torch.log10()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10_.html#torch.Tensor.log10_" target="_blank"><code>Tensor.log10_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10" target="_blank"><code>log10()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p" target="_blank"><code>Tensor.log1p</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p" target="_blank"><code>torch.log1p()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_" target="_blank"><code>Tensor.log1p_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p" target="_blank"><code>log1p()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2" target="_blank"><code>Tensor.log2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2" target="_blank"><code>torch.log2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2_.html#torch.Tensor.log2_" target="_blank"><code>Tensor.log2_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2" target="_blank"><code>log2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_" target="_blank"><code>Tensor.log_normal_</code></a></td>
<td>Fills <code>self</code> tensor with numbers samples from the log-normal distribution parameterized by the given mean \mu<em>μ</em> and standard deviation \sigma<em>σ</em>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp" target="_blank"><code>Tensor.logaddexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp" target="_blank"><code>torch.logaddexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2" target="_blank"><code>Tensor.logaddexp2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2" target="_blank"><code>torch.logaddexp2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp" target="_blank"><code>Tensor.logsumexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp" target="_blank"><code>torch.logsumexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" target="_blank"><code>Tensor.logical_and</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and" target="_blank"><code>torch.logical_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_" target="_blank"><code>Tensor.logical_and_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" target="_blank"><code>logical_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" target="_blank"><code>Tensor.logical_not</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not" target="_blank"><code>torch.logical_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_" target="_blank"><code>Tensor.logical_not_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" target="_blank"><code>logical_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" target="_blank"><code>Tensor.logical_or</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or" target="_blank"><code>torch.logical_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_" target="_blank"><code>Tensor.logical_or_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" target="_blank"><code>logical_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" target="_blank"><code>Tensor.logical_xor</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor" target="_blank"><code>torch.logical_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_" target="_blank"><code>Tensor.logical_xor_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" target="_blank"><code>logical_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit" target="_blank"><code>Tensor.logit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit" target="_blank"><code>torch.logit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit_.html#torch.Tensor.logit_" target="_blank"><code>Tensor.logit_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit" target="_blank"><code>logit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.long.html#torch.Tensor.long" target="_blank"><code>Tensor.long</code></a></td>
<td><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt" target="_blank"><code>Tensor.lt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt" target="_blank"><code>torch.lt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt_.html#torch.Tensor.lt_" target="_blank"><code>Tensor.lt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt" target="_blank"><code>lt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less" target="_blank"><code>Tensor.less</code></a></td>
<td>lt(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_.html#torch.Tensor.less_" target="_blank"><code>Tensor.less_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less" target="_blank"><code>less()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu.html#torch.Tensor.lu" target="_blank"><code>Tensor.lu</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu" target="_blank"><code>torch.lu()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve" target="_blank"><code>Tensor.lu_solve</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve" target="_blank"><code>torch.lu_solve()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass" target="_blank"><code>Tensor.as_subclass</code></a></td>
<td>Makes a <code>cls</code> instance with the same data pointer as <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.map_.html#torch.Tensor.map_" target="_blank"><code>Tensor.map_</code></a></td>
<td>Applies <code>callable</code> for each element in <code>self</code> tensor and the given <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank"><code>tensor</code></a> and stores the results in <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" target="_blank"><code>Tensor.masked_scatter_</code></a></td>
<td>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is True.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter" target="_blank"><code>Tensor.masked_scatter</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" target="_blank"><code>torch.Tensor.masked_scatter_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" target="_blank"><code>Tensor.masked_fill_</code></a></td>
<td>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is True.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill" target="_blank"><code>Tensor.masked_fill</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" target="_blank"><code>torch.Tensor.masked_fill_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select" target="_blank"><code>Tensor.masked_select</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select" target="_blank"><code>torch.masked_select()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matmul.html#torch.Tensor.matmul" target="_blank"><code>Tensor.matmul</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul" target="_blank"><code>torch.matmul()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power" target="_blank"><code>Tensor.matrix_power</code></a></td>
<td>NOTE<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power" target="_blank"><code>matrix_power()</code></a> is deprecated, use <a href="https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power" target="_blank"><code>torch.linalg.matrix_power()</code></a> instead.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp" target="_blank"><code>Tensor.matrix_exp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp" target="_blank"><code>torch.matrix_exp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.max.html#torch.Tensor.max" target="_blank"><code>Tensor.max</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.max.html#torch.max" target="_blank"><code>torch.max()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.maximum.html#torch.Tensor.maximum" target="_blank"><code>Tensor.maximum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum" target="_blank"><code>torch.maximum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean" target="_blank"><code>Tensor.mean</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean" target="_blank"><code>torch.mean()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean" target="_blank"><code>Tensor.nanmean</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nanmean.html#torch.nanmean" target="_blank"><code>torch.nanmean()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.median.html#torch.Tensor.median" target="_blank"><code>Tensor.median</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.median.html#torch.median" target="_blank"><code>torch.median()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian" target="_blank"><code>Tensor.nanmedian</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian" target="_blank"><code>torch.nanmedian()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.min.html#torch.Tensor.min" target="_blank"><code>Tensor.min</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.min.html#torch.min" target="_blank"><code>torch.min()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.minimum.html#torch.Tensor.minimum" target="_blank"><code>Tensor.minimum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum" target="_blank"><code>torch.minimum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mm.html#torch.Tensor.mm" target="_blank"><code>Tensor.mm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm" target="_blank"><code>torch.mm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.smm.html#torch.Tensor.smm" target="_blank"><code>Tensor.smm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.smm.html#torch.smm" target="_blank"><code>torch.smm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mode.html#torch.Tensor.mode" target="_blank"><code>Tensor.mode</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode" target="_blank"><code>torch.mode()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.movedim.html#torch.Tensor.movedim" target="_blank"><code>Tensor.movedim</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim" target="_blank"><code>torch.movedim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis" target="_blank"><code>Tensor.moveaxis</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis" target="_blank"><code>torch.moveaxis()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.msort.html#torch.Tensor.msort" target="_blank"><code>Tensor.msort</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort" target="_blank"><code>torch.msort()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul" target="_blank"><code>Tensor.mul</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul" target="_blank"><code>torch.mul()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul_.html#torch.Tensor.mul_" target="_blank"><code>Tensor.mul_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul" target="_blank"><code>mul()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply" target="_blank"><code>Tensor.multiply</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply" target="_blank"><code>torch.multiply()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_" target="_blank"><code>Tensor.multiply_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply" target="_blank"><code>multiply()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial" target="_blank"><code>Tensor.multinomial</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial" target="_blank"><code>torch.multinomial()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mv.html#torch.Tensor.mv" target="_blank"><code>Tensor.mv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv" target="_blank"><code>torch.mv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" target="_blank"><code>Tensor.mvlgamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma" target="_blank"><code>torch.mvlgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_" target="_blank"><code>Tensor.mvlgamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" target="_blank"><code>mvlgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nansum.html#torch.Tensor.nansum" target="_blank"><code>Tensor.nansum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum" target="_blank"><code>torch.nansum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow.html#torch.Tensor.narrow" target="_blank"><code>Tensor.narrow</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow" target="_blank"><code>torch.narrow()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy" target="_blank"><code>Tensor.narrow_copy</code></a></td>
<td>See <code>torch.narrow_copy()</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension" target="_blank"><code>Tensor.ndimension</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim" target="_blank"><code>dim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" target="_blank"><code>Tensor.nan_to_num</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num" target="_blank"><code>torch.nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_" target="_blank"><code>Tensor.nan_to_num_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" target="_blank"><code>nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne" target="_blank"><code>Tensor.ne</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne" target="_blank"><code>torch.ne()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne_.html#torch.Tensor.ne_" target="_blank"><code>Tensor.ne_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne" target="_blank"><code>ne()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" target="_blank"><code>Tensor.not_equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal" target="_blank"><code>torch.not_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_" target="_blank"><code>Tensor.not_equal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" target="_blank"><code>not_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg" target="_blank"><code>Tensor.neg</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg" target="_blank"><code>torch.neg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg_.html#torch.Tensor.neg_" target="_blank"><code>Tensor.neg_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg" target="_blank"><code>neg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative" target="_blank"><code>Tensor.negative</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative" target="_blank"><code>torch.negative()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative_.html#torch.Tensor.negative_" target="_blank"><code>Tensor.negative_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative" target="_blank"><code>negative()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nelement.html#torch.Tensor.nelement" target="_blank"><code>Tensor.nelement</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel" target="_blank"><code>numel()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" target="_blank"><code>Tensor.nextafter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter" target="_blank"><code>torch.nextafter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_" target="_blank"><code>Tensor.nextafter_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" target="_blank"><code>nextafter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero" target="_blank"><code>Tensor.nonzero</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero" target="_blank"><code>torch.nonzero()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm" target="_blank"><code>Tensor.norm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm" target="_blank"><code>torch.norm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_" target="_blank"><code>Tensor.normal_</code></a></td>
<td>Fills <code>self</code> tensor with elements samples from the normal distribution parameterized by <a href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean" target="_blank"><code>mean</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std" target="_blank"><code>std</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel" target="_blank"><code>Tensor.numel</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel" target="_blank"><code>torch.numel()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy" target="_blank"><code>Tensor.numpy</code></a></td>
<td>Returns the tensor as a NumPy <code>ndarray</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr" target="_blank"><code>Tensor.orgqr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr" target="_blank"><code>torch.orgqr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr" target="_blank"><code>Tensor.ormqr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr" target="_blank"><code>torch.ormqr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.outer.html#torch.Tensor.outer" target="_blank"><code>Tensor.outer</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer" target="_blank"><code>torch.outer()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html#torch.Tensor.permute" target="_blank"><code>Tensor.permute</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute" target="_blank"><code>torch.permute()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory" target="_blank"><code>Tensor.pin_memory</code></a></td>
<td>Copies the tensor to pinned memory, if it's not already pinned.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse" target="_blank"><code>Tensor.pinverse</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse" target="_blank"><code>torch.pinverse()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" target="_blank"><code>Tensor.polygamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma" target="_blank"><code>torch.polygamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_" target="_blank"><code>Tensor.polygamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" target="_blank"><code>polygamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.positive.html#torch.Tensor.positive" target="_blank"><code>Tensor.positive</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive" target="_blank"><code>torch.positive()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow" target="_blank"><code>Tensor.pow</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow" target="_blank"><code>torch.pow()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_" target="_blank"><code>Tensor.pow_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow" target="_blank"><code>pow()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.prod.html#torch.Tensor.prod" target="_blank"><code>Tensor.prod</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod" target="_blank"><code>torch.prod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_" target="_blank"><code>Tensor.put_</code></a></td>
<td>Copies the elements from <code>source</code> into the positions specified by <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.qr.html#torch.Tensor.qr" target="_blank"><code>Tensor.qr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr" target="_blank"><code>torch.qr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme" target="_blank"><code>Tensor.qscheme</code></a></td>
<td>Returns the quantization scheme of a given QTensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.quantile.html#torch.Tensor.quantile" target="_blank"><code>Tensor.quantile</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile" target="_blank"><code>torch.quantile()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile" target="_blank"><code>Tensor.nanquantile</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile" target="_blank"><code>torch.nanquantile()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale" target="_blank"><code>Tensor.q_scale</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point" target="_blank"><code>Tensor.q_zero_point</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales" target="_blank"><code>Tensor.q_per_channel_scales</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points" target="_blank"><code>Tensor.q_per_channel_zero_points</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis" target="_blank"><code>Tensor.q_per_channel_axis</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg" target="_blank"><code>Tensor.rad2deg</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg" target="_blank"><code>torch.rad2deg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_" target="_blank"><code>Tensor.random_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the discrete uniform distribution over <code>[from, to - 1]</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ravel.html#torch.Tensor.ravel" target="_blank"><code>Tensor.ravel</code></a></td>
<td>see <a href="https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel" target="_blank"><code>torch.ravel()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" target="_blank"><code>Tensor.reciprocal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal" target="_blank"><code>torch.reciprocal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_" target="_blank"><code>Tensor.reciprocal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" target="_blank"><code>reciprocal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream" target="_blank"><code>Tensor.record_stream</code></a></td>
<td>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook" target="_blank"><code>Tensor.register_hook</code></a></td>
<td>Registers a backward hook.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder" target="_blank"><code>Tensor.remainder</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder" target="_blank"><code>torch.remainder()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_" target="_blank"><code>Tensor.remainder_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder" target="_blank"><code>remainder()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm" target="_blank"><code>Tensor.renorm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm" target="_blank"><code>torch.renorm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_" target="_blank"><code>Tensor.renorm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm" target="_blank"><code>renorm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat" target="_blank"><code>Tensor.repeat</code></a></td>
<td>Repeats this tensor along the specified dimensions.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave" target="_blank"><code>Tensor.repeat_interleave</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave" target="_blank"><code>torch.repeat_interleave()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad" target="_blank"><code>Tensor.requires_grad</code></a></td>
<td>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_" target="_blank"><code>Tensor.requires_grad_</code></a></td>
<td>Change if autograd should record operations on this tensor: sets this tensor's <code>requires_grad</code> attribute in-place.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape" target="_blank"><code>Tensor.reshape</code></a></td>
<td>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as" target="_blank"><code>Tensor.reshape_as</code></a></td>
<td>Returns this tensor as the same shape as <code>other</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_" target="_blank"><code>Tensor.resize_</code></a></td>
<td>Resizes <code>self</code> tensor to the specified size.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_" target="_blank"><code>Tensor.resize_as_</code></a></td>
<td>Resizes the <code>self</code> tensor to be the same size as the specified <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank"><code>tensor</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad" target="_blank"><code>Tensor.retain_grad</code></a></td>
<td>Enables this Tensor to have their <code>grad</code> populated during <code>backward()</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad" target="_blank"><code>Tensor.retains_grad</code></a></td>
<td>Is <code>True</code> if this Tensor is non-leaf and its <code>grad</code> is enabled to be populated during <code>backward()</code>, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.roll.html#torch.Tensor.roll" target="_blank"><code>Tensor.roll</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll" target="_blank"><code>torch.roll()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rot90.html#torch.Tensor.rot90" target="_blank"><code>Tensor.rot90</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90" target="_blank"><code>torch.rot90()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round" target="_blank"><code>Tensor.round</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round" target="_blank"><code>torch.round()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round_.html#torch.Tensor.round_" target="_blank"><code>Tensor.round_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round" target="_blank"><code>round()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" target="_blank"><code>Tensor.rsqrt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt" target="_blank"><code>torch.rsqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_" target="_blank"><code>Tensor.rsqrt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" target="_blank"><code>rsqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter.html#torch.Tensor.scatter" target="_blank"><code>Tensor.scatter</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" target="_blank"><code>torch.Tensor.scatter_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" target="_blank"><code>Tensor.scatter_</code></a></td>
<td>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" target="_blank"><code>Tensor.scatter_add_</code></a></td>
<td>Adds all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor in a similar fashion as <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" target="_blank"><code>scatter_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add" target="_blank"><code>Tensor.scatter_add</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" target="_blank"><code>torch.Tensor.scatter_add_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" target="_blank"><code>Tensor.scatter_reduce_</code></a></td>
<td>Reduces all values from the <code>src</code> tensor to the indices specified in the <code>index</code> tensor in the <code>self</code> tensor using the applied reduction defined via the <code>reduce</code> argument (<code>"sum"</code>, <code>"prod"</code>, <code>"mean"</code>, <code>"amax"</code>, <code>"amin"</code>).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce" target="_blank"><code>Tensor.scatter_reduce</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" target="_blank"><code>torch.Tensor.scatter_reduce_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.select.html#torch.Tensor.select" target="_blank"><code>Tensor.select</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.select.html#torch.select" target="_blank"><code>torch.select()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter" target="_blank"><code>Tensor.select_scatter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.select_scatter.html#torch.select_scatter" target="_blank"><code>torch.select_scatter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html#torch.Tensor.set_" target="_blank"><code>Tensor.set_</code></a></td>
<td>Sets the underlying storage, size, and strides.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_" target="_blank"><code>Tensor.share_memory_</code></a></td>
<td>Moves the underlying storage to shared memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.short.html#torch.Tensor.short" target="_blank"><code>Tensor.short</code></a></td>
<td><code>self.short()</code> is equivalent to <code>self.to(torch.int16)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" target="_blank"><code>Tensor.sigmoid</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid" target="_blank"><code>torch.sigmoid()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_" target="_blank"><code>Tensor.sigmoid_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" target="_blank"><code>sigmoid()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign" target="_blank"><code>Tensor.sign</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign" target="_blank"><code>torch.sign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign_.html#torch.Tensor.sign_" target="_blank"><code>Tensor.sign_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign" target="_blank"><code>sign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.signbit.html#torch.Tensor.signbit" target="_blank"><code>Tensor.signbit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit" target="_blank"><code>torch.signbit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn" target="_blank"><code>Tensor.sgn</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn" target="_blank"><code>torch.sgn()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_" target="_blank"><code>Tensor.sgn_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn" target="_blank"><code>sgn()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin" target="_blank"><code>Tensor.sin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" target="_blank"><code>torch.sin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin_.html#torch.Tensor.sin_" target="_blank"><code>Tensor.sin_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin" target="_blank"><code>sin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc" target="_blank"><code>Tensor.sinc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc" target="_blank"><code>torch.sinc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_" target="_blank"><code>Tensor.sinc_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc" target="_blank"><code>sinc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh" target="_blank"><code>Tensor.sinh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh" target="_blank"><code>torch.sinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_" target="_blank"><code>Tensor.sinh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh" target="_blank"><code>sinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh" target="_blank"><code>Tensor.asinh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh" target="_blank"><code>torch.asinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_" target="_blank"><code>Tensor.asinh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh" target="_blank"><code>asinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" target="_blank"><code>Tensor.arcsinh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh" target="_blank"><code>torch.arcsinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_" target="_blank"><code>Tensor.arcsinh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" target="_blank"><code>arcsinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.size.html#torch.Tensor.size" target="_blank"><code>Tensor.size</code></a></td>
<td>Returns the size of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet" target="_blank"><code>Tensor.slogdet</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet" target="_blank"><code>torch.slogdet()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter" target="_blank"><code>Tensor.slice_scatter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.slice_scatter.html#torch.slice_scatter" target="_blank"><code>torch.slice_scatter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sort.html#torch.Tensor.sort" target="_blank"><code>Tensor.sort</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort" target="_blank"><code>torch.sort()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.split.html#torch.Tensor.split" target="_blank"><code>Tensor.split</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split" target="_blank"><code>torch.split()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask" target="_blank"><code>Tensor.sparse_mask</code></a></td>
<td>Returns a new <a href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" target="_blank">sparse tensor</a> with values from a strided tensor <code>self</code> filtered by the indices of the sparse tensor <code>mask</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" target="_blank"><code>Tensor.sparse_dim</code></a></td>
<td>Return the number of sparse dimensions in a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" target="_blank">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" target="_blank"><code>Tensor.sqrt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt" target="_blank"><code>torch.sqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_" target="_blank"><code>Tensor.sqrt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" target="_blank"><code>sqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square" target="_blank"><code>Tensor.square</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.square.html#torch.square" target="_blank"><code>torch.square()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.square_.html#torch.Tensor.square_" target="_blank"><code>Tensor.square_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square" target="_blank"><code>square()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" target="_blank"><code>Tensor.squeeze</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze" target="_blank"><code>torch.squeeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_" target="_blank"><code>Tensor.squeeze_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" target="_blank"><code>squeeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.std.html#torch.Tensor.std" target="_blank"><code>Tensor.std</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std" target="_blank"><code>torch.std()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.stft.html#torch.Tensor.stft" target="_blank"><code>Tensor.stft</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft" target="_blank"><code>torch.stft()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage.html#torch.Tensor.storage" target="_blank"><code>Tensor.storage</code></a></td>
<td>Returns the underlying storage.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset" target="_blank"><code>Tensor.storage_offset</code></a></td>
<td>Returns <code>self</code> tensor's offset in the underlying storage in terms of number of storage elements (not bytes).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type" target="_blank"><code>Tensor.storage_type</code></a></td>
<td>Returns the type of the underlying storage.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html#torch.Tensor.stride" target="_blank"><code>Tensor.stride</code></a></td>
<td>Returns the stride of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub" target="_blank"><code>Tensor.sub</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub" target="_blank"><code>torch.sub()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub_.html#torch.Tensor.sub_" target="_blank"><code>Tensor.sub_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub" target="_blank"><code>sub()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract" target="_blank"><code>Tensor.subtract</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract" target="_blank"><code>torch.subtract()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_" target="_blank"><code>Tensor.subtract_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract" target="_blank"><code>subtract()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum" target="_blank"><code>Tensor.sum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum" target="_blank"><code>torch.sum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size" target="_blank"><code>Tensor.sum_to_size</code></a></td>
<td>Sum <code>this</code> tensor to <code>size</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.svd.html#torch.Tensor.svd" target="_blank"><code>Tensor.svd</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd" target="_blank"><code>torch.svd()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes" target="_blank"><code>Tensor.swapaxes</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes" target="_blank"><code>torch.swapaxes()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims" target="_blank"><code>Tensor.swapdims</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims" target="_blank"><code>torch.swapdims()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.symeig.html#torch.Tensor.symeig" target="_blank"><code>Tensor.symeig</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig" target="_blank"><code>torch.symeig()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t" target="_blank"><code>Tensor.t</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.t.html#torch.t" target="_blank"><code>torch.t()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.t_.html#torch.Tensor.t_" target="_blank"><code>Tensor.t_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t" target="_blank"><code>t()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split" target="_blank"><code>Tensor.tensor_split</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split" target="_blank"><code>torch.tensor_split()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tile.html#torch.Tensor.tile" target="_blank"><code>Tensor.tile</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile" target="_blank"><code>torch.tile()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to" target="_blank"><code>Tensor.to</code></a></td>
<td>Performs Tensor dtype and/or device conversion.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn" target="_blank"><code>Tensor.to_mkldnn</code></a></td>
<td>Returns a copy of the tensor in <code>torch.mkldnn</code> layout.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.take.html#torch.Tensor.take" target="_blank"><code>Tensor.take</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.take.html#torch.take" target="_blank"><code>torch.take()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim" target="_blank"><code>Tensor.take_along_dim</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim" target="_blank"><code>torch.take_along_dim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan" target="_blank"><code>Tensor.tan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan" target="_blank"><code>torch.tan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan_.html#torch.Tensor.tan_" target="_blank"><code>Tensor.tan_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan" target="_blank"><code>tan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh" target="_blank"><code>Tensor.tanh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh" target="_blank"><code>torch.tanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_" target="_blank"><code>Tensor.tanh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh" target="_blank"><code>tanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh" target="_blank"><code>Tensor.atanh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh" target="_blank"><code>torch.atanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_" target="_blank"><code>Tensor.atanh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh" target="_blank"><code>atanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" target="_blank"><code>Tensor.arctanh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh" target="_blank"><code>torch.arctanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_" target="_blank"><code>Tensor.arctanh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" target="_blank"><code>arctanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist" target="_blank"><code>Tensor.tolist</code></a></td>
<td>Returns the tensor as a (nested) list.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html#torch.Tensor.topk" target="_blank"><code>Tensor.topk</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk" target="_blank"><code>torch.topk()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense" target="_blank"><code>Tensor.to_dense</code></a></td>
<td>Creates a strided copy of <code>self</code> if <code>self</code> is not a strided tensor, otherwise returns <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse" target="_blank"><code>Tensor.to_sparse</code></a></td>
<td>Returns a sparse copy of the tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr" target="_blank"><code>Tensor.to_sparse_csr</code></a></td>
<td>Convert a tensor to compressed row storage format (CSR).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc" target="_blank"><code>Tensor.to_sparse_csc</code></a></td>
<td>Convert a tensor to compressed column storage (CSC) format.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr" target="_blank"><code>Tensor.to_sparse_bsr</code></a></td>
<td>Convert a CSR tensor to a block sparse row (BSR) storage format of given blocksize.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc" target="_blank"><code>Tensor.to_sparse_bsc</code></a></td>
<td>Convert a CSR tensor to a block sparse column (BSC) storage format of given blocksize.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trace.html#torch.Tensor.trace" target="_blank"><code>Tensor.trace</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace" target="_blank"><code>torch.trace()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose" target="_blank"><code>Tensor.transpose</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose" target="_blank"><code>torch.transpose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_" target="_blank"><code>Tensor.transpose_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose" target="_blank"><code>transpose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve" target="_blank"><code>Tensor.triangular_solve</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve" target="_blank"><code>torch.triangular_solve()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril" target="_blank"><code>Tensor.tril</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril" target="_blank"><code>torch.tril()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril_.html#torch.Tensor.tril_" target="_blank"><code>Tensor.tril_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril" target="_blank"><code>tril()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu" target="_blank"><code>Tensor.triu</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu" target="_blank"><code>torch.triu()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu_.html#torch.Tensor.triu_" target="_blank"><code>Tensor.triu_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu" target="_blank"><code>triu()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide" target="_blank"><code>Tensor.true_divide</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide" target="_blank"><code>torch.true_divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" target="_blank"><code>Tensor.true_divide_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" target="_blank"><code>true_divide_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc" target="_blank"><code>Tensor.trunc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc" target="_blank"><code>torch.trunc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_" target="_blank"><code>Tensor.trunc_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc" target="_blank"><code>trunc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.type.html#torch.Tensor.type" target="_blank"><code>Tensor.type</code></a></td>
<td>Returns the type if dtype is not provided, else casts this object to the specified type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.type_as.html#torch.Tensor.type_as" target="_blank"><code>Tensor.type_as</code></a></td>
<td>Returns this tensor cast to the type of the given tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unbind.html#torch.Tensor.unbind" target="_blank"><code>Tensor.unbind</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind" target="_blank"><code>torch.unbind()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" target="_blank"><code>Tensor.unflatten</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten" target="_blank"><code>torch.unflatten()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html#torch.Tensor.unfold" target="_blank"><code>Tensor.unfold</code></a></td>
<td>Returns a view of the original tensor which contains all slices of size <code>size</code> from <code>self</code> tensor in the dimension <code>dimension</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_" target="_blank"><code>Tensor.uniform_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the continuous uniform distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique.html#torch.Tensor.unique" target="_blank"><code>Tensor.unique</code></a></td>
<td>Returns the unique elements of the input tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive" target="_blank"><code>Tensor.unique_consecutive</code></a></td>
<td>Eliminates all but the first element from every consecutive group of equivalent elements.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" target="_blank"><code>Tensor.unsqueeze</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze" target="_blank"><code>torch.unsqueeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_" target="_blank"><code>Tensor.unsqueeze_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" target="_blank"><code>unsqueeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.values.html#torch.Tensor.values" target="_blank"><code>Tensor.values</code></a></td>
<td>Return the values tensor of a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" target="_blank">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.var.html#torch.Tensor.var" target="_blank"><code>Tensor.var</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.var.html#torch.var" target="_blank"><code>torch.var()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.vdot.html#torch.Tensor.vdot" target="_blank"><code>Tensor.vdot</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot" target="_blank"><code>torch.vdot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view" target="_blank"><code>Tensor.view</code></a></td>
<td>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html#torch.Tensor.view_as" target="_blank"><code>Tensor.view_as</code></a></td>
<td>View this tensor as the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit" target="_blank"><code>Tensor.vsplit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit" target="_blank"><code>torch.vsplit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.where.html#torch.Tensor.where" target="_blank"><code>Tensor.where</code></a></td>
<td><code>self.where(condition, y)</code> is equivalent to <code>torch.where(condition, self, y)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" target="_blank"><code>Tensor.xlogy</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy" target="_blank"><code>torch.xlogy()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_" target="_blank"><code>Tensor.xlogy_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" target="_blank"><code>xlogy()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_" target="_blank"><code>Tensor.zero_</code></a></td>
<td>Fills <code>self</code> tensor with zeros.</td>
</tr>
</tbody>
</table>
<h2 id="storage">1.1 storage</h2>
<blockquote>
<p><a href="https://www.jianshu.com/p/ebd7f6395bf4" target="_blank">tensor的数据结构、storage()、stride()、storage_offset()</a></p>
</blockquote>
<p>pytorch中一个tensor对象分为<strong>头信息区（Tensor）</strong>和<strong>存储区（Storage）</strong>两部分</p>
<p><a data-lightbox="37c84140-34c6-46b2-bf5a-bfd1b911b21a" data-title="tensor结构示例图" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/pytorch学习_进阶知识/tensor结构示例图.webp" target="_blank"><img alt="tensor结构示例图" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/pytorch学习_进阶知识/tensor结构示例图.webp"/></a>
头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以<strong>连续一维数组</strong>的形式放在存储区，由torch.Storage实例管理着</p>
<p><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p>
<blockquote>
<p>获取tensor的storage</p>
</blockquote>
<pre><code class="lang-python">a = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">4.0</span>],[<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>],[<span class="hljs-number">3.0</span>, <span class="hljs-number">5.0</span>]])
a.storage()
Out[<span class="hljs-number">0</span>]: 
 <span class="hljs-number">1.0</span>
 <span class="hljs-number">4.0</span>
 <span class="hljs-number">2.0</span>
 <span class="hljs-number">1.0</span>
 <span class="hljs-number">3.0</span>
 <span class="hljs-number">5.0</span>
[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size <span class="hljs-number">6</span>]

a.storage()[<span class="hljs-number">2</span>] = <span class="hljs-number">9</span>

id(a.storage())
Out[<span class="hljs-number">1</span>]: <span class="hljs-number">1343354913168</span>
</code></pre>
<h1 id="实例">2 实例</h1>
<blockquote>
<p><a href="https://github.com/AccumulateMore/CV" target="_blank">小土堆+李沐课程笔记</a></p>
<p><a href="https://www.bilibili.com/video/BV1hE411t7RN/?p=11&amp;vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】</a></p>
</blockquote>
<h2 id="pytorch加载数据">2.1 Pytorch加载数据</h2>
<p>Pytorch中加载数据需要Dataset、Dataloader。</p>
<ul>
<li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li>
<li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li>
</ul>
<h2 id="tensorboard">2.2 Tensorboard</h2>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-comment"># 准备的测试数据集</span>
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor())               
<span class="hljs-comment"># batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      </span>
test_loader = DataLoader(dataset=test_data,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-keyword">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-keyword">False</span>)      
<span class="hljs-comment"># 用for循环取出DataLoader打包好的四个数据</span>
writer = SummaryWriter(<span class="hljs-string">"logs"</span>)
step = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:
    imgs, targets = data <span class="hljs-comment"># 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span>
    writer.add_images(<span class="hljs-string">"test_data"</span>,imgs,step)
    step = step + <span class="hljs-number">1</span>

writer.close()
</code></pre>
<h2 id="transforms">2.3 Transforms</h2>
<p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。</p>
<p>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

img_path = <span class="hljs-string">"Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg"</span>
img = Image.open(img_path)  

tensor_trans = transforms.ToTensor()  <span class="hljs-comment"># 创建 transforms.ToTensor类 的实例化对象</span>
tensor_img = tensor_trans(img)  <span class="hljs-comment"># 调用 transforms.ToTensor类 的__call__的魔术方法   </span>
print(tensor_img)
</code></pre>
<h2 id="torchvision数据集">2.4 torchvision数据集</h2>
<p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。</p>
<p>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
train_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,download=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># root为存放数据集的相对路线</span>
test_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,download=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># train=True是训练集，train=False是测试集  </span>

print(test_set[<span class="hljs-number">0</span>])       <span class="hljs-comment"># 输出的3是target </span>
print(test_set.classes)  <span class="hljs-comment"># 测试数据集中有多少种</span>

img, target = test_set[<span class="hljs-number">0</span>] <span class="hljs-comment"># 分别获得图片、target</span>
print(img)
print(target)

print(test_set.classes[target]) <span class="hljs-comment"># 3号target对应的种类</span>
img.show()
</code></pre>
<h2 id="损失函数">2.5 损失函数</h2>
<p>① Loss损失函数一方面计算实际输出和目标之间的差距。</p>
<p>② Loss损失函数另一方面为我们更新输出提供一定的依据</p>
<blockquote>
<p>L1loss损失函数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss
inputs = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)
targets = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)
inputs = torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
targets = torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
loss = L1Loss()  <span class="hljs-comment"># 默认为 maen</span>
result = loss(inputs,targets)
print(result)
</code></pre>
<blockquote>
<p>MSE损失函数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
inputs = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)
targets = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)
inputs = torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
targets = torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
loss_mse = nn.MSELoss()
result_mse = loss_mse(inputs,targets)
print(result_mse)
</code></pre>
<blockquote>
<p>交叉熵损失函数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

x = torch.tensor([<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>])
y = torch.tensor([<span class="hljs-number">1</span>])
x = torch.reshape(x,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)) <span class="hljs-comment"># 1的 batch_size，有三类</span>
loss_cross = nn.CrossEntropyLoss()
result_cross = loss_cross(x,y)
print(result_cross)
</code></pre>
<h2 id="优化器">2.6 优化器</h2>
<p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。</p>
<p>② 梯度要清零，如果梯度不清零会导致梯度累加</p>
<pre><code class="lang-python">loss = nn.CrossEntropyLoss() <span class="hljs-comment"># 交叉熵    </span>
tudui = Tudui()
optim = torch.optim.SGD(tudui.parameters(),lr=<span class="hljs-number">0.01</span>)   <span class="hljs-comment"># 随机梯度下降优化器</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
    imgs, targets = data
    outputs = tudui(imgs)
    result_loss = loss(outputs, targets) <span class="hljs-comment"># 计算实际输出与目标输出的差距</span>
    optim.zero_grad()  <span class="hljs-comment"># 梯度清零</span>
    result_loss.backward() <span class="hljs-comment"># 反向传播，计算损失函数的梯度</span>
    optim.step()   <span class="hljs-comment"># 根据梯度，对网络的参数进行调优</span>
    print(result_loss) <span class="hljs-comment"># 对数据只看了一遍，只看了一轮，所以loss下降不大</span>
</code></pre>
<blockquote>
<p>神经网络学习率优化</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn 
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">64</span>,drop_last=<span class="hljs-keyword">True</span>)

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tudui</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Tudui, self).__init__()        
        self.model1 = Sequential(
            Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),
            MaxPool2d(<span class="hljs-number">2</span>),
            Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),
            MaxPool2d(<span class="hljs-number">2</span>),
            Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),
            MaxPool2d(<span class="hljs-number">2</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>,<span class="hljs-number">10</span>)
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x

loss = nn.CrossEntropyLoss() <span class="hljs-comment"># 交叉熵    </span>
tudui = Tudui()
optim = torch.optim.SGD(tudui.parameters(),lr=<span class="hljs-number">0.01</span>)   <span class="hljs-comment"># 随机梯度下降优化器</span>
scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=<span class="hljs-number">5</span>, gamma=<span class="hljs-number">0.1</span>) <span class="hljs-comment"># 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">20</span>):
    running_loss = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
        imgs, targets = data
        outputs = tudui(imgs)
        result_loss = loss(outputs, targets) <span class="hljs-comment"># 计算实际输出与目标输出的差距</span>
        optim.zero_grad()  <span class="hljs-comment"># 梯度清零</span>
        result_loss.backward() <span class="hljs-comment"># 反向传播，计算损失函数的梯度</span>
        optim.step()   <span class="hljs-comment"># 根据梯度，对网络的参数进行调优</span>
        scheduler.step() <span class="hljs-comment"># 学习率太小了，所以20个轮次后，相当于没走多少</span>
        running_loss = running_loss + result_loss
    print(running_loss) <span class="hljs-comment"># 对这一轮所有误差的总和</span>
</code></pre>
<h2 id="网络模型使用及修改">2.7 网络模型使用及修改</h2>
<blockquote>
<p>网络模型添加</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
vgg16_true = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span>
vgg16_true.add_module(<span class="hljs-string">'add_linear'</span>,nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>)) <span class="hljs-comment"># 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span>

print(vgg16_true)
</code></pre>
<blockquote>
<p>网络模型修改</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

vgg16_false = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">False</span>) <span class="hljs-comment"># 没有预训练的参数     </span>
print(vgg16_false)
vgg16_false.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)
print(vgg16_false)
</code></pre>
<h2 id="网络模型保存与读取">2.8 网络模型保存与读取</h2>
<blockquote>
<p>模型结构 + 模型参数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">False</span>)
torch.save(vgg16,<span class="hljs-string">"./model/vgg16_method1.pth"</span>) <span class="hljs-comment"># 保存方式一：模型结构 + 模型参数      </span>
print(vgg16)

model = torch.load(<span class="hljs-string">"./model/vgg16_method1.pth"</span>) <span class="hljs-comment"># 保存方式一对应的加载模型    </span>
print(model)
</code></pre>
<blockquote>
<p>模型参数（官方推荐），不保存网络模型结构</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">False</span>)
torch.save(vgg16.state_dict(),<span class="hljs-string">"./model/vgg16_method2.pth"</span>) <span class="hljs-comment"># 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span>
print(vgg16)

model = torch.load(<span class="hljs-string">"./model/vgg16_method2.pth"</span>) <span class="hljs-comment"># 导入模型参数   </span>
print(model)
</code></pre>
<h2 id="固定模型参数">2.9 固定模型参数</h2>
<p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p>
<ol>
<li>一个是设置不要更新参数的<a href="https://so.csdn.net/so/search?q=网络层&amp;spm=1001.2101.3001.7020" target="_blank">网络层</a>为false</li>
<li>另一个就是在定义优化器时只传入要更新的参数</li>
</ol>
<p>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim


<span class="hljs-comment"># 定义一个简单的网络</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">net</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_class=<span class="hljs-number">3</span>)</span>:</span>
        super(net, self).__init__()
        self.fc1 = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>)
        self.fc2 = nn.Linear(<span class="hljs-number">4</span>, num_class)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-keyword">return</span> self.fc2(self.fc1(x))


model = net()

<span class="hljs-comment"># 冻结fc1层的参数</span>
<span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():
    <span class="hljs-keyword">if</span> <span class="hljs-string">"fc1"</span> <span class="hljs-keyword">in</span> name:
        param.requires_grad = <span class="hljs-keyword">False</span>

loss_fn = nn.CrossEntropyLoss()

<span class="hljs-comment"># 只传入requires_grad = True的参数</span>
optimizer = optim.SGD(filter(<span class="hljs-keyword">lambda</span> p: p.requires_grad, net.parameters(), lr=<span class="hljs-number">1e-2</span>)
print(<span class="hljs-string">"model.fc1.weight"</span>, model.fc1.weight)
print(<span class="hljs-string">"model.fc2.weight"</span>, model.fc2.weight)

model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
    x = torch.randn((<span class="hljs-number">3</span>, <span class="hljs-number">8</span>))
    label = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, [<span class="hljs-number">3</span>]).long()
    output = model(x)

    loss = loss_fn(output, label)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(<span class="hljs-string">"model.fc1.weight"</span>, model.fc1.weight)
print(<span class="hljs-string">"model.fc2.weight"</span>, model.fc2.weight)
</code></pre>
<h2 id="训练流程">2.10 训练流程</h2>
<blockquote>
<p>DataLoader加载数据集</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       

<span class="hljs-comment"># length 长度</span>
train_data_size = len(train_data)
test_data_size = len(test_data)
<span class="hljs-comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span>
print(<span class="hljs-string">"训练数据集的长度：{}"</span>.format(train_data_size))
print(<span class="hljs-string">"测试数据集的长度：{}"</span>.format(test_data_size))

<span class="hljs-comment"># 利用 Dataloader 来加载数据集</span>
train_dataloader = DataLoader(train_data_size, batch_size=<span class="hljs-number">64</span>)        
test_dataloader = DataLoader(test_data_size, batch_size=<span class="hljs-number">64</span>)
</code></pre>
<blockquote>
<p>测试网络正确</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-comment"># 搭建神经网络</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tudui</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Tudui, self).__init__()        
        self.model1 = nn.Sequential(
            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),  <span class="hljs-comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span>
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Flatten(),  <span class="hljs-comment"># 展平后变成 64*4*4 了</span>
            nn.Linear(<span class="hljs-number">64</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>,<span class="hljs-number">64</span>),
            nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">10</span>)
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    tudui = Tudui()
    input = torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))
    output = tudui(input)
    print(output.shape)  <span class="hljs-comment"># 测试输出的尺寸是不是我们想要的</span>
</code></pre>
<blockquote>
<p>网络训练数据</p>
</blockquote>
<p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。</p>
<p>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p>
<p>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p>
<p>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。</p>
<p>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-comment"># from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tudui</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Tudui, self).__init__()        
        self.model1 = nn.Sequential(
            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),  <span class="hljs-comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span>
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Flatten(),  <span class="hljs-comment"># 展平后变成 64*4*4 了</span>
            nn.Linear(<span class="hljs-number">64</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>,<span class="hljs-number">64</span>),
            nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">10</span>)
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       

<span class="hljs-comment"># length 长度</span>
train_data_size = len(train_data)
test_data_size = len(test_data)
<span class="hljs-comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span>
print(<span class="hljs-string">"训练数据集的长度：{}"</span>.format(train_data_size))
print(<span class="hljs-string">"测试数据集的长度：{}"</span>.format(test_data_size))

<span class="hljs-comment"># 利用 Dataloader 来加载数据集</span>
train_dataloader = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)        
test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)

<span class="hljs-comment"># 创建网络模型</span>
tudui = Tudui() 

<span class="hljs-comment"># 损失函数</span>
loss_fn = nn.CrossEntropyLoss() <span class="hljs-comment"># 交叉熵，fn 是 fuction 的缩写</span>

<span class="hljs-comment"># 优化器</span>
learning = <span class="hljs-number">0.01</span>  <span class="hljs-comment"># 1e-2 就是 0.01 的意思</span>
optimizer = torch.optim.SGD(tudui.parameters(),learning)   <span class="hljs-comment"># 随机梯度下降优化器  </span>

<span class="hljs-comment"># 设置网络的一些参数</span>
<span class="hljs-comment"># 记录训练的次数</span>
total_train_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 记录测试的次数</span>
total_test_step = <span class="hljs-number">0</span>

<span class="hljs-comment"># 训练的轮次</span>
epoch = <span class="hljs-number">10</span>

<span class="hljs-comment"># 添加 tensorboard</span>
writer = SummaryWriter(<span class="hljs-string">"logs"</span>)

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):
    print(<span class="hljs-string">"-----第 {} 轮训练开始-----"</span>.format(i+<span class="hljs-number">1</span>))

    <span class="hljs-comment"># 训练步骤开始</span>
    tudui.train() <span class="hljs-comment"># 当网络中有dropout层、batchnorm层时，这些层能起作用</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_dataloader:
        imgs, targets = data
        outputs = tudui(imgs)
        loss = loss_fn(outputs, targets) <span class="hljs-comment"># 计算实际输出与目标输出的差距</span>

        <span class="hljs-comment"># 优化器对模型调优</span>
        optimizer.zero_grad()  <span class="hljs-comment"># 梯度清零</span>
        loss.backward() <span class="hljs-comment"># 反向传播，计算损失函数的梯度</span>
        optimizer.step()   <span class="hljs-comment"># 根据梯度，对网络的参数进行调优</span>

        total_train_step = total_train_step + <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            print(<span class="hljs-string">"训练次数：{}，Loss：{}"</span>.format(total_train_step,loss.item()))  <span class="hljs-comment"># 方式二：获得loss值</span>
            writer.add_scalar(<span class="hljs-string">"train_loss"</span>,loss.item(),total_train_step)

    <span class="hljs-comment"># 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span>
    tudui.eval()  <span class="hljs-comment"># 当网络中有dropout层、batchnorm层时，这些层不能起作用</span>
    total_test_loss = <span class="hljs-number">0</span>
    total_accuracy = <span class="hljs-number">0</span>
    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 没有梯度了</span>
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader: <span class="hljs-comment"># 测试数据集提取数据</span>
            imgs, targets = data
            outputs = tudui(imgs)
            loss = loss_fn(outputs, targets) <span class="hljs-comment"># 仅data数据在网络模型上的损失</span>
            total_test_loss = total_test_loss + loss.item() <span class="hljs-comment"># 所有loss</span>
            accuracy = (outputs.argmax(<span class="hljs-number">1</span>) == targets).sum()
            total_accuracy = total_accuracy + accuracy

    print(<span class="hljs-string">"整体测试集上的Loss：{}"</span>.format(total_test_loss))
    print(<span class="hljs-string">"整体测试集上的正确率：{}"</span>.format(total_accuracy/test_data_size))
    writer.add_scalar(<span class="hljs-string">"test_loss"</span>,total_test_loss,total_test_step)
    writer.add_scalar(<span class="hljs-string">"test_accuracy"</span>,total_accuracy/test_data_size,total_test_step)  
    total_test_step = total_test_step + <span class="hljs-number">1</span>

    torch.save(tudui, <span class="hljs-string">"./model/tudui_{}.pth"</span>.format(i)) <span class="hljs-comment"># 保存每一轮训练后的结果</span>
    <span class="hljs-comment">#torch.save(tudui.state_dict(),"tudui_{}.path".format(i)) # 保存方式二         </span>
    print(<span class="hljs-string">"模型已保存"</span>)

writer.close()
</code></pre>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2024-03-06 03:16:50
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: pytorch学习_基础知识.md" class="navigation navigation-prev" href="pytorch学习_基础知识.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: transformer.md" class="navigation navigation-next" href="transformer.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":53040,"date":"2024/02/20 14:38:53","cover":"https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp","title":"pytorch学习_进阶知识.md","tags":["pytorch"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/蕾姆5.webp","mathjax":true,"categories":["deep-learning"],"description":"pytorch学习_进阶知识","level":"1.9","depth":1,"next":{"title":"transformer.md","level":"1.10","depth":1,"path":"chapters/transformer.md","ref":"chapters/transformer.md","articles":[]},"previous":{"title":"pytorch学习_基础知识.md","level":"1.8","depth":1,"path":"chapters/pytorch学习_基础知识.md","ref":"chapters/pytorch学习_基础知识.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2021","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipay打赏","button":"欢迎打赏","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechat打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"深度学习知识驿站","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"欢迎留下评论交流~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"深度学习相关学习记录","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"书籍主页":"https://study.hycbook.com"}},"gitbook":"*","description":"记录 深度学习 的学习和一些技巧的使用"},"file":{"path":"chapters/pytorch学习_进阶知识.md","mtime":"2024-03-06T03:16:50.260Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2024-03-06T03:18:25.446Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
