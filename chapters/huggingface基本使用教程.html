<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>huggingface基本使用教程.md · 深度学习相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="huggingface基本使用教程" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="nlp关键词和摘要提取技术整理.html" rel="next"/>
<link href="dl_in_vision_field.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">书籍主页</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="LLM Tokenizer分词系列.html" id="chapter_id_1">
<a href="LLM Tokenizer分词系列.html">
<b>1.2.</b>
                    
                    LLM Tokenizer分词系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="LLM模型微调系列.html" id="chapter_id_2">
<a href="LLM模型微调系列.html">
<b>1.3.</b>
                    
                    LLM模型微调系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="LLM模型部署调试推理.html" id="chapter_id_3">
<a href="LLM模型部署调试推理.html">
<b>1.4.</b>
                    
                    LLM模型部署调试推理.md
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="dl_in_vision_field.html" id="chapter_id_4">
<a href="dl_in_vision_field.html">
<b>1.5.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter active" data-level="1.6" data-path="huggingface基本使用教程.html" id="chapter_id_5">
<a href="huggingface基本使用教程.html">
<b>1.6.</b>
                    
                    huggingface基本使用教程.md
            
                </a>
</li>
<li class="chapter" data-level="1.7" data-path="nlp关键词和摘要提取技术整理.html" id="chapter_id_6">
<a href="nlp关键词和摘要提取技术整理.html">
<b>1.7.</b>
                    
                    nlp关键词和摘要提取技术整理.md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="pytorch学习_基础知识.html" id="chapter_id_7">
<a href="pytorch学习_基础知识.html">
<b>1.8.</b>
                    
                    pytorch学习_基础知识.md
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="pytorch学习_进阶知识.html" id="chapter_id_8">
<a href="pytorch学习_进阶知识.html">
<b>1.9.</b>
                    
                    pytorch学习_进阶知识.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="transformer.html" id="chapter_id_9">
<a href="transformer.html">
<b>1.10.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="图像分割算法.html" id="chapter_id_10">
<a href="图像分割算法.html">
<b>1.11.</b>
                    
                    图像分割算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="图像分类算法.html" id="chapter_id_11">
<a href="图像分类算法.html">
<b>1.12.</b>
                    
                    图像分类算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="图神经网络.html" id="chapter_id_12">
<a href="图神经网络.html">
<b>1.13.</b>
                    
                    图神经网络.md
            
                </a>
</li>
<li class="chapter" data-level="1.14" data-path="数据标注工具.html" id="chapter_id_13">
<a href="数据标注工具.html">
<b>1.14.</b>
                    
                    数据标注工具.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="深度学习核心之优化器.html" id="chapter_id_14">
<a href="深度学习核心之优化器.html">
<b>1.15.</b>
                    
                    深度学习核心之优化器.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="深度学习核心之损失函数.html" id="chapter_id_15">
<a href="深度学习核心之损失函数.html">
<b>1.16.</b>
                    
                    深度学习核心之损失函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="深度学习核心之激活函数.html" id="chapter_id_16">
<a href="深度学习核心之激活函数.html">
<b>1.17.</b>
                    
                    深度学习核心之激活函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.18" data-path="深度学习核心基础知识点.html" id="chapter_id_17">
<a href="深度学习核心基础知识点.html">
<b>1.18.</b>
                    
                    深度学习核心基础知识点.md
            
                </a>
</li>
<li class="chapter" data-level="1.19" data-path="深度学习模型压缩技术.html" id="chapter_id_18">
<a href="深度学习模型压缩技术.html">
<b>1.19.</b>
                    
                    深度学习模型压缩技术.md
            
                </a>
</li>
<li class="chapter" data-level="1.20" data-path="目标检测与跟踪算法.html" id="chapter_id_19">
<a href="目标检测与跟踪算法.html">
<b>1.20.</b>
                    
                    目标检测与跟踪算法.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">huggingface基本使用教程.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#huggingface">1 huggingface</a></li><ul><li><span class="title-icon"></span><a href="#概述">1.1 概述</a></li><li><span class="title-icon"></span><a href="#安装">1.2 安装</a></li><li><span class="title-icon"></span><a href="#模型下载加速">1.3 模型下载加速</a></li><ul><li><span class="title-icon"></span><a href="#git-clone">1.3.1 git clone</a></li><li><span class="title-icon"></span><a href="#huggingface-cli">1.3.2 huggingface-cli</a></li><li><span class="title-icon"></span><a href="#多线程下载器">1.3.3 多线程下载器</a></li><li><span class="title-icon"></span><a href="#镜像网站">1.3.4 镜像网站</a></li></ul><li><span class="title-icon"></span><a href="#快速开始">1.4 快速开始</a></li></ul><li><span class="title-icon"></span><a href="#datasets">2 datasets</a></li><ul><li><span class="title-icon"></span><a href="#安装_1">2.1 安装</a></li><li><span class="title-icon"></span><a href="#快速开始_1">2.2 快速开始</a></li><ul><li><span class="title-icon"></span><a href="#视觉">2.2.1 视觉</a></li><li><span class="title-icon"></span><a href="#nlp">2.2.2 nlp</a></li></ul><li><span class="title-icon"></span><a href="#概述_1">2.3 概述</a></li><ul><li><span class="title-icon"></span><a href="#datacollator类">2.3.1 DataCollator类</a></li></ul><li><span class="title-icon"></span><a href="#加载数据集">2.4 加载数据集</a></li><li><span class="title-icon"></span><a href="#进阶加载数据集">2.5 进阶加载数据集</a></li><li><span class="title-icon"></span><a href="#探索数据集">2.6 探索数据集</a></li><li><span class="title-icon"></span><a href="#preprocess处理">2.7 Preprocess处理</a></li><li><span class="title-icon"></span><a href="#构建数据集">2.8 构建数据集</a></li><li><span class="title-icon"></span><a href="#分享数据集">2.9 分享数据集</a></li></ul><li><span class="title-icon"></span><a href="#评估指标">3 评估指标</a></li><ul><li><span class="title-icon"></span><a href="#安装_2">3.1 安装</a></li><li><span class="title-icon"></span><a href="#快速开始_2">3.2 快速开始</a></li><ul><li><span class="title-icon"></span><a href="#指标种类">3.2.1 指标种类</a></li><li><span class="title-icon"></span><a href="#指标加载">3.2.2 指标加载</a></li><li><span class="title-icon"></span><a href="#指标计算">3.2.3 指标计算</a></li><li><span class="title-icon"></span><a href="#结果存储">3.2.4 结果存储</a></li><li><span class="title-icon"></span><a href="#可视化">3.2.5 可视化</a></li><li><span class="title-icon"></span><a href="#选择合适指标">3.2.6 选择合适指标</a></li></ul></ul><li><span class="title-icon"></span><a href="#transformers">4 transformers</a></li><ul><li><span class="title-icon"></span><a href="#概述_2">4.1 概述</a></li><li><span class="title-icon"></span><a href="#安装_3">4.2 安装</a></li><li><span class="title-icon"></span><a href="#快速开始_3">4.3 快速开始</a></li><ul><li><span class="title-icon"></span><a href="#pipeline">4.3.1 Pipeline</a></li><li><span class="title-icon"></span><a href="#autoclass">4.3.2 AutoClass</a></li><li><span class="title-icon"></span><a href="#autoconfig">4.3.3 AutoConfig</a></li><li><span class="title-icon"></span><a href="#tokenizer">4.3.4 tokenizer</a></li><li><span class="title-icon"></span><a href="#trainer">4.3.5 Trainer</a></li></ul></ul><li><span class="title-icon"></span><a href="#教程">5 教程</a></li><ul><li><span class="title-icon"></span><a href="#模型训练">5.1 模型训练</a></li><li><span class="title-icon"></span><a href="#分布式加速">5.2 分布式加速</a></li><li><span class="title-icon"></span><a href="#示例代码">5.3 示例代码</a></li></ul><li><span class="title-icon"></span><a href="#peft模块">6 PEFT模块</a></li><ul><li><span class="title-icon"></span><a href="#基本使用">6.1 基本使用</a></li><ul><li><span class="title-icon"></span><a href="#加载peft-adapter">6.1.1 加载PEFT adapter</a></li><li><span class="title-icon"></span><a href="#以8或4位加载">6.1.2 以8或4位加载</a></li><li><span class="title-icon"></span><a href="#添加新adapter">6.1.3 添加新adapter</a></li><li><span class="title-icon"></span><a href="#启用和禁用adapters">6.1.4 启用和禁用adapters</a></li><li><span class="title-icon"></span><a href="#训练peft-adapter">6.1.5 训练PEFT adapter</a></li></ul></ul><li><span class="title-icon"></span><a href="#其他模块">7 其他模块</a></li><ul><li><span class="title-icon"></span><a href="#核心类">7.1 核心类</a></li><ul><li><span class="title-icon"></span><a href="#modeloutput">7.1.1 ModelOutput</a></li><li><span class="title-icon"></span><a href="#pretrainedmodel">7.1.2 PreTrainedModel</a></li></ul><li><span class="title-icon"></span><a href="#autotrain">7.2 AutoTrain</a></li><li><span class="title-icon"></span><a href="#gradio">7.3 Gradio</a></li><li><span class="title-icon"></span><a href="#diffusers">7.4 Diffusers</a></li><li><span class="title-icon"></span><a href="#accelerate">7.5 Accelerate</a></li></ul></ul></div><a href="#huggingface" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><hr/>
<h1 id="huggingface">1 huggingface</h1>
<h2 id="概述">1.1 概述</h2>
<blockquote>
<p><a href="https://huggingface.co/docs" target="_blank">Hugging Face</a></p>
<p><a href="https://huggingface.co/tasks" target="_blank">官网任务分类和示例</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/624393922" target="_blank">LLM高效调参_PEFT库简介及使用</a></p>
</blockquote>
<p><code>Hugging Face</code>是一个知名的开源社区和公司，专注于自然语言处理(NLP)和机器学习(ML)领域。他们开发了许多流行的开源工具和库，使得构建和应用NLP模型更加便捷</p>
<p>Hugging face起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，10,000个数据集，变成了机器学习界的github</p>
<blockquote>
<p>在这里主要有以下大家需要的资源</p>
</blockquote>
<ol>
<li><p><strong>Datasets</strong>：数据集，以及数据集的下载地址</p>
</li>
<li><p><strong>Models</strong>：包括各种处理CV和NLP等任务的模型，上面模型都是可以免费获得</p>
<p>主要包括计算机视觉、自然语言处理、语音处理、多模态、表格处理、强化学习</p>
</li>
<li><p><strong>course</strong>：免费的nlp课程</p>
</li>
<li><p><strong>docs</strong>：文档</p>
</li>
</ol>
<blockquote>
<p>展开细节</p>
</blockquote>
<ul>
<li><strong>Computer Vision(计算机视觉任务)</strong>：包括lmage Classification(图像分类)，lmage Segmentation(图像分割)、zero-Shot lmage Classification(零样本图像分类)、lmage-to-Image(图像到图像的任务)、Unconditional lmage Generation(无条件图像生成)、Object Detection(目标检测)、Video Classification(视频分类)、Depth Estimation(深度估计，估计拍摄者距离图像各处的距离)</li>
<li><strong>Natural Language Processing(自然语言处理)</strong>：包括Translation(机器翻译)、Fill-Mask(填充掩码，预测句子中被遮掩的词)、Token Classification(词分类)、Sentence Similarity(句子相似度)、Question Answering(问答系统)，Summarization(总结，缩句)、Zero-Shot Classification (零样本分类)、Text Classification(文本分类)、Text2Text(文本到文本的生成)、Text Generation(文本生成)、Conversational(聊天)、Table Question Answer(表问答，1.预测表格中被遮掩单词2.数字推理，判断句子是否被表格数据支持)</li>
<li><strong>Audio(语音)</strong>：Automatic Speech Recognition(语音识别)、Audio Classification(语音分类)、Text-to-Speech(文本到语音的生成)、Audio-to-Audio(语音到语音的生成)、Voice Activity Detection(声音检测、检测识别出需要的声音部分)</li>
<li><strong>Multimodal(多模态)</strong>：Feature Extraction(特征提取)、Text-to-Image(文本到图像)、Visual Question Answering(视觉问答)、Image2Text(图像到文本)、Document Question Answering(文档问答)</li>
<li><strong>Tabular(表格)</strong>：Tabular Classification(表分类)、Tabular Regression(表回归)</li>
<li><strong>Reinforcement Learning(强化学习)</strong>：Reinforcement Learning(强化学习)、Robotics(机器人)</li>
</ul>
<h2 id="安装">1.2 安装</h2>
<blockquote>
<p>安装transformers库</p>
</blockquote>
<pre><code class="lang-sh">pip install transformers
</code></pre>
<h2 id="模型下载加速">1.3 模型下载加速</h2>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/663712983" target="_blank">如何快速下载huggingface模型——全方法总结</a></p>
</blockquote>
<h3 id="git-clone">1.3.1 git clone</h3>
<p>官方提供了 <code>git clone repo_url</code> 的方式下载，这种方法相当简单，然而却是<strong>最不推荐直接用的方法</strong>，缺点有二：</p>
<ol>
<li>不支持断点续传，断了重头再来</li>
<li>clone 会下载历史版本占用磁盘空间，即使没有历史版本</li>
</ol>
<h3 id="huggingface-cli">1.3.2 huggingface-cli</h3>
<blockquote>
<p><a href="https://huggingface.co/docs/hub/models-adding-libraries#download-files-from-the-hub" target="_blank">hf的模型下载工具: download-files-from-the-hub</a></p>
</blockquote>
<p><code>huggingface-cli</code> 隶属于 <code>huggingface_hub</code> 库，不仅可以下载模型、数据，还可以可以登录huggingface、上传模型、数据等</p>
<p>huggingface-cli 属于官方工具，其长期支持肯定是最好的。<strong>优先推荐！</strong></p>
<ol>
<li><p>安装依赖</p>
<pre><code class="lang-bash">pip install -U huggingface_hub
</code></pre>
<p>注意：huggingface_hub 依赖于 Python&gt;=3.8，此外需要安装 0.17.0 及以上的版本，推荐0.19.0+</p>
</li>
<li><p>基本用法</p>
<pre><code class="lang-bash">huggingface-cli download --resume-download bigscience/bloom-560m --local-dir bloom-560m
</code></pre>
</li>
<li><p>下载数据集</p>
<pre><code class="lang-bash">huggingface-cli download --resume-download --repo-type dataset lavita/medical-qa-shared-task-v1-toy
</code></pre>
<p>值得注意的是，有个<code>--local-dir-use-symlinks False</code> 参数可选，因为huggingface的工具链默认会使用符号链接来存储下载的文件，导致<code>--local-dir</code>指定的目录中都是一些<strong>链接文件</strong>，真实模型则存储在<code>~/.cache/huggingface</code>下，如果不喜欢这个可以用 <code>--local-dir-use-symlinks False</code>取消这个逻辑</p>
</li>
</ol>
<h3 id="多线程下载器">1.3.3 多线程下载器</h3>
<p>多线程加速是一种有效、显著提高下载速度的方法</p>
<p>经典多线程工具推荐两个：<strong>IDM、Aria2</strong>。 IDM 适用于 Windows、aria2 适用于 Linux。本文头图就是 IDM 工具。因此获取URL后，可以利用这些多线程工具来下载。以我的一次实测为例，单线程700KB/s，IDM 8线程 6MB/s。千兆宽带下，利用IDM能跑到80MB/s+</p>
<p>手动获取仓库中所有 URL 比较麻烦，<strong>作者写了一个命令行脚本</strong> <a href="https://link.zhihu.com/?target=https%3A//padeoe.com/file/hfd/hfd.sh" target="_blank">hdf.sh</a>（<a href="https://link.zhihu.com/?target=https%3A//gist.github.com/padeoe/697678ab8e528b85a2a7bddafea1fa4f" target="_blank">Gitst链接</a>），结合自动获取 url 以及 <code>aria2</code> 多线程下载，适合于 Linux</p>
<pre><code class="lang-bash"><span class="hljs-meta">#!/usr/bin/env bash</span>
<span class="hljs-comment"># Color definitions</span>
RED=<span class="hljs-string">'\033[0;31m'</span>
GREEN=<span class="hljs-string">'\033[0;32m'</span>
YELLOW=<span class="hljs-string">'\033[1;33m'</span>
NC=<span class="hljs-string">'\033[0m'</span> <span class="hljs-comment"># No Color</span>

<span class="hljs-built_in">trap</span> <span class="hljs-string">'printf "${YELLOW}\nDownload interrupted. If you re-run the command, you can resume the download from the breakpoint.\n${NC}"; exit 1'</span> INT

<span class="hljs-function"><span class="hljs-title">display_help</span></span>() {
    cat &lt;&lt; EOF
Usage:
  hfd &lt;model_id&gt; [--include include_pattern] [--exclude exclude_pattern] [--hf_username username] [--hf_token token] [--tool wget|aria2c] [-x threads] [--dataset]

Description:
  Downloads a model or dataset from Hugging Face using the provided model ID.

Parameters:
  model_id        The Hugging Face model ID <span class="hljs-keyword">in</span> the format <span class="hljs-string">'repo/model_name'</span>.
  --include       (Optional) Flag to specify a string pattern to include files <span class="hljs-keyword">for</span> downloading.
  --exclude       (Optional) Flag to specify a string pattern to exclude files from downloading.
  exclude_pattern The pattern to match against filenames <span class="hljs-keyword">for</span> exclusion.
  --hf_username   (Optional) Hugging Face username <span class="hljs-keyword">for</span> authentication.
  --hf_token      (Optional) Hugging Face token <span class="hljs-keyword">for</span> authentication.
  --tool          (Optional) Download tool to use. Can be wget (default) or aria2c.
  -x              (Optional) Number of download threads <span class="hljs-keyword">for</span> aria2c.
  --dataset       (Optional) Flag to indicate downloading a dataset.

Example:
  hfd bigscience/bloom-560m --exclude safetensors
  hfd meta-llama/Llama-2-7b --hf_username myuser --hf_token mytoken --tool aria2c -x 8
  hfd lavita/medical-qa-shared-task-v1-toy --dataset
EOF
    <span class="hljs-built_in">exit</span> 1
}

MODEL_ID=<span class="hljs-variable">$1</span>
<span class="hljs-built_in">shift</span>

<span class="hljs-comment"># Default values</span>
TOOL=<span class="hljs-string">"wget"</span>
THREADS=1
HF_ENDPOINT=<span class="hljs-variable">${HF_ENDPOINT:-"https://huggingface.co"}</span>

<span class="hljs-keyword">while</span> [[ <span class="hljs-variable">$#</span> <span class="hljs-_">-gt</span> 0 ]]; <span class="hljs-keyword">do</span>
    <span class="hljs-keyword">case</span> <span class="hljs-variable">$1</span> <span class="hljs-keyword">in</span>
        --include) INCLUDE_PATTERN=<span class="hljs-string">"<span class="hljs-variable">$2</span>"</span>; <span class="hljs-built_in">shift</span> 2 ;;
        --exclude) EXCLUDE_PATTERN=<span class="hljs-string">"<span class="hljs-variable">$2</span>"</span>; <span class="hljs-built_in">shift</span> 2 ;;
        --hf_username) HF_USERNAME=<span class="hljs-string">"<span class="hljs-variable">$2</span>"</span>; <span class="hljs-built_in">shift</span> 2 ;;
        --hf_token) HF_TOKEN=<span class="hljs-string">"<span class="hljs-variable">$2</span>"</span>; <span class="hljs-built_in">shift</span> 2 ;;
        --tool) TOOL=<span class="hljs-string">"<span class="hljs-variable">$2</span>"</span>; <span class="hljs-built_in">shift</span> 2 ;;
        -x) THREADS=<span class="hljs-string">"<span class="hljs-variable">$2</span>"</span>; <span class="hljs-built_in">shift</span> 2 ;;
        --dataset) DATASET=1; <span class="hljs-built_in">shift</span> ;;
        *) <span class="hljs-built_in">shift</span> ;;
    <span class="hljs-keyword">esac</span>
<span class="hljs-keyword">done</span>

<span class="hljs-comment"># Check if aria2, wget, curl, git, and git-lfs are installed</span>
<span class="hljs-function"><span class="hljs-title">check_command</span></span>() {
    <span class="hljs-keyword">if</span> ! <span class="hljs-built_in">command</span> -v <span class="hljs-variable">$1</span> &amp;&gt;/dev/null; <span class="hljs-keyword">then</span>
        <span class="hljs-built_in">echo</span> <span class="hljs-_">-e</span> <span class="hljs-string">"<span class="hljs-variable">${RED}</span><span class="hljs-variable">$1</span> is not installed. Please install it first.<span class="hljs-variable">${NC}</span>"</span>
        <span class="hljs-built_in">exit</span> 1
    <span class="hljs-keyword">fi</span>
}

[[ <span class="hljs-string">"<span class="hljs-variable">$TOOL</span>"</span> == <span class="hljs-string">"aria2c"</span> ]] &amp;&amp; check_<span class="hljs-built_in">command</span> aria2c
[[ <span class="hljs-string">"<span class="hljs-variable">$TOOL</span>"</span> == <span class="hljs-string">"wget"</span> ]] &amp;&amp; check_<span class="hljs-built_in">command</span> wget
check_<span class="hljs-built_in">command</span> curl; check_<span class="hljs-built_in">command</span> git; check_<span class="hljs-built_in">command</span> git-lfs

[[ -z <span class="hljs-string">"<span class="hljs-variable">$MODEL_ID</span>"</span> || <span class="hljs-string">"<span class="hljs-variable">$MODEL_ID</span>"</span> =~ ^-h ]] &amp;&amp; display_<span class="hljs-built_in">help</span>

MODEL_DIR=<span class="hljs-string">"<span class="hljs-variable">${MODEL_ID#*/}</span>"</span>

<span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$DATASET</span>"</span> == 1 ]]; <span class="hljs-keyword">then</span>
    MODEL_ID=<span class="hljs-string">"datasets/<span class="hljs-variable">$MODEL_ID</span>"</span>
<span class="hljs-keyword">fi</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"Downloading to ./<span class="hljs-variable">$MODEL_DIR</span>"</span>

<span class="hljs-keyword">if</span> [ <span class="hljs-_">-d</span> <span class="hljs-string">"<span class="hljs-variable">$MODEL_DIR</span>/.git"</span> ]; <span class="hljs-keyword">then</span>
    <span class="hljs-built_in">printf</span> <span class="hljs-string">"<span class="hljs-variable">${YELLOW}</span>%s exists, Skip Clone.\n<span class="hljs-variable">${NC}</span>"</span> <span class="hljs-string">"<span class="hljs-variable">$MODEL_DIR</span>"</span>
    <span class="hljs-built_in">cd</span> <span class="hljs-string">"<span class="hljs-variable">$MODEL_DIR</span>"</span> &amp;&amp; GIT_LFS_SKIP_SMUDGE=1 git pull || { <span class="hljs-built_in">printf</span> <span class="hljs-string">"Git pull failed.\n"</span>; <span class="hljs-built_in">exit</span> 1; }
<span class="hljs-keyword">else</span>
    REPO_URL=<span class="hljs-string">"<span class="hljs-variable">$HF_ENDPOINT</span>/<span class="hljs-variable">$MODEL_ID</span>"</span>
    GIT_REFS_URL=<span class="hljs-string">"<span class="hljs-variable">${REPO_URL}</span>/info/refs?service=git-upload-pack"</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"Test GIT_REFS_URL: <span class="hljs-variable">$GIT_REFS_URL</span>"</span>
    response=$(curl <span class="hljs-_">-s</span> -o /dev/null -w <span class="hljs-string">"%{http_code}"</span> <span class="hljs-string">"<span class="hljs-variable">$GIT_REFS_URL</span>"</span>)
    <span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$response</span>"</span> == <span class="hljs-string">"401"</span> ] || [ <span class="hljs-string">"<span class="hljs-variable">$response</span>"</span> == <span class="hljs-string">"403"</span> ]; <span class="hljs-keyword">then</span>
        <span class="hljs-keyword">if</span> [[ -z <span class="hljs-string">"<span class="hljs-variable">$HF_USERNAME</span>"</span> || -z <span class="hljs-string">"<span class="hljs-variable">$HF_TOKEN</span>"</span> ]]; <span class="hljs-keyword">then</span>
            <span class="hljs-built_in">printf</span> <span class="hljs-string">"<span class="hljs-variable">${RED}</span>HTTP Status Code: <span class="hljs-variable">$response</span>.\nThe repository requires authentication, but --hf_username and --hf_token is not passed. Please get token from https://huggingface.co/settings/tokens.\nExiting.\n<span class="hljs-variable">${NC}</span>"</span>
            <span class="hljs-built_in">exit</span> 1
        <span class="hljs-keyword">fi</span>
        REPO_URL=<span class="hljs-string">"https://<span class="hljs-variable">$HF_USERNAME</span>:<span class="hljs-variable">$HF_TOKEN</span>@<span class="hljs-variable">${HF_ENDPOINT#https://}</span>/<span class="hljs-variable">$MODEL_ID</span>"</span>
    <span class="hljs-keyword">elif</span> [ <span class="hljs-string">"<span class="hljs-variable">$response</span>"</span> != <span class="hljs-string">"200"</span> ]; <span class="hljs-keyword">then</span>
        <span class="hljs-built_in">echo</span> <span class="hljs-_">-e</span> <span class="hljs-string">"<span class="hljs-variable">${RED}</span>Unexpected HTTP Status Code: <span class="hljs-variable">$response</span>.\nExiting.\n<span class="hljs-variable">${NC}</span>"</span>; <span class="hljs-built_in">exit</span> 1
    <span class="hljs-keyword">fi</span>
    <span class="hljs-built_in">echo</span> <span class="hljs-string">"git clone <span class="hljs-variable">$REPO_URL</span>"</span>

    GIT_LFS_SKIP_SMUDGE=1 git <span class="hljs-built_in">clone</span> <span class="hljs-string">"<span class="hljs-variable">$REPO_URL</span>"</span> &amp;&amp; <span class="hljs-built_in">cd</span> <span class="hljs-string">"<span class="hljs-variable">$MODEL_DIR</span>"</span> || { <span class="hljs-built_in">printf</span> <span class="hljs-string">"<span class="hljs-variable">${RED}</span>Git clone failed.\n<span class="hljs-variable">${NC}</span>"</span>; <span class="hljs-built_in">exit</span> 1; }
    <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> $(git lfs ls-files | awk <span class="hljs-string">'{print $3}'</span>); <span class="hljs-keyword">do</span>
        truncate <span class="hljs-_">-s</span> 0 <span class="hljs-string">"<span class="hljs-variable">$file</span>"</span>
    <span class="hljs-keyword">done</span>
<span class="hljs-keyword">fi</span>

<span class="hljs-built_in">printf</span> <span class="hljs-string">"\nStart Downloading lfs files, bash script:\n"</span>
files=$(git lfs ls-files | awk <span class="hljs-string">'{print $3}'</span>)
<span class="hljs-built_in">declare</span> <span class="hljs-_">-a</span> urls

<span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> <span class="hljs-variable">$files</span>; <span class="hljs-keyword">do</span>
    url=<span class="hljs-string">"<span class="hljs-variable">$HF_ENDPOINT</span>/<span class="hljs-variable">$MODEL_ID</span>/resolve/main/<span class="hljs-variable">$file</span>"</span>
    file_dir=$(dirname <span class="hljs-string">"<span class="hljs-variable">$file</span>"</span>)
    mkdir -p <span class="hljs-string">"<span class="hljs-variable">$file_dir</span>"</span>
    <span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$TOOL</span>"</span> == <span class="hljs-string">"wget"</span> ]]; <span class="hljs-keyword">then</span>
        download_cmd=<span class="hljs-string">"wget -c \"<span class="hljs-variable">$url</span>\" -O \"<span class="hljs-variable">$file</span>\""</span>
        [[ -n <span class="hljs-string">"<span class="hljs-variable">$HF_TOKEN</span>"</span> ]] &amp;&amp; download_cmd=<span class="hljs-string">"wget --header=\"Authorization: Bearer <span class="hljs-variable">${HF_TOKEN}</span>\" -c \"<span class="hljs-variable">$url</span>\" -O \"<span class="hljs-variable">$file</span>\""</span>
    <span class="hljs-keyword">else</span>
        download_cmd=<span class="hljs-string">"aria2c -x <span class="hljs-variable">$THREADS</span> -s <span class="hljs-variable">$THREADS</span> -k 1M -c \"<span class="hljs-variable">$url</span>\" -d \"<span class="hljs-variable">$file_dir</span>\" -o \"<span class="hljs-variable">$(basename "$file")</span>\""</span>
        [[ -n <span class="hljs-string">"<span class="hljs-variable">$HF_TOKEN</span>"</span> ]] &amp;&amp; download_cmd=<span class="hljs-string">"aria2c --header=\"Authorization: Bearer <span class="hljs-variable">${HF_TOKEN}</span>\" -x <span class="hljs-variable">$THREADS</span> -s <span class="hljs-variable">$THREADS</span> -k 1M -c \"<span class="hljs-variable">$url</span>\" -d \"<span class="hljs-variable">$file_dir</span>\" -o \"<span class="hljs-variable">$(basename "$file")</span>\""</span>
    <span class="hljs-keyword">fi</span>
    [[ -n <span class="hljs-string">"<span class="hljs-variable">$INCLUDE_PATTERN</span>"</span> &amp;&amp; <span class="hljs-variable">$file</span> != *<span class="hljs-string">"<span class="hljs-variable">$INCLUDE_PATTERN</span>"</span>* ]] &amp;&amp; <span class="hljs-built_in">printf</span> <span class="hljs-string">"# %s\n"</span> <span class="hljs-string">"<span class="hljs-variable">$download_cmd</span>"</span> &amp;&amp; <span class="hljs-built_in">continue</span>
    [[ -n <span class="hljs-string">"<span class="hljs-variable">$EXCLUDE_PATTERN</span>"</span> &amp;&amp; <span class="hljs-variable">$file</span> == *<span class="hljs-string">"<span class="hljs-variable">$EXCLUDE_PATTERN</span>"</span>* ]] &amp;&amp; <span class="hljs-built_in">printf</span> <span class="hljs-string">"# %s\n"</span> <span class="hljs-string">"<span class="hljs-variable">$download_cmd</span>"</span> &amp;&amp; <span class="hljs-built_in">continue</span>
    <span class="hljs-built_in">printf</span> <span class="hljs-string">"%s\n"</span> <span class="hljs-string">"<span class="hljs-variable">$download_cmd</span>"</span>
    urls+=(<span class="hljs-string">"<span class="hljs-variable">$url</span>|<span class="hljs-variable">$file</span>"</span>)
<span class="hljs-keyword">done</span>

<span class="hljs-keyword">for</span> url_file <span class="hljs-keyword">in</span> <span class="hljs-string">"<span class="hljs-variable">${urls[@]}</span>"</span>; <span class="hljs-keyword">do</span>
    IFS=<span class="hljs-string">'|'</span> <span class="hljs-built_in">read</span> -r url file &lt;&lt;&lt; <span class="hljs-string">"<span class="hljs-variable">$url_file</span>"</span>
    file_dir=$(dirname <span class="hljs-string">"<span class="hljs-variable">$file</span>"</span>)
    <span class="hljs-keyword">if</span> [[ <span class="hljs-string">"<span class="hljs-variable">$TOOL</span>"</span> == <span class="hljs-string">"wget"</span> ]]; <span class="hljs-keyword">then</span>
        [[ -n <span class="hljs-string">"<span class="hljs-variable">$HF_TOKEN</span>"</span> ]] &amp;&amp; wget --header=<span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">${HF_TOKEN}</span>"</span> -c <span class="hljs-string">"<span class="hljs-variable">$url</span>"</span> -O <span class="hljs-string">"<span class="hljs-variable">$file</span>"</span> || wget -c <span class="hljs-string">"<span class="hljs-variable">$url</span>"</span> -O <span class="hljs-string">"<span class="hljs-variable">$file</span>"</span>
    <span class="hljs-keyword">else</span>
        [[ -n <span class="hljs-string">"<span class="hljs-variable">$HF_TOKEN</span>"</span> ]] &amp;&amp; aria2c --header=<span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">${HF_TOKEN}</span>"</span> -x <span class="hljs-variable">$THREADS</span> <span class="hljs-_">-s</span> <span class="hljs-variable">$THREADS</span> -k 1M -c <span class="hljs-string">"<span class="hljs-variable">$url</span>"</span> <span class="hljs-_">-d</span> <span class="hljs-string">"<span class="hljs-variable">$file_dir</span>"</span> -o <span class="hljs-string">"<span class="hljs-variable">$(basename "$file")</span>"</span> || aria2c -x <span class="hljs-variable">$THREADS</span> <span class="hljs-_">-s</span> <span class="hljs-variable">$THREADS</span> -k 1M -c <span class="hljs-string">"<span class="hljs-variable">$url</span>"</span> <span class="hljs-_">-d</span> <span class="hljs-string">"<span class="hljs-variable">$file_dir</span>"</span> -o <span class="hljs-string">"<span class="hljs-variable">$(basename "$file")</span>"</span>
    <span class="hljs-keyword">fi</span>
    [[ $? <span class="hljs-_">-eq</span> 0 ]] &amp;&amp; <span class="hljs-built_in">printf</span> <span class="hljs-string">"Downloaded %s successfully.\n"</span> <span class="hljs-string">"<span class="hljs-variable">$url</span>"</span> || { <span class="hljs-built_in">printf</span> <span class="hljs-string">"<span class="hljs-variable">${RED}</span>Failed to download %s.\n<span class="hljs-variable">${NC}</span>"</span> <span class="hljs-string">"<span class="hljs-variable">$url</span>"</span>; <span class="hljs-built_in">exit</span> 1; }
<span class="hljs-keyword">done</span>

<span class="hljs-built_in">printf</span> <span class="hljs-string">"<span class="hljs-variable">${GREEN}</span>Download completed successfully.\n<span class="hljs-variable">${NC}</span>"</span>
</code></pre>
<p>该工具同样支持设置镜像端点的环境变量:</p>
<pre><code class="lang-bash"><span class="hljs-built_in">export</span> HF_ENDPOINT=<span class="hljs-string">"https://hf-mirror.com"</span>
</code></pre>
<p><strong>基本命令：</strong></p>
<pre><code class="lang-bash">./hdf.sh bigscience/bloom-560m --tool aria2c -x 4
</code></pre>
<p>如果没有安装 aria2，则可以默认用 wget：</p>
<pre><code class="lang-text">./hdf.sh bigscience/bloom-560m
</code></pre>
<h3 id="镜像网站">1.3.4 镜像网站</h3>
<blockquote>
<p><a href="https://hf-mirror.com/" target="_blank">Huggingface-镜像网站</a></p>
</blockquote>
<p>可下载模型和数据集，解决Huggingface无法访问问题</p>
<p>使用以下py脚本可以快速生成下载模型等文件的sh脚本</p>
<pre><code class="lang-python"><span class="hljs-comment">#!/usr/bin/env Python</span>
<span class="hljs-comment"># -- coding: utf-8 --</span>

<span class="hljs-string">"""
@version: v1.0
@author: huangyc
@file: download_hf_models.py
@Description: 
@time: 2024/1/18 11:08
"""</span>
<span class="hljs-keyword">import</span> contextlib
<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> List
<span class="hljs-keyword">from</span> urllib.parse <span class="hljs-keyword">import</span> unquote, urlparse

<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> basic_support.logger.logger_config <span class="hljs-keyword">import</span> logger


<span class="hljs-meta">@contextlib.contextmanager</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_to_file</span><span class="hljs-params">(file: str, mode: str = <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>, errors=None, newline=None, closefd=True)</span>:</span>
    <span class="hljs-string">"""
    将print重定向输出到文件
    :param file: 文件名
    :param mode: 读写模式
    :param encoding: 文件编码
    :param errors:
    :param newline:
    :param closefd:
    :return:
    """</span>
    f = open(file=file, mode=mode, encoding=encoding, errors=errors, newline=newline, closefd=closefd)
    <span class="hljs-comment"># 保存原来的sys.stdout</span>
    original_stdout = sys.stdout

    <span class="hljs-comment"># 将sys.stdout重定向到文件流</span>
    sys.stdout = f
    <span class="hljs-keyword">yield</span>

    <span class="hljs-comment"># 恢复原来的sys.stdout</span>
    sys.stdout = original_stdout
    f.close()


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract_main_domain</span><span class="hljs-params">(url)</span>:</span>
    parsed_url = urlparse(url)
    main_domain = f<span class="hljs-string">"{parsed_url.scheme}://{parsed_url.hostname}"</span>
    <span class="hljs-keyword">return</span> main_domain


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gen_download_hf_models_script</span><span class="hljs-params">(model_url: str, file_name: str, filter_types: List[str] = None)</span>:</span>
    <span class="hljs-string">"""
    产生下载hf模型的脚本
    :param model_url: 支持 https://hf-mirror.com 和 https://huggingface.co/models
                    如: https://hf-mirror.com/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0
    :param file_name: 输出文件名字, 如nohup_download_baichuan2.sh
    :param filter_types: 需要过滤的文件类型[暂时没实现]
    :return:
    """</span>
    <span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup

    <span class="hljs-comment"># 获取主要域名</span>
    main_domain = extract_main_domain(model_url)

    <span class="hljs-comment"># 输出主要域名</span>
    logger.info(f<span class="hljs-string">"解析到域名为：{main_domain}"</span>)

    <span class="hljs-comment"># 发送HTTP GET请求获取网页内容</span>
    logger.info(<span class="hljs-string">"开始解析下载"</span>)
    response = requests.get(model_url)
    logger.info(<span class="hljs-string">"网页下载完成, 准备解析下载地址"</span>)
    html_content = response.text

    <span class="hljs-comment"># 使用BeautifulSoup对HTML内容进行解析</span>
    soup = BeautifulSoup(html_content, <span class="hljs-string">'html.parser'</span>)

    download_files = soup.findAll(<span class="hljs-string">'a'</span>, {<span class="hljs-string">'title'</span>: <span class="hljs-string">'Download file'</span>})

    <span class="hljs-keyword">with</span> print_to_file(file_name):
        print(<span class="hljs-string">'echo "开始下载模型等文件"\n'</span>)

        <span class="hljs-keyword">for</span> idx, download_file <span class="hljs-keyword">in</span> enumerate(download_files):
            href = unquote(download_file.get(<span class="hljs-string">'href'</span>))
            print(<span class="hljs-string">'date +"当前时间为: %Y-%m-%d %H:%M:%S"'</span>)
            url = f<span class="hljs-string">"{main_domain}{href}"</span>

            file_name =os.path.basename(href).split(<span class="hljs-string">'?'</span>)[<span class="hljs-number">0</span>]
            print(f<span class="hljs-string">'wget -O "{file_name}" "{url}"'</span>)
            print(f<span class="hljs-string">'echo "下载完成"\n'</span>)

        print(<span class="hljs-string">'date +"当前时间为: %Y-%m-%d %H:%M:%S"'</span>)
        print(<span class="hljs-string">'echo "Download completed successfully."'</span>)

    logger.info(<span class="hljs-string">"下载地址解析完毕"</span>)
    logger.info(f<span class="hljs-string">"脚本输出路径为: {file_name}"</span>)


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    test_url = <span class="hljs-string">r"https://hf-mirror.com/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0"</span>

    gen_download_hf_models_script(model_url=test_url, file_name=<span class="hljs-string">'noup_download_baichuan2.sh'</span>)
</code></pre>
<h2 id="快速开始">1.4 快速开始</h2>
<blockquote>
<p><a href="https://huggingface.co/docs/transformers/quicktour#quick-tour" target="_blank">hf快速开始教程</a></p>
</blockquote>
<p>下图是huggingface模块关系图</p>
<p><a data-lightbox="4cdf5b22-1c25-45ac-bdfd-bbd5dafd6590" data-title="huggingface模块关系图" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/huggingface模块关系图.svg" target="_blank"><img alt="huggingface模块关系图" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/huggingface模块关系图.svg"/></a></p>
<pre><code class="lang-python"><span class="hljs-comment">#!/usr/bin/env Python</span>
<span class="hljs-comment"># -- coding: utf-8 --</span>

<span class="hljs-string">"""
@version: v1.0
@author: huangyc
@file: noup_huggingface.py
@Description:
@time: 2024/2/3 9:49
"""</span>

<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">()</span>:</span>
    model_name = <span class="hljs-string">"distilbert-base-uncased"</span>
    output_dir = <span class="hljs-string">"path/to/save/folder/"</span>

    <span class="hljs-comment"># 加载预训练模型</span>
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    <span class="hljs-comment"># 加载分词器</span>
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    <span class="hljs-comment"># 加载数据集</span>
    dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_dataset</span><span class="hljs-params">(p_dataset)</span>:</span>
        <span class="hljs-string">"""
            定义数据处理函数
            @param p_dataset:
            @return:
            """</span>
        <span class="hljs-keyword">return</span> tokenizer(p_dataset[<span class="hljs-string">"text"</span>])

    <span class="hljs-comment"># 对数据集调用处理函数(这里主要做分词)</span>
    dataset = dataset.map(tokenize_dataset, batched=<span class="hljs-keyword">True</span>)

    <span class="hljs-comment"># 有些还需要做标签对齐</span>
    <span class="hljs-comment"># label2id = {"contradiction": 0, "neutral": 1, "entailment": 2}</span>
    <span class="hljs-comment"># mnli = load_dataset("glue", "mnli", split="train")</span>
    <span class="hljs-comment"># mnli_aligned = mnli.align_labels_with_mapping(label2id, "label")</span>

    <span class="hljs-comment"># 定义一个数据收集器</span>
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    <span class="hljs-comment"># 配置训练参数</span>
    training_args = TrainingArguments(output_dir, learning_rate=<span class="hljs-number">2e-5</span>, per_device_train_batch_size=<span class="hljs-number">8</span>,
                                      per_device_eval_batch_size=<span class="hljs-number">8</span>, num_train_epochs=<span class="hljs-number">2</span>, )

    <span class="hljs-comment"># 定义一个trainer</span>
    trainer = Trainer(model=model, args=training_args, train_dataset=dataset[<span class="hljs-string">"train"</span>], eval_dataset=dataset[<span class="hljs-string">"test"</span>],
                      tokenizer=tokenizer, data_collator=data_collator, )

    <span class="hljs-comment"># 开始训练</span>
    trainer.train()


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    run()
</code></pre>
<blockquote>
<p>对于使用序列到序列模型（Seq2Seq）的任务，如翻译或摘要，应该使用Seq2SeqTrainer和Seq2SeqTrainingArguments类</p>
</blockquote>
<p>您可以通过继承Trainer内部的方法来自定义训练循环的行为。这使您能够自定义特征，如损失函数、优化器和调度器。查看Trainer参考以了解哪些方法可以被继承</p>
<p>自定义训练循环的另一种方式是使用回调（Callbacks）。您可以使用回调与其他库集成以及检查训练循环，以报告进度或提前停止训练</p>
<p>回调不会修改训练循环本身的任何内容。若要自定义像损失函数这样的内容，您需要继承Trainer</p>
<h1 id="datasets">2 datasets</h1>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/582687507" target="_blank">HuggingFace datasets库总结</a></p>
</blockquote>
<h2 id="安装_1">2.1 安装</h2>
<p>下面三个命令都用于安装Hugging Face的<code>datasets</code>库的不同配置</p>
<ol>
<li><code>pip install datasets</code>：这个命令安装的是<code>datasets</code>库的基本配置，它提供了对常见的自然语言处理(NLP)任务和数据集的支持，例如文本分类、命名实体识别、问答系统等。如果您只需要处理文本数据或进行常见的NLP任务，这个基本配置就足够了</li>
<li><code>pip install datasets[audio]</code>：这个命令安装的是<code>datasets</code>库的"audio"配置。它包含了对声音和音频数据集的支持，例如自动语音识别(ASR)和音频分类任务。如果您需要处理声音和音频数据，比如进行语音识别或音频分类，安装这个配置会提供相应的功能和数据集支持</li>
<li><code>pip install datasets[vision]</code>：这个命令安装的是<code>datasets</code>库的"vision"配置。它包含了对图像和计算机视觉任务的支持，例如图像分类、目标检测和分割等。如果您需要处理图像数据或进行计算机视觉任务，安装这个配置会提供相应的功能和数据集支持</li>
</ol>
<p>通过安装不同的配置，您可以选择仅安装您需要的功能和支持的任务类型，以减少库的安装和存储空间。根据您的具体需求，选择适合的配置进行安装即可</p>
<pre><code class="lang-bash"><span class="hljs-comment"># 安装基础版</span>
pip install datasets

<span class="hljs-comment"># 安装for声音</span>
pip install datasets[audio]

<span class="hljs-comment"># 安装for图像</span>
pip install datasets[vision]
</code></pre>
<h2 id="快速开始_1">2.2 快速开始</h2>
<h3 id="视觉">2.2.1 视觉</h3>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Image
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, ColorJitter, ToTensor

<span class="hljs-comment"># 加载数据集</span>
dataset = load_dataset(<span class="hljs-string">"beans"</span>, split=<span class="hljs-string">"train"</span>)

jitter = Compose(
    [ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor()]
)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transforms</span><span class="hljs-params">(examples)</span>:</span>
    examples[<span class="hljs-string">"pixel_values"</span>] = [jitter(image.convert(<span class="hljs-string">"RGB"</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"image"</span>]]
    <span class="hljs-keyword">return</span> examples

dataset = dataset.with_transform(transforms)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span><span class="hljs-params">(examples)</span>:</span>
    images = []
    labels = []
    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples:
        images.append((example[<span class="hljs-string">"pixel_values"</span>]))
        labels.append(example[<span class="hljs-string">"labels"</span>])

    pixel_values = torch.stack(images)
    labels = torch.tensor(labels)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"pixel_values"</span>: pixel_values, <span class="hljs-string">"labels"</span>: labels}

<span class="hljs-comment"># 定义DataLoader</span>
dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=<span class="hljs-number">4</span>)
</code></pre>
<h3 id="nlp">2.2.2 nlp</h3>
<p>使用 Hugging Face 提供的<code>datasets</code>库加载了<a href="https://huggingface.co/datasets/glue/viewer/mrpc/test" target="_blank">GLUE</a>(General Language Understanding Evaluation)数据集中的MRPC(Microsoft Research Paraphrase Corpus)部分的训练集。这个数据集用于句子对的相似性判断任务</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-keyword">import</span> torch

dataset = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>, split=<span class="hljs-string">"test"</span>)

<span class="hljs-comment"># load a pretrained BERT model and its corresponding tokenizer from the 🤗 Transformers library.</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode</span><span class="hljs-params">(examples)</span>:</span>
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">"sentence1"</span>], examples[<span class="hljs-string">"sentence2"</span>], truncation=<span class="hljs-keyword">True</span>, padding=<span class="hljs-string">"max_length"</span>)

dataset = dataset.map(encode, batched=<span class="hljs-keyword">True</span>)
dataset[<span class="hljs-number">0</span>]

{<span class="hljs-string">'sentence1'</span>: <span class="hljs-string">'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .'</span>,
<span class="hljs-string">'sentence2'</span>: <span class="hljs-string">'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'</span>,
<span class="hljs-string">'label'</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">'idx'</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">'input_ids'</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">'token_type_ids'</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">'attention_mask'</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}

<span class="hljs-comment">#  Rename the label column to labels, which is the expected input name in BertForSequenceClassification</span>
dataset = dataset.map(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">"labels"</span>: examples[<span class="hljs-string">"label"</span>]}, batched=<span class="hljs-keyword">True</span>)

dataset.set_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"labels"</span>])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
</code></pre>
<h2 id="概述_1">2.3 概述</h2>
<p><code>datasets</code>库中的<code>Dataset</code>对象通常用来处理和存储数据。当数据需要载入模型进行训练或评估时，<code>DataLoader</code>被用来创建数据的迭代器，允许批量处理和并行加载</p>
<p><code>DataCollator</code>则用于将这些批次的数据整理成模型需要的格式，进行适当的填充或其他预处理步骤</p>
<p>简单来说，你可以这样想象它们的工作流：</p>
<ol>
<li><code>Dataset</code>负责数据的存储和预处理</li>
<li><code>DataLoader</code>负责从<code>Dataset</code>中抽取数据，组成批次，并可选择并行加载数据</li>
<li><code>DataCollator</code>负责将<code>DataLoader</code>提供的批次数据进行填充和整理，以确保模型可以正确处理</li>
</ol>
<h3 id="datacollator类">2.3.1 DataCollator类</h3>
<blockquote>
<p>huggingface的DataCollator和pytorch的collate_fn的关系</p>
</blockquote>
<p>在PyTorch中，<code>collate_fn</code> 是 <code>DataLoader</code> 的一个参数，用于指定如何将多个数据样本组合成一个批次</p>
<p>这个函数接受一个样本列表作为输入，然后返回一个批次，通常是通过堆叠（stacking）或填充（padding）样本来实现</p>
<p><code>collate_fn</code> 在处理长度不一致的数据时特别有用，例如文本数据或时间序列数据</p>
<p>Hugging Face的 <code>DataCollator</code> 基本上是 <code>collate_fn</code> 的一个扩展或包装器。它通常是一个类，实现了一个 <code>__call__</code> 方法，该方法的功能与 <code>collate_fn</code> 相同</p>
<p>在Hugging Face的Transformers库中，有预先定义的 <code>DataCollator</code> 类，它们被设计用来处理特定类型的数据和模型需求，如填充到相同长度，或者为了语言模型训练而进行数据掩蔽的任务</p>
<p>下面是一个示例，展示了在PyTorch和Hugging Face的Transformers中如何使用 <code>collate_fn</code> 和 <code>DataCollator</code>：</p>
<p><strong>在PyTorch中使用自定义 <code>collate_fn</code>：</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">custom_collate_fn</span><span class="hljs-params">(batch)</span>:</span>
    <span class="hljs-comment"># 自定义的堆叠、填充逻辑</span>
    <span class="hljs-keyword">pass</span>

data_loader = DataLoader(dataset, batch_size=<span class="hljs-number">32</span>, collate_fn=custom_collate_fn)
</code></pre>
<p><strong>在Hugging Face的Transformers中使用 <code>DataCollator</code>：</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-comment"># 对于一些特定的任务，Transformers库提供了预定义的DataCollator</span>
data_collator = DataCollatorWithPadding(tokenizer)

data_loader = DataLoader(dataset, batch_size=<span class="hljs-number">32</span>, collate_fn=data_collator)
</code></pre>
<p>在这个例子中，<code>DataCollatorWithPadding</code> 是Hugging Face提供的一个类，它使用给定的tokenizer来自动处理批次的填充</p>
<p>当创建 <code>DataLoader</code> 实例时，你可以直接将 <code>data_collator</code> 作为 <code>collate_fn</code> 的值传入，这是因为 <code>DataCollatorWithPadding</code> 类的实例是可调用的，这样 <code>DataLoader</code> 在每个批次准备数据时会调用 <code>data_collator</code></p>
<p>总的来说，Hugging Face的 <code>DataCollator</code> 提供了一个更高级别、更方便的接口，尤其是为了与 <code>Transformers</code> 库中的NLP模型和任务配合使用，而PyTorch的 <code>collate_fn</code> 是这个接口的底层机制，提供了自定义数据组合逻辑的基础功能</p>
<blockquote>
<p>小结</p>
</blockquote>
<p>可以查看huggingface的Trainer类，很容易发现他们之间的关系：Hugging Face的 <code>DataCollator</code> 基本上是 <code>collate_fn</code> 的一个扩展或包装器。它通常是一个类，实现了一个 <code>__call__</code> 方法，该方法的功能与 <code>collate_fn</code> 相同</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Trainer</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(
            self,
            model: Union[PreTrainedModel, nn.Module] = None,
            args: TrainingArguments = None,
            data_collator: Optional[DataCollator] = None,
            train_dataset: Optional[Dataset] = None,
            eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
            tokenizer: Optional[PreTrainedTokenizerBase] = None,
            model_init: Optional[Callable[[], PreTrainedModel]] = None,
            compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
            callbacks: Optional[List[TrainerCallback]] = None,
            optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = <span class="hljs-params">(None, None)</span>,
            preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
    )</span>:</span>
        ...
        default_collator = default_data_collator <span class="hljs-keyword">if</span> tokenizer <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">else</span> DataCollatorWithPadding(tokenizer)
        self.data_collator = data_collator <span class="hljs-keyword">if</span> data_collator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">else</span> default_collator
        ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_train_dataloader</span><span class="hljs-params">(self)</span> -&gt; DataLoader:</span>
        <span class="hljs-string">"""
        Returns the training [`~torch.utils.data.DataLoader`].

        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed
        training if necessary) otherwise.

        Subclass and override this method if you want to inject some custom behavior.
        """</span>
        <span class="hljs-keyword">if</span> self.train_dataset <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"Trainer: training requires a train_dataset."</span>)

        train_dataset = self.train_dataset
        data_collator = self.data_collator
        <span class="hljs-keyword">if</span> is_datasets_available() <span class="hljs-keyword">and</span> isinstance(train_dataset, datasets.Dataset):
            train_dataset = self._remove_unused_columns(train_dataset, description=<span class="hljs-string">"training"</span>)
        <span class="hljs-keyword">else</span>:
            data_collator = self._get_collator_with_removed_columns(data_collator, description=<span class="hljs-string">"training"</span>)

        <span class="hljs-keyword">if</span> isinstance(train_dataset, torch.utils.data.IterableDataset):
            <span class="hljs-keyword">if</span> self.args.world_size &gt; <span class="hljs-number">1</span>:
                train_dataset = IterableDatasetShard(
                    train_dataset,
                    batch_size=self._train_batch_size,
                    drop_last=self.args.dataloader_drop_last,
                    num_processes=self.args.world_size,
                    process_index=self.args.process_index,
                )

            <span class="hljs-keyword">return</span> DataLoader(
                train_dataset,
                batch_size=self._train_batch_size,
                collate_fn=data_collator,
                num_workers=self.args.dataloader_num_workers,
                pin_memory=self.args.dataloader_pin_memory,
            )

        train_sampler = self._get_train_sampler()

        <span class="hljs-keyword">return</span> DataLoader(
            train_dataset,
            batch_size=self._train_batch_size,
            sampler=train_sampler,
            collate_fn=data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
            worker_init_fn=seed_worker,
        )
</code></pre>
<h2 id="加载数据集">2.4 加载数据集</h2>
<blockquote>
<p>查看数据集描述</p>
</blockquote>
<pre><code class="lang-python">from datasets import load_dataset_builder
ds_builder = load_dataset_builder("rotten_tomatoes")

ds_builder.info.description
Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.


ds_builder.info.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
</code></pre>
<blockquote>
<p>加载数据集</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
<p>当一个数据集由多个文件(我们称之为<strong>分片</strong>)组成时，可以显著加快数据集的下载和准备步骤</p>
<p>您可以使用num_proc参数选择并行准备数据集时要使用的进程数。在这种情况下，每个进程被分配了一部分分片来进行准备</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

oscar_afrikaans = load_dataset(<span class="hljs-string">"oscar-corpus/OSCAR-2201"</span>, <span class="hljs-string">"af"</span>, num_proc=<span class="hljs-number">8</span>)
imagenet = load_dataset(<span class="hljs-string">"imagenet-1k"</span>, num_proc=<span class="hljs-number">8</span>)
ml_librispeech_spanish = load_dataset(<span class="hljs-string">"facebook/multilingual_librispeech"</span>, <span class="hljs-string">"spanish"</span>, num_proc=<span class="hljs-number">8</span>)
</code></pre>
<blockquote>
<p>查看数据集的分片名称，并加载指定的分片名称</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_split_names
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

get_dataset_split_names(<span class="hljs-string">"rotten_tomatoes"</span>)
[<span class="hljs-string">'train'</span>, <span class="hljs-string">'validation'</span>, <span class="hljs-string">'test'</span>]

<span class="hljs-comment"># 加载指定分片</span>
dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>, split=<span class="hljs-string">"train"</span>)

Dataset({
    features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
    num_rows: <span class="hljs-number">8530</span>
})

<span class="hljs-comment"># 还可以这么写：</span>
train_test_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train+test"</span>)
train_10_20_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[10:20]"</span>)
train_10pct_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[:10%]"</span>)
train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[:10%]+train[-80%:]"</span>)
val_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=[f<span class="hljs-string">"train[{k}%:{k+10}%]"</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
train_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=[f<span class="hljs-string">"train[:{k}%]+train[{k+10}%:]"</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
train_50_52_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[50%:52%]"</span>)
train_52_54_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[52%:54%]"</span>)

<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=datasets.ReadInstruction(<span class="hljs-string">"train"</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">"%"</span>, rounding=<span class="hljs-string">"pct1_dropremainder"</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=datasets.ReadInstruction(<span class="hljs-string">"train"</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">"%"</span>, rounding=<span class="hljs-string">"pct1_dropremainder"</span>))
<span class="hljs-comment"># Or equivalently:</span>
train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[50%:52%](pct1_dropremainder)"</span>)
train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[52%:54%](pct1_dropremainder)"</span>)

<span class="hljs-comment"># 加载全部数据</span>
dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>)
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
        num_rows: <span class="hljs-number">8530</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
        num_rows: <span class="hljs-number">1066</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
        num_rows: <span class="hljs-number">1066</span>
    })
})
</code></pre>
<blockquote>
<p>查看数据集子集，一个数据下可能还有很多子数据集</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_config_names

configs = get_dataset_config_names(<span class="hljs-string">"PolyAI/minds14"</span>)
print(configs)

[<span class="hljs-string">'cs-CZ'</span>, <span class="hljs-string">'de-DE'</span>, <span class="hljs-string">'en-AU'</span>, <span class="hljs-string">'en-GB'</span>, <span class="hljs-string">'en-US'</span>, <span class="hljs-string">'es-ES'</span>, <span class="hljs-string">'fr-FR'</span>, <span class="hljs-string">'it-IT'</span>, <span class="hljs-string">'ko-KR'</span>, <span class="hljs-string">'nl-NL'</span>, <span class="hljs-string">'pl-PL'</span>, <span class="hljs-string">'pt-PT'</span>, <span class="hljs-string">'ru-RU'</span>, <span class="hljs-string">'zh-CN'</span>, <span class="hljs-string">'all'</span>]
</code></pre>
<blockquote>
<p>加载指定子数据集</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

mindsFR = load_dataset(<span class="hljs-string">"PolyAI/minds14"</span>, <span class="hljs-string">"fr-FR"</span>, split=<span class="hljs-string">"train"</span>) <span class="hljs-comment"># 指定子数据集是fr-FR</span>
</code></pre>
<blockquote>
<p>指定数据集的文件, 避免load过多的数据</p>
</blockquote>
<pre><code class="lang-python">data_files = {<span class="hljs-string">"validation"</span>: <span class="hljs-string">"en/c4-validation.*.json.gz"</span>}
c4_validation = load_dataset(<span class="hljs-string">"allenai/c4"</span>, data_files=data_files, split=<span class="hljs-string">"validation"</span>)
</code></pre>
<blockquote>
<p>load本地的json、csv文件等，可以load远程文件、sql等</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment">#{"version": "0.1.0",</span>
<span class="hljs-comment"># "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},</span>
<span class="hljs-comment">#          {"a": 4, "b": -5.5, "c": null, "d": true}]</span>
<span class="hljs-comment">#}</span>

<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"my_file.json"</span>, field=<span class="hljs-string">"data"</span>)
</code></pre>
<blockquote>
<p>通过python对象来创建dataset</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-comment"># 字典方式</span>
my_dict = {<span class="hljs-string">"a"</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
dataset = Dataset.from_dict(my_dict)

<span class="hljs-comment"># list方式</span>
my_list = [{<span class="hljs-string">"a"</span>: <span class="hljs-number">1</span>}, {<span class="hljs-string">"a"</span>: <span class="hljs-number">2</span>}, {<span class="hljs-string">"a"</span>: <span class="hljs-number">3</span>}]
dataset = Dataset.from_list(my_list)

<span class="hljs-comment"># pandas方式</span>
df = pd.DataFrame({<span class="hljs-string">"a"</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
dataset = Dataset.from_pandas(df)
</code></pre>
<blockquote>
<p>load多个文本文件: 文本必须一行就是一条样本</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
dataset = load_dataset(<span class="hljs-string">"text"</span>, data_files={<span class="hljs-string">"train"</span>: [<span class="hljs-string">"my_text_1.txt"</span>, <span class="hljs-string">"my_text_2.txt"</span>], <span class="hljs-string">"test"</span>: <span class="hljs-string">"my_test_file.txt"</span>})

<span class="hljs-comment"># Load from a directory</span>
dataset = load_dataset(<span class="hljs-string">"text"</span>, data_dir=<span class="hljs-string">"path/to/text/dataset"</span>)
</code></pre>
<p>离线load: 将环境变量<code>HF_DATASETS_OFFLINE</code>设置为1以启用完全离线模式</p>
<h2 id="进阶加载数据集">2.5 进阶加载数据集</h2>
<blockquote>
<p>从脚本加载数据集</p>
</blockquote>
<p>您可能在本地计算机上有一个🤗Datasets的加载脚本。在这种情况下，通过将以下路径之一传递给load_dataset()来加载数据集：</p>
<p>加载脚本文件的本地路径。 包含加载脚本文件的目录的本地路径(仅当脚本文件与目录具有相同的名称时)</p>
<pre><code class="lang-python">dataset = load_dataset(<span class="hljs-string">"path/to/local/loading_script/loading_script.py"</span>, split=<span class="hljs-string">"train"</span>)

<span class="hljs-comment"># equivalent because the file has the same name as the directory</span>
dataset = load_dataset(<span class="hljs-string">"path/to/local/loading_script"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
<p>可以从Hub上下载加载脚本，并对其进行编辑以添加自己的修改。将数据集仓库下载到本地，以便加载脚本中相对路径引用的任何数据文件都可以被加载</p>
<pre><code class="lang-cmd">git clone https://huggingface.co/datasets/eli5
</code></pre>
<p>在加载脚本上进行编辑后，通过将其本地路径传递给load_dataset()来加载它</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

eli5 = load_dataset(<span class="hljs-string">"path/to/local/eli5"</span>)
</code></pre>
<blockquote>
<p>csv+json方式</p>
</blockquote>
<p>数据集可以从存储在计算机上的本地文件和远程文件中加载。这些数据集很可能以csv、json、txt或parquet文件的形式存储。load_dataset()函数可以加载这些文件类型的数据集</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># csv方式</span>
dataset = load_dataset(<span class="hljs-string">"csv"</span>, data_files=<span class="hljs-string">"my_file.csv"</span>)

<span class="hljs-comment"># json方式</span>
<span class="hljs-comment"># {"a": 1, "b": 2.0, "c": "foo", "d": false}</span>
<span class="hljs-comment"># {"a": 4, "b": -5.5, "c": null, "d": true}</span>
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"my_file.json"</span>)

<span class="hljs-comment"># {"version": "0.1.0",</span>
<span class="hljs-comment">#  "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},</span>
<span class="hljs-comment">#           {"a": 4, "b": -5.5, "c": null, "d": true}]</span>
<span class="hljs-comment"># }</span>
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"my_file.json"</span>, field=<span class="hljs-string">"data"</span>)

<span class="hljs-comment"># 从http方式加载csv</span>
base_url = <span class="hljs-string">"https://rajpurkar.github.io/SQuAD-explorer/dataset/"</span>
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files={<span class="hljs-string">"train"</span>: base_url + <span class="hljs-string">"train-v1.1.json"</span>, <span class="hljs-string">"validation"</span>: base_url + <span class="hljs-string">"dev-v1.1.json"</span>}, field=<span class="hljs-string">"data"</span>)

<span class="hljs-comment"># Parquet方式</span>
dataset = load_dataset(<span class="hljs-string">"parquet"</span>, data_files={<span class="hljs-string">'train'</span>: <span class="hljs-string">'train.parquet'</span>, <span class="hljs-string">'test'</span>: <span class="hljs-string">'test.parquet'</span>})

base_url = <span class="hljs-string">"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"</span>
data_files = {<span class="hljs-string">"train"</span>: base_url + <span class="hljs-string">"wikipedia-train.parquet"</span>}
wiki = load_dataset(<span class="hljs-string">"parquet"</span>, data_files=data_files, split=<span class="hljs-string">"train"</span>)
</code></pre>
<blockquote>
<p>sql方式</p>
</blockquote>
<p>使用from_sql()方法可以通过指定连接到数据库的URI来读取数据库内容。您可以读取表名或执行查询操作</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

dataset = Dataset.from_sql(<span class="hljs-string">"data_table_name"</span>, con=<span class="hljs-string">"sqlite:///sqlite_file.db"</span>)
dataset = Dataset.from_sql(<span class="hljs-string">"SELECT text FROM table WHERE length(text) &gt; 100 LIMIT 10"</span>, con=<span class="hljs-string">"sqlite:///sqlite_file.db"</span>)
</code></pre>
<p>For more details, check out the <a href="https://huggingface.co/docs/datasets/tabular_load#databases" target="_blank">how to load tabular datasets from SQL databases</a> guide.</p>
<h2 id="探索数据集">2.6 探索数据集</h2>
<blockquote>
<p>下标</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># 第一个样本</span>
dataset[<span class="hljs-number">0</span>]
<span class="hljs-comment">#{'label': 1,</span>
<span class="hljs-comment"># 'text': 'the rock is destined to be the 21st century\'s new " conan " and that he\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}</span>

<span class="hljs-comment"># 最后一个样本</span>
dataset[<span class="hljs-number">-1</span>]

<span class="hljs-comment"># 只取text列</span>
dataset[<span class="hljs-string">"text"</span>] <span class="hljs-comment"># 返回a list of 样本列</span>

<span class="hljs-comment"># 第一个样本text列</span>
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"text"</span>] <span class="hljs-comment"># 性能：dataset[0]['text']比dataset['text'][0]快2倍。</span>
</code></pre>
<blockquote>
<p>数据切片</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># Get the first three rows</span>
dataset[:<span class="hljs-number">3</span>]

<span class="hljs-comment"># Get rows between three and six</span>
dataset[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>]
</code></pre>
<blockquote>
<p>迭代方式，streaming=True</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

iterable_dataset = load_dataset(<span class="hljs-string">"food101"</span>, split=<span class="hljs-string">"train"</span>, streaming=<span class="hljs-keyword">True</span>)
<span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> iterable_dataset:
    print(example)
    <span class="hljs-keyword">break</span>

{<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">384</span>x512 at <span class="hljs-number">0x7F0681F5C520</span>&gt;, <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>}

<span class="hljs-comment"># Get first three examples</span>
list(iterable_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">384</span>x512 at <span class="hljs-number">0x7F7479DEE9D0</span>&gt;,
  <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>},
 {<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">512</span>x512 at <span class="hljs-number">0x7F7479DE8190</span>&gt;,
  <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>},
 {<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">512</span>x383 at <span class="hljs-number">0x7F7479DE8310</span>&gt;,
  <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>}]
</code></pre>
<blockquote>
<p>排序+shuffle+选择+filter+切分数据集+分片</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># sort: 按某一列排序</span>
dataset.sort(<span class="hljs-string">"label"</span>)

<span class="hljs-comment"># 打乱</span>
shuffled_dataset = sorted_dataset.shuffle(seed=<span class="hljs-number">42</span>)

<span class="hljs-comment"># 选择</span>
small_dataset = dataset.select([<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>])

<span class="hljs-comment"># 匹配查找</span>
start_with_ar = dataset.filter(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">"sentence1"</span>].startswith(<span class="hljs-string">"Ar"</span>))
len(start_with_ar)
start_with_ar[<span class="hljs-string">"sentence1"</span>]
<span class="hljs-comment"># 匹配查找：根据下标</span>
even_dataset = dataset.filter(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-keyword">True</span>)

<span class="hljs-comment"># 切分</span>
dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)

<span class="hljs-comment"># 分片</span>
<span class="hljs-comment"># 数据集支持分片，将非常大的数据集划分为预定义数量的块。 在 shard() 中指定 num_shards 参数以确定要将数据集拆分成的分片数。 您还需要使用 index 参数提供要返回的分片。</span>
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
datasets = load_dataset(<span class="hljs-string">"imdb"</span>, split=<span class="hljs-string">"train"</span>)
print(dataset)
dataset.shard(num_shards=<span class="hljs-number">4</span>, index=<span class="hljs-number">0</span>) <span class="hljs-comment"># 四分之一</span>
</code></pre>
<blockquote>
<p>列重命名+移除列+转换格式+flatten</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># 列重命名</span>
dataset = dataset.rename_column(<span class="hljs-string">"sentence1"</span>, <span class="hljs-string">"sentenceA"</span>)

<span class="hljs-comment"># 去掉某一列</span>
dataset = dataset.remove_columns([<span class="hljs-string">"sentence1"</span>, <span class="hljs-string">"sentence2"</span>])

<span class="hljs-comment"># 转换格式：一列或者多列</span>
new_features = dataset.features.copy()
new_features[<span class="hljs-string">"label"</span>] = ClassLabel(names=[<span class="hljs-string">"negative"</span>, <span class="hljs-string">"positive"</span>])
new_features[<span class="hljs-string">"idx"</span>] = Value(<span class="hljs-string">"int64"</span>)
dataset = dataset.cast(new_features)

<span class="hljs-comment"># 转换格式：一列</span>
dataset = dataset.cast_column(<span class="hljs-string">"audio"</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))

<span class="hljs-comment"># 将某一列的key\value拉平</span>
dataset = load_dataset(<span class="hljs-string">"squad"</span>, split=<span class="hljs-string">"train"</span>) <span class="hljs-comment"># ???</span>
</code></pre>
<blockquote>
<p>map转换</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> multiprocess <span class="hljs-keyword">import</span> set_start_method
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> os

set_start_method(<span class="hljs-string">"spawn"</span>)


<span class="hljs-comment"># remove_columns 转换的同时去掉某一列</span>
updated_dataset = dataset.map(<span class="hljs-keyword">lambda</span> example: {<span class="hljs-string">"new_sentence"</span>: example[<span class="hljs-string">"sentence1"</span>]}, remove_columns=[<span class="hljs-string">"sentence1"</span>])
updated_dataset.column_names

<span class="hljs-comment"># with_indices: 对下标处理</span>
updated_dataset = dataset.map(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">"sentence2"</span>: f<span class="hljs-string">"{idx}: "</span> + example[<span class="hljs-string">"sentence2"</span>]}, with_indices=<span class="hljs-keyword">True</span>)
updated_dataset[<span class="hljs-string">"sentence2"</span>][:<span class="hljs-number">5</span>]

<span class="hljs-comment">#如果您设置with_rank=True，map()也适用于进程的等级。 这类似于with_indices参数。 映射函数中的with_rank参数位于索引1之后(如果它已经存在)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gpu_computation</span><span class="hljs-params">(example, rank)</span>:</span>
    os.environ[<span class="hljs-string">"CUDA_VISIBLE_DEVICES"</span>] = str(rank % torch.cuda.device_count())
    <span class="hljs-comment"># Your big GPU call goes here</span>
    <span class="hljs-keyword">return</span> examples
updated_dataset = dataset.map(gpu_computation, with_rank=<span class="hljs-keyword">True</span>)

<span class="hljs-comment"># 多线程</span>
updated_dataset = dataset.map(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">"sentence2"</span>: f<span class="hljs-string">"{idx}: "</span> + example[<span class="hljs-string">"sentence2"</span>]}, num_proc=<span class="hljs-number">4</span>)

<span class="hljs-comment"># batched</span>
chunked_dataset = dataset.map(chunk_examples, batched=<span class="hljs-keyword">True</span>, remove_columns=dataset.column_names)

<span class="hljs-comment"># 数据增强</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">augment_data</span><span class="hljs-params">(examples)</span>:</span>
    outputs = []
    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"sentence1"</span>]:
        words = sentence.split(<span class="hljs-string">' '</span>)
        K = randint(<span class="hljs-number">1</span>, len(words)<span class="hljs-number">-1</span>)
        masked_sentence = <span class="hljs-string">" "</span>.join(words[:K]  + [mask_token] + words[K+<span class="hljs-number">1</span>:])
        predictions = fillmask(masked_sentence)
        augmented_sequences = [predictions[i][<span class="hljs-string">"sequence"</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>)]
        outputs += [sentence] + augmented_sequences
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"data"</span>: outputs}
augmented_dataset = smaller_dataset.map(augment_data, batched=<span class="hljs-keyword">True</span>, remove_columns=dataset.column_names, batch_size=<span class="hljs-number">8</span>)
augmented_dataset[:<span class="hljs-number">9</span>][<span class="hljs-string">"data"</span>]

<span class="hljs-comment"># 处理多split</span>
dataset = load_dataset(<span class="hljs-string">'glue'</span>, <span class="hljs-string">'mrpc'</span>)
encoded_dataset = dataset.map(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">"sentence1"</span>]), batched=<span class="hljs-keyword">True</span>)
encoded_dataset[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]
</code></pre>
<blockquote>
<p>合并+拼接数据集</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, load_dataset
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

<span class="hljs-comment"># 加载数据集</span>
bookcorpus = load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train"</span>)
wiki = load_dataset(<span class="hljs-string">"wikipedia"</span>, <span class="hljs-string">"20220301.en"</span>, split=<span class="hljs-string">"train"</span>)
wiki = wiki.remove_columns([col <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> wiki.column_names <span class="hljs-keyword">if</span> col != <span class="hljs-string">"text"</span>])  <span class="hljs-comment"># only keep the 'text' column</span>

<span class="hljs-keyword">assert</span> bookcorpus.features.type == wiki.features.type
bert_dataset = concatenate_datasets([bookcorpus, wiki])

<span class="hljs-comment"># 可以换concate的方向</span>
bookcorpus_ids = Dataset.from_dict({<span class="hljs-string">"ids"</span>: list(range(len(bookcorpus)))})
bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=<span class="hljs-number">1</span>)
</code></pre>
<blockquote>
<p>相互穿插</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch

<span class="hljs-comment"># 按概率穿插</span>
seed = <span class="hljs-number">42</span>
probabilities = [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.2</span>]
d1 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})
d2 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]})
d3 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>]})
dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
dataset[<span class="hljs-string">"a"</span>]

<span class="hljs-comment"># 按所有的样本都出现过一次后，马上停止</span>
d1 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})
d2 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]})
d3 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>]})
dataset = interleave_datasets([d1, d2, d3], stopping_strategy=<span class="hljs-string">"all_exhausted"</span>)
dataset[<span class="hljs-string">"a"</span>]
</code></pre>
<blockquote>
<p>format</p>
</blockquote>
<pre><code class="lang-python">dataset.set_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"label"</span>])

<span class="hljs-comment"># 返回一个新dataset</span>
dataset = dataset.with_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"label"</span>])

<span class="hljs-comment"># 查看</span>
dataset.format
</code></pre>
<blockquote>
<p>保存</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

encoded_dataset.save_to_disk(<span class="hljs-string">"path/of/my/dataset/directory"</span>)

<span class="hljs-comment"># 从本地load上来</span>
reloaded_dataset = load_from_disk(<span class="hljs-string">"path/of/my/dataset/directory"</span>)
encoded_dataset.to_csv(<span class="hljs-string">"path/of/my/dataset.csv"</span>)
Dataset.to_json()
</code></pre>
<h2 id="preprocess处理">2.7 Preprocess处理</h2>
<blockquote>
<p>文本处理：用transformers的tokenizer</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)
dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>, split=<span class="hljs-string">"train"</span>)

tokenizer(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"text"</span>])

{<span class="hljs-string">'input_ids'</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">2067</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">17348</span>, <span class="hljs-number">1106</span>, <span class="hljs-number">1129</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">6880</span>, <span class="hljs-number">1432</span>, <span class="hljs-number">112</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1207</span>, <span class="hljs-number">107</span>, <span class="hljs-number">14255</span>, <span class="hljs-number">1389</span>, <span class="hljs-number">107</span>, <span class="hljs-number">1105</span>, <span class="hljs-number">1115</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">112</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1280</span>, <span class="hljs-number">1106</span>, <span class="hljs-number">1294</span>, <span class="hljs-number">170</span>, <span class="hljs-number">24194</span>, <span class="hljs-number">1256</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">1190</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11791</span>, <span class="hljs-number">5253</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1732</span>, <span class="hljs-number">7200</span>, <span class="hljs-number">10947</span>, <span class="hljs-number">12606</span>, <span class="hljs-number">2895</span>, <span class="hljs-number">117</span>, <span class="hljs-number">179</span>, <span class="hljs-number">7766</span>, <span class="hljs-number">118</span>, <span class="hljs-number">172</span>, <span class="hljs-number">15554</span>, <span class="hljs-number">1181</span>, <span class="hljs-number">3498</span>, <span class="hljs-number">6961</span>, <span class="hljs-number">3263</span>, <span class="hljs-number">1137</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1566</span>, <span class="hljs-number">7912</span>, <span class="hljs-number">14516</span>, <span class="hljs-number">6997</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">'token_type_ids'</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">'attention_mask'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}
</code></pre>
<p>分词器返回一个包含三个项目的字典：</p>
<ul>
<li><strong>input_ids</strong>：表示文本中各个标记的数字</li>
<li><strong>token_type_ids</strong>：如果有多个序列，指示一个标记属于哪个序列</li>
<li><strong>attention_mask</strong>：指示一个标记是否应该被掩盖(masked)</li>
</ul>
<pre><code class="lang-python">dataset.set_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"labels"</span>])
</code></pre>
<blockquote>
<p>音频信号：重新采样音频信号</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">"facebook/wav2vec2-base-960h"</span>)
dataset = load_dataset(<span class="hljs-string">"PolyAI/minds14"</span>, <span class="hljs-string">"en-US"</span>, split=<span class="hljs-string">"train"</span>)

dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"audio"</span>]

{<span class="hljs-string">'array'</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, <span class="hljs-number">-0.00024414</span>, ..., <span class="hljs-number">-0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">'path'</span>: <span class="hljs-string">'/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav'</span>,
 <span class="hljs-string">'sampling_rate'</span>: <span class="hljs-number">8000</span>}
</code></pre>
<p>MInDS-14数据集卡会告诉您采样率为8kHz</p>
<p>Wav2Vec2模型卡说它是在16kHz语音音频上采样的。 这意味着您需要对MInDS-14数据集进行上采样以匹配模型的采样率</p>
<p>使用cast_column()函数并在Audio功能中设置sampling_rate参数以对音频信号进行上采样。 当您现在调用音频列时，它会被解码并重新采样到16kHz：</p>
<pre><code class="lang-python">dataset = dataset.cast_column(<span class="hljs-string">"audio"</span>, Audio(sampling_rate=<span class="hljs-number">16</span>_000))
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"audio"</span>]

<span class="hljs-comment"># 加速：使用 map() 函数将整个数据集重新采样到16kHz</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_function</span><span class="hljs-params">(examples)</span>:</span>
    audio_arrays = [x[<span class="hljs-string">"array"</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"audio"</span>]]
    inputs = feature_extractor(
        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=<span class="hljs-number">16000</span>, truncation=<span class="hljs-keyword">True</span>
    )
    <span class="hljs-keyword">return</span> inputs

dataset = dataset.map(preprocess_function, batched=<span class="hljs-keyword">True</span>)
</code></pre>
<blockquote>
<p>图像增强</p>
</blockquote>
<p>在图像数据集中，最常见的预处理操作之一是<code>数据增强</code>(data augmentation)，这是一种在不改变数据含义的情况下对图像引入随机变化的过程</p>
<p>这可以包括改变图像的颜色属性或随机裁剪图像。您可以自由选择任何数据增强库，并且🤗Datasets将帮助您将数据增强应用到您的数据集中</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Image
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomRotation


feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">"google/vit-base-patch16-224-in21k"</span>)
dataset = load_dataset(<span class="hljs-string">"beans"</span>, split=<span class="hljs-string">"train"</span>)

rotate = RandomRotation(degrees=(<span class="hljs-number">0</span>, <span class="hljs-number">90</span>))
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transforms</span><span class="hljs-params">(examples)</span>:</span>
    examples[<span class="hljs-string">"pixel_values"</span>] = [rotate(image.convert(<span class="hljs-string">"RGB"</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"image"</span>]]
    <span class="hljs-keyword">return</span> examples

<span class="hljs-comment"># 应用图像转换</span>
dataset.set_transform(transforms)
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"pixel_values"</span>]
</code></pre>
<blockquote>
<p>label id对齐</p>
</blockquote>
<p>在Transformers库中，<strong>label id对齐</strong>(label ID alignment)通常指的是将标签与模型输出的预测结果对齐。当使用预训练模型进行分类或回归等任务时，通常需要将标签映射为模型期望的标签ID</p>
<p>具体来说，对于分类任务，常见的做法是将标签映射为整数标签ID。例如，如果有三个类别["cat", "dog", "bird"]，可以将它们映射为[0, 1, 2]，并将模型的输出标签预测结果与这些标签ID进行对齐</p>
<p>对于回归任务，可能需要将连续值的标签进行离散化或归一化，并将其映射为标签ID。例如，将一个连续的目标值范围映射为一组离散的标签ID</p>
<p>在使用Transformers库进行训练或评估时，您需要确保标签与模型的输出结果具有相同的标签ID对齐，以便正确计算损失、评估指标和解码预测结果</p>
<p>需要注意的是，标签ID对齐的具体实现方式可能因任务和库的使用而有所不同。在具体的代码实现中，您可能需要根据您的数据集和模型设置进行相应的标签ID对齐操作</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset


label2id = {<span class="hljs-string">"contradiction"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"neutral"</span>: <span class="hljs-number">1</span>, <span class="hljs-string">"entailment"</span>: <span class="hljs-number">2</span>}
mnli = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mnli"</span>, split=<span class="hljs-string">"train"</span>)
mnli_aligned = mnli.align_labels_with_mapping(label2id, <span class="hljs-string">"label"</span>)
</code></pre>
<h2 id="构建数据集">2.8 构建数据集</h2>
<p>如果您使用自己的数据，可能需要创建一个数据集。使用🤗Datasets创建数据集可以享受到该库的所有优势：快速加载和处理数据、流式处理大型数据集、内存映射等等。您可以使用🤗Datasets的低代码方法轻松快速地创建数据集，减少启动训练模型所需的时间。在许多情况下，只需将数据文件拖放到Hub上的数据集仓库中，就可以轻松完成</p>
<p>在本教程中，您将学习如何使用🤗Datasets的低代码方法创建各种类型的数据集：</p>
<ul>
<li>基于文件夹的构建器(Folder-based builders)，用于快速创建<strong>图像或音频数据集</strong></li>
<li>使用from_方法从本地文件创建数据集</li>
</ul>
<blockquote>
<p>基于文件夹的构建器</p>
</blockquote>
<p>有两个基于文件夹的构建器：<code>ImageFolder(图像文件夹构建器)</code>和<code>AudioFolder(音频文件夹构建器)</code></p>
<p>它们是低代码方法，可以快速创建包含数千个示例的图像、语音和音频数据集。它们非常适用于在扩展到更大的数据集之前，快速原型化计算机视觉和语音模型</p>
<p>基于文件夹的构建器会使用您的数据，并自动生成数据集的特征、划分和标签。具体来说：</p>
<ul>
<li>ImageFolder使用Image特征来解码图像文件。它支持许多图像扩展格式，例如jpg和png，还支持其他格式。您可以查看支持的图像扩展格式的完整列表</li>
<li>AudioFolder使用Audio特征来解码音频文件。它支持音频扩展格式，如wav和mp3，您可以查看支持的音频扩展格式的完整列表</li>
</ul>
<p>例如，如果您的图像数据集(对于音频数据集也是一样)存储如下所示：</p>
<pre><code class="lang-sh">pokemon/train/grass/bulbasaur.png
pokemon/train/fire/charmander.png
pokemon/train/water/squirtle.png

pokemon/<span class="hljs-built_in">test</span>/grass/ivysaur.png
pokemon/<span class="hljs-built_in">test</span>/fire/charmeleon.png
pokemon/<span class="hljs-built_in">test</span>/water/wartortle.png
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ImageFolder
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> AudioFolder

dataset = load_dataset(<span class="hljs-string">"imagefolder"</span>, data_dir=<span class="hljs-string">"/path/to/pokemon"</span>)
dataset = load_dataset(<span class="hljs-string">"audiofolder"</span>, data_dir=<span class="hljs-string">"/path/to/folder"</span>)
</code></pre>
<p>数据集中可以包含有关数据集的其他信息，例如文本标题或转录，可以使用包含在数据集文件夹中的metadata.csv文件来进行存储</p>
<p>metadata文件需要有一个file_name列，将图像或音频文件与其相应的元数据进行关联</p>
<pre><code class="lang-txt">file_name, text
bulbasaur.png, There is a plant seed on its back right from the day this Pokémon is born.
charmander.png, It has a preference for hot things.
squirtle.png, When it retracts its long neck into its shell, it squirts out water with vigorous force.
</code></pre>
<p>To learn more about each of these folder-based builders, check out the and <a href="https://huggingface.co/docs/datasets/image_dataset#imagefolder" target="_blank">ImageFolder</a> or <a href="https://huggingface.co/docs/datasets/audio_dataset#audiofolder" target="_blank">AudioFolder</a> guides.</p>
<blockquote>
<p>基于文件的构建器</p>
</blockquote>
<p>使用 from_generator() 方法是从生成器创建数据集的最节省内存的方式，这是由于生成器的迭代行为。这在处理非常大的数据集时特别有用，因为数据集是逐步在磁盘上生成的，然后进行内存映射，这样可以避免将整个数据集加载到内存中</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gen</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">yield</span> {<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
    <span class="hljs-keyword">yield</span> {<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"squirtle"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"water"</span>}
ds = Dataset.from_generator(gen)
ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
</code></pre>
<p>基于生成器的IterableDataset需要使用for循环进行迭代，例如：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> IterableDataset
ds = IterableDataset.from_generator(gen)
<span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> ds:
    print(example)

{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"squirtle"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"water"</span>}
</code></pre>
<p>使用from_dict()方法是从字典创建数据集的简单直接的方式：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
ds = Dataset.from_dict({<span class="hljs-string">"pokemon"</span>: [<span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"squirtle"</span>], <span class="hljs-string">"type"</span>: [<span class="hljs-string">"grass"</span>, <span class="hljs-string">"water"</span>]})
ds[<span class="hljs-number">0</span>]

{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
</code></pre>
<h2 id="分享数据集">2.9 分享数据集</h2>
<p>点击您的个人资料并选择新的数据集以创建一个新的数据集仓库。 为您的数据集选择一个名称，并选择它是一个公共数据集还是私有数据集。公共数据集对任何人可见，而私有数据集只能由您或您组织的成员查看</p>
<p>一旦您的数据集存储在Hub上，任何人都可以使用load_dataset()函数加载它：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"stevhliu/demo"</span>)
</code></pre>
<blockquote>
<p>使用Python进行上传</p>
</blockquote>
<p>喜欢以编程方式上传数据集的用户可以使用huggingface_hub库。该库允许用户从Python中与Hub进行交互</p>
<p>首先安装该库：</p>
<pre><code class="lang-sh">pip install huggingface_hub
</code></pre>
<p>要在Hub上使用Python上传数据集，您需要登录到您的Hugging Face账户：</p>
<pre><code class="lang-sh">huggingface-cli login
</code></pre>
<p>使用push_to_hub()函数帮助您将文件添加、提交和推送到您的仓库：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"stevhliu/demo"</span>)
<span class="hljs-comment"># dataset = dataset.map(...)  # 在这里进行所有的数据处理</span>
dataset.push_to_hub(<span class="hljs-string">"stevhliu/processed_demo"</span>)
</code></pre>
<p>如果要将数据集设置为私有，请将private参数设置为True。该参数仅在首次创建仓库时有效</p>
<pre><code class="lang-python">dataset.push_to_hub(<span class="hljs-string">"stevhliu/private_processed_demo"</span>, private=<span class="hljs-keyword">True</span>)
</code></pre>
<h1 id="评估指标">3 评估指标</h1>
<h2 id="安装_2">3.1 安装</h2>
<p>一种用于轻松评估机器学习模型和数据集的库</p>
<p>只需一行代码，您就可以访问数十种不同领域(自然语言处理、计算机视觉、强化学习等)的评估方法</p>
<p>无论是在本地机器上还是在分布式训练环境中，您都可以以一种一致且可重复的方式评估您的模型</p>
<blockquote>
<p>安装</p>
</blockquote>
<pre><code class="lang-sh">pip install evaluate
</code></pre>
<blockquote>
<p>测试</p>
</blockquote>
<pre><code class="lang-sh">python -c <span class="hljs-string">"import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))"</span>

{<span class="hljs-string">'exact_match'</span>: 1.0}
</code></pre>
<h2 id="快速开始_2">3.2 快速开始</h2>
<h3 id="指标种类">3.2.1 指标种类</h3>
<blockquote>
<p><a href="https://huggingface.co/evaluate-metric" target="_blank">Evaluate Metric卡片实例</a></p>
</blockquote>
<p>🤗Evaluate提供了广泛的评估工具。它涵盖了文本、计算机视觉、音频等多种形式，并提供了用于评估模型或数据集的工具。这些工具分为三个类别</p>
<p>评估类型 典型的机器学习流程涉及到不同方面的评估，对于每个方面，🤗 Evaluate都提供了相应的工具：</p>
<ul>
<li><strong>指标(Metric)</strong>：用于评估模型的性能，通常涉及模型的预测结果和一些真实标签。您可以在evaluate-metric中找到所有集成的指标</li>
<li><strong>比较(Comparison)</strong>：用于比较两个模型。可以通过将它们的预测结果与真实标签进行比较并计算它们的一致性来进行比较。您可以在evaluate-comparison中找到所有集成的比较方法</li>
<li><strong>测量(Measurement)</strong>：数据集和训练在其上的模型同样重要。通过测量可以研究数据集的特性。您可以在evaluate-measurement中找到所有集成的测量方法</li>
</ul>
<p>每个评估模块都作为一个Space存储在Hugging Face Hub上。它们提供了一个交互式小部件和一个文档卡片，用于记录其使用方法和限制</p>
<blockquote>
<p>评估工具之间的关系和区别</p>
</blockquote>
<p>Evaluate库中的<code>Metric(指标)</code>、<code>Comparison(比较)</code>和<code>Measurement(测量)</code>是三种不同的评估工具，用于评估机器学习模型和数据集。它们之间的关系和区别如下：</p>
<ol>
<li>Metric(指标)：<ul>
<li>用途：<strong>用于评估模型的性能</strong></li>
<li>具体含义：指标通过将模型的预测结果与真实标签进行比较来衡量模型的表现</li>
<li>示例：准确率、精确率、召回率、F1分数等</li>
<li>目的：提供了对模型性能的定量评估，帮助衡量模型在特定任务上的表现</li>
</ul>
</li>
<li>Comparison(比较)：<ul>
<li>用途：用于<strong>比较两个模型之间的差异</strong></li>
<li>具体含义：比较工具将两个模型的预测结果与真实标签进行对比，计算它们之间的一致性或差异程度</li>
<li>示例：一致性指标、相对误差等</li>
<li>目的：帮助评估不同模型之间的性能差异，找到更好的模型或进行模型选择</li>
</ul>
</li>
<li>Measurement(测量)：<ul>
<li>用途：用于<strong>研究数据集的属性和特性</strong></li>
<li>具体含义：测量工具用于对数据集进行分析，探索数据集的结构、分布、偏差等方面的信息</li>
<li>示例：数据集大小、样本分布、类别不平衡度等</li>
<li>目的：提供对数据集的详细了解，帮助了解数据集的特点和潜在问题</li>
</ul>
</li>
</ol>
<p>这三种评估工具在Evaluate库中各自独立，用于不同的评估目的。Metric用于衡量模型性能，Comparison用于比较不同模型之间的性能差异，Measurement用于研究和了解数据集的特性。通过使用这些工具，可以全面评估和理解机器学习模型和数据集的表现和特点</p>
<h3 id="指标加载">3.2.2 指标加载</h3>
<blockquote>
<p>官方+社区 指标</p>
</blockquote>
<p>在使用Hugging Face的Evaluate库加载评估工具时，可以通过显式指定评估的类型来确保加载正确的工具。这可以防止名称冲突或混淆，确保您使用的是期望的评估工具</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> evaluate

accuracy = evaluate.load(<span class="hljs-string">"accuracy"</span>)

<span class="hljs-comment"># 显式指定评估的类型</span>
word_length = evaluate.load(<span class="hljs-string">"word_length"</span>, module_type=<span class="hljs-string">"measurement"</span>)

<span class="hljs-comment"># 社区指标</span>
element_count = evaluate.load(<span class="hljs-string">"lvwerra/element_count"</span>, module_type=<span class="hljs-string">"measurement"</span>)
</code></pre>
<blockquote>
<p>查看可用的模块方法</p>
</blockquote>
<pre><code class="lang-python">evaluate.list_evaluation_modules(
  module_type=<span class="hljs-string">"comparison"</span>,
  include_community=<span class="hljs-keyword">False</span>,
  with_details=<span class="hljs-keyword">True</span>)

[{<span class="hljs-string">'name'</span>: <span class="hljs-string">'mcnemar'</span>, <span class="hljs-string">'type'</span>: <span class="hljs-string">'comparison'</span>, <span class="hljs-string">'community'</span>: <span class="hljs-keyword">False</span>, <span class="hljs-string">'likes'</span>: <span class="hljs-number">1</span>},
 {<span class="hljs-string">'name'</span>: <span class="hljs-string">'exact_match'</span>, <span class="hljs-string">'type'</span>: <span class="hljs-string">'comparison'</span>, <span class="hljs-string">'community'</span>: <span class="hljs-keyword">False</span>, <span class="hljs-string">'likes'</span>: <span class="hljs-number">0</span>}]
</code></pre>
<h3 id="指标计算">3.2.3 指标计算</h3>
<blockquote>
<p>计算指标</p>
</blockquote>
<p>当涉及到计算实际得分时，有两种主要的方法：</p>
<ul>
<li><p><strong>一体式计算(All-in-one)</strong>：通过一次性将所有必要的输入传递给compute()方法来计算得分</p>
<pre><code class="lang-python">accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])

{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.5</span>}
</code></pre>
</li>
<li><p><strong>逐步计算(Incremental)</strong>：通过使用EvaluationModule.add()或EvaluationModule.add_batch()将必要的输入逐步添加到模块中，然后在最后使用 EvaluationModule.compute()计算得分</p>
<pre><code class="lang-python"><span class="hljs-comment"># add的方式</span>
<span class="hljs-keyword">for</span> ref, pred <span class="hljs-keyword">in</span> zip([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]):
    accuracy.add(references=ref, predictions=pred)
accuracy.compute()
{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.5</span>}

<span class="hljs-comment"># add_batch的方式</span>
<span class="hljs-keyword">for</span> refs, preds <span class="hljs-keyword">in</span> zip([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]]):
    accuracy.add_batch(references=refs, predictions=preds)
accuracy.compute()
{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.5</span>}
</code></pre>
</li>
</ul>
<p>在你需要以批量方式从模型中获取预测结果时特别有用：</p>
<pre><code class="lang-python"><span class="hljs-keyword">for</span> model_inputs, gold_standards <span class="hljs-keyword">in</span> evaluation_dataset:
    predictions = model(model_inputs)
    metric.add_batch(references=gold_standards, predictions=predictions)
metric.compute()
</code></pre>
<blockquote>
<p>分布式指标</p>
</blockquote>
<p>在分布式环境中计算指标可能会有些棘手。指标评估是在不同的数据子集上的单独Python进程或节点中执行的</p>
<p>通常情况下，当一个指标得分是可加的(<script type="math/tex; "> f(A \cup B) = f(A) + f(B)</script>)时，你可以使用分布式的reduce操作来收集每个数据子集的得分。但是当指标是非可加的(<script type="math/tex; "> f(A \cup B) \neq f(A) + f(B)</script>)时，情况就不那么简单了。例如，你不能将每个数据子集的F1分数相加作为最终的指标</p>
<p><strong>克服这个问题的常见方法是回退到单进程评估，但指标在单个GPU上进行评估，这会导致效率降低</strong></p>
<ol>
<li>🤗Evaluate通过仅在第一个节点上计算最终的指标来解决了这个问题</li>
<li><strong>预测结果和参考结果被分别计算并提供给每个节点的指标</strong>，这些结果暂时存储在Apache Arrow表中，避免了GPU或CPU内存的混乱</li>
<li>当你准备使用compute()计算最终指标时，第一个节点能够访问所有其他节点上存储的预测结果和参考结果。一旦它收集到所有的预测结果和参考结果，compute()将进行最终的指标评估</li>
</ol>
<p>这个解决方案使得🤗Evaluate能够在分布式设置中执行分布式预测，这对于提高评估速度非常重要。同时，你还可以使用复杂的非可加指标，而不浪费宝贵的GPU或CPU内存</p>
<blockquote>
<p>组合评估</p>
</blockquote>
<p>通常情况下，我们不仅想评估单个指标，而是想评估一系列不同的指标，以捕捉模型性能的不同方面</p>
<p>例如，对于分类问题，除了准确度外，通常还会计算F1分数、召回率和精确度，以便更好地了解模型的性能。当然，你可以加载一系列指标并依次调用它们。然而，一种更方便的方法是使用combine()函数将它们捆绑在一起：</p>
<pre><code class="lang-python">clf_metrics = evaluate.combine([<span class="hljs-string">"accuracy"</span>, <span class="hljs-string">"f1"</span>, <span class="hljs-string">"precision"</span>, <span class="hljs-string">"recall"</span>])
clf_metrics.compute(predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

{
  <span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.667</span>,
  <span class="hljs-string">'f1'</span>: <span class="hljs-number">0.667</span>,
  <span class="hljs-string">'precision'</span>: <span class="hljs-number">1.0</span>,
  <span class="hljs-string">'recall'</span>: <span class="hljs-number">0.5</span>
}
</code></pre>
<blockquote>
<p>自动化评估</p>
</blockquote>
<p><strong>使用evaluate.evaluator()提供了自动化的评估功能</strong>，只需要一个模型、数据集和度量指标，与EvaluationModules中的度量指标相比，它不需要模型的预测结果。因此，使用给定的度量指标在数据集上评估模型更容易，因为推理过程是在内部处理的</p>
<p>为了实现这一点，它使用了transformers库中的pipeline抽象。然而，只要符合pipeline接口，你也可以使用自己的框架</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-keyword">import</span> evaluate
</code></pre>
<p>为了使用evaluator进行评估，让我们加载一个基于IMDb训练的transformers pipeline（但你也可以传递自己的自定义推理类来适应任何遵循pipeline调用API的框架），并使用IMDb的测试集和准确度度量指标进行评估</p>
<pre><code class="lang-python">pipe = pipeline(<span class="hljs-string">"text-classification"</span>, model=<span class="hljs-string">"lvwerra/distilbert-imdb"</span>, device=<span class="hljs-number">0</span>)
data = load_dataset(<span class="hljs-string">"imdb"</span>, split=<span class="hljs-string">"test"</span>).shuffle().select(range(<span class="hljs-number">1000</span>))
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)

task_evaluator = evaluator(<span class="hljs-string">"text-classification"</span>)
results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={<span class="hljs-string">"NEGATIVE"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"POSITIVE"</span>: <span class="hljs-number">1</span>},)

{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.934</span>}
</code></pre>
<p>仅仅计算度量指标的值通常还不足以知道一个模型是否显著优于另一个模型。通过使用<code>自助法(bootstrapping)</code>，evaluate计算置信区间和标准误差，这有助于估计分数的稳定性</p>
<pre><code class="lang-python">results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={<span class="hljs-string">"NEGATIVE"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"POSITIVE"</span>: <span class="hljs-number">1</span>},
                       strategy=<span class="hljs-string">"bootstrap"</span>, n_resamples=<span class="hljs-number">200</span>)

{<span class="hljs-string">'accuracy'</span>:
    {
      <span class="hljs-string">'confidence_interval'</span>: (<span class="hljs-number">0.906</span>, <span class="hljs-number">0.9406749892841922</span>),
      <span class="hljs-string">'standard_error'</span>: <span class="hljs-number">0.00865213251082787</span>,
      <span class="hljs-string">'score'</span>: <span class="hljs-number">0.923</span>
    }
}
</code></pre>
<p>评估器期望数据输入具有"text"和"label"列。如果您的数据集不同，可以使用关键字参数input_column="text"和label_column="label"来提供列名</p>
<p>目前只支持"text-classification"任务，将来可能会添加更多的任务类型</p>
<h3 id="结果存储">3.2.4 结果存储</h3>
<blockquote>
<p>评估结果save和push</p>
</blockquote>
<p>保存和分享评估结果是一个重要的步骤。我们提供evaluate.save()函数来方便地保存指标结果。你可以传递一个特定的文件名或目录。在后一种情况下，结果将保存在一个带有自动创建的文件名的文件中</p>
<p>除了目录或文件名，该函数还接受任意的键值对作为输入，并将它们存储在一个JSON文件中</p>
<pre><code class="lang-python">result = accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])

hyperparams = {<span class="hljs-string">"model"</span>: <span class="hljs-string">"bert-base-uncased"</span>}
evaluate.save(<span class="hljs-string">"./results/"</span>, experiment=<span class="hljs-string">"run 42"</span>, **result, **hyperparams)

PosixPath(<span class="hljs-string">'results/result-2022_05_30-22_09_11.json'</span>)

<span class="hljs-comment"># result-2022_05_30-22_09_11.json</span>
{
    <span class="hljs-string">"experiment"</span>: <span class="hljs-string">"run 42"</span>,
    <span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.5</span>,
    <span class="hljs-string">"model"</span>: <span class="hljs-string">"bert-base-uncased"</span>,
    <span class="hljs-string">"_timestamp"</span>: <span class="hljs-string">"2022-05-30T22:09:11.959469"</span>,
    <span class="hljs-string">"_git_commit_hash"</span>: <span class="hljs-string">"123456789abcdefghijkl"</span>,
    <span class="hljs-string">"_evaluate_version"</span>: <span class="hljs-string">"0.1.0"</span>,
    <span class="hljs-string">"_python_version"</span>: <span class="hljs-string">"3.9.12 (main, Mar 26 2022, 15:51:15) \n[Clang 13.1.6 (clang-1316.0.21.2)]"</span>,
    <span class="hljs-string">"_interpreter_path"</span>: <span class="hljs-string">"/Users/leandro/git/evaluate/env/bin/python"</span>
}
</code></pre>
<p>除了指定的字段，它还包含有用的系统信息，用于重现结果，你还应该将它们报告到模型在Hub上的存储库中</p>
<pre><code class="lang-python">evaluate.push_to_hub(
  model_id=<span class="hljs-string">"huggingface/gpt2-wikitext2"</span>,  <span class="hljs-comment"># model repository on hub</span>
  metric_value=<span class="hljs-number">0.5</span>,                       <span class="hljs-comment"># metric value</span>
  metric_type=<span class="hljs-string">"bleu"</span>,                     <span class="hljs-comment"># metric name, e.g. accuracy.name</span>
  metric_name=<span class="hljs-string">"BLEU"</span>,                     <span class="hljs-comment"># pretty name which is displayed</span>
  dataset_type=<span class="hljs-string">"wikitext"</span>,                <span class="hljs-comment"># dataset name on the hub</span>
  dataset_name=<span class="hljs-string">"WikiText"</span>,                <span class="hljs-comment"># pretty name</span>
  dataset_split=<span class="hljs-string">"test"</span>,                   <span class="hljs-comment"># dataset split used</span>
  task_type=<span class="hljs-string">"text-generation"</span>,            <span class="hljs-comment"># task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json</span>
  task_name=<span class="hljs-string">"Text Generation"</span>             <span class="hljs-comment"># pretty name for task</span>
)
</code></pre>
<blockquote>
<p>上传自己的指标<a href="https://huggingface.co/docs/evaluate/main/en/creating_and_sharing" target="_blank">Creating and sharing a new evaluation</a></p>
</blockquote>
<h3 id="可视化">3.2.5 可视化</h3>
<p>当比较多个模型时，仅通过查看它们的得分往往很难发现它们之间的差异。而且通常情况下，并没有一个单一的最佳模型，而是在准确性和延迟等方面存在着权衡，因为较大的模型可能具有更好的性能但也更慢。我们正在逐步添加不同的可视化方法，例如绘图，以便更轻松地选择适合特定用例的最佳模型。</p>
<p>例如，如果您有多个模型的结果列表（以字典形式），您可以将它们传递给radar_plot()函数进行可视化：</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> evaluate
<span class="hljs-keyword">from</span> evaluate.visualization <span class="hljs-keyword">import</span> radar_plot

data = [
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.99</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.8</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.95</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">33.6</span>},
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.98</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.87</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.91</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">11.2</span>},
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.98</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.78</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">87.6</span>}, 
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.78</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.81</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">101.6</span>}
   ]
model_names = [<span class="hljs-string">"Model 1"</span>, <span class="hljs-string">"Model 2"</span>, <span class="hljs-string">"Model 3"</span>, <span class="hljs-string">"Model 4"</span>]
plot = radar_plot(data=data, model_names=model_names)
plot.show()
</code></pre>
<p><a data-lightbox="4a97af1c-0f3f-4405-a27e-c39d357cbce3" data-title="模型比较指标图" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/模型比较指标图.webp" target="_blank"><img alt="模型比较指标图" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/模型比较指标图.webp"/></a></p>
<h3 id="选择合适指标">3.2.6 选择合适指标</h3>
<p>评估指标可以分为三个高级类别：</p>
<ul>
<li><p><strong>通用指标</strong>：适用于各种情况和数据集的指标，例如精确度和准确度</p>
<pre><code class="lang-python">precision_metric = evaluate.load(<span class="hljs-string">"precision"</span>)
results = precision_metric.compute(references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
print(results)

{<span class="hljs-string">'precision'</span>: <span class="hljs-number">1.0</span>}
</code></pre>
</li>
<li><p><strong>任务特定指标</strong>：仅适用于特定任务的指标，例如机器翻译(通常使用BLEU或ROUGE指标进行评估)或命名实体识别(通常使用seqeval进行评估)</p>
</li>
<li><p><strong>数据集特定指标</strong>：旨在衡量模型在特定基准数据集上的性能，例如GLUE基准测试具有专门的评估指标</p>
</li>
</ul>
<h1 id="transformers">4 transformers</h1>
<h2 id="概述_2">4.1 概述</h2>
<blockquote>
<p><a href="https://huggingface.co/docs/transformers/v4.30.0/en/task_summary" target="_blank">What 🤗 Transformers can do</a></p>
</blockquote>
<p>🤗 Transformers提供了API和工具，可轻松下载和训练最先进的预训练模型。使用预训练模型可以减少计算成本、碳足迹，并节省从头开始训练模型所需的时间和资源。这些模型支持不同领域的常见任务，包括：</p>
<ul>
<li>📝 自然语言处理：文本分类、命名实体识别、问答系统、语言建模、摘要生成、翻译、多项选择和文本生成</li>
<li>🖼️ 计算机视觉：图像分类、目标检测和分割</li>
<li>🗣️ 音频：自动语音识别和音频分类</li>
<li>🐙 多模态：表格问答、光学字符识别、从扫描文档中提取信息、视频分类和视觉问答</li>
</ul>
<p>🤗 Transformers支持在PyTorch、TensorFlow和JAX之间进行框架互操作。这提供了在模型的不同阶段使用不同框架的灵活性；可以在一个框架中用三行代码训练模型，然后在另一个框架中加载模型进行推理。模型还可以导出为ONNX和TorchScript等格式，以便在生产环境中进行部署</p>
<h2 id="安装_3">4.2 安装</h2>
<pre><code class="lang-cmd">pip install transformers datasets
</code></pre>
<h2 id="快速开始_3">4.3 快速开始</h2>
<h3 id="pipeline">4.3.1 Pipeline</h3>
<p>pipeline()是使用预训练模型进行推理的最简单和最快捷的方法。您可以直接使用pipeline()进行许多任务的推理，涵盖了不同的模态，下表列出了其中一些任务</p>
<table>
<thead>
<tr>
<th><strong>Task</strong></th>
<th><strong>Description</strong></th>
<th><strong>Modality</strong></th>
<th><strong>Pipeline identifier</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Text classification</td>
<td>assign a label to a given sequence of text</td>
<td>NLP</td>
<td>pipeline(task=“sentiment-analysis”)</td>
</tr>
<tr>
<td>Text generation</td>
<td>generate text given a prompt</td>
<td>NLP</td>
<td>pipeline(task=“text-generation”)</td>
</tr>
<tr>
<td>Summarization</td>
<td>generate a summary of a sequence of text or document</td>
<td>NLP</td>
<td>pipeline(task=“summarization”)</td>
</tr>
<tr>
<td>Image classification</td>
<td>assign a label to an image</td>
<td>CV</td>
<td>pipeline(task=“image-classification”)</td>
</tr>
<tr>
<td>Image segmentation</td>
<td>assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)</td>
<td>CV</td>
<td>pipeline(task=“image-segmentation”)</td>
</tr>
<tr>
<td>Object detection</td>
<td>predict the bounding boxes and classes of objects in an image</td>
<td>CV</td>
<td>pipeline(task=“object-detection”)</td>
</tr>
<tr>
<td>Audio classification</td>
<td>assign a label to some audio data</td>
<td>Audio</td>
<td>pipeline(task=“audio-classification”)</td>
</tr>
<tr>
<td>Automatic speech recognition</td>
<td>transcribe speech into text</td>
<td>Audio</td>
<td>pipeline(task=“automatic-speech-recognition”)</td>
</tr>
<tr>
<td>Visual question answering</td>
<td>answer a question about the image, given an image and a question</td>
<td>Multimodal</td>
<td>pipeline(task=“vqa”)</td>
</tr>
<tr>
<td>Document question answering</td>
<td>answer a question about a document, given an image and a question</td>
<td>Multimodal</td>
<td>pipeline(task=“document-question-answering”)</td>
</tr>
<tr>
<td>Image captioning</td>
<td>generate a caption for a given image</td>
<td>Multimodal</td>
<td>pipeline(task=“image-to-text”)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>基本使用</p>
</blockquote>
<p>首先，通过创建pipeline()的实例并指定要使用的任务，开始使用它。在本指南中，我们以情感分析的pipeline()为例：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">"sentiment-analysis"</span>)
</code></pre>
<p>pipeline()会下载并缓存用于情感分析的默认预训练模型和分词器。现在，您可以在目标文本上使用分类器了：</p>
<pre><code class="lang-python">classifier(<span class="hljs-string">"We are very happy to show you the 🤗 Transformers library."</span>)

[{<span class="hljs-string">'label'</span>: <span class="hljs-string">'POSITIVE'</span>, <span class="hljs-string">'score'</span>: <span class="hljs-number">0.9998</span>}]
</code></pre>
<p>如果您有多个输入，请将输入作为列表传递给pipeline()，以返回一个字典列表</p>
<pre><code class="lang-python">results = classifier([<span class="hljs-string">"We are very happy to show you the 🤗 Transformers library."</span>, <span class="hljs-string">"We hope you don't hate it."</span>])
<span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
    print(f<span class="hljs-string">"label: {result['label']}, with score: {round(result['score'], 4)}"</span>)

label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>
</code></pre>
<p>pipeline()还可以对任何您喜欢的任务迭代整个数据集。在这个例子中，让我们选择自动语音识别作为我们的任务</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

speech_recognizer = pipeline(<span class="hljs-string">"automatic-speech-recognition"</span>, model=<span class="hljs-string">"facebook/wav2vec2-base-960h"</span>)
</code></pre>
<p>加载您想要迭代的音频数据集(有关更多详细信息，请参阅🤗 Datasets快速入门)。例如，加载MInDS-14数据集：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

dataset = load_dataset(<span class="hljs-string">"PolyAI/minds14"</span>, name=<span class="hljs-string">"en-US"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
<p>您需要确保数据集的采样率与facebook/wav2vec2-base-960h 训练时使用的采样率相匹配</p>
<pre><code class="lang-python">dataset = dataset.cast_column(<span class="hljs-string">"audio"</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
</code></pre>
<p>调用"audio"列时，音频文件会自动加载和重新采样。从前四个样本中提取原始波形数组，并将其作为列表传递给pipeline：</p>
<pre><code class="lang-python">result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">"audio"</span>])
print([d[<span class="hljs-string">"text"</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])

[<span class="hljs-string">'I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT'</span>, <span class="hljs-string">"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE"</span>, <span class="hljs-string">"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS"</span>, <span class="hljs-string">'HOW DO I FURN A JOINA COUT'</span>]
</code></pre>
<p>对于输入较大的更大数据集(如语音或视觉数据)，您可以将生成器传递给pipeline，而不是将其作为列表加载到内存中</p>
<p>在pipeline中使用其他模型和分词器pipeline()可以适应Hub中的任何模型，这使得对pipeline()进行其他用途的调整变得容易</p>
<p>例如，如果您想要一个能够处理法语文本的模型，请使用Hub上的标签来过滤合适的模型。通过对过滤结果进行排序，您可以获得一个针对法语文本进行情感分析的多语言BERT模型</p>
<blockquote>
<p>在pipeline中使用另一个模型和分词器</p>
</blockquote>
<p>pipeline()可以适应Hub中的任何模型，这使得将pipeline()适应其他用例变得容易</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

model_name = <span class="hljs-string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

classifier = pipeline(<span class="hljs-string">"sentiment-analysis"</span>, model=model, tokenizer=tokenizer)
classifier(<span class="hljs-string">"Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers."</span>)
</code></pre>
<h3 id="autoclass">4.3.2 AutoClass</h3>
<p>AutoClass是一种快捷方式，它可以根据模型的名称或路径自动获取预训练模型的架构。您只需要选择与您的任务相匹配的AutoClass和相应的预处理类</p>
<h4 id="autotokenizer"><a class="anchor-navigation-ex-anchor" href="#autotokenizer" name="autotokenizer"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#autotokenizer" name="autotokenizer"><i aria-hidden="true" class="fa fa-link"></i></a>AutoTokenizer</h4>
<p>AutoTokenizer分词器负责将文本预处理为模型输入的数字数组。有多个规则来规定分词的过程，包括如何拆分一个单词以及以何种级别拆分单词</p>
<p>最重要的是，您<strong>需要使用相同的模型名称来实例化一个分词器，以确保您使用了与预训练模型相同的分词规则</strong></p>
<blockquote>
<p>使用AutoTokenizer加载一个分词器</p>
</blockquote>
<p>将<code>return_tensors</code>参数设置为<code>pt</code>以返回适用于PyTorch的张量，或者设置为<code>tf</code>以返回适用于TensorFlow的张量</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_name = <span class="hljs-string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
tokenizer = AutoTokenizer.from_pretrained(model_name)
encoding = tokenizer(<span class="hljs-string">"We are very happy to show you the 🤗 Transformers library."</span>, return_tensors=<span class="hljs-string">"pt"</span>)

{<span class="hljs-string">'input_ids'</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">'token_type_ids'</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">'attention_mask'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}

tokenizer.decode(encoding[<span class="hljs-string">"input_ids"</span>])
<span class="hljs-string">"We are very happy to show you the 🤗 Transformers library."</span>
</code></pre>
<p>分词器返回一个包含三个项目的字典：</p>
<ul>
<li><strong>input_ids</strong>：表示文本中各个标记的数字</li>
<li><strong>token_type_ids</strong>：如果有多个序列，指示一个标记属于哪个序列</li>
<li><strong>attention_mask</strong>：指示一个标记是否应该被掩盖(masked)</li>
</ul>
<p>分词器还可以接受一个输入列表，并对文本进行填充和截断，以返回具有统一长度的批处理数据</p>
<pre><code class="lang-python">pt_batch = tokenizer(
    [<span class="hljs-string">"We are very happy to show you the 🤗 Transformers library."</span>, <span class="hljs-string">"We hope you don't hate it."</span>],
    padding=<span class="hljs-keyword">True</span>,
    truncation=<span class="hljs-keyword">True</span>,
    max_length=<span class="hljs-number">512</span>,
    return_tensors=<span class="hljs-string">"pt"</span>)
</code></pre>
<blockquote>
<p>pad + truncation</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># padding</span>
batch_sentences = [
    <span class="hljs-string">"But what about second breakfast?"</span>,
    <span class="hljs-string">"Don't think he knows about second breakfast, Pip."</span>,
    <span class="hljs-string">"What about elevensies?"</span>,
]
encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-keyword">True</span>)

{<span class="hljs-string">'input_ids'</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'token_type_ids'</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'attention_mask'</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}

<span class="hljs-comment"># truncation  将truncation参数设置为True，可以将序列截断为模型所能接受的最大长度</span>
batch_sentences = [
    <span class="hljs-string">"But what about second breakfast?"</span>,
    <span class="hljs-string">"Don't think he knows about second breakfast, Pip."</span>,
    <span class="hljs-string">"What about elevensies?"</span>,
]
encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-keyword">True</span>, truncation=<span class="hljs-keyword">True</span>)

{<span class="hljs-string">'input_ids'</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'token_type_ids'</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'attention_mask'</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}
</code></pre>
<h4 id="automodel"><a class="anchor-navigation-ex-anchor" href="#automodel" name="automodel"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#automodel" name="automodel"><i aria-hidden="true" class="fa fa-link"></i></a>AutoModel</h4>
<p>🤗Transformers提供了一种简单而统一的方法来加载预训练模型实例。这意味着您可以像加载AutoTokenizer一样加载AutoModel</p>
<p>唯一的区别是<strong>选择正确的AutoModel来适应任务</strong>。对于文本(或序列)分类，您应该加载AutoModelForSequenceClassification</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

model_name = <span class="hljs-string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)

pt_outputs = pt_model(**pt_batch)
</code></pre>
<p>模型将最终的激活值存储在logits属性中。应用softmax函数到logits上以获取概率值</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=<span class="hljs-number">-1</span>)


tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)
</code></pre>
<blockquote>
<p>在huggingface库中，AutoModel类可以根据给定的checkpoint自动选择并加载适合的模型。它支持各种不同的模型架构，包括：</p>
</blockquote>
<ul>
<li>AutoModel: 用于通用的模型加载，根据checkpoint自动选择适合的模型架构</li>
<li>AutoModelForSequenceClassification: 用于序列分类任务的模型，如文本分类</li>
<li>AutoModelForQuestionAnswering: 用于问答任务的模型，如阅读理解</li>
<li>AutoModelForTokenClassification: 用于标记分类任务的模型，如命名实体识别</li>
<li>AutoModelForMaskedLM: 用于遮蔽语言建模任务的模型，如BERT</li>
<li>AutoModelForCausalLM: 用于有因果关系的语言建模任务的模型，如GPT</li>
<li>AutoModelForImageClassification: 用于图像分类任务的模型，如ResNet</li>
<li>AutoModelForImageSegmentation: 用于图像分割任务的模型，如Mask R-CNN</li>
</ul>
<p>这些仅是AutoModel类的一些示例，实际上还有更多可用的模型架构。您可以根据具体的任务需求选择适合的AutoModel类进行加载和使用</p>
<h4 id="其他的auto类"><a class="anchor-navigation-ex-anchor" href="#其他的auto类" name="其他的auto类"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#其他的auto类" name="其他的auto类"><i aria-hidden="true" class="fa fa-link"></i></a>其他的Auto类</h4>
<blockquote>
<p>AutoImageProcessor</p>
</blockquote>
<p>对于视觉任务，图像处理器将图像处理为正确的输入格式</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">"google/vit-base-patch16-224"</span>)
</code></pre>
<blockquote>
<p>AutoFeatureExtractor</p>
</blockquote>
<p>对于音频任务，特征提取器将音频信号处理为正确的输入格式</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(
    <span class="hljs-string">"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"</span>
)
</code></pre>
<blockquote>
<p>AutoProcessor</p>
</blockquote>
<p>多模态任务需要一个处理器来结合两种类型的预处理工具。例如，LayoutLMV2模型需要一个图像处理器来处理图像，还需要一个分词器来处理文本；处理器将两者结合起来</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">"microsoft/layoutlmv2-base-uncased"</span>)
</code></pre>
<h4 id="模型保存"><a class="anchor-navigation-ex-anchor" href="#模型保存" name="模型保存"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#模型保存" name="模型保存"><i aria-hidden="true" class="fa fa-link"></i></a>模型保存</h4>
<p>一旦您的模型经过微调，您可以使用PreTrainedModel.save_pretrained()将其与其标记器一起保存起来：</p>
<pre><code class="lang-python"><span class="hljs-comment"># 模型+分词器 保存</span>
pt_save_directory = <span class="hljs-string">"./pt_save_pretrained"</span>
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)

<span class="hljs-comment"># 加载</span>
pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory)
</code></pre>
<h3 id="autoconfig">4.3.3 AutoConfig</h3>
<p>您可以修改模型的<strong>配置类</strong>来更改模型的构建方式。配置类指定了模型的属性，例如隐藏层的数量或注意力头数</p>
<p>当您从自定义配置类初始化模型时，您将从头开始。模型的属性将被随机初始化，您需要在使用模型之前对其进行训练以获得有意义的结果</p>
<p>首先导入AutoConfig，然后加载要修改的预训练模型。在AutoConfig.from_pretrained()中，您可以指定要更改的属性，例如注意力头的数量：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

my_config = AutoConfig.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>, n_heads=<span class="hljs-number">12</span>)
my_model = AutoModel.from_config(my_config)
</code></pre>
<h3 id="tokenizer">4.3.4 tokenizer</h3>
<blockquote>
<p><a href="https://huggingface.co/docs/transformers/v4.37.2/zh/tokenizer_summary" target="_blank">huggingface的分词器的摘要</a></p>
<p><a href="http://www.360doc.com/content/23/0605/20/7673502_1083606595.shtml" target="_blank">【LLM系列之Tokenizer】如何科学地训练一个LLM分词器</a></p>
</blockquote>
<p>参见：<a href="https://study.hycbook.com/article/57252.html" target="_blank">LLM Tokenizer分词系列</a></p>
<h3 id="trainer">4.3.5 Trainer</h3>
<p>对于PyTorch，所有模型都是标准的torch.nn.Module，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，但🤗Transformers提供了<code>Trainer</code>类，其中包含基本的训练循环，并添加了其他功能，如分布式训练、混合精度等</p>
<p>根据您的任务，通常会向Trainer传递以下参数：</p>
<ol>
<li><p><a href="https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/model#transformers.PreTrainedModel" target="_blank">PreTrainedModel</a>或<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" target="_blank"><code>torch.nn.Module</code></a>对象</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>)
</code></pre>
</li>
<li><p><strong>TrainingArguments</strong>包含了可以修改的模型超参数，比如学习率、批大小和训练的轮数。如果你不指定任何训练参数，将使用默认值</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">"path/to/save/folder/"</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">8</span>,
    per_device_eval_batch_size=<span class="hljs-number">8</span>,
    num_train_epochs=<span class="hljs-number">2</span>,
)
</code></pre>
</li>
<li><p><strong>Preprocessing</strong>类，例如tokenizer(标记器)、image processor(图像处理器)、feature extractor(特征提取器)或processor(处理器)</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>)
</code></pre>
</li>
<li><p>加载数据集</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>
</code></pre>
</li>
<li><p>创建一个函数来对数据集进行<strong>标记化</strong>处理，然后使用<code>map</code>函数将其应用于整个数据集</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_dataset</span><span class="hljs-params">(dataset)</span>:</span>
    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">"text"</span>])

dataset = dataset.map(tokenize_dataset, batched=<span class="hljs-keyword">True</span>)
</code></pre>
</li>
<li><p>使用<code>DataCollatorWithPadding</code>来从数据集中创建一个批次的示例</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
</code></pre>
<p><code>DataCollatorWithPadding</code>是Hugging Face的<code>transformers</code>库中的一个类，用于在训练过程中创建批次数据。它的作用是将不同长度的样本填充到相同长度，以便能够同时进行批处理</p>
<p>具体来说，<code>DataCollatorWithPadding</code>会根据给定的数据集，找到其中最长的样本，并将其他样本填充到相同的长度。填充通常使用特定的填充令牌(token)来完成，这样模型在处理时可以识别出填充部分，并进行相应的处理</p>
<p>使用<code>DataCollatorWithPadding</code>可以确保批次数据的长度一致，从而提高训练效率，并避免由于不同长度样本导致的错误</p>
</li>
</ol>
<p>现在将所有这些类组合在Trainer中</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset[<span class="hljs-string">"train"</span>],
    eval_dataset=dataset[<span class="hljs-string">"test"</span>],
    tokenizer=tokenizer,
    data_collator=data_collator,
)  <span class="hljs-comment"># doctest: +SKIP</span>

trainer.train()
</code></pre>
<p>Trainer类提供了自定义训练循环行为的方法，你可以通过继承Trainer类并重写其中的方法来实现自定义行为。这样你就可以定制诸如损失函数、优化器和学习率调度器等功能。你可以参考Trainer类的文档了解可以重写的方法</p>
<p>另一种定制训练循环的方式是使用回调函数(Callbacks)。你可以使用回调函数与其他库进行集成，监视训练过程并报告进展，或在必要时提前停止训练。回调函数不会修改训练循环本身的行为。如果你需要定制损失函数等内容，你需要继承Trainer类来实现</p>
<h1 id="教程">5 教程</h1>
<h2 id="模型训练">5.1 模型训练</h2>
<p>使用预训练模型有很多好处。它可以减少计算成本和碳足迹，并且可以让您使用最先进的模型，而无需从头开始训练</p>
<p>🤗Transformers提供了对各种任务的数千个预训练模型的访问。当您使用预训练模型时，您可以在特定于您任务的数据集上进行微调训练。这被称为<code>微调</code>，是一种非常强大的训练技术</p>
<blockquote>
<p>数据准备</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-comment"># 1. 加载数据集</span>
dataset = load_dataset(<span class="hljs-string">"yelp_review_full"</span>)
dataset[<span class="hljs-string">"train"</span>][<span class="hljs-number">100</span>]
{<span class="hljs-string">'label'</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">'text'</span>: <span class="hljs-string">'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'</span>}
<span class="hljs-comment"># 可以创建一个较小的数据集子集，用于微调，以减少所需的时间</span>
small_train_dataset = tokenized_datasets[<span class="hljs-string">"train"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(range(<span class="hljs-number">1000</span>))
small_eval_dataset = tokenized_datasets[<span class="hljs-string">"test"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(range(<span class="hljs-number">1000</span>))

<span class="hljs-comment"># 2. 分词器</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_function</span><span class="hljs-params">(examples)</span>:</span>
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">"text"</span>], padding=<span class="hljs-string">"max_length"</span>, truncation=<span class="hljs-keyword">True</span>)
tokenized_datasets = dataset.map(tokenize_function, batched=<span class="hljs-keyword">True</span>)
</code></pre>
<blockquote>
<p>Train with PyTorch Trainer</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> evaluate

<span class="hljs-comment"># 1. 加载模型</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">5</span>)

<span class="hljs-comment"># 2. 定义训练参数  在训练参数中指定evaluation_strategy参数，以在每个epoch结束时报告评估指标</span>
training_args = TrainingArguments(output_dir=<span class="hljs-string">"test_trainer"</span>, evaluation_strategy=<span class="hljs-string">"epoch"</span>)

<span class="hljs-comment"># 3. 加载评估器</span>
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)
<span class="hljs-comment"># 在计算度量标准的时候调用compute，以计算您的预测的准确率。在将预测结果传递给compute之前，您需要将预测结果转换为logits</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_metrics</span><span class="hljs-params">(eval_pred)</span>:</span>
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=<span class="hljs-number">-1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)

<span class="hljs-comment"># 4. 定义Trainer</span>
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

<span class="hljs-comment"># 5. 开始训练</span>
trainer.train()
</code></pre>
<blockquote>
<p>Train in native PyTorch</p>
</blockquote>
<p>Trainer负责训练循环，并允许您通过一行代码对模型进行微调。对于喜欢编写自己的训练循环的用户，您也可以在原生PyTorch中对🤗Transformers模型进行微调</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> evaluate


<span class="hljs-comment"># 1. 数据集预处理</span>
tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">"text"</span>])
tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">"label"</span>, <span class="hljs-string">"labels"</span>)
tokenized_datasets.set_format(<span class="hljs-string">"torch"</span>)

<span class="hljs-comment"># 2. 定义DataLoader</span>
train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="hljs-keyword">True</span>, batch_size=<span class="hljs-number">8</span>)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="hljs-number">8</span>)

<span class="hljs-comment"># 3. 加载模型</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">5</span>)

<span class="hljs-comment"># 4. 定义优化器</span>
optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)

<span class="hljs-comment"># 5. 定义scheduler</span>
num_epochs = <span class="hljs-number">3</span>
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name=<span class="hljs-string">"linear"</span>, optimizer=optimizer, num_warmup_steps=<span class="hljs-number">0</span>, num_training_steps=num_training_steps
)

<span class="hljs-comment"># 6. 移动模型到指定设备 </span>
device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">"cpu"</span>)
model.to(device)

<span class="hljs-comment"># 7. 开始训练</span>
progress_bar = tqdm(range(num_training_steps))
model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

<span class="hljs-comment"># 8. 验证集评估</span>
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)
model.eval()
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
    <span class="hljs-keyword">with</span> torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=<span class="hljs-number">-1</span>)
    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">"labels"</span>])

metric.compute()
</code></pre>
<h2 id="分布式加速">5.2 分布式加速</h2>
<blockquote>
<p><a href="https://huggingface.co/docs/accelerate/quicktour" target="_blank">huggingface的accelerate模块</a></p>
</blockquote>
<p>🤗Accelerate是Hugging Face提供的用于简化分布式训练的库。它旨在使分布式训练更加容易和高效，支持多种深度学习框架，包括PyTorch和TensorFlow</p>
<p>Accelerate提供了以下功能：</p>
<ol>
<li><strong>数据并行</strong>：Accelerate使用<code>accelerator.DataParallel</code>类来实现数据并行，可以在多个GPU上同时训练模型</li>
<li><strong>混合精度训练</strong>：Accelerate支持自动混合精度训练，通过将模型参数和梯度转换为半精度浮点数来减少内存占用和计算量</li>
<li><strong>分布式训练</strong>：Accelerate使用<code>accelerator.DistributedDataParallel</code>类来实现分布式训练，可以在多个机器上并行训练模型</li>
<li>训练循环的自动管理：Accelerate提供了一个<code>accelerator.Trainer</code>类，它封装了训练循环，自动处理数据加载、前向传播、反向传播、优化器更新等过程</li>
</ol>
<p>使用Accelerate可以简化分布式训练的配置和管理，使用户能够更轻松地利用多个GPU或多台机器进行训练，并获得更高的训练效率</p>
<blockquote>
<p>安装</p>
</blockquote>
<pre><code class="lang-cmd">pip install accelerate
</code></pre>
<blockquote>
<p>示例代码，以下代码只列出改变的部分代码</p>
</blockquote>
<p>只需要在训练循环中添加四行额外的代码即可启用分布式训练</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

<span class="hljs-comment"># 1. 定义加速器</span>
accelerator = Accelerator()

<span class="hljs-comment"># 2. dataloader包装</span>
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

<span class="hljs-comment"># 3. 反向传播</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)
</code></pre>
<p>完整代码如下</p>
<pre><code class="lang-python">+ <span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator
  <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)
  optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">3e-5</span>)

- device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">"cpu"</span>)
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = <span class="hljs-number">3</span>
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      <span class="hljs-string">"linear"</span>,
      optimizer=optimizer,
      num_warmup_steps=<span class="hljs-number">0</span>,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
      <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
-         batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(<span class="hljs-number">1</span>)
</code></pre>
<h2 id="示例代码">5.3 示例代码</h2>
<p>包括<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/sequence_classification" target="_blank">自然语言处理</a>、<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/audio_classification" target="_blank">语音</a>、<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/image_classification" target="_blank">计算机视觉</a>和<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/image_captioning" target="_blank">多模态</a></p>
<h1 id="peft模块">6 PEFT模块</h1>
<blockquote>
<p><a href="https://huggingface.co/docs/peft/index#supported-models" target="_blank">huggingface PEFT模块</a></p>
</blockquote>
<p>详见<a href="https://study.hycbook.com/article/59381.html" target="_blank">兼一书虫-LLM模型微调系列</a></p>
<p>🤗<code>PEFT</code>，即<strong>Parameter-Efficient Fine-Tuning(参数高效微调)</strong>，是一个用于高效地将预训练语言模型(PLM)适应于各种下游应用的库，而无需对所有模型参数进行微调</p>
<p>PEFT方法只微调少量的(额外的)模型参数，显著降低了计算和存储成本，因为对大规模PLM进行完整微调代价过高。最近的最先进的PEFT技术达到了与完整微调相当的性能</p>
<p>PEFT与🤗Accelerate库无缝集成，用于利用DeepSpeed和Big Model Inference进行大规模模型微调</p>
<blockquote>
<p>Supported methods (截至23-06-15)</p>
</blockquote>
<ol>
<li>LoRA: <a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li>
<li>Prefix Tuning: <a href="https://aclanthology.org/2021.acl-long.353/" target="_blank">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a>, <a href="https://arxiv.org/pdf/2110.07602.pdf" target="_blank">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a></li>
<li>P-Tuning: <a href="https://arxiv.org/pdf/2103.10385.pdf" target="_blank">GPT Understands, Too</a></li>
<li>Prompt Tuning: <a href="https://arxiv.org/pdf/2104.08691.pdf" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li>AdaLoRA: <a href="https://arxiv.org/abs/2303.10512" target="_blank">Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a></li>
<li><a href="https://github.com/ZrrSkywalker/LLaMA-Adapter" target="_blank">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a></li>
</ol>
<h2 id="基本使用">6.1 基本使用</h2>
<h3 id="加载peft-adapter">6.1.1 加载PEFT adapter</h3>
<p><a data-lightbox="2db4495f-ffd5-4a7f-820f-0af8bacfbe0a" data-title="PEFT-Lora文件示例" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/PEFT-Lora文件示例.webp" target="_blank"><img alt="PEFT-Lora文件示例" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/PEFT-Lora文件示例.webp"/></a></p>
<p>从🤗Transformers加载并使用PEFT adapter模型时，请确保Hub仓库或本地目录包含一个<code>adapter_config.json</code>文件和<code>adapter</code>权重，如上面的示例图片所示</p>
<p>然后，你可以使用AutoModelFor类加载PEFT adapter模型。例如，要加载用于因果语言建模的PEFT adapter模型：</p>
<ul>
<li>指定PEFT模型的id</li>
<li>将其传递给AutoModelForCausalLM类</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

peft_model_id = <span class="hljs-string">"ybelkada/opt-350m-lora"</span>
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
</code></pre>
<p>你可以使用AutoModelFor类或基础模型类(如OPTForCausalLM或LlamaForCausalLM)加载PEFT adapter</p>
<p>你也可以通过调用<code>load_adapter</code>方法加载PEFT adapter：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">"facebook/opt-350m"</span>
peft_model_id = <span class="hljs-string">"ybelkada/opt-350m-lora"</span>

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
</code></pre>
<h3 id="以8或4位加载">6.1.2 以8或4位加载</h3>
<p>bitsandbytes集成支持8位和4位精度数据类型，这对于加载大型模型很有用，因为它可以节省内存</p>
<p>添加<code>load_in_8bit</code>或<code>load_in_4bit</code>参数到<code>from_pretrained()</code>并设置<code>device_map="auto"</code>以有效地将模型分配到你的硬件：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

peft_model_id = <span class="hljs-string">"ybelkada/opt-350m-lora"</span>
model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map=<span class="hljs-string">"auto"</span>, load_in_8bit=<span class="hljs-keyword">True</span>)
</code></pre>
<h3 id="添加新adapter">6.1.3 添加新adapter</h3>
<p>你可以使用<code>~peft.PeftModel.add_adapter</code>向具有现有adapter的模型添加一个新的adapter，只要新的adapter是与当前的adapter类型相同的</p>
<p>例如，如果你的模型已经附加了一个LoRA adapter：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig

model_id = <span class="hljs-string">"facebook/opt-350m"</span>
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=[<span class="hljs-string">"q_proj"</span>, <span class="hljs-string">"k_proj"</span>],
    init_lora_weights=<span class="hljs-keyword">False</span>
)

model.add_adapter(lora_config, adapter_name=<span class="hljs-string">"adapter_1"</span>)
</code></pre>
<p>要添加新的adapter：</p>
<pre><code class="lang-python"><span class="hljs-comment"># 使用相同配置附加新的 adapter</span>
model.add_adapter(lora_config, adapter_name=<span class="hljs-string">"adapter_2"</span>)
</code></pre>
<p>现在你可以使用<code>~peft.PeftModel.set_adapter</code>设置使用哪个adapter：</p>
<pre><code class="lang-python"><span class="hljs-comment"># 使用 adapter_1</span>
model.set_adapter(<span class="hljs-string">"adapter_1"</span>)
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-keyword">True</span>))

<span class="hljs-comment"># 使用 adapter_2</span>
model.set_adapter(<span class="hljs-string">"adapter_2"</span>)
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-keyword">True</span>))
</code></pre>
<h3 id="启用和禁用adapters">6.1.4 启用和禁用adapters</h3>
<p>一旦你向模型中添加了一个adapter，你可以启用或禁用adapter模块。要启用adapter模块：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftConfig

model_id = <span class="hljs-string">"facebook/opt-350m"</span>
adapter_model_id = <span class="hljs-string">"ybelkada/opt-350m-lora"</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = <span class="hljs-string">"Hello"</span>
inputs = tokenizer(text, return_tensors=<span class="hljs-string">"pt"</span>)

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

<span class="hljs-comment"># 初始化为随机权重</span>
peft_config.init_lora_weights = <span class="hljs-keyword">False</span>

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
</code></pre>
<p>要禁用adapter模块：</p>
<pre><code class="lang-python">model.disable_adapters()
output = model.generate(**inputs)
</code></pre>
<h3 id="训练peft-adapter">6.1.5 训练PEFT adapter</h3>
<p>Trainer类支持PEFT adapters，因此你可以为你的特定用例训练adapter，只需要添加几行代码</p>
<p>例如，训练LoRA adapter，如果你不熟悉使用Trainer细调模型，请<a href="https://huggingface.co/docs/transformers/v4.38.2/zh/main_classes/trainer" target="_blank">Trainer查看教程</a></p>
<p>使用任务类型和超参数定义adapter配置(查看<code>~peft.LoraConfig</code>以了解更多关于超参数的信息)</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig

peft_config = LoraConfig(
    lora_alpha=<span class="hljs-number">16</span>,
    lora_dropout=<span class="hljs-number">0.1</span>,
    r=<span class="hljs-number">64</span>,
    bias=<span class="hljs-string">"none"</span>,
    task_type=<span class="hljs-string">"CAUSAL_LM"</span>,
)
</code></pre>
<p>将adapter添加到模型中</p>
<pre><code class="lang-python">model.add_adapter(peft_config)
</code></pre>
<p>现在你可以将模型传递给Trainer</p>
<pre><code class="lang-python">trainer = Trainer(model=model, ...)
trainer.train()
</code></pre>
<p>要保存你训练的adapter并重新加载它：</p>
<pre><code class="lang-python">model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
</code></pre>
<p>向PEFT adapter添加额外的可训练层</p>
<p>你也可以在附有adapter的模型上通过在你的PEFT配置中传递<code>modules_to_save</code>来调整额外的可训练adapter</p>
<p>例如，如果你想在带有LoRA adapter的模型上同时微调<code>lm_head</code>：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig

model_id = <span class="hljs-string">"facebook/opt-350m"</span>
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=[<span class="hljs-string">"q_proj"</span>, <span class="hljs-string">"k_proj"</span>],
    modules_to_save=[<span class="hljs-string">"lm_head"</span>],
)

model.add_adapter(lora_config)
</code></pre>
<h1 id="其他模块">7 其他模块</h1>
<h2 id="核心类">7.1 核心类</h2>
<blockquote>
<p><a href="[LLM%20入门笔记-transformers库简单介绍%20(qq.com">LLM 入门笔记-transformers库简单介绍</a>](<a href="https://mp.weixin.qq.com/s/svJMXj9UwI2sd_SAtFExig" target="_blank">https://mp.weixin.qq.com/s/svJMXj9UwI2sd_SAtFExig</a>))</p>
</blockquote>
<h3 id="modeloutput">7.1.1 ModelOutput</h3>
<p><code>ModelOutput</code>(transformers.utils.ModelOutput)是所有模型输出的基类。简单理解它就是一个字典，在模型的 <code>forward</code>函数里把原本的输出做了一下封装而已，方便用户能直观地知道输出是什么</p>
<p>例如<code>CausalLMOutput</code>顾名思义就是用于像 GPT 这样自回归模型的输出，<code>ModelOutput</code>是所有模型输出的基类</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelOutput</span><span class="hljs-params">(OrderedDict)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init_subclass__</span><span class="hljs-params">(cls)</span> -&gt; <span class="hljs-keyword">None</span>:</span>
        <span class="hljs-string">"""
        这个方法允许对 ModelOutput 的子类进行定制，使得子类在被创建时能够执行特定的操作或注册到某个系统中。
        """</span>
        ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, *args, **kwargs)</span>:</span>
        <span class="hljs-string">"""
        初始化 ModelOutput 类的实例。
        """</span>
        super().__init__(*args, **kwargs)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__post_init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        在初始化 ModelOutput 类的实例之后执行的操作，允许进一步对实例进行处理或设置属性。子类需要用 dataclass 装饰器
        """</span>
        ...
</code></pre>
<p>基于 <code>ModelOutput</code>，hf 预先定义了 40 多种不同的 sub-class，这些类是 Hugging Face Transformers 库中用于表示不同类型模型输出的基础类，每个类都提供了特定类型模型输出的结构和信息，以便于在实际任务中对模型输出进行处理和使用</p>
<p>每个 sub-class 都需要用装饰器 <code>@dataclass</code>，我们以<code>CausalLMOutputWithPast</code>为例看一下源码</p>
<pre><code class="lang-python"><span class="hljs-meta">@dataclass</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CausalLMOutputWithPast</span><span class="hljs-params">(ModelOutput)</span>:</span>
    loss: Optional[torch.FloatTensor] = <span class="hljs-keyword">None</span>
    logits: torch.FloatTensor = <span class="hljs-keyword">None</span>
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = <span class="hljs-keyword">None</span>
    hidden_states: Optional[Tuple[torch.FloatTensor]] = <span class="hljs-keyword">None</span>
    attentions: Optional[Tuple[torch.FloatTensor]] = <span class="hljs-keyword">None</span>
</code></pre>
<p>为了保持代码规范，我们需要在模型的<code>forward</code>函数中对输出结果进行封装，示例如下：</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span><span class="hljs-params">(PretrainedModel)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        self.model = ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs, labels)</span>:</span>
        output = self.model(**inputs)
        hidden_states = ...
        loss = loss_fn(outputs, labels)
        <span class="hljs-keyword">return</span> CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
</code></pre>
<p>这里简单介绍以下几种，更多的可以查看官方文档和源码：</p>
<ul>
<li><p><code>BaseModelOutput</code>: 该类是许多基本模型输出的基础，包含模型的一般输出，如 logits、hidden_states 等</p>
</li>
<li><p><code>BaseModelOutputWithNoAttention</code>: 在模型输出中不包含注意力（attention）信息</p>
</li>
<li><p><code>BaseModelOutputWithPast</code>: 包含过去隐藏状态的模型输出，适用于能够迭代生成文本的模型，例如语言模型</p>
</li>
<li><p><code>BaseModelOutputWithCrossAttentions</code>: 在模型输出中包含交叉注意力（cross attentions）信息</p>
<p>通常用于特定任务中需要跨注意力的情况，比如机器翻译</p>
</li>
<li><p><code>BaseModelOutputWithPastAndCrossAttentions</code>: 同时包含过去隐藏状态和交叉注意力的模型输出</p>
</li>
<li><p><code>MoEModelOutput</code>: 包含混合专家模型（Mixture of Experts）输出的模型</p>
</li>
<li><p><code>MoECausalLMOutputWithPast</code>: 混合专家语言模型的输出，包括过去隐藏状态</p>
</li>
<li><p><code>Seq2SeqModelOutput</code>: 序列到序列模型输出的基类，适用于需要生成序列的模型</p>
</li>
<li><p><code>CausalLMOutput</code>: 用于生成式语言模型输出的基础类，提供生成文本的基本信息</p>
</li>
<li><p><code>CausalLMOutputWithPast</code>: 生成式语言模型输出的类，包含过去隐藏状态，用于连续生成文本的模型</p>
</li>
</ul>
<h3 id="pretrainedmodel">7.1.2 PreTrainedModel</h3>
<p><code>PreTrainedModel</code> (transformers.modeling_utils.PretrainedModel) 是所有模型的基类</p>
<p>所以你如果看到一个模型取名为<code>LlamaForCausalLM</code>，那你就可以知道这个模型的输出格式<strong>大概率</strong>就是自回归输出，即前面提到的<code>CausalLMOutput</code></p>
<p><code>PreTrainedModel</code> 是 Hugging Face Transformers 库中定义预训练模型的基类</p>
<p>它继承了 <code>nn.Module</code>，同时混合了几个不同的 mixin 类，如 <code>ModuleUtilsMixin</code>、<code>GenerationMixin</code>、<code>PushToHubMixin</code> 和 <code>PeftAdapterMixin</code></p>
<p>这个基类提供了创建和定义预训练模型所需的核心功能和属性</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PreTrainedModel</span><span class="hljs-params">(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin)</span>:</span>
    config_class = <span class="hljs-keyword">None</span>
    base_model_prefix = <span class="hljs-string">""</span>
    main_input_name = <span class="hljs-string">"input_ids"</span>
    _auto_class = <span class="hljs-keyword">None</span>
    _no_split_modules = <span class="hljs-keyword">None</span>
    _skip_keys_device_placement = <span class="hljs-keyword">None</span>
    _keep_in_fp32_modules = <span class="hljs-keyword">None</span>
    ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config: PretrainedConfig, *inputs, **kwargs)</span>:</span>
        super().__init__()
        ...
</code></pre>
<p>在这个基类中，我们可以看到一些重要的属性和方法：</p>
<ul>
<li><code>config_class</code>：指向特定预训练模型类的配置文件，用于定义模型的配置</li>
<li><code>base_model_prefix</code>：基本模型前缀，在模型的命名中使用，例如在加载预训练模型的权重时使用</li>
<li><code>main_input_name</code>：指定模型的主要输入名称，通常是 input_ids</li>
<li><code>_init_weights</code> 方法：用于初始化模型权重的方法</li>
</ul>
<p>在这个基类中，大多数属性都被定义为 None 或空字符串，这些属性在具体的预训练模型类中会被重写或填充</p>
<blockquote>
<p>LLama例子</p>
</blockquote>
<p>接下来我们将看到如何使用 PretrainedModel 类定义 llama 模型</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LlamaPreTrainedModel</span><span class="hljs-params">(PreTrainedModel)</span>:</span>
    config_class = LlamaConfig
    base_model_prefix = <span class="hljs-string">"model"</span>
    supports_gradient_checkpointing = <span class="hljs-keyword">True</span>
    _no_split_modules = [<span class="hljs-string">"LlamaDecoderLayer"</span>]
    _skip_keys_device_placement = <span class="hljs-string">"past_key_values"</span>
    _supports_flash_attn_2 = <span class="hljs-keyword">True</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_init_weights</span><span class="hljs-params">(self, module)</span>:</span>
        std = self.config.initializer_range
        <span class="hljs-keyword">if</span> isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=std)
            <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
                module.bias.data.zero_()
        <span class="hljs-keyword">elif</span> isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=std)
            <span class="hljs-keyword">if</span> module.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
                module.weight.data[module.padding_idx].zero_()
</code></pre>
<p>在这个例子中，首先定义了 <code>LlamaPreTrainedModel</code> 类作为 llama 模型的基类，它继承自 <code>PreTrainedModel</code>。在这个基类中，我们指定了一些 llama 模型特有的属性，比如配置类 <code>LlamaConfig</code>、模型前缀 model、支持梯度检查点（gradient checkpointing）、跳过的模块列表 _no_split_modules 等等。</p>
<p>然后，我们基于这个基类分别定义了 <code>LlamaModel</code>、<code>LlamaForCausalLM</code> 和 <code>LlamaForSequenceClassification</code>。这些模型的逻辑关系如下图所示</p>
<p><a data-lightbox="254e43c7-4f0d-4dd6-8539-83804fdbc904" data-title="Llama模型结构关系图" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/Llama模型结构关系图.webp" target="_blank"><img alt="Llama模型结构关系图" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/Llama模型结构关系图.webp"/></a></p>
<p><code>LlamaModel</code>是 llama 模型的主体定义类，也就是我们最常见的普pytorch 定义模型的方法、默认的输出格式为<code>BaseModelOutputWithPast</code></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LlamaModel</span><span class="hljs-params">(LlamaPreTrainedModel)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config: LlamaConfig)</span>:</span>
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList([LlamaDecoderLayer(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(config.num_hidden_layers)])
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, ...)</span>:</span>
        ...
        <span class="hljs-keyword">return</span> BaseModelOutputWithPast(...)
</code></pre>
<p><code>LlamaForCausalLM</code> 适用于生成式语言模型的 llama 模型，可以看到 backbone 就是 <code>LlamaModel</code>，增加了<code>lm_head</code>作为分类器，输出长度为词汇表达大小，用来预测下一个单词。输出格式为<code>CausalLMOutputWithPast</code></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LlamaForCausalLM</span><span class="hljs-params">(LlamaPreTrainedModel)</span>:</span>
    <span class="hljs-comment"># 适用于生成式语言模型的 Llama 模型定义</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=<span class="hljs-keyword">False</span>)
        ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, ...)</span>:</span>
        outputs = self.model(...)
        ... <span class="hljs-comment"># 后处理 outputs，以满足输出格式要求</span>
        <span class="hljs-keyword">return</span> CausalLMOutputWithPast(...)
</code></pre>
<p><code>LlamaForSequenceClassification</code> 适用于序列分类任务的 llama 模型，同样把 <code>LlamaModel</code>作为 backbone， 不过增加了<code>score</code>作为分类器，输出长度为 label 的数量，用来预测类别。输出格式为<code>SequenceClassifierOutputWithPast</code></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LlamaForSequenceClassification</span><span class="hljs-params">(LlamaPreTrainedModel)</span>:</span>
    <span class="hljs-comment"># 适用于序列分类任务的 Llama 模型定义</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, config)</span>:</span>
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = LlamaModel(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=<span class="hljs-keyword">False</span>)
        ...

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, ...)</span>:</span>
        outputs = self.model(...)
        ... <span class="hljs-comment"># 后处理 outputs，以满足输出格式要求</span>
        <span class="hljs-keyword">return</span> SequenceClassifierOutputWithPast(...)
</code></pre>
<p>每个子类根据特定的任务或应用场景进行了定制，以满足不同任务的需求</p>
<p>另外可以看到 hf 定义的模型都是由传入的 <code>config</code>参数定义的，所以不同模型对应不同的配置啦，这也是为什么我们经常能看到有像 <code>BertConfig</code>，<code>GPTConfig</code>这些预先定义好的类</p>
<p>例如我们可以很方便地通过指定的字符串或者文件获取和修改不同的参数配置</p>
<pre><code class="lang-python">config = BertConfig.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)  <span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
config = BertConfig.from_pretrained(<span class="hljs-string">"./test/saved_model/"</span>)  <span class="hljs-comment"># E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*</span>
config = BertConfig.from_pretrained(<span class="hljs-string">"./test/saved_model/my_configuration.json"</span>)
config = BertConfig.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>, output_attentions=<span class="hljs-keyword">True</span>, foo=<span class="hljs-keyword">False</span>)
</code></pre>
<p>hf 为了造福懒人，提供了更加简便的 API，即 Auto 系列 API。至于有多简便，看看下面的 demo 就知道了</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
config = AutoConfig.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)
model = AutoModel.from_config(config)
</code></pre>
<h2 id="autotrain">7.2 AutoTrain</h2>
<p><code>AutoTrain</code>是一个用于自动化训练的库，旨在简化模型训练的过程。它提供了一种简单的方法来定义和训练深度学习模型，自动处理数据加载、批处理、优化器、损失函数等训练过程中的细节。通过使用AutoTrain，你可以更快速地搭建和训练模型，减少样板代码的编写，并且能够轻松地进行超参数搜索和模型选择</p>
<h2 id="gradio">7.3 Gradio</h2>
<p><code>Gradio</code>是一个用于构建交互式界面的库，使你能够轻松地为你的深度学习模型创建Web应用程序。Gradio提供了一个简单而强大的API，可以将模型与用户界面组件(如文本框、滑块、图像上传器等)相连接，从而实现模型的实时推理和可视化。通过Gradio，你可以快速构建一个交互式的演示或部署你的模型到Web上，无需编写复杂的前端代码</p>
<h2 id="diffusers">7.4 Diffusers</h2>
<p><code>Diffusers</code>是一个用于生成图像、音频甚至分子的三维结构的最新预训练扩散模型的库。无论您是寻找一个简单的推理解决方案，还是想要训练自己的扩散模型，🤗Diffusers都是一个支持两者的模块化工具箱。我们的库着重于易用性而非性能，简洁而非复杂，可定制性而非抽象性，该库主要包含以下三个组件：</p>
<ol>
<li>最新的扩散推理流程，只需几行代码即可实现</li>
<li>可互换的噪声调度器，用于在生成速度和质量之间平衡权衡</li>
<li>可用作构建块的预训练模型，可以与调度器结合使用，创建您自己的端到端扩散系统</li>
</ol>
<h2 id="accelerate">7.5 Accelerate</h2>
<p>Hugging Face的<code>Accelerate</code>是一个旨在简化和加速深度学习模型训练和推理过程的库</p>
<p>它提供了一个高级API，抽象了分布式训练、混合精度和梯度累积等复杂性，使用户能够轻松地充分利用硬件资源的潜力</p>
<p>Accelerate兼容PyTorch和TensorFlow，并提供了一套工具和实用程序，以实现跨多个GPU或多台机器的高效分布式训练。它包括以下功能：</p>
<ol>
<li><strong>分布式训练</strong>：Accelerate提供了简单易用的接口，使用户能够将训练过程分布到多个GPU或多台机器上。它支持常见的分布式训练策略，如数据并行和模型并行，并自动处理数据的分发和梯度的聚合，使用户无需手动编写复杂的分布式训练代码</li>
<li><strong>混合精度训练</strong>：Accelerate支持混合精度训练，通过同时使用浮点16位和浮点32位精度来加快模型的训练速度。它自动处理数据类型转换和梯度缩放，用户只需简单地指定使用混合精度训练即可</li>
<li><strong>梯度累积</strong>：Accelerate支持梯度累积，这在GPU显存有限的情况下特别有用。梯度累积允许在多个小批次上累积梯度，然后进行一次大批次的参数更新，从而减少显存占用并提高训练效率</li>
<li><strong>自动调节批次大小</strong>：Accelerate可以自动调整批次大小以适应可用的GPU内存。它会动态调整批次大小，以达到最佳的GPU利用率和训练性能</li>
</ol>
<p>总之，Hugging Face的Accelerate是一个功能强大的库，旨在简化和加速深度学习模型的训练和推理过程。它提供了高级API和一系列工具，使用户能够轻松地实现分布式训练、混合精度训练和梯度累积等高效训练策略</p>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2024-03-17 09:11:41
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: dl_in_vision_field.md" class="navigation navigation-prev" href="dl_in_vision_field.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: nlp关键词和摘要提取技术整理.md" class="navigation navigation-next" href="nlp关键词和摘要提取技术整理.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":57912,"date":"2023/06/10 18:36:10","cover":"https://pic.hycbook.com/i/hexo/post_cover/蕾姆12.webp","title":"huggingface基本使用教程.md","tags":["huggingface","transformers"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/蕾姆12.webp","mathjax":true,"categories":["deep-learning"],"description":"huggingface基本使用教程","level":"1.6","depth":1,"next":{"title":"nlp关键词和摘要提取技术整理.md","level":"1.7","depth":1,"path":"chapters/nlp关键词和摘要提取技术整理.md","ref":"chapters/nlp关键词和摘要提取技术整理.md","articles":[]},"previous":{"title":"dl_in_vision_field.md","level":"1.5","depth":1,"path":"chapters/dl_in_vision_field.md","ref":"chapters/dl_in_vision_field.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2021","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipay打赏","button":"欢迎打赏","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechat打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"深度学习知识驿站","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"欢迎留下评论交流~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"深度学习相关学习记录","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"书籍主页":"https://study.hycbook.com"}},"gitbook":"*","description":"记录 深度学习 的学习和一些技巧的使用"},"file":{"path":"chapters/huggingface基本使用教程.md","mtime":"2024-03-17T09:11:41.958Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2024-03-17T09:12:03.805Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
