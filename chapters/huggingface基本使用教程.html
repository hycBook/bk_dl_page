<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.md Â· æ·±åº¦å­¦ä¹ ç›¸å…³å­¦ä¹ è®°å½•</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.html" rel="next"/>
<link href="dl_in_vision_field.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="è¾“å…¥å¹¶æœç´¢" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">ä¹¦ç±ä¸»é¡µ</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.html" id="chapter_id_1">
<a href="LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.html">
<b>1.2.</b>
                    
                    LLMæ¨¡å‹å¾®è°ƒç³»åˆ—.md
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.html" id="chapter_id_2">
<a href="LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.html">
<b>1.3.</b>
                    
                    LLMæ¨¡å‹éƒ¨ç½²è°ƒè¯•æ¨ç†.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="dl_in_vision_field.html" id="chapter_id_3">
<a href="dl_in_vision_field.html">
<b>1.4.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter active" data-level="1.5" data-path="huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.html" id="chapter_id_4">
<a href="huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.html">
<b>1.5.</b>
                    
                    huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.md
            
                </a>
</li>
<li class="chapter" data-level="1.6" data-path="nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.html" id="chapter_id_5">
<a href="nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.html">
<b>1.6.</b>
                    
                    nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.md
            
                </a>
</li>
<li class="chapter" data-level="1.7" data-path="pytorchå­¦ä¹ .html" id="chapter_id_6">
<a href="pytorchå­¦ä¹ .html">
<b>1.7.</b>
                    
                    pytorchå­¦ä¹ .md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="transformer.html" id="chapter_id_7">
<a href="transformer.html">
<b>1.8.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="å›¾åƒåˆ†å‰²ç®—æ³•.html" id="chapter_id_8">
<a href="å›¾åƒåˆ†å‰²ç®—æ³•.html">
<b>1.9.</b>
                    
                    å›¾åƒåˆ†å‰²ç®—æ³•.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="å›¾åƒåˆ†ç±»ç®—æ³•.html" id="chapter_id_9">
<a href="å›¾åƒåˆ†ç±»ç®—æ³•.html">
<b>1.10.</b>
                    
                    å›¾åƒåˆ†ç±»ç®—æ³•.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="å›¾ç¥ç»ç½‘ç»œ.html" id="chapter_id_10">
<a href="å›¾ç¥ç»ç½‘ç»œ.html">
<b>1.11.</b>
                    
                    å›¾ç¥ç»ç½‘ç»œ.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="æ•°æ®æ ‡æ³¨å·¥å…·.html" id="chapter_id_11">
<a href="æ•°æ®æ ‡æ³¨å·¥å…·.html">
<b>1.12.</b>
                    
                    æ•°æ®æ ‡æ³¨å·¥å…·.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹ä¼˜åŒ–å™¨.html" id="chapter_id_12">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹ä¼˜åŒ–å™¨.html">
<b>1.13.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹ä¼˜åŒ–å™¨.md
            
                </a>
</li>
<li class="chapter" data-level="1.14" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æŸå¤±å‡½æ•°.html" id="chapter_id_13">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æŸå¤±å‡½æ•°.html">
<b>1.14.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æŸå¤±å‡½æ•°.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æ¿€æ´»å‡½æ•°.html" id="chapter_id_14">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æ¿€æ´»å‡½æ•°.html">
<b>1.15.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒä¹‹æ¿€æ´»å‡½æ•°.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸºç¡€çŸ¥è¯†ç‚¹.html" id="chapter_id_15">
<a href="æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸºç¡€çŸ¥è¯†ç‚¹.html">
<b>1.16.</b>
                    
                    æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸºç¡€çŸ¥è¯†ç‚¹.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æŠ€æœ¯.html" id="chapter_id_16">
<a href="æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æŠ€æœ¯.html">
<b>1.17.</b>
                    
                    æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©æŠ€æœ¯.md
            
                </a>
</li>
<li class="chapter" data-level="1.18" data-path="ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªç®—æ³•.html" id="chapter_id_17">
<a href="ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªç®—æ³•.html">
<b>1.18.</b>
                    
                    ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªç®—æ³•.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            æœ¬ä¹¦ä½¿ç”¨ GitBook å‘å¸ƒ
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#huggingface">1 huggingface</a></li><ul><li><span class="title-icon"></span><a href="#æ¦‚è¿°">1.1 æ¦‚è¿°</a></li><li><span class="title-icon"></span><a href="#å®‰è£…">1.2 å®‰è£…</a></li></ul><li><span class="title-icon"></span><a href="#datasets">2 datasets</a></li><ul><li><span class="title-icon"></span><a href="#å®‰è£…_1">2.1 å®‰è£…</a></li><li><span class="title-icon"></span><a href="#å¿«é€Ÿå¼€å§‹">2.2 å¿«é€Ÿå¼€å§‹</a></li><ul><li><span class="title-icon"></span><a href="#è§†è§‰">2.2.1 è§†è§‰</a></li><li><span class="title-icon"></span><a href="#nlp">2.2.2 nlp</a></li></ul><li><span class="title-icon"></span><a href="#åŠ è½½æ•°æ®é›†">2.3 åŠ è½½æ•°æ®é›†</a></li><li><span class="title-icon"></span><a href="#è¿›é˜¶åŠ è½½æ•°æ®é›†">2.4 è¿›é˜¶åŠ è½½æ•°æ®é›†</a></li><li><span class="title-icon"></span><a href="#æ¢ç´¢æ•°æ®é›†">2.5 æ¢ç´¢æ•°æ®é›†</a></li><li><span class="title-icon"></span><a href="#preprocesså¤„ç†">2.6 Preprocesså¤„ç†</a></li><li><span class="title-icon"></span><a href="#æ„å»ºæ•°æ®é›†">2.7 æ„å»ºæ•°æ®é›†</a></li><li><span class="title-icon"></span><a href="#åˆ†äº«æ•°æ®é›†">2.8 åˆ†äº«æ•°æ®é›†</a></li></ul><li><span class="title-icon"></span><a href="#è¯„ä¼°æŒ‡æ ‡">3 è¯„ä¼°æŒ‡æ ‡</a></li><ul><li><span class="title-icon"></span><a href="#å®‰è£…_2">3.1 å®‰è£…</a></li><li><span class="title-icon"></span><a href="#å¿«é€Ÿå¼€å§‹_1">3.2 å¿«é€Ÿå¼€å§‹</a></li><ul><li><span class="title-icon"></span><a href="#æŒ‡æ ‡ç§ç±»">3.2.1 æŒ‡æ ‡ç§ç±»</a></li><li><span class="title-icon"></span><a href="#æŒ‡æ ‡åŠ è½½">3.2.2 æŒ‡æ ‡åŠ è½½</a></li><li><span class="title-icon"></span><a href="#æŒ‡æ ‡è®¡ç®—">3.2.3 æŒ‡æ ‡è®¡ç®—</a></li><li><span class="title-icon"></span><a href="#ç»“æœå­˜å‚¨">3.2.4 ç»“æœå­˜å‚¨</a></li><li><span class="title-icon"></span><a href="#å¯è§†åŒ–">3.2.5 å¯è§†åŒ–</a></li><li><span class="title-icon"></span><a href="#é€‰æ‹©åˆé€‚æŒ‡æ ‡">3.2.6 é€‰æ‹©åˆé€‚æŒ‡æ ‡</a></li></ul></ul><li><span class="title-icon"></span><a href="#transformers">4 transformers</a></li><ul><li><span class="title-icon"></span><a href="#æ¦‚è¿°_1">4.1 æ¦‚è¿°</a></li><li><span class="title-icon"></span><a href="#å®‰è£…_3">4.2 å®‰è£…</a></li><li><span class="title-icon"></span><a href="#å¿«é€Ÿå¼€å§‹_2">4.3 å¿«é€Ÿå¼€å§‹</a></li><ul><li><span class="title-icon"></span><a href="#pipeline">4.3.1 Pipeline</a></li><li><span class="title-icon"></span><a href="#autoclass">4.3.2 AutoClass</a></li><li><span class="title-icon"></span><a href="#autoconfig">4.3.3 AutoConfig</a></li><li><span class="title-icon"></span><a href="#trainer">4.3.4 Trainer</a></li></ul></ul><li><span class="title-icon"></span><a href="#æ•™ç¨‹">5 æ•™ç¨‹</a></li><ul><li><span class="title-icon"></span><a href="#æ¨¡å‹è®­ç»ƒ">5.1 æ¨¡å‹è®­ç»ƒ</a></li><li><span class="title-icon"></span><a href="#åˆ†å¸ƒå¼åŠ é€Ÿ">5.2 åˆ†å¸ƒå¼åŠ é€Ÿ</a></li><li><span class="title-icon"></span><a href="#ç¤ºä¾‹ä»£ç ">5.3 ç¤ºä¾‹ä»£ç </a></li></ul><li><span class="title-icon"></span><a href="#peftæ¨¡å—">6 PEFTæ¨¡å—</a></li><li><span class="title-icon"></span><a href="#å…¶ä»–æ¨¡å—">7 å…¶ä»–æ¨¡å—</a></li><ul><li><span class="title-icon"></span><a href="#autotrain">7.1 AutoTrain</a></li><li><span class="title-icon"></span><a href="#gradio">7.2 Gradio</a></li><li><span class="title-icon"></span><a href="#diffusers">7.3 Diffusers</a></li><li><span class="title-icon"></span><a href="#accelerate">7.4 Accelerate</a></li></ul></ul></div><a href="#huggingface" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><hr/>
<h1 id="huggingface">1 huggingface</h1>
<h2 id="æ¦‚è¿°">1.1 æ¦‚è¿°</h2>
<blockquote>
<p><a href="https://huggingface.co/docs" target="_blank">Hugging Face</a></p>
<p><a href="https://huggingface.co/tasks" target="_blank">å®˜ç½‘ä»»åŠ¡åˆ†ç±»å’Œç¤ºä¾‹</a></p>
</blockquote>
<p><code>Hugging Face</code>æ˜¯ä¸€ä¸ªçŸ¥åçš„å¼€æºç¤¾åŒºå’Œå…¬å¸ï¼Œä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†(NLP)å’Œæœºå™¨å­¦ä¹ (ML)é¢†åŸŸã€‚ä»–ä»¬å¼€å‘äº†è®¸å¤šæµè¡Œçš„å¼€æºå·¥å…·å’Œåº“ï¼Œä½¿å¾—æ„å»ºå’Œåº”ç”¨NLPæ¨¡å‹æ›´åŠ ä¾¿æ·</p>
<p>Hugging faceèµ·åˆæ˜¯ä¸€å®¶æ€»éƒ¨ä½äºçº½çº¦çš„èŠå¤©æœºå™¨äººåˆåˆ›æœåŠ¡å•†ï¼Œä»–ä»¬æœ¬æ¥æ‰“ç®—åˆ›ä¸šåšèŠå¤©æœºå™¨äººï¼Œç„¶ååœ¨githubä¸Šå¼€æºäº†ä¸€ä¸ªTransformersåº“ï¼Œè™½ç„¶èŠå¤©æœºå™¨äººä¸šåŠ¡æ²¡æèµ·æ¥ï¼Œä½†æ˜¯ä»–ä»¬çš„è¿™ä¸ªåº“åœ¨æœºå™¨å­¦ä¹ ç¤¾åŒºè¿…é€Ÿå¤§ç«èµ·æ¥ã€‚ç›®å‰å·²ç»å…±äº«äº†è¶…100,000ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œ10,000ä¸ªæ•°æ®é›†ï¼Œå˜æˆäº†æœºå™¨å­¦ä¹ ç•Œçš„github</p>
<blockquote>
<p>åœ¨è¿™é‡Œä¸»è¦æœ‰ä»¥ä¸‹å¤§å®¶éœ€è¦çš„èµ„æº</p>
</blockquote>
<ol>
<li><p><strong>Datasets</strong>ï¼šæ•°æ®é›†ï¼Œä»¥åŠæ•°æ®é›†çš„ä¸‹è½½åœ°å€</p>
</li>
<li><p><strong>Models</strong>ï¼šåŒ…æ‹¬å„ç§å¤„ç†CVå’ŒNLPç­‰ä»»åŠ¡çš„æ¨¡å‹ï¼Œä¸Šé¢æ¨¡å‹éƒ½æ˜¯å¯ä»¥å…è´¹è·å¾—</p>
<p>ä¸»è¦åŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³å¤„ç†ã€å¤šæ¨¡æ€ã€è¡¨æ ¼å¤„ç†ã€å¼ºåŒ–å­¦ä¹ </p>
</li>
<li><p><strong>course</strong>ï¼šå…è´¹çš„nlpè¯¾ç¨‹</p>
</li>
<li><p><strong>docs</strong>ï¼šæ–‡æ¡£</p>
</li>
</ol>
<blockquote>
<p>å±•å¼€ç»†èŠ‚</p>
</blockquote>
<ul>
<li><strong>Computer Vision(è®¡ç®—æœºè§†è§‰ä»»åŠ¡)</strong>ï¼šåŒ…æ‹¬lmage Classification(å›¾åƒåˆ†ç±»)ï¼Œlmage Segmentation(å›¾åƒåˆ†å‰²)ã€zero-Shot lmage Classification(é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ã€lmage-to-Image(å›¾åƒåˆ°å›¾åƒçš„ä»»åŠ¡)ã€Unconditional lmage Generation(æ— æ¡ä»¶å›¾åƒç”Ÿæˆ)ã€Object Detection(ç›®æ ‡æ£€æµ‹)ã€Video Classification(è§†é¢‘åˆ†ç±»)ã€Depth Estimation(æ·±åº¦ä¼°è®¡ï¼Œä¼°è®¡æ‹æ‘„è€…è·ç¦»å›¾åƒå„å¤„çš„è·ç¦»)</li>
<li><strong>Natural Language Processing(è‡ªç„¶è¯­è¨€å¤„ç†)</strong>ï¼šåŒ…æ‹¬Translation(æœºå™¨ç¿»è¯‘)ã€Fill-Mask(å¡«å……æ©ç ï¼Œé¢„æµ‹å¥å­ä¸­è¢«é®æ©çš„è¯)ã€Token Classification(è¯åˆ†ç±»)ã€Sentence Similarity(å¥å­ç›¸ä¼¼åº¦)ã€Question Answering(é—®ç­”ç³»ç»Ÿ)ï¼ŒSummarization(æ€»ç»“ï¼Œç¼©å¥)ã€Zero-Shot Classification (é›¶æ ·æœ¬åˆ†ç±»)ã€Text Classification(æ–‡æœ¬åˆ†ç±»)ã€Text2Text(æ–‡æœ¬åˆ°æ–‡æœ¬çš„ç”Ÿæˆ)ã€Text Generation(æ–‡æœ¬ç”Ÿæˆ)ã€Conversational(èŠå¤©)ã€Table Question Answer(è¡¨é—®ç­”ï¼Œ1.é¢„æµ‹è¡¨æ ¼ä¸­è¢«é®æ©å•è¯2.æ•°å­—æ¨ç†ï¼Œåˆ¤æ–­å¥å­æ˜¯å¦è¢«è¡¨æ ¼æ•°æ®æ”¯æŒ)</li>
<li><strong>Audio(è¯­éŸ³)</strong>ï¼šAutomatic Speech Recognition(è¯­éŸ³è¯†åˆ«)ã€Audio Classification(è¯­éŸ³åˆ†ç±»)ã€Text-to-Speech(æ–‡æœ¬åˆ°è¯­éŸ³çš„ç”Ÿæˆ)ã€Audio-to-Audio(è¯­éŸ³åˆ°è¯­éŸ³çš„ç”Ÿæˆ)ã€Voice Activity Detection(å£°éŸ³æ£€æµ‹ã€æ£€æµ‹è¯†åˆ«å‡ºéœ€è¦çš„å£°éŸ³éƒ¨åˆ†)</li>
<li><strong>Multimodal(å¤šæ¨¡æ€)</strong>ï¼šFeature Extraction(ç‰¹å¾æå–)ã€Text-to-Image(æ–‡æœ¬åˆ°å›¾åƒ)ã€Visual Question Answering(è§†è§‰é—®ç­”)ã€Image2Text(å›¾åƒåˆ°æ–‡æœ¬)ã€Document Question Answering(æ–‡æ¡£é—®ç­”)</li>
<li><strong>Tabular(è¡¨æ ¼)</strong>ï¼šTabular Classification(è¡¨åˆ†ç±»)ã€Tabular Regression(è¡¨å›å½’)</li>
<li><strong>Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )</strong>ï¼šReinforcement Learning(å¼ºåŒ–å­¦ä¹ )ã€Robotics(æœºå™¨äºº)</li>
</ul>
<h2 id="å®‰è£…">1.2 å®‰è£…</h2>
<blockquote>
<p>å®‰è£…transformersåº“</p>
</blockquote>
<pre><code class="lang-sh">pip install transformers
</code></pre>
<h1 id="datasets">2 datasets</h1>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/582687507" target="_blank">HuggingFace datasetsåº“æ€»ç»“</a></p>
</blockquote>
<h2 id="å®‰è£…_1">2.1 å®‰è£…</h2>
<p>ä¸‹é¢ä¸‰ä¸ªå‘½ä»¤éƒ½ç”¨äºå®‰è£…Hugging Faceçš„<code>datasets</code>åº“çš„ä¸åŒé…ç½®</p>
<ol>
<li><code>pip install datasets</code>ï¼šè¿™ä¸ªå‘½ä»¤å®‰è£…çš„æ˜¯<code>datasets</code>åº“çš„åŸºæœ¬é…ç½®ï¼Œå®ƒæä¾›äº†å¯¹å¸¸è§çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡å’Œæ•°æ®é›†çš„æ”¯æŒï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚å¦‚æœæ‚¨åªéœ€è¦å¤„ç†æ–‡æœ¬æ•°æ®æˆ–è¿›è¡Œå¸¸è§çš„NLPä»»åŠ¡ï¼Œè¿™ä¸ªåŸºæœ¬é…ç½®å°±è¶³å¤Ÿäº†</li>
<li><code>pip install datasets[audio]</code>ï¼šè¿™ä¸ªå‘½ä»¤å®‰è£…çš„æ˜¯<code>datasets</code>åº“çš„"audio"é…ç½®ã€‚å®ƒåŒ…å«äº†å¯¹å£°éŸ³å’ŒéŸ³é¢‘æ•°æ®é›†çš„æ”¯æŒï¼Œä¾‹å¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)å’ŒéŸ³é¢‘åˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœæ‚¨éœ€è¦å¤„ç†å£°éŸ³å’ŒéŸ³é¢‘æ•°æ®ï¼Œæ¯”å¦‚è¿›è¡Œè¯­éŸ³è¯†åˆ«æˆ–éŸ³é¢‘åˆ†ç±»ï¼Œå®‰è£…è¿™ä¸ªé…ç½®ä¼šæä¾›ç›¸åº”çš„åŠŸèƒ½å’Œæ•°æ®é›†æ”¯æŒ</li>
<li><code>pip install datasets[vision]</code>ï¼šè¿™ä¸ªå‘½ä»¤å®‰è£…çš„æ˜¯<code>datasets</code>åº“çš„"vision"é…ç½®ã€‚å®ƒåŒ…å«äº†å¯¹å›¾åƒå’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ”¯æŒï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ç­‰ã€‚å¦‚æœæ‚¨éœ€è¦å¤„ç†å›¾åƒæ•°æ®æˆ–è¿›è¡Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå®‰è£…è¿™ä¸ªé…ç½®ä¼šæä¾›ç›¸åº”çš„åŠŸèƒ½å’Œæ•°æ®é›†æ”¯æŒ</li>
</ol>
<p>é€šè¿‡å®‰è£…ä¸åŒçš„é…ç½®ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä»…å®‰è£…æ‚¨éœ€è¦çš„åŠŸèƒ½å’Œæ”¯æŒçš„ä»»åŠ¡ç±»å‹ï¼Œä»¥å‡å°‘åº“çš„å®‰è£…å’Œå­˜å‚¨ç©ºé—´ã€‚æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚ï¼Œé€‰æ‹©é€‚åˆçš„é…ç½®è¿›è¡Œå®‰è£…å³å¯</p>
<pre><code class="lang-sh"><span class="hljs-comment"># å®‰è£…åŸºç¡€ç‰ˆ</span>
pip install datasets
<span class="hljs-comment"># å®‰è£…forå£°éŸ³</span>
pip install datasets[audio]
<span class="hljs-comment"># å®‰è£…forå›¾åƒ</span>
pip install datasets[vision]
</code></pre>
<h2 id="å¿«é€Ÿå¼€å§‹">2.2 å¿«é€Ÿå¼€å§‹</h2>
<h3 id="è§†è§‰">2.2.1 è§†è§‰</h3>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Image
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, ColorJitter, ToTensor

<span class="hljs-comment"># åŠ è½½æ•°æ®é›†</span>
dataset = load_dataset(<span class="hljs-string">"beans"</span>, split=<span class="hljs-string">"train"</span>)

jitter = Compose(
    [ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor()]
)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transforms</span><span class="hljs-params">(examples)</span>:</span>
    examples[<span class="hljs-string">"pixel_values"</span>] = [jitter(image.convert(<span class="hljs-string">"RGB"</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"image"</span>]]
    <span class="hljs-keyword">return</span> examples

dataset = dataset.with_transform(transforms)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">collate_fn</span><span class="hljs-params">(examples)</span>:</span>
    images = []
    labels = []
    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples:
        images.append((example[<span class="hljs-string">"pixel_values"</span>]))
        labels.append(example[<span class="hljs-string">"labels"</span>])

    pixel_values = torch.stack(images)
    labels = torch.tensor(labels)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"pixel_values"</span>: pixel_values, <span class="hljs-string">"labels"</span>: labels}

<span class="hljs-comment"># å®šä¹‰DataLoader</span>
dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=<span class="hljs-number">4</span>)
</code></pre>
<h3 id="nlp">2.2.2 nlp</h3>
<p>ä½¿ç”¨ Hugging Face æä¾›çš„<code>datasets</code>åº“åŠ è½½äº†<a href="https://huggingface.co/datasets/glue/viewer/mrpc/test" target="_blank">GLUE</a>(General Language Understanding Evaluation)æ•°æ®é›†ä¸­çš„MRPC(Microsoft Research Paraphrase Corpus)éƒ¨åˆ†çš„è®­ç»ƒé›†ã€‚è¿™ä¸ªæ•°æ®é›†ç”¨äºå¥å­å¯¹çš„ç›¸ä¼¼æ€§åˆ¤æ–­ä»»åŠ¡</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-keyword">import</span> torch

dataset = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>, split=<span class="hljs-string">"test"</span>)

<span class="hljs-comment"># load a pretrained BERT model and its corresponding tokenizer from the ğŸ¤— Transformers library.</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">encode</span><span class="hljs-params">(examples)</span>:</span>
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">"sentence1"</span>], examples[<span class="hljs-string">"sentence2"</span>], truncation=<span class="hljs-keyword">True</span>, padding=<span class="hljs-string">"max_length"</span>)

dataset = dataset.map(encode, batched=<span class="hljs-keyword">True</span>)
dataset[<span class="hljs-number">0</span>]

{<span class="hljs-string">'sentence1'</span>: <span class="hljs-string">'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .'</span>,
<span class="hljs-string">'sentence2'</span>: <span class="hljs-string">'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'</span>,
<span class="hljs-string">'label'</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">'idx'</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">'input_ids'</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">'token_type_ids'</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">'attention_mask'</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}

<span class="hljs-comment">#  Rename the label column to labels, which is the expected input name in BertForSequenceClassification</span>
dataset = dataset.map(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">"labels"</span>: examples[<span class="hljs-string">"label"</span>]}, batched=<span class="hljs-keyword">True</span>)

dataset.set_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"labels"</span>])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
</code></pre>
<h2 id="åŠ è½½æ•°æ®é›†">2.3 åŠ è½½æ•°æ®é›†</h2>
<blockquote>
<p>æŸ¥çœ‹æ•°æ®é›†æè¿°</p>
</blockquote>
<pre><code class="lang-python">from datasets import load_dataset_builder
ds_builder = load_dataset_builder("rotten_tomatoes")

ds_builder.info.description
Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.


ds_builder.info.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
</code></pre>
<blockquote>
<p>åŠ è½½æ•°æ®é›†</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
<p>å½“ä¸€ä¸ªæ•°æ®é›†ç”±å¤šä¸ªæ–‡ä»¶(æˆ‘ä»¬ç§°ä¹‹ä¸º<strong>åˆ†ç‰‡</strong>)ç»„æˆæ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«æ•°æ®é›†çš„ä¸‹è½½å’Œå‡†å¤‡æ­¥éª¤</p>
<p>æ‚¨å¯ä»¥ä½¿ç”¨num_procå‚æ•°é€‰æ‹©å¹¶è¡Œå‡†å¤‡æ•°æ®é›†æ—¶è¦ä½¿ç”¨çš„è¿›ç¨‹æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªè¿›ç¨‹è¢«åˆ†é…äº†ä¸€éƒ¨åˆ†åˆ†ç‰‡æ¥è¿›è¡Œå‡†å¤‡</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

oscar_afrikaans = load_dataset(<span class="hljs-string">"oscar-corpus/OSCAR-2201"</span>, <span class="hljs-string">"af"</span>, num_proc=<span class="hljs-number">8</span>)
imagenet = load_dataset(<span class="hljs-string">"imagenet-1k"</span>, num_proc=<span class="hljs-number">8</span>)
ml_librispeech_spanish = load_dataset(<span class="hljs-string">"facebook/multilingual_librispeech"</span>, <span class="hljs-string">"spanish"</span>, num_proc=<span class="hljs-number">8</span>)
</code></pre>
<blockquote>
<p>æŸ¥çœ‹æ•°æ®é›†çš„åˆ†ç‰‡åç§°ï¼Œå¹¶åŠ è½½æŒ‡å®šçš„åˆ†ç‰‡åç§°</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_split_names
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

get_dataset_split_names(<span class="hljs-string">"rotten_tomatoes"</span>)
[<span class="hljs-string">'train'</span>, <span class="hljs-string">'validation'</span>, <span class="hljs-string">'test'</span>]

<span class="hljs-comment"># åŠ è½½æŒ‡å®šåˆ†ç‰‡</span>
dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>, split=<span class="hljs-string">"train"</span>)

Dataset({
    features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
    num_rows: <span class="hljs-number">8530</span>
})

<span class="hljs-comment"># è¿˜å¯ä»¥è¿™ä¹ˆå†™ï¼š</span>
train_test_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train+test"</span>)
train_10_20_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[10:20]"</span>)
train_10pct_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[:10%]"</span>)
train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[:10%]+train[-80%:]"</span>)
val_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=[f<span class="hljs-string">"train[{k}%:{k+10}%]"</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
train_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=[f<span class="hljs-string">"train[:{k}%]+train[{k+10}%:]"</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
train_50_52_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[50%:52%]"</span>)
train_52_54_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[52%:54%]"</span>)

<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=datasets.ReadInstruction(<span class="hljs-string">"train"</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">"%"</span>, rounding=<span class="hljs-string">"pct1_dropremainder"</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=datasets.ReadInstruction(<span class="hljs-string">"train"</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">"%"</span>, rounding=<span class="hljs-string">"pct1_dropremainder"</span>))
<span class="hljs-comment"># Or equivalently:</span>
train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[50%:52%](pct1_dropremainder)"</span>)
train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train[52%:54%](pct1_dropremainder)"</span>)

<span class="hljs-comment"># åŠ è½½å…¨éƒ¨æ•°æ®</span>
dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>)
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
        num_rows: <span class="hljs-number">8530</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
        num_rows: <span class="hljs-number">1066</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">'text'</span>, <span class="hljs-string">'label'</span>],
        num_rows: <span class="hljs-number">1066</span>
    })
})
</code></pre>
<blockquote>
<p>æŸ¥çœ‹æ•°æ®é›†å­é›†ï¼Œä¸€ä¸ªæ•°æ®ä¸‹å¯èƒ½è¿˜æœ‰å¾ˆå¤šå­æ•°æ®é›†</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_config_names

configs = get_dataset_config_names(<span class="hljs-string">"PolyAI/minds14"</span>)
print(configs)

[<span class="hljs-string">'cs-CZ'</span>, <span class="hljs-string">'de-DE'</span>, <span class="hljs-string">'en-AU'</span>, <span class="hljs-string">'en-GB'</span>, <span class="hljs-string">'en-US'</span>, <span class="hljs-string">'es-ES'</span>, <span class="hljs-string">'fr-FR'</span>, <span class="hljs-string">'it-IT'</span>, <span class="hljs-string">'ko-KR'</span>, <span class="hljs-string">'nl-NL'</span>, <span class="hljs-string">'pl-PL'</span>, <span class="hljs-string">'pt-PT'</span>, <span class="hljs-string">'ru-RU'</span>, <span class="hljs-string">'zh-CN'</span>, <span class="hljs-string">'all'</span>]
</code></pre>
<blockquote>
<p>åŠ è½½æŒ‡å®šå­æ•°æ®é›†</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

mindsFR = load_dataset(<span class="hljs-string">"PolyAI/minds14"</span>, <span class="hljs-string">"fr-FR"</span>, split=<span class="hljs-string">"train"</span>) <span class="hljs-comment"># æŒ‡å®šå­æ•°æ®é›†æ˜¯fr-FR</span>
</code></pre>
<blockquote>
<p>æŒ‡å®šæ•°æ®é›†çš„æ–‡ä»¶, é¿å…loadè¿‡å¤šçš„æ•°æ®</p>
</blockquote>
<pre><code class="lang-python">data_files = {<span class="hljs-string">"validation"</span>: <span class="hljs-string">"en/c4-validation.*.json.gz"</span>}
c4_validation = load_dataset(<span class="hljs-string">"allenai/c4"</span>, data_files=data_files, split=<span class="hljs-string">"validation"</span>)
</code></pre>
<blockquote>
<p>loadæœ¬åœ°çš„jsonã€csvæ–‡ä»¶ç­‰ï¼Œå¯ä»¥loadè¿œç¨‹æ–‡ä»¶ã€sqlç­‰</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment">#{"version": "0.1.0",</span>
<span class="hljs-comment"># "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},</span>
<span class="hljs-comment">#          {"a": 4, "b": -5.5, "c": null, "d": true}]</span>
<span class="hljs-comment">#}</span>

<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"my_file.json"</span>, field=<span class="hljs-string">"data"</span>)
</code></pre>
<blockquote>
<p>é€šè¿‡pythonå¯¹è±¡æ¥åˆ›å»ºdataset</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-comment"># å­—å…¸æ–¹å¼</span>
my_dict = {<span class="hljs-string">"a"</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
dataset = Dataset.from_dict(my_dict)

<span class="hljs-comment"># listæ–¹å¼</span>
my_list = [{<span class="hljs-string">"a"</span>: <span class="hljs-number">1</span>}, {<span class="hljs-string">"a"</span>: <span class="hljs-number">2</span>}, {<span class="hljs-string">"a"</span>: <span class="hljs-number">3</span>}]
dataset = Dataset.from_list(my_list)

<span class="hljs-comment"># pandasæ–¹å¼</span>
df = pd.DataFrame({<span class="hljs-string">"a"</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
dataset = Dataset.from_pandas(df)
</code></pre>
<blockquote>
<p>loadå¤šä¸ªæ–‡æœ¬æ–‡ä»¶: æ–‡æœ¬å¿…é¡»ä¸€è¡Œå°±æ˜¯ä¸€æ¡æ ·æœ¬</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
dataset = load_dataset(<span class="hljs-string">"text"</span>, data_files={<span class="hljs-string">"train"</span>: [<span class="hljs-string">"my_text_1.txt"</span>, <span class="hljs-string">"my_text_2.txt"</span>], <span class="hljs-string">"test"</span>: <span class="hljs-string">"my_test_file.txt"</span>})

<span class="hljs-comment"># Load from a directory</span>
dataset = load_dataset(<span class="hljs-string">"text"</span>, data_dir=<span class="hljs-string">"path/to/text/dataset"</span>)
</code></pre>
<p>ç¦»çº¿load: å°†ç¯å¢ƒå˜é‡<code>HF_DATASETS_OFFLINE</code>è®¾ç½®ä¸º1ä»¥å¯ç”¨å®Œå…¨ç¦»çº¿æ¨¡å¼</p>
<h2 id="è¿›é˜¶åŠ è½½æ•°æ®é›†">2.4 è¿›é˜¶åŠ è½½æ•°æ®é›†</h2>
<blockquote>
<p>ä»è„šæœ¬åŠ è½½æ•°æ®é›†</p>
</blockquote>
<p>æ‚¨å¯èƒ½åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šæœ‰ä¸€ä¸ªğŸ¤—Datasetsçš„åŠ è½½è„šæœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šè¿‡å°†ä»¥ä¸‹è·¯å¾„ä¹‹ä¸€ä¼ é€’ç»™load_dataset()æ¥åŠ è½½æ•°æ®é›†ï¼š</p>
<p>åŠ è½½è„šæœ¬æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚ åŒ…å«åŠ è½½è„šæœ¬æ–‡ä»¶çš„ç›®å½•çš„æœ¬åœ°è·¯å¾„(ä»…å½“è„šæœ¬æ–‡ä»¶ä¸ç›®å½•å…·æœ‰ç›¸åŒçš„åç§°æ—¶)</p>
<pre><code class="lang-python">dataset = load_dataset(<span class="hljs-string">"path/to/local/loading_script/loading_script.py"</span>, split=<span class="hljs-string">"train"</span>)

<span class="hljs-comment"># equivalent because the file has the same name as the directory</span>
dataset = load_dataset(<span class="hljs-string">"path/to/local/loading_script"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
<p>å¯ä»¥ä»Hubä¸Šä¸‹è½½åŠ è½½è„šæœ¬ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç¼–è¾‘ä»¥æ·»åŠ è‡ªå·±çš„ä¿®æ”¹ã€‚å°†æ•°æ®é›†ä»“åº“ä¸‹è½½åˆ°æœ¬åœ°ï¼Œä»¥ä¾¿åŠ è½½è„šæœ¬ä¸­ç›¸å¯¹è·¯å¾„å¼•ç”¨çš„ä»»ä½•æ•°æ®æ–‡ä»¶éƒ½å¯ä»¥è¢«åŠ è½½</p>
<pre><code class="lang-cmd">git clone https://huggingface.co/datasets/eli5
</code></pre>
<p>åœ¨åŠ è½½è„šæœ¬ä¸Šè¿›è¡Œç¼–è¾‘åï¼Œé€šè¿‡å°†å…¶æœ¬åœ°è·¯å¾„ä¼ é€’ç»™load_dataset()æ¥åŠ è½½å®ƒ</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

eli5 = load_dataset(<span class="hljs-string">"path/to/local/eli5"</span>)
</code></pre>
<blockquote>
<p>csv+jsonæ–¹å¼</p>
</blockquote>
<p>æ•°æ®é›†å¯ä»¥ä»å­˜å‚¨åœ¨è®¡ç®—æœºä¸Šçš„æœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹æ–‡ä»¶ä¸­åŠ è½½ã€‚è¿™äº›æ•°æ®é›†å¾ˆå¯èƒ½ä»¥csvã€jsonã€txtæˆ–parquetæ–‡ä»¶çš„å½¢å¼å­˜å‚¨ã€‚load_dataset()å‡½æ•°å¯ä»¥åŠ è½½è¿™äº›æ–‡ä»¶ç±»å‹çš„æ•°æ®é›†</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># csvæ–¹å¼</span>
dataset = load_dataset(<span class="hljs-string">"csv"</span>, data_files=<span class="hljs-string">"my_file.csv"</span>)

<span class="hljs-comment"># jsonæ–¹å¼</span>
<span class="hljs-comment"># {"a": 1, "b": 2.0, "c": "foo", "d": false}</span>
<span class="hljs-comment"># {"a": 4, "b": -5.5, "c": null, "d": true}</span>
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"my_file.json"</span>)

<span class="hljs-comment"># {"version": "0.1.0",</span>
<span class="hljs-comment">#  "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},</span>
<span class="hljs-comment">#           {"a": 4, "b": -5.5, "c": null, "d": true}]</span>
<span class="hljs-comment"># }</span>
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"my_file.json"</span>, field=<span class="hljs-string">"data"</span>)

<span class="hljs-comment"># ä»httpæ–¹å¼åŠ è½½csv</span>
base_url = <span class="hljs-string">"https://rajpurkar.github.io/SQuAD-explorer/dataset/"</span>
dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files={<span class="hljs-string">"train"</span>: base_url + <span class="hljs-string">"train-v1.1.json"</span>, <span class="hljs-string">"validation"</span>: base_url + <span class="hljs-string">"dev-v1.1.json"</span>}, field=<span class="hljs-string">"data"</span>)

<span class="hljs-comment"># Parquetæ–¹å¼</span>
dataset = load_dataset(<span class="hljs-string">"parquet"</span>, data_files={<span class="hljs-string">'train'</span>: <span class="hljs-string">'train.parquet'</span>, <span class="hljs-string">'test'</span>: <span class="hljs-string">'test.parquet'</span>})

base_url = <span class="hljs-string">"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"</span>
data_files = {<span class="hljs-string">"train"</span>: base_url + <span class="hljs-string">"wikipedia-train.parquet"</span>}
wiki = load_dataset(<span class="hljs-string">"parquet"</span>, data_files=data_files, split=<span class="hljs-string">"train"</span>)
</code></pre>
<blockquote>
<p>sqlæ–¹å¼</p>
</blockquote>
<p>ä½¿ç”¨from_sql()æ–¹æ³•å¯ä»¥é€šè¿‡æŒ‡å®šè¿æ¥åˆ°æ•°æ®åº“çš„URIæ¥è¯»å–æ•°æ®åº“å†…å®¹ã€‚æ‚¨å¯ä»¥è¯»å–è¡¨åæˆ–æ‰§è¡ŒæŸ¥è¯¢æ“ä½œ</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

dataset = Dataset.from_sql(<span class="hljs-string">"data_table_name"</span>, con=<span class="hljs-string">"sqlite:///sqlite_file.db"</span>)
dataset = Dataset.from_sql(<span class="hljs-string">"SELECT text FROM table WHERE length(text) &gt; 100 LIMIT 10"</span>, con=<span class="hljs-string">"sqlite:///sqlite_file.db"</span>)
</code></pre>
<p>For more details, check out the <a href="https://huggingface.co/docs/datasets/tabular_load#databases" target="_blank">how to load tabular datasets from SQL databases</a> guide.</p>
<h2 id="æ¢ç´¢æ•°æ®é›†">2.5 æ¢ç´¢æ•°æ®é›†</h2>
<blockquote>
<p>ä¸‹æ ‡</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># ç¬¬ä¸€ä¸ªæ ·æœ¬</span>
dataset[<span class="hljs-number">0</span>]
<span class="hljs-comment">#{'label': 1,</span>
<span class="hljs-comment"># 'text': 'the rock is destined to be the 21st century\'s new " conan " and that he\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}</span>

<span class="hljs-comment"># æœ€åä¸€ä¸ªæ ·æœ¬</span>
dataset[<span class="hljs-number">-1</span>]

<span class="hljs-comment"># åªå–textåˆ—</span>
dataset[<span class="hljs-string">"text"</span>] <span class="hljs-comment"># è¿”å›a list of æ ·æœ¬åˆ—</span>

<span class="hljs-comment"># ç¬¬ä¸€ä¸ªæ ·æœ¬textåˆ—</span>
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"text"</span>] <span class="hljs-comment"># æ€§èƒ½ï¼šdataset[0]['text']æ¯”dataset['text'][0]å¿«2å€ã€‚</span>
</code></pre>
<blockquote>
<p>æ•°æ®åˆ‡ç‰‡</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># Get the first three rows</span>
dataset[:<span class="hljs-number">3</span>]

<span class="hljs-comment"># Get rows between three and six</span>
dataset[<span class="hljs-number">3</span>:<span class="hljs-number">6</span>]
</code></pre>
<blockquote>
<p>è¿­ä»£æ–¹å¼ï¼Œstreaming=True</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

iterable_dataset = load_dataset(<span class="hljs-string">"food101"</span>, split=<span class="hljs-string">"train"</span>, streaming=<span class="hljs-keyword">True</span>)
<span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> iterable_dataset:
    print(example)
    <span class="hljs-keyword">break</span>

{<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">384</span>x512 at <span class="hljs-number">0x7F0681F5C520</span>&gt;, <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>}

<span class="hljs-comment"># Get first three examples</span>
list(iterable_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">384</span>x512 at <span class="hljs-number">0x7F7479DEE9D0</span>&gt;,
  <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>},
 {<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">512</span>x512 at <span class="hljs-number">0x7F7479DE8190</span>&gt;,
  <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>},
 {<span class="hljs-string">'image'</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=<span class="hljs-number">512</span>x383 at <span class="hljs-number">0x7F7479DE8310</span>&gt;,
  <span class="hljs-string">'label'</span>: <span class="hljs-number">6</span>}]
</code></pre>
<blockquote>
<p>æ’åº+shuffle+é€‰æ‹©+filter+åˆ‡åˆ†æ•°æ®é›†+åˆ†ç‰‡</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># sort: æŒ‰æŸä¸€åˆ—æ’åº</span>
dataset.sort(<span class="hljs-string">"label"</span>)

<span class="hljs-comment"># æ‰“ä¹±</span>
shuffled_dataset = sorted_dataset.shuffle(seed=<span class="hljs-number">42</span>)

<span class="hljs-comment"># é€‰æ‹©</span>
small_dataset = dataset.select([<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>])

<span class="hljs-comment"># åŒ¹é…æŸ¥æ‰¾</span>
start_with_ar = dataset.filter(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">"sentence1"</span>].startswith(<span class="hljs-string">"Ar"</span>))
len(start_with_ar)
start_with_ar[<span class="hljs-string">"sentence1"</span>]
<span class="hljs-comment"># åŒ¹é…æŸ¥æ‰¾ï¼šæ ¹æ®ä¸‹æ ‡</span>
even_dataset = dataset.filter(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-keyword">True</span>)

<span class="hljs-comment"># åˆ‡åˆ†</span>
dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)

<span class="hljs-comment"># åˆ†ç‰‡</span>
<span class="hljs-comment"># æ•°æ®é›†æ”¯æŒåˆ†ç‰‡ï¼Œå°†éå¸¸å¤§çš„æ•°æ®é›†åˆ’åˆ†ä¸ºé¢„å®šä¹‰æ•°é‡çš„å—ã€‚ åœ¨ shard() ä¸­æŒ‡å®š num_shards å‚æ•°ä»¥ç¡®å®šè¦å°†æ•°æ®é›†æ‹†åˆ†æˆçš„åˆ†ç‰‡æ•°ã€‚ æ‚¨è¿˜éœ€è¦ä½¿ç”¨ index å‚æ•°æä¾›è¦è¿”å›çš„åˆ†ç‰‡ã€‚</span>
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
datasets = load_dataset(<span class="hljs-string">"imdb"</span>, split=<span class="hljs-string">"train"</span>)
print(dataset)
dataset.shard(num_shards=<span class="hljs-number">4</span>, index=<span class="hljs-number">0</span>) <span class="hljs-comment"># å››åˆ†ä¹‹ä¸€</span>
</code></pre>
<blockquote>
<p>åˆ—é‡å‘½å+ç§»é™¤åˆ—+è½¬æ¢æ ¼å¼+flatten</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># åˆ—é‡å‘½å</span>
dataset = dataset.rename_column(<span class="hljs-string">"sentence1"</span>, <span class="hljs-string">"sentenceA"</span>)

<span class="hljs-comment"># å»æ‰æŸä¸€åˆ—</span>
dataset = dataset.remove_columns([<span class="hljs-string">"sentence1"</span>, <span class="hljs-string">"sentence2"</span>])

<span class="hljs-comment"># è½¬æ¢æ ¼å¼ï¼šä¸€åˆ—æˆ–è€…å¤šåˆ—</span>
new_features = dataset.features.copy()
new_features[<span class="hljs-string">"label"</span>] = ClassLabel(names=[<span class="hljs-string">"negative"</span>, <span class="hljs-string">"positive"</span>])
new_features[<span class="hljs-string">"idx"</span>] = Value(<span class="hljs-string">"int64"</span>)
dataset = dataset.cast(new_features)

<span class="hljs-comment"># è½¬æ¢æ ¼å¼ï¼šä¸€åˆ—</span>
dataset = dataset.cast_column(<span class="hljs-string">"audio"</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))

<span class="hljs-comment"># å°†æŸä¸€åˆ—çš„key\valueæ‹‰å¹³</span>
dataset = load_dataset(<span class="hljs-string">"squad"</span>, split=<span class="hljs-string">"train"</span>) <span class="hljs-comment"># ???</span>
</code></pre>
<blockquote>
<p>mapè½¬æ¢</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> multiprocess <span class="hljs-keyword">import</span> set_start_method
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> os

set_start_method(<span class="hljs-string">"spawn"</span>)


<span class="hljs-comment"># remove_columns è½¬æ¢çš„åŒæ—¶å»æ‰æŸä¸€åˆ—</span>
updated_dataset = dataset.map(<span class="hljs-keyword">lambda</span> example: {<span class="hljs-string">"new_sentence"</span>: example[<span class="hljs-string">"sentence1"</span>]}, remove_columns=[<span class="hljs-string">"sentence1"</span>])
updated_dataset.column_names

<span class="hljs-comment"># with_indices: å¯¹ä¸‹æ ‡å¤„ç†</span>
updated_dataset = dataset.map(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">"sentence2"</span>: f<span class="hljs-string">"{idx}: "</span> + example[<span class="hljs-string">"sentence2"</span>]}, with_indices=<span class="hljs-keyword">True</span>)
updated_dataset[<span class="hljs-string">"sentence2"</span>][:<span class="hljs-number">5</span>]

<span class="hljs-comment">#å¦‚æœæ‚¨è®¾ç½®with_rank=Trueï¼Œmap()ä¹Ÿé€‚ç”¨äºè¿›ç¨‹çš„ç­‰çº§ã€‚ è¿™ç±»ä¼¼äºwith_indiceså‚æ•°ã€‚ æ˜ å°„å‡½æ•°ä¸­çš„with_rankå‚æ•°ä½äºç´¢å¼•1ä¹‹å(å¦‚æœå®ƒå·²ç»å­˜åœ¨)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gpu_computation</span><span class="hljs-params">(example, rank)</span>:</span>
    os.environ[<span class="hljs-string">"CUDA_VISIBLE_DEVICES"</span>] = str(rank % torch.cuda.device_count())
    <span class="hljs-comment"># Your big GPU call goes here</span>
    <span class="hljs-keyword">return</span> examples
updated_dataset = dataset.map(gpu_computation, with_rank=<span class="hljs-keyword">True</span>)

<span class="hljs-comment"># å¤šçº¿ç¨‹</span>
updated_dataset = dataset.map(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">"sentence2"</span>: f<span class="hljs-string">"{idx}: "</span> + example[<span class="hljs-string">"sentence2"</span>]}, num_proc=<span class="hljs-number">4</span>)

<span class="hljs-comment"># batched</span>
chunked_dataset = dataset.map(chunk_examples, batched=<span class="hljs-keyword">True</span>, remove_columns=dataset.column_names)

<span class="hljs-comment"># æ•°æ®å¢å¼º</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">augment_data</span><span class="hljs-params">(examples)</span>:</span>
    outputs = []
    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"sentence1"</span>]:
        words = sentence.split(<span class="hljs-string">' '</span>)
        K = randint(<span class="hljs-number">1</span>, len(words)<span class="hljs-number">-1</span>)
        masked_sentence = <span class="hljs-string">" "</span>.join(words[:K]  + [mask_token] + words[K+<span class="hljs-number">1</span>:])
        predictions = fillmask(masked_sentence)
        augmented_sequences = [predictions[i][<span class="hljs-string">"sequence"</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>)]
        outputs += [sentence] + augmented_sequences
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"data"</span>: outputs}
augmented_dataset = smaller_dataset.map(augment_data, batched=<span class="hljs-keyword">True</span>, remove_columns=dataset.column_names, batch_size=<span class="hljs-number">8</span>)
augmented_dataset[:<span class="hljs-number">9</span>][<span class="hljs-string">"data"</span>]

<span class="hljs-comment"># å¤„ç†å¤šsplit</span>
dataset = load_dataset(<span class="hljs-string">'glue'</span>, <span class="hljs-string">'mrpc'</span>)
encoded_dataset = dataset.map(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">"sentence1"</span>]), batched=<span class="hljs-keyword">True</span>)
encoded_dataset[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]
</code></pre>
<blockquote>
<p>åˆå¹¶+æ‹¼æ¥æ•°æ®é›†</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, load_dataset
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset

<span class="hljs-comment"># åŠ è½½æ•°æ®é›†</span>
bookcorpus = load_dataset(<span class="hljs-string">"bookcorpus"</span>, split=<span class="hljs-string">"train"</span>)
wiki = load_dataset(<span class="hljs-string">"wikipedia"</span>, <span class="hljs-string">"20220301.en"</span>, split=<span class="hljs-string">"train"</span>)
wiki = wiki.remove_columns([col <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> wiki.column_names <span class="hljs-keyword">if</span> col != <span class="hljs-string">"text"</span>])  <span class="hljs-comment"># only keep the 'text' column</span>

<span class="hljs-keyword">assert</span> bookcorpus.features.type == wiki.features.type
bert_dataset = concatenate_datasets([bookcorpus, wiki])

<span class="hljs-comment"># å¯ä»¥æ¢concateçš„æ–¹å‘</span>
bookcorpus_ids = Dataset.from_dict({<span class="hljs-string">"ids"</span>: list(range(len(bookcorpus)))})
bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=<span class="hljs-number">1</span>)
</code></pre>
<blockquote>
<p>ç›¸äº’ç©¿æ’</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch

<span class="hljs-comment"># æŒ‰æ¦‚ç‡ç©¿æ’</span>
seed = <span class="hljs-number">42</span>
probabilities = [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.2</span>]
d1 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})
d2 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]})
d3 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>]})
dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
dataset[<span class="hljs-string">"a"</span>]

<span class="hljs-comment"># æŒ‰æ‰€æœ‰çš„æ ·æœ¬éƒ½å‡ºç°è¿‡ä¸€æ¬¡åï¼Œé©¬ä¸Šåœæ­¢</span>
d1 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})
d2 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]})
d3 = Dataset.from_dict({<span class="hljs-string">"a"</span>: [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>]})
dataset = interleave_datasets([d1, d2, d3], stopping_strategy=<span class="hljs-string">"all_exhausted"</span>)
dataset[<span class="hljs-string">"a"</span>]
</code></pre>
<blockquote>
<p>format</p>
</blockquote>
<pre><code class="lang-python">dataset.set_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"label"</span>])

<span class="hljs-comment"># è¿”å›ä¸€ä¸ªæ–°dataset</span>
dataset = dataset.with_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"label"</span>])

<span class="hljs-comment"># æŸ¥çœ‹</span>
dataset.format
</code></pre>
<blockquote>
<p>ä¿å­˜</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

encoded_dataset.save_to_disk(<span class="hljs-string">"path/of/my/dataset/directory"</span>)

<span class="hljs-comment"># ä»æœ¬åœ°loadä¸Šæ¥</span>
reloaded_dataset = load_from_disk(<span class="hljs-string">"path/of/my/dataset/directory"</span>)
encoded_dataset.to_csv(<span class="hljs-string">"path/of/my/dataset.csv"</span>)
Dataset.to_json()
</code></pre>
<h2 id="preprocesså¤„ç†">2.6 Preprocesså¤„ç†</h2>
<blockquote>
<p>æ–‡æœ¬å¤„ç†ï¼šç”¨transformersçš„tokenizer</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)
dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>, split=<span class="hljs-string">"train"</span>)

tokenizer(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"text"</span>])

{<span class="hljs-string">'input_ids'</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">2067</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">17348</span>, <span class="hljs-number">1106</span>, <span class="hljs-number">1129</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">6880</span>, <span class="hljs-number">1432</span>, <span class="hljs-number">112</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1207</span>, <span class="hljs-number">107</span>, <span class="hljs-number">14255</span>, <span class="hljs-number">1389</span>, <span class="hljs-number">107</span>, <span class="hljs-number">1105</span>, <span class="hljs-number">1115</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">112</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1280</span>, <span class="hljs-number">1106</span>, <span class="hljs-number">1294</span>, <span class="hljs-number">170</span>, <span class="hljs-number">24194</span>, <span class="hljs-number">1256</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">1190</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11791</span>, <span class="hljs-number">5253</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1732</span>, <span class="hljs-number">7200</span>, <span class="hljs-number">10947</span>, <span class="hljs-number">12606</span>, <span class="hljs-number">2895</span>, <span class="hljs-number">117</span>, <span class="hljs-number">179</span>, <span class="hljs-number">7766</span>, <span class="hljs-number">118</span>, <span class="hljs-number">172</span>, <span class="hljs-number">15554</span>, <span class="hljs-number">1181</span>, <span class="hljs-number">3498</span>, <span class="hljs-number">6961</span>, <span class="hljs-number">3263</span>, <span class="hljs-number">1137</span>, <span class="hljs-number">188</span>, <span class="hljs-number">1566</span>, <span class="hljs-number">7912</span>, <span class="hljs-number">14516</span>, <span class="hljs-number">6997</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">'token_type_ids'</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">'attention_mask'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}
</code></pre>
<p>åˆ†è¯å™¨è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé¡¹ç›®çš„å­—å…¸ï¼š</p>
<ul>
<li><strong>input_ids</strong>ï¼šè¡¨ç¤ºæ–‡æœ¬ä¸­å„ä¸ªæ ‡è®°çš„æ•°å­—</li>
<li><strong>token_type_ids</strong>ï¼šå¦‚æœæœ‰å¤šä¸ªåºåˆ—ï¼ŒæŒ‡ç¤ºä¸€ä¸ªæ ‡è®°å±äºå“ªä¸ªåºåˆ—</li>
<li><strong>attention_mask</strong>ï¼šæŒ‡ç¤ºä¸€ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥è¢«æ©ç›–(masked)</li>
</ul>
<pre><code class="lang-python">dataset.set_format(type=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>, <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"labels"</span>])
</code></pre>
<blockquote>
<p>éŸ³é¢‘ä¿¡å·ï¼šé‡æ–°é‡‡æ ·éŸ³é¢‘ä¿¡å·</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">"facebook/wav2vec2-base-960h"</span>)
dataset = load_dataset(<span class="hljs-string">"PolyAI/minds14"</span>, <span class="hljs-string">"en-US"</span>, split=<span class="hljs-string">"train"</span>)

dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"audio"</span>]

{<span class="hljs-string">'array'</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, <span class="hljs-number">-0.00024414</span>, ..., <span class="hljs-number">-0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">'path'</span>: <span class="hljs-string">'/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav'</span>,
 <span class="hljs-string">'sampling_rate'</span>: <span class="hljs-number">8000</span>}
</code></pre>
<p>MInDS-14æ•°æ®é›†å¡ä¼šå‘Šè¯‰æ‚¨é‡‡æ ·ç‡ä¸º8kHz</p>
<p>Wav2Vec2æ¨¡å‹å¡è¯´å®ƒæ˜¯åœ¨16kHzè¯­éŸ³éŸ³é¢‘ä¸Šé‡‡æ ·çš„ã€‚ è¿™æ„å‘³ç€æ‚¨éœ€è¦å¯¹MInDS-14æ•°æ®é›†è¿›è¡Œä¸Šé‡‡æ ·ä»¥åŒ¹é…æ¨¡å‹çš„é‡‡æ ·ç‡</p>
<p>ä½¿ç”¨cast_column()å‡½æ•°å¹¶åœ¨AudioåŠŸèƒ½ä¸­è®¾ç½®sampling_rateå‚æ•°ä»¥å¯¹éŸ³é¢‘ä¿¡å·è¿›è¡Œä¸Šé‡‡æ ·ã€‚ å½“æ‚¨ç°åœ¨è°ƒç”¨éŸ³é¢‘åˆ—æ—¶ï¼Œå®ƒä¼šè¢«è§£ç å¹¶é‡æ–°é‡‡æ ·åˆ°16kHzï¼š</p>
<pre><code class="lang-python">dataset = dataset.cast_column(<span class="hljs-string">"audio"</span>, Audio(sampling_rate=<span class="hljs-number">16</span>_000))
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"audio"</span>]

<span class="hljs-comment"># åŠ é€Ÿï¼šä½¿ç”¨ map() å‡½æ•°å°†æ•´ä¸ªæ•°æ®é›†é‡æ–°é‡‡æ ·åˆ°16kHz</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_function</span><span class="hljs-params">(examples)</span>:</span>
    audio_arrays = [x[<span class="hljs-string">"array"</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"audio"</span>]]
    inputs = feature_extractor(
        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=<span class="hljs-number">16000</span>, truncation=<span class="hljs-keyword">True</span>
    )
    <span class="hljs-keyword">return</span> inputs

dataset = dataset.map(preprocess_function, batched=<span class="hljs-keyword">True</span>)
</code></pre>
<blockquote>
<p>å›¾åƒå¢å¼º</p>
</blockquote>
<p>åœ¨å›¾åƒæ•°æ®é›†ä¸­ï¼Œæœ€å¸¸è§çš„é¢„å¤„ç†æ“ä½œä¹‹ä¸€æ˜¯<code>æ•°æ®å¢å¼º</code>(data augmentation)ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ä¸æ”¹å˜æ•°æ®å«ä¹‰çš„æƒ…å†µä¸‹å¯¹å›¾åƒå¼•å…¥éšæœºå˜åŒ–çš„è¿‡ç¨‹</p>
<p>è¿™å¯ä»¥åŒ…æ‹¬æ”¹å˜å›¾åƒçš„é¢œè‰²å±æ€§æˆ–éšæœºè£å‰ªå›¾åƒã€‚æ‚¨å¯ä»¥è‡ªç”±é€‰æ‹©ä»»ä½•æ•°æ®å¢å¼ºåº“ï¼Œå¹¶ä¸”ğŸ¤—Datasetså°†å¸®åŠ©æ‚¨å°†æ•°æ®å¢å¼ºåº”ç”¨åˆ°æ‚¨çš„æ•°æ®é›†ä¸­</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Image
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomRotation


feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">"google/vit-base-patch16-224-in21k"</span>)
dataset = load_dataset(<span class="hljs-string">"beans"</span>, split=<span class="hljs-string">"train"</span>)

rotate = RandomRotation(degrees=(<span class="hljs-number">0</span>, <span class="hljs-number">90</span>))
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transforms</span><span class="hljs-params">(examples)</span>:</span>
    examples[<span class="hljs-string">"pixel_values"</span>] = [rotate(image.convert(<span class="hljs-string">"RGB"</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">"image"</span>]]
    <span class="hljs-keyword">return</span> examples

<span class="hljs-comment"># åº”ç”¨å›¾åƒè½¬æ¢</span>
dataset.set_transform(transforms)
dataset[<span class="hljs-number">0</span>][<span class="hljs-string">"pixel_values"</span>]
</code></pre>
<blockquote>
<p>label idå¯¹é½</p>
</blockquote>
<p>åœ¨Transformersåº“ä¸­ï¼Œ<strong>label idå¯¹é½</strong>(label ID alignment)é€šå¸¸æŒ‡çš„æ˜¯å°†æ ‡ç­¾ä¸æ¨¡å‹è¾“å‡ºçš„é¢„æµ‹ç»“æœå¯¹é½ã€‚å½“ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†ç±»æˆ–å›å½’ç­‰ä»»åŠ¡æ—¶ï¼Œé€šå¸¸éœ€è¦å°†æ ‡ç­¾æ˜ å°„ä¸ºæ¨¡å‹æœŸæœ›çš„æ ‡ç­¾ID</p>
<p>å…·ä½“æ¥è¯´ï¼Œå¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œå¸¸è§çš„åšæ³•æ˜¯å°†æ ‡ç­¾æ˜ å°„ä¸ºæ•´æ•°æ ‡ç­¾IDã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸‰ä¸ªç±»åˆ«["cat", "dog", "bird"]ï¼Œå¯ä»¥å°†å®ƒä»¬æ˜ å°„ä¸º[0, 1, 2]ï¼Œå¹¶å°†æ¨¡å‹çš„è¾“å‡ºæ ‡ç­¾é¢„æµ‹ç»“æœä¸è¿™äº›æ ‡ç­¾IDè¿›è¡Œå¯¹é½</p>
<p>å¯¹äºå›å½’ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦å°†è¿ç»­å€¼çš„æ ‡ç­¾è¿›è¡Œç¦»æ•£åŒ–æˆ–å½’ä¸€åŒ–ï¼Œå¹¶å°†å…¶æ˜ å°„ä¸ºæ ‡ç­¾IDã€‚ä¾‹å¦‚ï¼Œå°†ä¸€ä¸ªè¿ç»­çš„ç›®æ ‡å€¼èŒƒå›´æ˜ å°„ä¸ºä¸€ç»„ç¦»æ•£çš„æ ‡ç­¾ID</p>
<p>åœ¨ä½¿ç”¨Transformersåº“è¿›è¡Œè®­ç»ƒæˆ–è¯„ä¼°æ—¶ï¼Œæ‚¨éœ€è¦ç¡®ä¿æ ‡ç­¾ä¸æ¨¡å‹çš„è¾“å‡ºç»“æœå…·æœ‰ç›¸åŒçš„æ ‡ç­¾IDå¯¹é½ï¼Œä»¥ä¾¿æ­£ç¡®è®¡ç®—æŸå¤±ã€è¯„ä¼°æŒ‡æ ‡å’Œè§£ç é¢„æµ‹ç»“æœ</p>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ ‡ç­¾IDå¯¹é½çš„å…·ä½“å®ç°æ–¹å¼å¯èƒ½å› ä»»åŠ¡å’Œåº“çš„ä½¿ç”¨è€Œæœ‰æ‰€ä¸åŒã€‚åœ¨å…·ä½“çš„ä»£ç å®ç°ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®æ‚¨çš„æ•°æ®é›†å’Œæ¨¡å‹è®¾ç½®è¿›è¡Œç›¸åº”çš„æ ‡ç­¾IDå¯¹é½æ“ä½œ</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset


label2id = {<span class="hljs-string">"contradiction"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"neutral"</span>: <span class="hljs-number">1</span>, <span class="hljs-string">"entailment"</span>: <span class="hljs-number">2</span>}
mnli = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mnli"</span>, split=<span class="hljs-string">"train"</span>)
mnli_aligned = mnli.align_labels_with_mapping(label2id, <span class="hljs-string">"label"</span>)
</code></pre>
<h2 id="æ„å»ºæ•°æ®é›†">2.7 æ„å»ºæ•°æ®é›†</h2>
<p>å¦‚æœæ‚¨ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Œå¯èƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚ä½¿ç”¨ğŸ¤—Datasetsåˆ›å»ºæ•°æ®é›†å¯ä»¥äº«å—åˆ°è¯¥åº“çš„æ‰€æœ‰ä¼˜åŠ¿ï¼šå¿«é€ŸåŠ è½½å’Œå¤„ç†æ•°æ®ã€æµå¼å¤„ç†å¤§å‹æ•°æ®é›†ã€å†…å­˜æ˜ å°„ç­‰ç­‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤—Datasetsçš„ä½ä»£ç æ–¹æ³•è½»æ¾å¿«é€Ÿåœ°åˆ›å»ºæ•°æ®é›†ï¼Œå‡å°‘å¯åŠ¨è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„æ—¶é—´ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œåªéœ€å°†æ•°æ®æ–‡ä»¶æ‹–æ”¾åˆ°Hubä¸Šçš„æ•°æ®é›†ä»“åº“ä¸­ï¼Œå°±å¯ä»¥è½»æ¾å®Œæˆ</p>
<p>åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ğŸ¤—Datasetsçš„ä½ä»£ç æ–¹æ³•åˆ›å»ºå„ç§ç±»å‹çš„æ•°æ®é›†ï¼š</p>
<ul>
<li>åŸºäºæ–‡ä»¶å¤¹çš„æ„å»ºå™¨(Folder-based builders)ï¼Œç”¨äºå¿«é€Ÿåˆ›å»º<strong>å›¾åƒæˆ–éŸ³é¢‘æ•°æ®é›†</strong></li>
<li>ä½¿ç”¨from_æ–¹æ³•ä»æœ¬åœ°æ–‡ä»¶åˆ›å»ºæ•°æ®é›†</li>
</ul>
<blockquote>
<p>åŸºäºæ–‡ä»¶å¤¹çš„æ„å»ºå™¨</p>
</blockquote>
<p>æœ‰ä¸¤ä¸ªåŸºäºæ–‡ä»¶å¤¹çš„æ„å»ºå™¨ï¼š<code>ImageFolder(å›¾åƒæ–‡ä»¶å¤¹æ„å»ºå™¨)</code>å’Œ<code>AudioFolder(éŸ³é¢‘æ–‡ä»¶å¤¹æ„å»ºå™¨)</code></p>
<p>å®ƒä»¬æ˜¯ä½ä»£ç æ–¹æ³•ï¼Œå¯ä»¥å¿«é€Ÿåˆ›å»ºåŒ…å«æ•°åƒä¸ªç¤ºä¾‹çš„å›¾åƒã€è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®é›†ã€‚å®ƒä»¬éå¸¸é€‚ç”¨äºåœ¨æ‰©å±•åˆ°æ›´å¤§çš„æ•°æ®é›†ä¹‹å‰ï¼Œå¿«é€ŸåŸå‹åŒ–è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³æ¨¡å‹</p>
<p>åŸºäºæ–‡ä»¶å¤¹çš„æ„å»ºå™¨ä¼šä½¿ç”¨æ‚¨çš„æ•°æ®ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆæ•°æ®é›†çš„ç‰¹å¾ã€åˆ’åˆ†å’Œæ ‡ç­¾ã€‚å…·ä½“æ¥è¯´ï¼š</p>
<ul>
<li>ImageFolderä½¿ç”¨Imageç‰¹å¾æ¥è§£ç å›¾åƒæ–‡ä»¶ã€‚å®ƒæ”¯æŒè®¸å¤šå›¾åƒæ‰©å±•æ ¼å¼ï¼Œä¾‹å¦‚jpgå’Œpngï¼Œè¿˜æ”¯æŒå…¶ä»–æ ¼å¼ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹æ”¯æŒçš„å›¾åƒæ‰©å±•æ ¼å¼çš„å®Œæ•´åˆ—è¡¨</li>
<li>AudioFolderä½¿ç”¨Audioç‰¹å¾æ¥è§£ç éŸ³é¢‘æ–‡ä»¶ã€‚å®ƒæ”¯æŒéŸ³é¢‘æ‰©å±•æ ¼å¼ï¼Œå¦‚wavå’Œmp3ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æ”¯æŒçš„éŸ³é¢‘æ‰©å±•æ ¼å¼çš„å®Œæ•´åˆ—è¡¨</li>
</ul>
<p>ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„å›¾åƒæ•°æ®é›†(å¯¹äºéŸ³é¢‘æ•°æ®é›†ä¹Ÿæ˜¯ä¸€æ ·)å­˜å‚¨å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<pre><code class="lang-sh">pokemon/train/grass/bulbasaur.png
pokemon/train/fire/charmander.png
pokemon/train/water/squirtle.png

pokemon/<span class="hljs-built_in">test</span>/grass/ivysaur.png
pokemon/<span class="hljs-built_in">test</span>/fire/charmeleon.png
pokemon/<span class="hljs-built_in">test</span>/water/wartortle.png
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ImageFolder
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> AudioFolder

dataset = load_dataset(<span class="hljs-string">"imagefolder"</span>, data_dir=<span class="hljs-string">"/path/to/pokemon"</span>)
dataset = load_dataset(<span class="hljs-string">"audiofolder"</span>, data_dir=<span class="hljs-string">"/path/to/folder"</span>)
</code></pre>
<p>æ•°æ®é›†ä¸­å¯ä»¥åŒ…å«æœ‰å…³æ•°æ®é›†çš„å…¶ä»–ä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡æœ¬æ ‡é¢˜æˆ–è½¬å½•ï¼Œå¯ä»¥ä½¿ç”¨åŒ…å«åœ¨æ•°æ®é›†æ–‡ä»¶å¤¹ä¸­çš„metadata.csvæ–‡ä»¶æ¥è¿›è¡Œå­˜å‚¨</p>
<p>metadataæ–‡ä»¶éœ€è¦æœ‰ä¸€ä¸ªfile_nameåˆ—ï¼Œå°†å›¾åƒæˆ–éŸ³é¢‘æ–‡ä»¶ä¸å…¶ç›¸åº”çš„å…ƒæ•°æ®è¿›è¡Œå…³è”</p>
<pre><code class="lang-txt">file_name, text
bulbasaur.png, There is a plant seed on its back right from the day this PokÃ©mon is born.
charmander.png, It has a preference for hot things.
squirtle.png, When it retracts its long neck into its shell, it squirts out water with vigorous force.
</code></pre>
<p>To learn more about each of these folder-based builders, check out the and <a href="https://huggingface.co/docs/datasets/image_dataset#imagefolder" target="_blank">ImageFolder</a> or <a href="https://huggingface.co/docs/datasets/audio_dataset#audiofolder" target="_blank">AudioFolder</a> guides.</p>
<blockquote>
<p>åŸºäºæ–‡ä»¶çš„æ„å»ºå™¨</p>
</blockquote>
<p>ä½¿ç”¨ from_generator() æ–¹æ³•æ˜¯ä»ç”Ÿæˆå™¨åˆ›å»ºæ•°æ®é›†çš„æœ€èŠ‚çœå†…å­˜çš„æ–¹å¼ï¼Œè¿™æ˜¯ç”±äºç”Ÿæˆå™¨çš„è¿­ä»£è¡Œä¸ºã€‚è¿™åœ¨å¤„ç†éå¸¸å¤§çš„æ•°æ®é›†æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºæ•°æ®é›†æ˜¯é€æ­¥åœ¨ç£ç›˜ä¸Šç”Ÿæˆçš„ï¼Œç„¶åè¿›è¡Œå†…å­˜æ˜ å°„ï¼Œè¿™æ ·å¯ä»¥é¿å…å°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜ä¸­</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gen</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">yield</span> {<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
    <span class="hljs-keyword">yield</span> {<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"squirtle"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"water"</span>}
ds = Dataset.from_generator(gen)
ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
</code></pre>
<p>åŸºäºç”Ÿæˆå™¨çš„IterableDatasetéœ€è¦ä½¿ç”¨forå¾ªç¯è¿›è¡Œè¿­ä»£ï¼Œä¾‹å¦‚ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> IterableDataset
ds = IterableDataset.from_generator(gen)
<span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> ds:
    print(example)

{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"squirtle"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"water"</span>}
</code></pre>
<p>ä½¿ç”¨from_dict()æ–¹æ³•æ˜¯ä»å­—å…¸åˆ›å»ºæ•°æ®é›†çš„ç®€å•ç›´æ¥çš„æ–¹å¼ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
ds = Dataset.from_dict({<span class="hljs-string">"pokemon"</span>: [<span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"squirtle"</span>], <span class="hljs-string">"type"</span>: [<span class="hljs-string">"grass"</span>, <span class="hljs-string">"water"</span>]})
ds[<span class="hljs-number">0</span>]

{<span class="hljs-string">"pokemon"</span>: <span class="hljs-string">"bulbasaur"</span>, <span class="hljs-string">"type"</span>: <span class="hljs-string">"grass"</span>}
</code></pre>
<h2 id="åˆ†äº«æ•°æ®é›†">2.8 åˆ†äº«æ•°æ®é›†</h2>
<p>ç‚¹å‡»æ‚¨çš„ä¸ªäººèµ„æ–™å¹¶é€‰æ‹©æ–°çš„æ•°æ®é›†ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ä»“åº“ã€‚ ä¸ºæ‚¨çš„æ•°æ®é›†é€‰æ‹©ä¸€ä¸ªåç§°ï¼Œå¹¶é€‰æ‹©å®ƒæ˜¯ä¸€ä¸ªå…¬å…±æ•°æ®é›†è¿˜æ˜¯ç§æœ‰æ•°æ®é›†ã€‚å…¬å…±æ•°æ®é›†å¯¹ä»»ä½•äººå¯è§ï¼Œè€Œç§æœ‰æ•°æ®é›†åªèƒ½ç”±æ‚¨æˆ–æ‚¨ç»„ç»‡çš„æˆå‘˜æŸ¥çœ‹</p>
<p>ä¸€æ—¦æ‚¨çš„æ•°æ®é›†å­˜å‚¨åœ¨Hubä¸Šï¼Œä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨load_dataset()å‡½æ•°åŠ è½½å®ƒï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"stevhliu/demo"</span>)
</code></pre>
<blockquote>
<p>ä½¿ç”¨Pythonè¿›è¡Œä¸Šä¼ </p>
</blockquote>
<p>å–œæ¬¢ä»¥ç¼–ç¨‹æ–¹å¼ä¸Šä¼ æ•°æ®é›†çš„ç”¨æˆ·å¯ä»¥ä½¿ç”¨huggingface_hubåº“ã€‚è¯¥åº“å…è®¸ç”¨æˆ·ä»Pythonä¸­ä¸Hubè¿›è¡Œäº¤äº’</p>
<p>é¦–å…ˆå®‰è£…è¯¥åº“ï¼š</p>
<pre><code class="lang-sh">pip install huggingface_hub
</code></pre>
<p>è¦åœ¨Hubä¸Šä½¿ç”¨Pythonä¸Šä¼ æ•°æ®é›†ï¼Œæ‚¨éœ€è¦ç™»å½•åˆ°æ‚¨çš„Hugging Faceè´¦æˆ·ï¼š</p>
<pre><code class="lang-sh">huggingface-cli login
</code></pre>
<p>ä½¿ç”¨push_to_hub()å‡½æ•°å¸®åŠ©æ‚¨å°†æ–‡ä»¶æ·»åŠ ã€æäº¤å’Œæ¨é€åˆ°æ‚¨çš„ä»“åº“ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"stevhliu/demo"</span>)
<span class="hljs-comment"># dataset = dataset.map(...)  # åœ¨è¿™é‡Œè¿›è¡Œæ‰€æœ‰çš„æ•°æ®å¤„ç†</span>
dataset.push_to_hub(<span class="hljs-string">"stevhliu/processed_demo"</span>)
</code></pre>
<p>å¦‚æœè¦å°†æ•°æ®é›†è®¾ç½®ä¸ºç§æœ‰ï¼Œè¯·å°†privateå‚æ•°è®¾ç½®ä¸ºTrueã€‚è¯¥å‚æ•°ä»…åœ¨é¦–æ¬¡åˆ›å»ºä»“åº“æ—¶æœ‰æ•ˆ</p>
<pre><code class="lang-python">dataset.push_to_hub(<span class="hljs-string">"stevhliu/private_processed_demo"</span>, private=<span class="hljs-keyword">True</span>)
</code></pre>
<h1 id="è¯„ä¼°æŒ‡æ ‡">3 è¯„ä¼°æŒ‡æ ‡</h1>
<h2 id="å®‰è£…_2">3.1 å®‰è£…</h2>
<p>ä¸€ç§ç”¨äºè½»æ¾è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ•°æ®é›†çš„åº“</p>
<p>åªéœ€ä¸€è¡Œä»£ç ï¼Œæ‚¨å°±å¯ä»¥è®¿é—®æ•°åç§ä¸åŒé¢†åŸŸ(è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€å¼ºåŒ–å­¦ä¹ ç­‰)çš„è¯„ä¼°æ–¹æ³•</p>
<p>æ— è®ºæ˜¯åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿˜æ˜¯åœ¨åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­ï¼Œæ‚¨éƒ½å¯ä»¥ä»¥ä¸€ç§ä¸€è‡´ä¸”å¯é‡å¤çš„æ–¹å¼è¯„ä¼°æ‚¨çš„æ¨¡å‹</p>
<blockquote>
<p>å®‰è£…</p>
</blockquote>
<pre><code class="lang-sh">pip install evaluate
</code></pre>
<blockquote>
<p>æµ‹è¯•</p>
</blockquote>
<pre><code class="lang-sh">python -c <span class="hljs-string">"import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))"</span>

{<span class="hljs-string">'exact_match'</span>: 1.0}
</code></pre>
<h2 id="å¿«é€Ÿå¼€å§‹_1">3.2 å¿«é€Ÿå¼€å§‹</h2>
<h3 id="æŒ‡æ ‡ç§ç±»">3.2.1 æŒ‡æ ‡ç§ç±»</h3>
<blockquote>
<p><a href="https://huggingface.co/evaluate-metric" target="_blank">Evaluate Metricå¡ç‰‡å®ä¾‹</a></p>
</blockquote>
<p>ğŸ¤—Evaluateæä¾›äº†å¹¿æ³›çš„è¯„ä¼°å·¥å…·ã€‚å®ƒæ¶µç›–äº†æ–‡æœ¬ã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘ç­‰å¤šç§å½¢å¼ï¼Œå¹¶æä¾›äº†ç”¨äºè¯„ä¼°æ¨¡å‹æˆ–æ•°æ®é›†çš„å·¥å…·ã€‚è¿™äº›å·¥å…·åˆ†ä¸ºä¸‰ä¸ªç±»åˆ«</p>
<p>è¯„ä¼°ç±»å‹ å…¸å‹çš„æœºå™¨å­¦ä¹ æµç¨‹æ¶‰åŠåˆ°ä¸åŒæ–¹é¢çš„è¯„ä¼°ï¼Œå¯¹äºæ¯ä¸ªæ–¹é¢ï¼ŒğŸ¤— Evaluateéƒ½æä¾›äº†ç›¸åº”çš„å·¥å…·ï¼š</p>
<ul>
<li><strong>æŒ‡æ ‡(Metric)</strong>ï¼šç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œé€šå¸¸æ¶‰åŠæ¨¡å‹çš„é¢„æµ‹ç»“æœå’Œä¸€äº›çœŸå®æ ‡ç­¾ã€‚æ‚¨å¯ä»¥åœ¨evaluate-metricä¸­æ‰¾åˆ°æ‰€æœ‰é›†æˆçš„æŒ‡æ ‡</li>
<li><strong>æ¯”è¾ƒ(Comparison)</strong>ï¼šç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ã€‚å¯ä»¥é€šè¿‡å°†å®ƒä»¬çš„é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒå¹¶è®¡ç®—å®ƒä»¬çš„ä¸€è‡´æ€§æ¥è¿›è¡Œæ¯”è¾ƒã€‚æ‚¨å¯ä»¥åœ¨evaluate-comparisonä¸­æ‰¾åˆ°æ‰€æœ‰é›†æˆçš„æ¯”è¾ƒæ–¹æ³•</li>
<li><strong>æµ‹é‡(Measurement)</strong>ï¼šæ•°æ®é›†å’Œè®­ç»ƒåœ¨å…¶ä¸Šçš„æ¨¡å‹åŒæ ·é‡è¦ã€‚é€šè¿‡æµ‹é‡å¯ä»¥ç ”ç©¶æ•°æ®é›†çš„ç‰¹æ€§ã€‚æ‚¨å¯ä»¥åœ¨evaluate-measurementä¸­æ‰¾åˆ°æ‰€æœ‰é›†æˆçš„æµ‹é‡æ–¹æ³•</li>
</ul>
<p>æ¯ä¸ªè¯„ä¼°æ¨¡å—éƒ½ä½œä¸ºä¸€ä¸ªSpaceå­˜å‚¨åœ¨Hugging Face Hubä¸Šã€‚å®ƒä»¬æä¾›äº†ä¸€ä¸ªäº¤äº’å¼å°éƒ¨ä»¶å’Œä¸€ä¸ªæ–‡æ¡£å¡ç‰‡ï¼Œç”¨äºè®°å½•å…¶ä½¿ç”¨æ–¹æ³•å’Œé™åˆ¶</p>
<blockquote>
<p>è¯„ä¼°å·¥å…·ä¹‹é—´çš„å…³ç³»å’ŒåŒºåˆ«</p>
</blockquote>
<p>Evaluateåº“ä¸­çš„<code>Metric(æŒ‡æ ‡)</code>ã€<code>Comparison(æ¯”è¾ƒ)</code>å’Œ<code>Measurement(æµ‹é‡)</code>æ˜¯ä¸‰ç§ä¸åŒçš„è¯„ä¼°å·¥å…·ï¼Œç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒä»¬ä¹‹é—´çš„å…³ç³»å’ŒåŒºåˆ«å¦‚ä¸‹ï¼š</p>
<ol>
<li>Metric(æŒ‡æ ‡)ï¼š<ul>
<li>ç”¨é€”ï¼š<strong>ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½</strong></li>
<li>å…·ä½“å«ä¹‰ï¼šæŒ‡æ ‡é€šè¿‡å°†æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒæ¥è¡¡é‡æ¨¡å‹çš„è¡¨ç°</li>
<li>ç¤ºä¾‹ï¼šå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰</li>
<li>ç›®çš„ï¼šæä¾›äº†å¯¹æ¨¡å‹æ€§èƒ½çš„å®šé‡è¯„ä¼°ï¼Œå¸®åŠ©è¡¡é‡æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°</li>
</ul>
</li>
<li>Comparison(æ¯”è¾ƒ)ï¼š<ul>
<li>ç”¨é€”ï¼šç”¨äº<strong>æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„å·®å¼‚</strong></li>
<li>å…·ä½“å«ä¹‰ï¼šæ¯”è¾ƒå·¥å…·å°†ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾è¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ä¸€è‡´æ€§æˆ–å·®å¼‚ç¨‹åº¦</li>
<li>ç¤ºä¾‹ï¼šä¸€è‡´æ€§æŒ‡æ ‡ã€ç›¸å¯¹è¯¯å·®ç­‰</li>
<li>ç›®çš„ï¼šå¸®åŠ©è¯„ä¼°ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œæ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹æˆ–è¿›è¡Œæ¨¡å‹é€‰æ‹©</li>
</ul>
</li>
<li>Measurement(æµ‹é‡)ï¼š<ul>
<li>ç”¨é€”ï¼šç”¨äº<strong>ç ”ç©¶æ•°æ®é›†çš„å±æ€§å’Œç‰¹æ€§</strong></li>
<li>å…·ä½“å«ä¹‰ï¼šæµ‹é‡å·¥å…·ç”¨äºå¯¹æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œæ¢ç´¢æ•°æ®é›†çš„ç»“æ„ã€åˆ†å¸ƒã€åå·®ç­‰æ–¹é¢çš„ä¿¡æ¯</li>
<li>ç¤ºä¾‹ï¼šæ•°æ®é›†å¤§å°ã€æ ·æœ¬åˆ†å¸ƒã€ç±»åˆ«ä¸å¹³è¡¡åº¦ç­‰</li>
<li>ç›®çš„ï¼šæä¾›å¯¹æ•°æ®é›†çš„è¯¦ç»†äº†è§£ï¼Œå¸®åŠ©äº†è§£æ•°æ®é›†çš„ç‰¹ç‚¹å’Œæ½œåœ¨é—®é¢˜</li>
</ul>
</li>
</ol>
<p>è¿™ä¸‰ç§è¯„ä¼°å·¥å…·åœ¨Evaluateåº“ä¸­å„è‡ªç‹¬ç«‹ï¼Œç”¨äºä¸åŒçš„è¯„ä¼°ç›®çš„ã€‚Metricç”¨äºè¡¡é‡æ¨¡å‹æ€§èƒ½ï¼ŒComparisonç”¨äºæ¯”è¾ƒä¸åŒæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼ŒMeasurementç”¨äºç ”ç©¶å’Œäº†è§£æ•°æ®é›†çš„ç‰¹æ€§ã€‚é€šè¿‡ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œå¯ä»¥å…¨é¢è¯„ä¼°å’Œç†è§£æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ•°æ®é›†çš„è¡¨ç°å’Œç‰¹ç‚¹</p>
<h3 id="æŒ‡æ ‡åŠ è½½">3.2.2 æŒ‡æ ‡åŠ è½½</h3>
<blockquote>
<p>å®˜æ–¹+ç¤¾åŒº æŒ‡æ ‡</p>
</blockquote>
<p>åœ¨ä½¿ç”¨Hugging Faceçš„Evaluateåº“åŠ è½½è¯„ä¼°å·¥å…·æ—¶ï¼Œå¯ä»¥é€šè¿‡æ˜¾å¼æŒ‡å®šè¯„ä¼°çš„ç±»å‹æ¥ç¡®ä¿åŠ è½½æ­£ç¡®çš„å·¥å…·ã€‚è¿™å¯ä»¥é˜²æ­¢åç§°å†²çªæˆ–æ··æ·†ï¼Œç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯æœŸæœ›çš„è¯„ä¼°å·¥å…·</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> evaluate

accuracy = evaluate.load(<span class="hljs-string">"accuracy"</span>)

<span class="hljs-comment"># æ˜¾å¼æŒ‡å®šè¯„ä¼°çš„ç±»å‹</span>
word_length = evaluate.load(<span class="hljs-string">"word_length"</span>, module_type=<span class="hljs-string">"measurement"</span>)

<span class="hljs-comment"># ç¤¾åŒºæŒ‡æ ‡</span>
element_count = evaluate.load(<span class="hljs-string">"lvwerra/element_count"</span>, module_type=<span class="hljs-string">"measurement"</span>)
</code></pre>
<blockquote>
<p>æŸ¥çœ‹å¯ç”¨çš„æ¨¡å—æ–¹æ³•</p>
</blockquote>
<pre><code class="lang-python">evaluate.list_evaluation_modules(
  module_type=<span class="hljs-string">"comparison"</span>,
  include_community=<span class="hljs-keyword">False</span>,
  with_details=<span class="hljs-keyword">True</span>)

[{<span class="hljs-string">'name'</span>: <span class="hljs-string">'mcnemar'</span>, <span class="hljs-string">'type'</span>: <span class="hljs-string">'comparison'</span>, <span class="hljs-string">'community'</span>: <span class="hljs-keyword">False</span>, <span class="hljs-string">'likes'</span>: <span class="hljs-number">1</span>},
 {<span class="hljs-string">'name'</span>: <span class="hljs-string">'exact_match'</span>, <span class="hljs-string">'type'</span>: <span class="hljs-string">'comparison'</span>, <span class="hljs-string">'community'</span>: <span class="hljs-keyword">False</span>, <span class="hljs-string">'likes'</span>: <span class="hljs-number">0</span>}]
</code></pre>
<h3 id="æŒ‡æ ‡è®¡ç®—">3.2.3 æŒ‡æ ‡è®¡ç®—</h3>
<blockquote>
<p>è®¡ç®—æŒ‡æ ‡</p>
</blockquote>
<p>å½“æ¶‰åŠåˆ°è®¡ç®—å®é™…å¾—åˆ†æ—¶ï¼Œæœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•ï¼š</p>
<ul>
<li><p><strong>ä¸€ä½“å¼è®¡ç®—(All-in-one)</strong>ï¼šé€šè¿‡ä¸€æ¬¡æ€§å°†æ‰€æœ‰å¿…è¦çš„è¾“å…¥ä¼ é€’ç»™compute()æ–¹æ³•æ¥è®¡ç®—å¾—åˆ†</p>
<pre><code class="lang-python">accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])

{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.5</span>}
</code></pre>
</li>
<li><p><strong>é€æ­¥è®¡ç®—(Incremental)</strong>ï¼šé€šè¿‡ä½¿ç”¨EvaluationModule.add()æˆ–EvaluationModule.add_batch()å°†å¿…è¦çš„è¾“å…¥é€æ­¥æ·»åŠ åˆ°æ¨¡å—ä¸­ï¼Œç„¶ååœ¨æœ€åä½¿ç”¨ EvaluationModule.compute()è®¡ç®—å¾—åˆ†</p>
<pre><code class="lang-python"><span class="hljs-comment"># addçš„æ–¹å¼</span>
<span class="hljs-keyword">for</span> ref, pred <span class="hljs-keyword">in</span> zip([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]):
    accuracy.add(references=ref, predictions=pred)
accuracy.compute()
{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.5</span>}

<span class="hljs-comment"># add_batchçš„æ–¹å¼</span>
<span class="hljs-keyword">for</span> refs, preds <span class="hljs-keyword">in</span> zip([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]]):
    accuracy.add_batch(references=refs, predictions=preds)
accuracy.compute()
{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.5</span>}
</code></pre>
</li>
</ul>
<p>åœ¨ä½ éœ€è¦ä»¥æ‰¹é‡æ–¹å¼ä»æ¨¡å‹ä¸­è·å–é¢„æµ‹ç»“æœæ—¶ç‰¹åˆ«æœ‰ç”¨ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">for</span> model_inputs, gold_standards <span class="hljs-keyword">in</span> evaluation_dataset:
    predictions = model(model_inputs)
    metric.add_batch(references=gold_standards, predictions=predictions)
metric.compute()
</code></pre>
<blockquote>
<p>åˆ†å¸ƒå¼æŒ‡æ ‡</p>
</blockquote>
<p>åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®¡ç®—æŒ‡æ ‡å¯èƒ½ä¼šæœ‰äº›æ£˜æ‰‹ã€‚æŒ‡æ ‡è¯„ä¼°æ˜¯åœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šçš„å•ç‹¬Pythonè¿›ç¨‹æˆ–èŠ‚ç‚¹ä¸­æ‰§è¡Œçš„</p>
<p>é€šå¸¸æƒ…å†µä¸‹ï¼Œå½“ä¸€ä¸ªæŒ‡æ ‡å¾—åˆ†æ˜¯å¯åŠ çš„(<script type="math/tex; "> f(A \cup B) = f(A) + f(B)</script>)æ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼çš„reduceæ“ä½œæ¥æ”¶é›†æ¯ä¸ªæ•°æ®å­é›†çš„å¾—åˆ†ã€‚ä½†æ˜¯å½“æŒ‡æ ‡æ˜¯éå¯åŠ çš„(<script type="math/tex; "> f(A \cup B) \neq f(A) + f(B)</script>)æ—¶ï¼Œæƒ…å†µå°±ä¸é‚£ä¹ˆç®€å•äº†ã€‚ä¾‹å¦‚ï¼Œä½ ä¸èƒ½å°†æ¯ä¸ªæ•°æ®å­é›†çš„F1åˆ†æ•°ç›¸åŠ ä½œä¸ºæœ€ç»ˆçš„æŒ‡æ ‡</p>
<p><strong>å…‹æœè¿™ä¸ªé—®é¢˜çš„å¸¸è§æ–¹æ³•æ˜¯å›é€€åˆ°å•è¿›ç¨‹è¯„ä¼°ï¼Œä½†æŒ‡æ ‡åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™ä¼šå¯¼è‡´æ•ˆç‡é™ä½</strong></p>
<ol>
<li>ğŸ¤—Evaluateé€šè¿‡ä»…åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè®¡ç®—æœ€ç»ˆçš„æŒ‡æ ‡æ¥è§£å†³äº†è¿™ä¸ªé—®é¢˜</li>
<li><strong>é¢„æµ‹ç»“æœå’Œå‚è€ƒç»“æœè¢«åˆ†åˆ«è®¡ç®—å¹¶æä¾›ç»™æ¯ä¸ªèŠ‚ç‚¹çš„æŒ‡æ ‡</strong>ï¼Œè¿™äº›ç»“æœæš‚æ—¶å­˜å‚¨åœ¨Apache Arrowè¡¨ä¸­ï¼Œé¿å…äº†GPUæˆ–CPUå†…å­˜çš„æ··ä¹±</li>
<li>å½“ä½ å‡†å¤‡ä½¿ç”¨compute()è®¡ç®—æœ€ç»ˆæŒ‡æ ‡æ—¶ï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹èƒ½å¤Ÿè®¿é—®æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸Šå­˜å‚¨çš„é¢„æµ‹ç»“æœå’Œå‚è€ƒç»“æœã€‚ä¸€æ—¦å®ƒæ”¶é›†åˆ°æ‰€æœ‰çš„é¢„æµ‹ç»“æœå’Œå‚è€ƒç»“æœï¼Œcompute()å°†è¿›è¡Œæœ€ç»ˆçš„æŒ‡æ ‡è¯„ä¼°</li>
</ol>
<p>è¿™ä¸ªè§£å†³æ–¹æ¡ˆä½¿å¾—ğŸ¤—Evaluateèƒ½å¤Ÿåœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­æ‰§è¡Œåˆ†å¸ƒå¼é¢„æµ‹ï¼Œè¿™å¯¹äºæé«˜è¯„ä¼°é€Ÿåº¦éå¸¸é‡è¦ã€‚åŒæ—¶ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨å¤æ‚çš„éå¯åŠ æŒ‡æ ‡ï¼Œè€Œä¸æµªè´¹å®è´µçš„GPUæˆ–CPUå†…å­˜</p>
<blockquote>
<p>ç»„åˆè¯„ä¼°</p>
</blockquote>
<p>é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä»…æƒ³è¯„ä¼°å•ä¸ªæŒ‡æ ‡ï¼Œè€Œæ˜¯æƒ³è¯„ä¼°ä¸€ç³»åˆ—ä¸åŒçš„æŒ‡æ ‡ï¼Œä»¥æ•æ‰æ¨¡å‹æ€§èƒ½çš„ä¸åŒæ–¹é¢</p>
<p>ä¾‹å¦‚ï¼Œå¯¹äºåˆ†ç±»é—®é¢˜ï¼Œé™¤äº†å‡†ç¡®åº¦å¤–ï¼Œé€šå¸¸è¿˜ä¼šè®¡ç®—F1åˆ†æ•°ã€å¬å›ç‡å’Œç²¾ç¡®åº¦ï¼Œä»¥ä¾¿æ›´å¥½åœ°äº†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚å½“ç„¶ï¼Œä½ å¯ä»¥åŠ è½½ä¸€ç³»åˆ—æŒ‡æ ‡å¹¶ä¾æ¬¡è°ƒç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œä¸€ç§æ›´æ–¹ä¾¿çš„æ–¹æ³•æ˜¯ä½¿ç”¨combine()å‡½æ•°å°†å®ƒä»¬æ†ç»‘åœ¨ä¸€èµ·ï¼š</p>
<pre><code class="lang-python">clf_metrics = evaluate.combine([<span class="hljs-string">"accuracy"</span>, <span class="hljs-string">"f1"</span>, <span class="hljs-string">"precision"</span>, <span class="hljs-string">"recall"</span>])
clf_metrics.compute(predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>], references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])

{
  <span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.667</span>,
  <span class="hljs-string">'f1'</span>: <span class="hljs-number">0.667</span>,
  <span class="hljs-string">'precision'</span>: <span class="hljs-number">1.0</span>,
  <span class="hljs-string">'recall'</span>: <span class="hljs-number">0.5</span>
}
</code></pre>
<blockquote>
<p>è‡ªåŠ¨åŒ–è¯„ä¼°</p>
</blockquote>
<p><strong>ä½¿ç”¨evaluate.evaluator()æä¾›äº†è‡ªåŠ¨åŒ–çš„è¯„ä¼°åŠŸèƒ½</strong>ï¼Œåªéœ€è¦ä¸€ä¸ªæ¨¡å‹ã€æ•°æ®é›†å’Œåº¦é‡æŒ‡æ ‡ï¼Œä¸EvaluationModulesä¸­çš„åº¦é‡æŒ‡æ ‡ç›¸æ¯”ï¼Œå®ƒä¸éœ€è¦æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚å› æ­¤ï¼Œä½¿ç”¨ç»™å®šçš„åº¦é‡æŒ‡æ ‡åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ›´å®¹æ˜“ï¼Œå› ä¸ºæ¨ç†è¿‡ç¨‹æ˜¯åœ¨å†…éƒ¨å¤„ç†çš„</p>
<p>ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œå®ƒä½¿ç”¨äº†transformersåº“ä¸­çš„pipelineæŠ½è±¡ã€‚ç„¶è€Œï¼Œåªè¦ç¬¦åˆpipelineæ¥å£ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„æ¡†æ¶</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-keyword">import</span> evaluate
</code></pre>
<p>ä¸ºäº†ä½¿ç”¨evaluatorè¿›è¡Œè¯„ä¼°ï¼Œè®©æˆ‘ä»¬åŠ è½½ä¸€ä¸ªåŸºäºIMDbè®­ç»ƒçš„transformers pipelineï¼ˆä½†ä½ ä¹Ÿå¯ä»¥ä¼ é€’è‡ªå·±çš„è‡ªå®šä¹‰æ¨ç†ç±»æ¥é€‚åº”ä»»ä½•éµå¾ªpipelineè°ƒç”¨APIçš„æ¡†æ¶ï¼‰ï¼Œå¹¶ä½¿ç”¨IMDbçš„æµ‹è¯•é›†å’Œå‡†ç¡®åº¦åº¦é‡æŒ‡æ ‡è¿›è¡Œè¯„ä¼°</p>
<pre><code class="lang-python">pipe = pipeline(<span class="hljs-string">"text-classification"</span>, model=<span class="hljs-string">"lvwerra/distilbert-imdb"</span>, device=<span class="hljs-number">0</span>)
data = load_dataset(<span class="hljs-string">"imdb"</span>, split=<span class="hljs-string">"test"</span>).shuffle().select(range(<span class="hljs-number">1000</span>))
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)

task_evaluator = evaluator(<span class="hljs-string">"text-classification"</span>)
results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={<span class="hljs-string">"NEGATIVE"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"POSITIVE"</span>: <span class="hljs-number">1</span>},)

{<span class="hljs-string">'accuracy'</span>: <span class="hljs-number">0.934</span>}
</code></pre>
<p>ä»…ä»…è®¡ç®—åº¦é‡æŒ‡æ ‡çš„å€¼é€šå¸¸è¿˜ä¸è¶³ä»¥çŸ¥é“ä¸€ä¸ªæ¨¡å‹æ˜¯å¦æ˜¾è‘—ä¼˜äºå¦ä¸€ä¸ªæ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨<code>è‡ªåŠ©æ³•(bootstrapping)</code>ï¼Œevaluateè®¡ç®—ç½®ä¿¡åŒºé—´å’Œæ ‡å‡†è¯¯å·®ï¼Œè¿™æœ‰åŠ©äºä¼°è®¡åˆ†æ•°çš„ç¨³å®šæ€§</p>
<pre><code class="lang-python">results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={<span class="hljs-string">"NEGATIVE"</span>: <span class="hljs-number">0</span>, <span class="hljs-string">"POSITIVE"</span>: <span class="hljs-number">1</span>},
                       strategy=<span class="hljs-string">"bootstrap"</span>, n_resamples=<span class="hljs-number">200</span>)

{<span class="hljs-string">'accuracy'</span>:
    {
      <span class="hljs-string">'confidence_interval'</span>: (<span class="hljs-number">0.906</span>, <span class="hljs-number">0.9406749892841922</span>),
      <span class="hljs-string">'standard_error'</span>: <span class="hljs-number">0.00865213251082787</span>,
      <span class="hljs-string">'score'</span>: <span class="hljs-number">0.923</span>
    }
}
</code></pre>
<p>è¯„ä¼°å™¨æœŸæœ›æ•°æ®è¾“å…¥å…·æœ‰"text"å’Œ"label"åˆ—ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†ä¸åŒï¼Œå¯ä»¥ä½¿ç”¨å…³é”®å­—å‚æ•°input_column="text"å’Œlabel_column="label"æ¥æä¾›åˆ—å</p>
<p>ç›®å‰åªæ”¯æŒ"text-classification"ä»»åŠ¡ï¼Œå°†æ¥å¯èƒ½ä¼šæ·»åŠ æ›´å¤šçš„ä»»åŠ¡ç±»å‹</p>
<h3 id="ç»“æœå­˜å‚¨">3.2.4 ç»“æœå­˜å‚¨</h3>
<blockquote>
<p>è¯„ä¼°ç»“æœsaveå’Œpush</p>
</blockquote>
<p>ä¿å­˜å’Œåˆ†äº«è¯„ä¼°ç»“æœæ˜¯ä¸€ä¸ªé‡è¦çš„æ­¥éª¤ã€‚æˆ‘ä»¬æä¾›evaluate.save()å‡½æ•°æ¥æ–¹ä¾¿åœ°ä¿å­˜æŒ‡æ ‡ç»“æœã€‚ä½ å¯ä»¥ä¼ é€’ä¸€ä¸ªç‰¹å®šçš„æ–‡ä»¶åæˆ–ç›®å½•ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œç»“æœå°†ä¿å­˜åœ¨ä¸€ä¸ªå¸¦æœ‰è‡ªåŠ¨åˆ›å»ºçš„æ–‡ä»¶åçš„æ–‡ä»¶ä¸­</p>
<p>é™¤äº†ç›®å½•æˆ–æ–‡ä»¶åï¼Œè¯¥å‡½æ•°è¿˜æ¥å—ä»»æ„çš„é”®å€¼å¯¹ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ä¸€ä¸ªJSONæ–‡ä»¶ä¸­</p>
<pre><code class="lang-python">result = accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])

hyperparams = {<span class="hljs-string">"model"</span>: <span class="hljs-string">"bert-base-uncased"</span>}
evaluate.save(<span class="hljs-string">"./results/"</span>, experiment=<span class="hljs-string">"run 42"</span>, **result, **hyperparams)

PosixPath(<span class="hljs-string">'results/result-2022_05_30-22_09_11.json'</span>)

<span class="hljs-comment"># result-2022_05_30-22_09_11.json</span>
{
    <span class="hljs-string">"experiment"</span>: <span class="hljs-string">"run 42"</span>,
    <span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.5</span>,
    <span class="hljs-string">"model"</span>: <span class="hljs-string">"bert-base-uncased"</span>,
    <span class="hljs-string">"_timestamp"</span>: <span class="hljs-string">"2022-05-30T22:09:11.959469"</span>,
    <span class="hljs-string">"_git_commit_hash"</span>: <span class="hljs-string">"123456789abcdefghijkl"</span>,
    <span class="hljs-string">"_evaluate_version"</span>: <span class="hljs-string">"0.1.0"</span>,
    <span class="hljs-string">"_python_version"</span>: <span class="hljs-string">"3.9.12 (main, Mar 26 2022, 15:51:15) \n[Clang 13.1.6 (clang-1316.0.21.2)]"</span>,
    <span class="hljs-string">"_interpreter_path"</span>: <span class="hljs-string">"/Users/leandro/git/evaluate/env/bin/python"</span>
}
</code></pre>
<p>é™¤äº†æŒ‡å®šçš„å­—æ®µï¼Œå®ƒè¿˜åŒ…å«æœ‰ç”¨çš„ç³»ç»Ÿä¿¡æ¯ï¼Œç”¨äºé‡ç°ç»“æœï¼Œä½ è¿˜åº”è¯¥å°†å®ƒä»¬æŠ¥å‘Šåˆ°æ¨¡å‹åœ¨Hubä¸Šçš„å­˜å‚¨åº“ä¸­</p>
<pre><code class="lang-python">evaluate.push_to_hub(
  model_id=<span class="hljs-string">"huggingface/gpt2-wikitext2"</span>,  <span class="hljs-comment"># model repository on hub</span>
  metric_value=<span class="hljs-number">0.5</span>,                       <span class="hljs-comment"># metric value</span>
  metric_type=<span class="hljs-string">"bleu"</span>,                     <span class="hljs-comment"># metric name, e.g. accuracy.name</span>
  metric_name=<span class="hljs-string">"BLEU"</span>,                     <span class="hljs-comment"># pretty name which is displayed</span>
  dataset_type=<span class="hljs-string">"wikitext"</span>,                <span class="hljs-comment"># dataset name on the hub</span>
  dataset_name=<span class="hljs-string">"WikiText"</span>,                <span class="hljs-comment"># pretty name</span>
  dataset_split=<span class="hljs-string">"test"</span>,                   <span class="hljs-comment"># dataset split used</span>
  task_type=<span class="hljs-string">"text-generation"</span>,            <span class="hljs-comment"># task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json</span>
  task_name=<span class="hljs-string">"Text Generation"</span>             <span class="hljs-comment"># pretty name for task</span>
)
</code></pre>
<blockquote>
<p>ä¸Šä¼ è‡ªå·±çš„æŒ‡æ ‡<a href="https://huggingface.co/docs/evaluate/main/en/creating_and_sharing" target="_blank">Creating and sharing a new evaluation</a></p>
</blockquote>
<h3 id="å¯è§†åŒ–">3.2.5 å¯è§†åŒ–</h3>
<p>å½“æ¯”è¾ƒå¤šä¸ªæ¨¡å‹æ—¶ï¼Œä»…é€šè¿‡æŸ¥çœ‹å®ƒä»¬çš„å¾—åˆ†å¾€å¾€å¾ˆéš¾å‘ç°å®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚è€Œä¸”é€šå¸¸æƒ…å†µä¸‹ï¼Œå¹¶æ²¡æœ‰ä¸€ä¸ªå•ä¸€çš„æœ€ä½³æ¨¡å‹ï¼Œè€Œæ˜¯åœ¨å‡†ç¡®æ€§å’Œå»¶è¿Ÿç­‰æ–¹é¢å­˜åœ¨ç€æƒè¡¡ï¼Œå› ä¸ºè¾ƒå¤§çš„æ¨¡å‹å¯èƒ½å…·æœ‰æ›´å¥½çš„æ€§èƒ½ä½†ä¹Ÿæ›´æ…¢ã€‚æˆ‘ä»¬æ­£åœ¨é€æ­¥æ·»åŠ ä¸åŒçš„å¯è§†åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚ç»˜å›¾ï¼Œä»¥ä¾¿æ›´è½»æ¾åœ°é€‰æ‹©é€‚åˆç‰¹å®šç”¨ä¾‹çš„æœ€ä½³æ¨¡å‹ã€‚</p>
<p>ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰å¤šä¸ªæ¨¡å‹çš„ç»“æœåˆ—è¡¨ï¼ˆä»¥å­—å…¸å½¢å¼ï¼‰ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä¼ é€’ç»™radar_plot()å‡½æ•°è¿›è¡Œå¯è§†åŒ–ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> evaluate
<span class="hljs-keyword">from</span> evaluate.visualization <span class="hljs-keyword">import</span> radar_plot

data = [
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.99</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.8</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.95</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">33.6</span>},
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.98</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.87</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.91</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">11.2</span>},
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.98</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.78</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">87.6</span>}, 
   {<span class="hljs-string">"accuracy"</span>: <span class="hljs-number">0.88</span>, <span class="hljs-string">"precision"</span>: <span class="hljs-number">0.78</span>, <span class="hljs-string">"f1"</span>: <span class="hljs-number">0.81</span>, <span class="hljs-string">"latency_in_seconds"</span>: <span class="hljs-number">101.6</span>}
   ]
model_names = [<span class="hljs-string">"Model 1"</span>, <span class="hljs-string">"Model 2"</span>, <span class="hljs-string">"Model 3"</span>, <span class="hljs-string">"Model 4"</span>]
plot = radar_plot(data=data, model_names=model_names)
plot.show()
</code></pre>
<p><a data-lightbox="132821b7-734e-48cb-a577-2f7ab4612786" data-title="æ¨¡å‹æ¯”è¾ƒæŒ‡æ ‡å›¾" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹/æ¨¡å‹æ¯”è¾ƒæŒ‡æ ‡å›¾.webp" target="_blank"><img alt="æ¨¡å‹æ¯”è¾ƒæŒ‡æ ‡å›¾" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹/æ¨¡å‹æ¯”è¾ƒæŒ‡æ ‡å›¾.webp"/></a></p>
<h3 id="é€‰æ‹©åˆé€‚æŒ‡æ ‡">3.2.6 é€‰æ‹©åˆé€‚æŒ‡æ ‡</h3>
<p>è¯„ä¼°æŒ‡æ ‡å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé«˜çº§ç±»åˆ«ï¼š</p>
<ul>
<li><p><strong>é€šç”¨æŒ‡æ ‡</strong>ï¼šé€‚ç”¨äºå„ç§æƒ…å†µå’Œæ•°æ®é›†çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ç²¾ç¡®åº¦å’Œå‡†ç¡®åº¦</p>
<pre><code class="lang-python">precision_metric = evaluate.load(<span class="hljs-string">"precision"</span>)
results = precision_metric.compute(references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
print(results)

{<span class="hljs-string">'precision'</span>: <span class="hljs-number">1.0</span>}
</code></pre>
</li>
<li><p><strong>ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡</strong>ï¼šä»…é€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘(é€šå¸¸ä½¿ç”¨BLEUæˆ–ROUGEæŒ‡æ ‡è¿›è¡Œè¯„ä¼°)æˆ–å‘½åå®ä½“è¯†åˆ«(é€šå¸¸ä½¿ç”¨seqevalè¿›è¡Œè¯„ä¼°)</p>
</li>
<li><p><strong>æ•°æ®é›†ç‰¹å®šæŒ‡æ ‡</strong>ï¼šæ—¨åœ¨è¡¡é‡æ¨¡å‹åœ¨ç‰¹å®šåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œä¾‹å¦‚GLUEåŸºå‡†æµ‹è¯•å…·æœ‰ä¸“é—¨çš„è¯„ä¼°æŒ‡æ ‡</p>
</li>
</ul>
<h1 id="transformers">4 transformers</h1>
<h2 id="æ¦‚è¿°_1">4.1 æ¦‚è¿°</h2>
<blockquote>
<p><a href="https://huggingface.co/docs/transformers/v4.30.0/en/task_summary" target="_blank">What ğŸ¤— Transformers can do</a></p>
</blockquote>
<p>ğŸ¤— Transformersæä¾›äº†APIå’Œå·¥å…·ï¼Œå¯è½»æ¾ä¸‹è½½å’Œè®­ç»ƒæœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å‡å°‘è®¡ç®—æˆæœ¬ã€ç¢³è¶³è¿¹ï¼Œå¹¶èŠ‚çœä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„æ—¶é—´å’Œèµ„æºã€‚è¿™äº›æ¨¡å‹æ”¯æŒä¸åŒé¢†åŸŸçš„å¸¸è§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼š</p>
<ul>
<li>ğŸ“ è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ç³»ç»Ÿã€è¯­è¨€å»ºæ¨¡ã€æ‘˜è¦ç”Ÿæˆã€ç¿»è¯‘ã€å¤šé¡¹é€‰æ‹©å’Œæ–‡æœ¬ç”Ÿæˆ</li>
<li>ğŸ–¼ï¸ è®¡ç®—æœºè§†è§‰ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²</li>
<li>ğŸ—£ï¸ éŸ³é¢‘ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘åˆ†ç±»</li>
<li>ğŸ™ å¤šæ¨¡æ€ï¼šè¡¨æ ¼é—®ç­”ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€ä»æ‰«ææ–‡æ¡£ä¸­æå–ä¿¡æ¯ã€è§†é¢‘åˆ†ç±»å’Œè§†è§‰é—®ç­”</li>
</ul>
<p>ğŸ¤— Transformersæ”¯æŒåœ¨PyTorchã€TensorFlowå’ŒJAXä¹‹é—´è¿›è¡Œæ¡†æ¶äº’æ“ä½œã€‚è¿™æä¾›äº†åœ¨æ¨¡å‹çš„ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒæ¡†æ¶çš„çµæ´»æ€§ï¼›å¯ä»¥åœ¨ä¸€ä¸ªæ¡†æ¶ä¸­ç”¨ä¸‰è¡Œä»£ç è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨å¦ä¸€ä¸ªæ¡†æ¶ä¸­åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚æ¨¡å‹è¿˜å¯ä»¥å¯¼å‡ºä¸ºONNXå’ŒTorchScriptç­‰æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œéƒ¨ç½²</p>
<h2 id="å®‰è£…_3">4.2 å®‰è£…</h2>
<pre><code class="lang-cmd">pip install transformers datasets
</code></pre>
<h2 id="å¿«é€Ÿå¼€å§‹_2">4.3 å¿«é€Ÿå¼€å§‹</h2>
<h3 id="pipeline">4.3.1 Pipeline</h3>
<p>pipeline()æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•å’Œæœ€å¿«æ·çš„æ–¹æ³•ã€‚æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨pipeline()è¿›è¡Œè®¸å¤šä»»åŠ¡çš„æ¨ç†ï¼Œæ¶µç›–äº†ä¸åŒçš„æ¨¡æ€ï¼Œä¸‹è¡¨åˆ—å‡ºäº†å…¶ä¸­ä¸€äº›ä»»åŠ¡</p>
<table>
<thead>
<tr>
<th><strong>Task</strong></th>
<th><strong>Description</strong></th>
<th><strong>Modality</strong></th>
<th><strong>Pipeline identifier</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Text classification</td>
<td>assign a label to a given sequence of text</td>
<td>NLP</td>
<td>pipeline(task=â€œsentiment-analysisâ€)</td>
</tr>
<tr>
<td>Text generation</td>
<td>generate text given a prompt</td>
<td>NLP</td>
<td>pipeline(task=â€œtext-generationâ€)</td>
</tr>
<tr>
<td>Summarization</td>
<td>generate a summary of a sequence of text or document</td>
<td>NLP</td>
<td>pipeline(task=â€œsummarizationâ€)</td>
</tr>
<tr>
<td>Image classification</td>
<td>assign a label to an image</td>
<td>CV</td>
<td>pipeline(task=â€œimage-classificationâ€)</td>
</tr>
<tr>
<td>Image segmentation</td>
<td>assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)</td>
<td>CV</td>
<td>pipeline(task=â€œimage-segmentationâ€)</td>
</tr>
<tr>
<td>Object detection</td>
<td>predict the bounding boxes and classes of objects in an image</td>
<td>CV</td>
<td>pipeline(task=â€œobject-detectionâ€)</td>
</tr>
<tr>
<td>Audio classification</td>
<td>assign a label to some audio data</td>
<td>Audio</td>
<td>pipeline(task=â€œaudio-classificationâ€)</td>
</tr>
<tr>
<td>Automatic speech recognition</td>
<td>transcribe speech into text</td>
<td>Audio</td>
<td>pipeline(task=â€œautomatic-speech-recognitionâ€)</td>
</tr>
<tr>
<td>Visual question answering</td>
<td>answer a question about the image, given an image and a question</td>
<td>Multimodal</td>
<td>pipeline(task=â€œvqaâ€)</td>
</tr>
<tr>
<td>Document question answering</td>
<td>answer a question about a document, given an image and a question</td>
<td>Multimodal</td>
<td>pipeline(task=â€œdocument-question-answeringâ€)</td>
</tr>
<tr>
<td>Image captioning</td>
<td>generate a caption for a given image</td>
<td>Multimodal</td>
<td>pipeline(task=â€œimage-to-textâ€)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>åŸºæœ¬ä½¿ç”¨</p>
</blockquote>
<p>é¦–å…ˆï¼Œé€šè¿‡åˆ›å»ºpipeline()çš„å®ä¾‹å¹¶æŒ‡å®šè¦ä½¿ç”¨çš„ä»»åŠ¡ï¼Œå¼€å§‹ä½¿ç”¨å®ƒã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä»¥æƒ…æ„Ÿåˆ†æçš„pipeline()ä¸ºä¾‹ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">"sentiment-analysis"</span>)
</code></pre>
<p>pipeline()ä¼šä¸‹è½½å¹¶ç¼“å­˜ç”¨äºæƒ…æ„Ÿåˆ†æçš„é»˜è®¤é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥åœ¨ç›®æ ‡æ–‡æœ¬ä¸Šä½¿ç”¨åˆ†ç±»å™¨äº†ï¼š</p>
<pre><code class="lang-python">classifier(<span class="hljs-string">"We are very happy to show you the ğŸ¤— Transformers library."</span>)

[{<span class="hljs-string">'label'</span>: <span class="hljs-string">'POSITIVE'</span>, <span class="hljs-string">'score'</span>: <span class="hljs-number">0.9998</span>}]
</code></pre>
<p>å¦‚æœæ‚¨æœ‰å¤šä¸ªè¾“å…¥ï¼Œè¯·å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™pipeline()ï¼Œä»¥è¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨</p>
<pre><code class="lang-python">results = classifier([<span class="hljs-string">"We are very happy to show you the ğŸ¤— Transformers library."</span>, <span class="hljs-string">"We hope you don't hate it."</span>])
<span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
    print(f<span class="hljs-string">"label: {result['label']}, with score: {round(result['score'], 4)}"</span>)

label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>
</code></pre>
<p>pipeline()è¿˜å¯ä»¥å¯¹ä»»ä½•æ‚¨å–œæ¬¢çš„ä»»åŠ¡è¿­ä»£æ•´ä¸ªæ•°æ®é›†ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè®©æˆ‘ä»¬é€‰æ‹©è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä½œä¸ºæˆ‘ä»¬çš„ä»»åŠ¡</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

speech_recognizer = pipeline(<span class="hljs-string">"automatic-speech-recognition"</span>, model=<span class="hljs-string">"facebook/wav2vec2-base-960h"</span>)
</code></pre>
<p>åŠ è½½æ‚¨æƒ³è¦è¿­ä»£çš„éŸ³é¢‘æ•°æ®é›†(æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— Datasetså¿«é€Ÿå…¥é—¨)ã€‚ä¾‹å¦‚ï¼ŒåŠ è½½MInDS-14æ•°æ®é›†ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

dataset = load_dataset(<span class="hljs-string">"PolyAI/minds14"</span>, name=<span class="hljs-string">"en-US"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
<p>æ‚¨éœ€è¦ç¡®ä¿æ•°æ®é›†çš„é‡‡æ ·ç‡ä¸facebook/wav2vec2-base-960h è®­ç»ƒæ—¶ä½¿ç”¨çš„é‡‡æ ·ç‡ç›¸åŒ¹é…</p>
<pre><code class="lang-python">dataset = dataset.cast_column(<span class="hljs-string">"audio"</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
</code></pre>
<p>è°ƒç”¨"audio"åˆ—æ—¶ï¼ŒéŸ³é¢‘æ–‡ä»¶ä¼šè‡ªåŠ¨åŠ è½½å’Œé‡æ–°é‡‡æ ·ã€‚ä»å‰å››ä¸ªæ ·æœ¬ä¸­æå–åŸå§‹æ³¢å½¢æ•°ç»„ï¼Œå¹¶å°†å…¶ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™pipelineï¼š</p>
<pre><code class="lang-python">result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">"audio"</span>])
print([d[<span class="hljs-string">"text"</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])

[<span class="hljs-string">'I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT'</span>, <span class="hljs-string">"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE"</span>, <span class="hljs-string">"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS"</span>, <span class="hljs-string">'HOW DO I FURN A JOINA COUT'</span>]
</code></pre>
<p>å¯¹äºè¾“å…¥è¾ƒå¤§çš„æ›´å¤§æ•°æ®é›†(å¦‚è¯­éŸ³æˆ–è§†è§‰æ•°æ®)ï¼Œæ‚¨å¯ä»¥å°†ç”Ÿæˆå™¨ä¼ é€’ç»™pipelineï¼Œè€Œä¸æ˜¯å°†å…¶ä½œä¸ºåˆ—è¡¨åŠ è½½åˆ°å†…å­˜ä¸­</p>
<p>åœ¨pipelineä¸­ä½¿ç”¨å…¶ä»–æ¨¡å‹å’Œåˆ†è¯å™¨pipeline()å¯ä»¥é€‚åº”Hubä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œè¿™ä½¿å¾—å¯¹pipeline()è¿›è¡Œå…¶ä»–ç”¨é€”çš„è°ƒæ•´å˜å¾—å®¹æ˜“</p>
<p>ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³è¦ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ³•è¯­æ–‡æœ¬çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨Hubä¸Šçš„æ ‡ç­¾æ¥è¿‡æ»¤åˆé€‚çš„æ¨¡å‹ã€‚é€šè¿‡å¯¹è¿‡æ»¤ç»“æœè¿›è¡Œæ’åºï¼Œæ‚¨å¯ä»¥è·å¾—ä¸€ä¸ªé’ˆå¯¹æ³•è¯­æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æçš„å¤šè¯­è¨€BERTæ¨¡å‹</p>
<blockquote>
<p>åœ¨pipelineä¸­ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹å’Œåˆ†è¯å™¨</p>
</blockquote>
<p>pipeline()å¯ä»¥é€‚åº”Hubä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œè¿™ä½¿å¾—å°†pipeline()é€‚åº”å…¶ä»–ç”¨ä¾‹å˜å¾—å®¹æ˜“</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

model_name = <span class="hljs-string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

classifier = pipeline(<span class="hljs-string">"sentiment-analysis"</span>, model=model, tokenizer=tokenizer)
classifier(<span class="hljs-string">"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers."</span>)
</code></pre>
<h3 id="autoclass">4.3.2 AutoClass</h3>
<p>AutoClassæ˜¯ä¸€ç§å¿«æ·æ–¹å¼ï¼Œå®ƒå¯ä»¥æ ¹æ®æ¨¡å‹çš„åç§°æˆ–è·¯å¾„è‡ªåŠ¨è·å–é¢„è®­ç»ƒæ¨¡å‹çš„æ¶æ„ã€‚æ‚¨åªéœ€è¦é€‰æ‹©ä¸æ‚¨çš„ä»»åŠ¡ç›¸åŒ¹é…çš„AutoClasså’Œç›¸åº”çš„é¢„å¤„ç†ç±»</p>
<h4 id="autotokenizer"><a class="anchor-navigation-ex-anchor" href="#autotokenizer" name="autotokenizer"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#autotokenizer" name="autotokenizer"><i aria-hidden="true" class="fa fa-link"></i></a>AutoTokenizer</h4>
<p>AutoTokenizeråˆ†è¯å™¨è´Ÿè´£å°†æ–‡æœ¬é¢„å¤„ç†ä¸ºæ¨¡å‹è¾“å…¥çš„æ•°å­—æ•°ç»„ã€‚æœ‰å¤šä¸ªè§„åˆ™æ¥è§„å®šåˆ†è¯çš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¦‚ä½•æ‹†åˆ†ä¸€ä¸ªå•è¯ä»¥åŠä»¥ä½•ç§çº§åˆ«æ‹†åˆ†å•è¯</p>
<p>æœ€é‡è¦çš„æ˜¯ï¼Œæ‚¨<strong>éœ€è¦ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹åç§°æ¥å®ä¾‹åŒ–ä¸€ä¸ªåˆ†è¯å™¨ï¼Œä»¥ç¡®ä¿æ‚¨ä½¿ç”¨äº†ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒçš„åˆ†è¯è§„åˆ™</strong></p>
<blockquote>
<p>ä½¿ç”¨AutoTokenizeråŠ è½½ä¸€ä¸ªåˆ†è¯å™¨</p>
</blockquote>
<p>å°†<code>return_tensors</code>å‚æ•°è®¾ç½®ä¸º<code>pt</code>ä»¥è¿”å›é€‚ç”¨äºPyTorchçš„å¼ é‡ï¼Œæˆ–è€…è®¾ç½®ä¸º<code>tf</code>ä»¥è¿”å›é€‚ç”¨äºTensorFlowçš„å¼ é‡</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_name = <span class="hljs-string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
tokenizer = AutoTokenizer.from_pretrained(model_name)
encoding = tokenizer(<span class="hljs-string">"We are very happy to show you the ğŸ¤— Transformers library."</span>, return_tensors=<span class="hljs-string">"pt"</span>)

{<span class="hljs-string">'input_ids'</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">'token_type_ids'</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">'attention_mask'</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}

tokenizer.decode(encoding[<span class="hljs-string">"input_ids"</span>])
<span class="hljs-string">"We are very happy to show you the ğŸ¤— Transformers library."</span>
</code></pre>
<p>åˆ†è¯å™¨è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé¡¹ç›®çš„å­—å…¸ï¼š</p>
<ul>
<li><strong>input_ids</strong>ï¼šè¡¨ç¤ºæ–‡æœ¬ä¸­å„ä¸ªæ ‡è®°çš„æ•°å­—</li>
<li><strong>token_type_ids</strong>ï¼šå¦‚æœæœ‰å¤šä¸ªåºåˆ—ï¼ŒæŒ‡ç¤ºä¸€ä¸ªæ ‡è®°å±äºå“ªä¸ªåºåˆ—</li>
<li><strong>attention_mask</strong>ï¼šæŒ‡ç¤ºä¸€ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥è¢«æ©ç›–(masked)</li>
</ul>
<p>åˆ†è¯å™¨è¿˜å¯ä»¥æ¥å—ä¸€ä¸ªè¾“å…¥åˆ—è¡¨ï¼Œå¹¶å¯¹æ–‡æœ¬è¿›è¡Œå¡«å……å’Œæˆªæ–­ï¼Œä»¥è¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹å¤„ç†æ•°æ®</p>
<pre><code class="lang-python">pt_batch = tokenizer(
    [<span class="hljs-string">"We are very happy to show you the ğŸ¤— Transformers library."</span>, <span class="hljs-string">"We hope you don't hate it."</span>],
    padding=<span class="hljs-keyword">True</span>,
    truncation=<span class="hljs-keyword">True</span>,
    max_length=<span class="hljs-number">512</span>,
    return_tensors=<span class="hljs-string">"pt"</span>)
</code></pre>
<blockquote>
<p>pad + truncation</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># padding</span>
batch_sentences = [
    <span class="hljs-string">"But what about second breakfast?"</span>,
    <span class="hljs-string">"Don't think he knows about second breakfast, Pip."</span>,
    <span class="hljs-string">"What about elevensies?"</span>,
]
encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-keyword">True</span>)

{<span class="hljs-string">'input_ids'</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'token_type_ids'</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'attention_mask'</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}

<span class="hljs-comment"># truncation  å°†truncationå‚æ•°è®¾ç½®ä¸ºTrueï¼Œå¯ä»¥å°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹æ‰€èƒ½æ¥å—çš„æœ€å¤§é•¿åº¦</span>
batch_sentences = [
    <span class="hljs-string">"But what about second breakfast?"</span>,
    <span class="hljs-string">"Don't think he knows about second breakfast, Pip."</span>,
    <span class="hljs-string">"What about elevensies?"</span>,
]
encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-keyword">True</span>, truncation=<span class="hljs-keyword">True</span>)

{<span class="hljs-string">'input_ids'</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'token_type_ids'</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">'attention_mask'</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}
</code></pre>
<h4 id="automodel"><a class="anchor-navigation-ex-anchor" href="#automodel" name="automodel"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#automodel" name="automodel"><i aria-hidden="true" class="fa fa-link"></i></a>AutoModel</h4>
<p>ğŸ¤—Transformersæä¾›äº†ä¸€ç§ç®€å•è€Œç»Ÿä¸€çš„æ–¹æ³•æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å®ä¾‹ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åƒåŠ è½½AutoTokenizerä¸€æ ·åŠ è½½AutoModel</p>
<p>å”¯ä¸€çš„åŒºåˆ«æ˜¯<strong>é€‰æ‹©æ­£ç¡®çš„AutoModelæ¥é€‚åº”ä»»åŠ¡</strong>ã€‚å¯¹äºæ–‡æœ¬(æˆ–åºåˆ—)åˆ†ç±»ï¼Œæ‚¨åº”è¯¥åŠ è½½AutoModelForSequenceClassification</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

model_name = <span class="hljs-string">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)

pt_outputs = pt_model(**pt_batch)
</code></pre>
<p>æ¨¡å‹å°†æœ€ç»ˆçš„æ¿€æ´»å€¼å­˜å‚¨åœ¨logitså±æ€§ä¸­ã€‚åº”ç”¨softmaxå‡½æ•°åˆ°logitsä¸Šä»¥è·å–æ¦‚ç‡å€¼</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=<span class="hljs-number">-1</span>)


tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)
</code></pre>
<blockquote>
<p>åœ¨huggingfaceåº“ä¸­ï¼ŒAutoModelç±»å¯ä»¥æ ¹æ®ç»™å®šçš„checkpointè‡ªåŠ¨é€‰æ‹©å¹¶åŠ è½½é€‚åˆçš„æ¨¡å‹ã€‚å®ƒæ”¯æŒå„ç§ä¸åŒçš„æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š</p>
</blockquote>
<ul>
<li>AutoModel: ç”¨äºé€šç”¨çš„æ¨¡å‹åŠ è½½ï¼Œæ ¹æ®checkpointè‡ªåŠ¨é€‰æ‹©é€‚åˆçš„æ¨¡å‹æ¶æ„</li>
<li>AutoModelForSequenceClassification: ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»</li>
<li>AutoModelForQuestionAnswering: ç”¨äºé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚é˜…è¯»ç†è§£</li>
<li>AutoModelForTokenClassification: ç”¨äºæ ‡è®°åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«</li>
<li>AutoModelForMaskedLM: ç”¨äºé®è”½è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚BERT</li>
<li>AutoModelForCausalLM: ç”¨äºæœ‰å› æœå…³ç³»çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚GPT</li>
<li>AutoModelForImageClassification: ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚ResNet</li>
<li>AutoModelForImageSegmentation: ç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡çš„æ¨¡å‹ï¼Œå¦‚Mask R-CNN</li>
</ul>
<p>è¿™äº›ä»…æ˜¯AutoModelç±»çš„ä¸€äº›ç¤ºä¾‹ï¼Œå®é™…ä¸Šè¿˜æœ‰æ›´å¤šå¯ç”¨çš„æ¨¡å‹æ¶æ„ã€‚æ‚¨å¯ä»¥æ ¹æ®å…·ä½“çš„ä»»åŠ¡éœ€æ±‚é€‰æ‹©é€‚åˆçš„AutoModelç±»è¿›è¡ŒåŠ è½½å’Œä½¿ç”¨</p>
<h4 id="å…¶ä»–çš„autoç±»"><a class="anchor-navigation-ex-anchor" href="#å…¶ä»–çš„autoç±»" name="å…¶ä»–çš„autoç±»"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#å…¶ä»–çš„autoç±»" name="å…¶ä»–çš„autoç±»"><i aria-hidden="true" class="fa fa-link"></i></a>å…¶ä»–çš„Autoç±»</h4>
<blockquote>
<p>AutoImageProcessor</p>
</blockquote>
<p>å¯¹äºè§†è§‰ä»»åŠ¡ï¼Œå›¾åƒå¤„ç†å™¨å°†å›¾åƒå¤„ç†ä¸ºæ­£ç¡®çš„è¾“å…¥æ ¼å¼</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">"google/vit-base-patch16-224"</span>)
</code></pre>
<blockquote>
<p>AutoFeatureExtractor</p>
</blockquote>
<p>å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œç‰¹å¾æå–å™¨å°†éŸ³é¢‘ä¿¡å·å¤„ç†ä¸ºæ­£ç¡®çš„è¾“å…¥æ ¼å¼</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(
    <span class="hljs-string">"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"</span>
)
</code></pre>
<blockquote>
<p>AutoProcessor</p>
</blockquote>
<p>å¤šæ¨¡æ€ä»»åŠ¡éœ€è¦ä¸€ä¸ªå¤„ç†å™¨æ¥ç»“åˆä¸¤ç§ç±»å‹çš„é¢„å¤„ç†å·¥å…·ã€‚ä¾‹å¦‚ï¼ŒLayoutLMV2æ¨¡å‹éœ€è¦ä¸€ä¸ªå›¾åƒå¤„ç†å™¨æ¥å¤„ç†å›¾åƒï¼Œè¿˜éœ€è¦ä¸€ä¸ªåˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬ï¼›å¤„ç†å™¨å°†ä¸¤è€…ç»“åˆèµ·æ¥</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

processor = AutoProcessor.from_pretrained(<span class="hljs-string">"microsoft/layoutlmv2-base-uncased"</span>)
</code></pre>
<h4 id="æ¨¡å‹ä¿å­˜"><a class="anchor-navigation-ex-anchor" href="#æ¨¡å‹ä¿å­˜" name="æ¨¡å‹ä¿å­˜"><i aria-hidden="true" class="fa fa-link"></i></a><a class="plugin-anchor" href="#æ¨¡å‹ä¿å­˜" name="æ¨¡å‹ä¿å­˜"><i aria-hidden="true" class="fa fa-link"></i></a>æ¨¡å‹ä¿å­˜</h4>
<p>ä¸€æ—¦æ‚¨çš„æ¨¡å‹ç»è¿‡å¾®è°ƒï¼Œæ‚¨å¯ä»¥ä½¿ç”¨PreTrainedModel.save_pretrained()å°†å…¶ä¸å…¶æ ‡è®°å™¨ä¸€èµ·ä¿å­˜èµ·æ¥ï¼š</p>
<pre><code class="lang-python"><span class="hljs-comment"># æ¨¡å‹+åˆ†è¯å™¨ ä¿å­˜</span>
pt_save_directory = <span class="hljs-string">"./pt_save_pretrained"</span>
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)

<span class="hljs-comment"># åŠ è½½</span>
pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory)
</code></pre>
<h3 id="autoconfig">4.3.3 AutoConfig</h3>
<p>æ‚¨å¯ä»¥ä¿®æ”¹æ¨¡å‹çš„<strong>é…ç½®ç±»</strong>æ¥æ›´æ”¹æ¨¡å‹çš„æ„å»ºæ–¹å¼ã€‚é…ç½®ç±»æŒ‡å®šäº†æ¨¡å‹çš„å±æ€§ï¼Œä¾‹å¦‚éšè—å±‚çš„æ•°é‡æˆ–æ³¨æ„åŠ›å¤´æ•°</p>
<p>å½“æ‚¨ä»è‡ªå®šä¹‰é…ç½®ç±»åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œæ‚¨å°†ä»å¤´å¼€å§‹ã€‚æ¨¡å‹çš„å±æ€§å°†è¢«éšæœºåˆå§‹åŒ–ï¼Œæ‚¨éœ€è¦åœ¨ä½¿ç”¨æ¨¡å‹ä¹‹å‰å¯¹å…¶è¿›è¡Œè®­ç»ƒä»¥è·å¾—æœ‰æ„ä¹‰çš„ç»“æœ</p>
<p>é¦–å…ˆå¯¼å…¥AutoConfigï¼Œç„¶ååŠ è½½è¦ä¿®æ”¹çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨AutoConfig.from_pretrained()ä¸­ï¼Œæ‚¨å¯ä»¥æŒ‡å®šè¦æ›´æ”¹çš„å±æ€§ï¼Œä¾‹å¦‚æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼š</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

my_config = AutoConfig.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>, n_heads=<span class="hljs-number">12</span>)
my_model = AutoModel.from_config(my_config)
</code></pre>
<h3 id="trainer">4.3.4 Trainer</h3>
<p>å¯¹äºPyTorchï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„torch.nn.Moduleï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨ä»»ä½•å…¸å‹çš„è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨å®ƒä»¬ã€‚è™½ç„¶æ‚¨å¯ä»¥ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯ï¼Œä½†ğŸ¤—Transformersæä¾›äº†<code>Trainer</code>ç±»ï¼Œå…¶ä¸­åŒ…å«åŸºæœ¬çš„è®­ç»ƒå¾ªç¯ï¼Œå¹¶æ·»åŠ äº†å…¶ä»–åŠŸèƒ½ï¼Œå¦‚åˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ç­‰</p>
<p>æ ¹æ®æ‚¨çš„ä»»åŠ¡ï¼Œé€šå¸¸ä¼šå‘Trainerä¼ é€’ä»¥ä¸‹å‚æ•°ï¼š</p>
<ol>
<li><p><a href="https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/model#transformers.PreTrainedModel" target="_blank">PreTrainedModel</a>æˆ–<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" target="_blank"><code>torch.nn.Module</code></a>å¯¹è±¡</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>)
</code></pre>
</li>
<li><p><strong>TrainingArguments</strong>åŒ…å«äº†å¯ä»¥ä¿®æ”¹çš„æ¨¡å‹è¶…å‚æ•°ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ã€æ‰¹å¤§å°å’Œè®­ç»ƒçš„è½®æ•°ã€‚å¦‚æœä½ ä¸æŒ‡å®šä»»ä½•è®­ç»ƒå‚æ•°ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">"path/to/save/folder/"</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">8</span>,
    per_device_eval_batch_size=<span class="hljs-number">8</span>,
    num_train_epochs=<span class="hljs-number">2</span>,
)
</code></pre>
</li>
<li><p><strong>Preprocessing</strong>ç±»ï¼Œä¾‹å¦‚tokenizer(æ ‡è®°å™¨)ã€image processor(å›¾åƒå¤„ç†å™¨)ã€feature extractor(ç‰¹å¾æå–å™¨)æˆ–processor(å¤„ç†å™¨)</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>)
</code></pre>
</li>
<li><p>åŠ è½½æ•°æ®é›†</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">"rotten_tomatoes"</span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>
</code></pre>
</li>
<li><p>åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¯¹æ•°æ®é›†è¿›è¡Œ<strong>æ ‡è®°åŒ–</strong>å¤„ç†ï¼Œç„¶åä½¿ç”¨<code>map</code>å‡½æ•°å°†å…¶åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_dataset</span><span class="hljs-params">(dataset)</span>:</span>
    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">"text"</span>])

dataset = dataset.map(tokenize_dataset, batched=<span class="hljs-keyword">True</span>)
</code></pre>
</li>
<li><p>ä½¿ç”¨<code>DataCollatorWithPadding</code>æ¥ä»æ•°æ®é›†ä¸­åˆ›å»ºä¸€ä¸ªæ‰¹æ¬¡çš„ç¤ºä¾‹</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
</code></pre>
<p><code>DataCollatorWithPadding</code>æ˜¯Hugging Faceçš„<code>transformers</code>åº“ä¸­çš„ä¸€ä¸ªç±»ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ›å»ºæ‰¹æ¬¡æ•°æ®ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†ä¸åŒé•¿åº¦çš„æ ·æœ¬å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œä»¥ä¾¿èƒ½å¤ŸåŒæ—¶è¿›è¡Œæ‰¹å¤„ç†</p>
<p>å…·ä½“æ¥è¯´ï¼Œ<code>DataCollatorWithPadding</code>ä¼šæ ¹æ®ç»™å®šçš„æ•°æ®é›†ï¼Œæ‰¾åˆ°å…¶ä¸­æœ€é•¿çš„æ ·æœ¬ï¼Œå¹¶å°†å…¶ä»–æ ·æœ¬å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ã€‚å¡«å……é€šå¸¸ä½¿ç”¨ç‰¹å®šçš„å¡«å……ä»¤ç‰Œ(token)æ¥å®Œæˆï¼Œè¿™æ ·æ¨¡å‹åœ¨å¤„ç†æ—¶å¯ä»¥è¯†åˆ«å‡ºå¡«å……éƒ¨åˆ†ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„å¤„ç†</p>
<p>ä½¿ç”¨<code>DataCollatorWithPadding</code>å¯ä»¥ç¡®ä¿æ‰¹æ¬¡æ•°æ®çš„é•¿åº¦ä¸€è‡´ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¹¶é¿å…ç”±äºä¸åŒé•¿åº¦æ ·æœ¬å¯¼è‡´çš„é”™è¯¯</p>
</li>
</ol>
<p>ç°åœ¨å°†æ‰€æœ‰è¿™äº›ç±»ç»„åˆåœ¨Trainerä¸­</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset[<span class="hljs-string">"train"</span>],
    eval_dataset=dataset[<span class="hljs-string">"test"</span>],
    tokenizer=tokenizer,
    data_collator=data_collator,
)  <span class="hljs-comment"># doctest: +SKIP</span>

trainer.train()
</code></pre>
<p>Trainerç±»æä¾›äº†è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è¡Œä¸ºçš„æ–¹æ³•ï¼Œä½ å¯ä»¥é€šè¿‡ç»§æ‰¿Trainerç±»å¹¶é‡å†™å…¶ä¸­çš„æ–¹æ³•æ¥å®ç°è‡ªå®šä¹‰è¡Œä¸ºã€‚è¿™æ ·ä½ å°±å¯ä»¥å®šåˆ¶è¯¸å¦‚æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰åŠŸèƒ½ã€‚ä½ å¯ä»¥å‚è€ƒTrainerç±»çš„æ–‡æ¡£äº†è§£å¯ä»¥é‡å†™çš„æ–¹æ³•</p>
<p>å¦ä¸€ç§å®šåˆ¶è®­ç»ƒå¾ªç¯çš„æ–¹å¼æ˜¯ä½¿ç”¨å›è°ƒå‡½æ•°(Callbacks)ã€‚ä½ å¯ä»¥ä½¿ç”¨å›è°ƒå‡½æ•°ä¸å…¶ä»–åº“è¿›è¡Œé›†æˆï¼Œç›‘è§†è®­ç»ƒè¿‡ç¨‹å¹¶æŠ¥å‘Šè¿›å±•ï¼Œæˆ–åœ¨å¿…è¦æ—¶æå‰åœæ­¢è®­ç»ƒã€‚å›è°ƒå‡½æ•°ä¸ä¼šä¿®æ”¹è®­ç»ƒå¾ªç¯æœ¬èº«çš„è¡Œä¸ºã€‚å¦‚æœä½ éœ€è¦å®šåˆ¶æŸå¤±å‡½æ•°ç­‰å†…å®¹ï¼Œä½ éœ€è¦ç»§æ‰¿Trainerç±»æ¥å®ç°</p>
<h1 id="æ•™ç¨‹">5 æ•™ç¨‹</h1>
<h2 id="æ¨¡å‹è®­ç»ƒ">5.1 æ¨¡å‹è®­ç»ƒ</h2>
<p>ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ‰å¾ˆå¤šå¥½å¤„ã€‚å®ƒå¯ä»¥å‡å°‘è®¡ç®—æˆæœ¬å’Œç¢³è¶³è¿¹ï¼Œå¹¶ä¸”å¯ä»¥è®©æ‚¨ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒ</p>
<p>ğŸ¤—Transformersæä¾›äº†å¯¹å„ç§ä»»åŠ¡çš„æ•°åƒä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„è®¿é—®ã€‚å½“æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥åœ¨ç‰¹å®šäºæ‚¨ä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚è¿™è¢«ç§°ä¸º<code>å¾®è°ƒ</code>ï¼Œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„è®­ç»ƒæŠ€æœ¯</p>
<blockquote>
<p>æ•°æ®å‡†å¤‡</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-comment"># 1. åŠ è½½æ•°æ®é›†</span>
dataset = load_dataset(<span class="hljs-string">"yelp_review_full"</span>)
dataset[<span class="hljs-string">"train"</span>][<span class="hljs-number">100</span>]
{<span class="hljs-string">'label'</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">'text'</span>: <span class="hljs-string">'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'</span>}
<span class="hljs-comment"># å¯ä»¥åˆ›å»ºä¸€ä¸ªè¾ƒå°çš„æ•°æ®é›†å­é›†ï¼Œç”¨äºå¾®è°ƒï¼Œä»¥å‡å°‘æ‰€éœ€çš„æ—¶é—´</span>
small_train_dataset = tokenized_datasets[<span class="hljs-string">"train"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(range(<span class="hljs-number">1000</span>))
small_eval_dataset = tokenized_datasets[<span class="hljs-string">"test"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(range(<span class="hljs-number">1000</span>))

<span class="hljs-comment"># 2. åˆ†è¯å™¨</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_function</span><span class="hljs-params">(examples)</span>:</span>
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">"text"</span>], padding=<span class="hljs-string">"max_length"</span>, truncation=<span class="hljs-keyword">True</span>)
tokenized_datasets = dataset.map(tokenize_function, batched=<span class="hljs-keyword">True</span>)
</code></pre>
<blockquote>
<p>Train with PyTorch Trainer</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> evaluate

<span class="hljs-comment"># 1. åŠ è½½æ¨¡å‹</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">5</span>)

<span class="hljs-comment"># 2. å®šä¹‰è®­ç»ƒå‚æ•°  åœ¨è®­ç»ƒå‚æ•°ä¸­æŒ‡å®ševaluation_strategyå‚æ•°ï¼Œä»¥åœ¨æ¯ä¸ªepochç»“æŸæ—¶æŠ¥å‘Šè¯„ä¼°æŒ‡æ ‡</span>
training_args = TrainingArguments(output_dir=<span class="hljs-string">"test_trainer"</span>, evaluation_strategy=<span class="hljs-string">"epoch"</span>)

<span class="hljs-comment"># 3. åŠ è½½è¯„ä¼°å™¨</span>
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)
<span class="hljs-comment"># åœ¨è®¡ç®—åº¦é‡æ ‡å‡†çš„æ—¶å€™è°ƒç”¨computeï¼Œä»¥è®¡ç®—æ‚¨çš„é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚åœ¨å°†é¢„æµ‹ç»“æœä¼ é€’ç»™computeä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºlogits</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_metrics</span><span class="hljs-params">(eval_pred)</span>:</span>
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=<span class="hljs-number">-1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)

<span class="hljs-comment"># 4. å®šä¹‰Trainer</span>
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

<span class="hljs-comment"># 5. å¼€å§‹è®­ç»ƒ</span>
trainer.train()
</code></pre>
<blockquote>
<p>Train in native PyTorch</p>
</blockquote>
<p>Trainerè´Ÿè´£è®­ç»ƒå¾ªç¯ï¼Œå¹¶å…è®¸æ‚¨é€šè¿‡ä¸€è¡Œä»£ç å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å¯¹äºå–œæ¬¢ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯çš„ç”¨æˆ·ï¼Œæ‚¨ä¹Ÿå¯ä»¥åœ¨åŸç”ŸPyTorchä¸­å¯¹ğŸ¤—Transformersæ¨¡å‹è¿›è¡Œå¾®è°ƒ</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> evaluate


<span class="hljs-comment"># 1. æ•°æ®é›†é¢„å¤„ç†</span>
tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">"text"</span>])
tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">"label"</span>, <span class="hljs-string">"labels"</span>)
tokenized_datasets.set_format(<span class="hljs-string">"torch"</span>)

<span class="hljs-comment"># 2. å®šä¹‰DataLoader</span>
train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="hljs-keyword">True</span>, batch_size=<span class="hljs-number">8</span>)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="hljs-number">8</span>)

<span class="hljs-comment"># 3. åŠ è½½æ¨¡å‹</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">5</span>)

<span class="hljs-comment"># 4. å®šä¹‰ä¼˜åŒ–å™¨</span>
optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)

<span class="hljs-comment"># 5. å®šä¹‰scheduler</span>
num_epochs = <span class="hljs-number">3</span>
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name=<span class="hljs-string">"linear"</span>, optimizer=optimizer, num_warmup_steps=<span class="hljs-number">0</span>, num_training_steps=num_training_steps
)

<span class="hljs-comment"># 6. ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡ </span>
device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">"cpu"</span>)
model.to(device)

<span class="hljs-comment"># 7. å¼€å§‹è®­ç»ƒ</span>
progress_bar = tqdm(range(num_training_steps))
model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

<span class="hljs-comment"># 8. éªŒè¯é›†è¯„ä¼°</span>
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)
model.eval()
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
    <span class="hljs-keyword">with</span> torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=<span class="hljs-number">-1</span>)
    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">"labels"</span>])

metric.compute()
</code></pre>
<h2 id="åˆ†å¸ƒå¼åŠ é€Ÿ">5.2 åˆ†å¸ƒå¼åŠ é€Ÿ</h2>
<blockquote>
<p><a href="https://huggingface.co/docs/accelerate/quicktour" target="_blank">huggingfaceçš„accelerateæ¨¡å—</a></p>
</blockquote>
<p>ğŸ¤—Accelerateæ˜¯Hugging Faceæä¾›çš„ç”¨äºç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒçš„åº“ã€‚å®ƒæ—¨åœ¨ä½¿åˆ†å¸ƒå¼è®­ç»ƒæ›´åŠ å®¹æ˜“å’Œé«˜æ•ˆï¼Œæ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬PyTorchå’ŒTensorFlow</p>
<p>Accelerateæä¾›äº†ä»¥ä¸‹åŠŸèƒ½ï¼š</p>
<ol>
<li><strong>æ•°æ®å¹¶è¡Œ</strong>ï¼šAccelerateä½¿ç”¨<code>accelerator.DataParallel</code>ç±»æ¥å®ç°æ•°æ®å¹¶è¡Œï¼Œå¯ä»¥åœ¨å¤šä¸ªGPUä¸ŠåŒæ—¶è®­ç»ƒæ¨¡å‹</li>
<li><strong>æ··åˆç²¾åº¦è®­ç»ƒ</strong>ï¼šAccelerateæ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œé€šè¿‡å°†æ¨¡å‹å‚æ•°å’Œæ¢¯åº¦è½¬æ¢ä¸ºåŠç²¾åº¦æµ®ç‚¹æ•°æ¥å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—é‡</li>
<li><strong>åˆ†å¸ƒå¼è®­ç»ƒ</strong>ï¼šAccelerateä½¿ç”¨<code>accelerator.DistributedDataParallel</code>ç±»æ¥å®ç°åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¯ä»¥åœ¨å¤šä¸ªæœºå™¨ä¸Šå¹¶è¡Œè®­ç»ƒæ¨¡å‹</li>
<li>è®­ç»ƒå¾ªç¯çš„è‡ªåŠ¨ç®¡ç†ï¼šAccelerateæä¾›äº†ä¸€ä¸ª<code>accelerator.Trainer</code>ç±»ï¼Œå®ƒå°è£…äº†è®­ç»ƒå¾ªç¯ï¼Œè‡ªåŠ¨å¤„ç†æ•°æ®åŠ è½½ã€å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€ä¼˜åŒ–å™¨æ›´æ–°ç­‰è¿‡ç¨‹</li>
</ol>
<p>ä½¿ç”¨Accelerateå¯ä»¥ç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒçš„é…ç½®å’Œç®¡ç†ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ›´è½»æ¾åœ°åˆ©ç”¨å¤šä¸ªGPUæˆ–å¤šå°æœºå™¨è¿›è¡Œè®­ç»ƒï¼Œå¹¶è·å¾—æ›´é«˜çš„è®­ç»ƒæ•ˆç‡</p>
<blockquote>
<p>å®‰è£…</p>
</blockquote>
<pre><code class="lang-cmd">pip install accelerate
</code></pre>
<blockquote>
<p>ç¤ºä¾‹ä»£ç ï¼Œä»¥ä¸‹ä»£ç åªåˆ—å‡ºæ”¹å˜çš„éƒ¨åˆ†ä»£ç </p>
</blockquote>
<p>åªéœ€è¦åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å››è¡Œé¢å¤–çš„ä»£ç å³å¯å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

<span class="hljs-comment"># 1. å®šä¹‰åŠ é€Ÿå™¨</span>
accelerator = Accelerator()

<span class="hljs-comment"># 2. dataloaderåŒ…è£…</span>
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

<span class="hljs-comment"># 3. åå‘ä¼ æ’­</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)
</code></pre>
<p>å®Œæ•´ä»£ç å¦‚ä¸‹</p>
<pre><code class="lang-python">+ <span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator
  <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)
  optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">3e-5</span>)

- device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">"cpu"</span>)
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = <span class="hljs-number">3</span>
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      <span class="hljs-string">"linear"</span>,
      optimizer=optimizer,
      num_warmup_steps=<span class="hljs-number">0</span>,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
      <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
-         batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(<span class="hljs-number">1</span>)
</code></pre>
<h2 id="ç¤ºä¾‹ä»£ç ">5.3 ç¤ºä¾‹ä»£ç </h2>
<p>åŒ…æ‹¬<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/sequence_classification" target="_blank">è‡ªç„¶è¯­è¨€å¤„ç†</a>ã€<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/audio_classification" target="_blank">è¯­éŸ³</a>ã€<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/image_classification" target="_blank">è®¡ç®—æœºè§†è§‰</a>å’Œ<a href="https://huggingface.co/docs/transformers/v4.30.0/en/tasks/image_captioning" target="_blank">å¤šæ¨¡æ€</a></p>
<h1 id="peftæ¨¡å—">6 PEFTæ¨¡å—</h1>
<blockquote>
<p><a href="https://huggingface.co/docs/peft/index#supported-models" target="_blank">huggingface PEFTæ¨¡å—</a></p>
</blockquote>
<p>ğŸ¤—<code>PEFT</code>ï¼Œå³<strong>Parameter-Efficient Fine-Tuning(å‚æ•°é«˜æ•ˆå¾®è°ƒ)</strong>ï¼Œæ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåœ°å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(PLM)é€‚åº”äºå„ç§ä¸‹æ¸¸åº”ç”¨çš„åº“ï¼Œè€Œæ— éœ€å¯¹æ‰€æœ‰æ¨¡å‹å‚æ•°è¿›è¡Œå¾®è°ƒ</p>
<p>PEFTæ–¹æ³•åªå¾®è°ƒå°‘é‡çš„(é¢å¤–çš„)æ¨¡å‹å‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ï¼Œå› ä¸ºå¯¹å¤§è§„æ¨¡PLMè¿›è¡Œå®Œæ•´å¾®è°ƒä»£ä»·è¿‡é«˜ã€‚æœ€è¿‘çš„æœ€å…ˆè¿›çš„PEFTæŠ€æœ¯è¾¾åˆ°äº†ä¸å®Œæ•´å¾®è°ƒç›¸å½“çš„æ€§èƒ½</p>
<p>PEFTä¸ğŸ¤—Accelerateåº“æ— ç¼é›†æˆï¼Œç”¨äºåˆ©ç”¨DeepSpeedå’ŒBig Model Inferenceè¿›è¡Œå¤§è§„æ¨¡æ¨¡å‹å¾®è°ƒ</p>
<blockquote>
<p>Supported methods (æˆªè‡³23-06-15)</p>
</blockquote>
<ol>
<li>LoRA: <a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li>
<li>Prefix Tuning: <a href="https://aclanthology.org/2021.acl-long.353/" target="_blank">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a>, <a href="https://arxiv.org/pdf/2110.07602.pdf" target="_blank">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a></li>
<li>P-Tuning: <a href="https://arxiv.org/pdf/2103.10385.pdf" target="_blank">GPT Understands, Too</a></li>
<li>Prompt Tuning: <a href="https://arxiv.org/pdf/2104.08691.pdf" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li>AdaLoRA: <a href="https://arxiv.org/abs/2303.10512" target="_blank">Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a></li>
<li><a href="https://github.com/ZrrSkywalker/LLaMA-Adapter" target="_blank">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a></li>
</ol>
<h1 id="å…¶ä»–æ¨¡å—">7 å…¶ä»–æ¨¡å—</h1>
<h2 id="autotrain">7.1 AutoTrain</h2>
<p><code>AutoTrain</code>æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–è®­ç»ƒçš„åº“ï¼Œæ—¨åœ¨ç®€åŒ–æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ã€‚å®ƒæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥å®šä¹‰å’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè‡ªåŠ¨å¤„ç†æ•°æ®åŠ è½½ã€æ‰¹å¤„ç†ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»†èŠ‚ã€‚é€šè¿‡ä½¿ç”¨AutoTrainï¼Œä½ å¯ä»¥æ›´å¿«é€Ÿåœ°æ­å»ºå’Œè®­ç»ƒæ¨¡å‹ï¼Œå‡å°‘æ ·æ¿ä»£ç çš„ç¼–å†™ï¼Œå¹¶ä¸”èƒ½å¤Ÿè½»æ¾åœ°è¿›è¡Œè¶…å‚æ•°æœç´¢å’Œæ¨¡å‹é€‰æ‹©</p>
<h2 id="gradio">7.2 Gradio</h2>
<p><code>Gradio</code>æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºäº¤äº’å¼ç•Œé¢çš„åº“ï¼Œä½¿ä½ èƒ½å¤Ÿè½»æ¾åœ°ä¸ºä½ çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ›å»ºWebåº”ç”¨ç¨‹åºã€‚Gradioæä¾›äº†ä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„APIï¼Œå¯ä»¥å°†æ¨¡å‹ä¸ç”¨æˆ·ç•Œé¢ç»„ä»¶(å¦‚æ–‡æœ¬æ¡†ã€æ»‘å—ã€å›¾åƒä¸Šä¼ å™¨ç­‰)ç›¸è¿æ¥ï¼Œä»è€Œå®ç°æ¨¡å‹çš„å®æ—¶æ¨ç†å’Œå¯è§†åŒ–ã€‚é€šè¿‡Gradioï¼Œä½ å¯ä»¥å¿«é€Ÿæ„å»ºä¸€ä¸ªäº¤äº’å¼çš„æ¼”ç¤ºæˆ–éƒ¨ç½²ä½ çš„æ¨¡å‹åˆ°Webä¸Šï¼Œæ— éœ€ç¼–å†™å¤æ‚çš„å‰ç«¯ä»£ç </p>
<h2 id="diffusers">7.3 Diffusers</h2>
<p><code>Diffusers</code>æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå›¾åƒã€éŸ³é¢‘ç”šè‡³åˆ†å­çš„ä¸‰ç»´ç»“æ„çš„æœ€æ–°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åº“ã€‚æ— è®ºæ‚¨æ˜¯å¯»æ‰¾ä¸€ä¸ªç®€å•çš„æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œè¿˜æ˜¯æƒ³è¦è®­ç»ƒè‡ªå·±çš„æ‰©æ•£æ¨¡å‹ï¼ŒğŸ¤—Diffuserséƒ½æ˜¯ä¸€ä¸ªæ”¯æŒä¸¤è€…çš„æ¨¡å—åŒ–å·¥å…·ç®±ã€‚æˆ‘ä»¬çš„åº“ç€é‡äºæ˜“ç”¨æ€§è€Œéæ€§èƒ½ï¼Œç®€æ´è€Œéå¤æ‚ï¼Œå¯å®šåˆ¶æ€§è€ŒéæŠ½è±¡æ€§ï¼Œè¯¥åº“ä¸»è¦åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªç»„ä»¶ï¼š</p>
<ol>
<li>æœ€æ–°çš„æ‰©æ•£æ¨ç†æµç¨‹ï¼Œåªéœ€å‡ è¡Œä»£ç å³å¯å®ç°</li>
<li>å¯äº’æ¢çš„å™ªå£°è°ƒåº¦å™¨ï¼Œç”¨äºåœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ä¹‹é—´å¹³è¡¡æƒè¡¡</li>
<li>å¯ç”¨ä½œæ„å»ºå—çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ä¸è°ƒåº¦å™¨ç»“åˆä½¿ç”¨ï¼Œåˆ›å»ºæ‚¨è‡ªå·±çš„ç«¯åˆ°ç«¯æ‰©æ•£ç³»ç»Ÿ</li>
</ol>
<h2 id="accelerate">7.4 Accelerate</h2>
<p>Hugging Faceçš„<code>Accelerate</code>æ˜¯ä¸€ä¸ªæ—¨åœ¨ç®€åŒ–å’ŒåŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹çš„åº“</p>
<p>å®ƒæä¾›äº†ä¸€ä¸ªé«˜çº§APIï¼ŒæŠ½è±¡äº†åˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦å’Œæ¢¯åº¦ç´¯ç§¯ç­‰å¤æ‚æ€§ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åœ°å……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æºçš„æ½œåŠ›</p>
<p>Accelerateå…¼å®¹PyTorchå’ŒTensorFlowï¼Œå¹¶æä¾›äº†ä¸€å¥—å·¥å…·å’Œå®ç”¨ç¨‹åºï¼Œä»¥å®ç°è·¨å¤šä¸ªGPUæˆ–å¤šå°æœºå™¨çš„é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒã€‚å®ƒåŒ…æ‹¬ä»¥ä¸‹åŠŸèƒ½ï¼š</p>
<ol>
<li><strong>åˆ†å¸ƒå¼è®­ç»ƒ</strong>ï¼šAccelerateæä¾›äº†ç®€å•æ˜“ç”¨çš„æ¥å£ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå°†è®­ç»ƒè¿‡ç¨‹åˆ†å¸ƒåˆ°å¤šä¸ªGPUæˆ–å¤šå°æœºå™¨ä¸Šã€‚å®ƒæ”¯æŒå¸¸è§çš„åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œå¦‚æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œï¼Œå¹¶è‡ªåŠ¨å¤„ç†æ•°æ®çš„åˆ†å‘å’Œæ¢¯åº¦çš„èšåˆï¼Œä½¿ç”¨æˆ·æ— éœ€æ‰‹åŠ¨ç¼–å†™å¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒä»£ç </li>
<li><strong>æ··åˆç²¾åº¦è®­ç»ƒ</strong>ï¼šAccelerateæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œé€šè¿‡åŒæ—¶ä½¿ç”¨æµ®ç‚¹16ä½å’Œæµ®ç‚¹32ä½ç²¾åº¦æ¥åŠ å¿«æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦ã€‚å®ƒè‡ªåŠ¨å¤„ç†æ•°æ®ç±»å‹è½¬æ¢å’Œæ¢¯åº¦ç¼©æ”¾ï¼Œç”¨æˆ·åªéœ€ç®€å•åœ°æŒ‡å®šä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå³å¯</li>
<li><strong>æ¢¯åº¦ç´¯ç§¯</strong>ï¼šAccelerateæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼Œè¿™åœ¨GPUæ˜¾å­˜æœ‰é™çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ç”¨ã€‚æ¢¯åº¦ç´¯ç§¯å…è®¸åœ¨å¤šä¸ªå°æ‰¹æ¬¡ä¸Šç´¯ç§¯æ¢¯åº¦ï¼Œç„¶åè¿›è¡Œä¸€æ¬¡å¤§æ‰¹æ¬¡çš„å‚æ•°æ›´æ–°ï¼Œä»è€Œå‡å°‘æ˜¾å­˜å ç”¨å¹¶æé«˜è®­ç»ƒæ•ˆç‡</li>
<li><strong>è‡ªåŠ¨è°ƒèŠ‚æ‰¹æ¬¡å¤§å°</strong>ï¼šAccelerateå¯ä»¥è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”å¯ç”¨çš„GPUå†…å­˜ã€‚å®ƒä¼šåŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼Œä»¥è¾¾åˆ°æœ€ä½³çš„GPUåˆ©ç”¨ç‡å’Œè®­ç»ƒæ€§èƒ½</li>
</ol>
<p>æ€»ä¹‹ï¼ŒHugging Faceçš„Accelerateæ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„åº“ï¼Œæ—¨åœ¨ç®€åŒ–å’ŒåŠ é€Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚å®ƒæä¾›äº†é«˜çº§APIå’Œä¸€ç³»åˆ—å·¥å…·ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åœ°å®ç°åˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦è®­ç»ƒå’Œæ¢¯åº¦ç´¯ç§¯ç­‰é«˜æ•ˆè®­ç»ƒç­–ç•¥</p>
<p></p><footer class="page-footer"><span class="copyright">Copyright Â© narutohyc.com 2021 all right reservedï¼Œpowered by Gitbook</span><span class="footer-modification">è¯¥æ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š
2023-08-19 10:32:45
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: 'æ¬¢è¿ç•™ä¸‹è¯„è®ºäº¤æµ~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: dl_in_vision_field.md" class="navigation navigation-prev" href="dl_in_vision_field.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.md" class="navigation navigation-next" href="nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":57912,"date":"2023/06/10 18:36:10","cover":"https://pic.hycbook.com/i/hexo/post_cover/è•¾å§†0.webp","title":"huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.md","tags":["huggingface","transformers"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/è•¾å§†0.webp","mathjax":true,"categories":["deep_learning"],"description":"huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹","level":"1.5","depth":1,"next":{"title":"nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.md","level":"1.6","depth":1,"path":"chapters/nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.md","ref":"chapters/nlpå…³é”®è¯å’Œæ‘˜è¦æå–æŠ€æœ¯æ•´ç†.md","articles":[]},"previous":{"title":"dl_in_vision_field.md","level":"1.4","depth":1,"path":"chapters/dl_in_vision_field.md","ref":"chapters/dl_in_vision_field.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright Â© narutohyc.com 2021","modify_label":"è¯¥æ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipayæ‰“èµ","button":"æ¬¢è¿æ‰“èµ","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechatæ‰“èµ"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"æ·±åº¦å­¦ä¹ çŸ¥è¯†é©¿ç«™","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"æ¬¢è¿ç•™ä¸‹è¯„è®ºäº¤æµ~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"æ·±åº¦å­¦ä¹ ç›¸å…³å­¦ä¹ è®°å½•","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"ä¹¦ç±ä¸»é¡µ":"https://study.hycbook.com"}},"gitbook":"*","description":"è®°å½• æ·±åº¦å­¦ä¹  çš„å­¦ä¹ å’Œä¸€äº›æŠ€å·§çš„ä½¿ç”¨"},"file":{"path":"chapters/huggingfaceåŸºæœ¬ä½¿ç”¨æ•™ç¨‹.md","mtime":"2023-08-19T10:32:45.759Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-08-19T10:33:59.132Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
