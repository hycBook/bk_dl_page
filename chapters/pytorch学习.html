<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>pytorch学习.md · 深度学习相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="pytorch学习记录" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="transformer.html" rel="next"/>
<link href="nlp关键词和摘要提取技术整理.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">书籍主页</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="LLM模型微调系列.html" id="chapter_id_1">
<a href="LLM模型微调系列.html">
<b>1.2.</b>
                    
                    LLM模型微调系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="LLM模型部署调试推理.html" id="chapter_id_2">
<a href="LLM模型部署调试推理.html">
<b>1.3.</b>
                    
                    LLM模型部署调试推理.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="dl_in_vision_field.html" id="chapter_id_3">
<a href="dl_in_vision_field.html">
<b>1.4.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="huggingface基本使用教程.html" id="chapter_id_4">
<a href="huggingface基本使用教程.html">
<b>1.5.</b>
                    
                    huggingface基本使用教程.md
            
                </a>
</li>
<li class="chapter" data-level="1.6" data-path="nlp关键词和摘要提取技术整理.html" id="chapter_id_5">
<a href="nlp关键词和摘要提取技术整理.html">
<b>1.6.</b>
                    
                    nlp关键词和摘要提取技术整理.md
            
                </a>
</li>
<li class="chapter active" data-level="1.7" data-path="pytorch学习.html" id="chapter_id_6">
<a href="pytorch学习.html">
<b>1.7.</b>
                    
                    pytorch学习.md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="transformer.html" id="chapter_id_7">
<a href="transformer.html">
<b>1.8.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="图像分割算法.html" id="chapter_id_8">
<a href="图像分割算法.html">
<b>1.9.</b>
                    
                    图像分割算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="图像分类算法.html" id="chapter_id_9">
<a href="图像分类算法.html">
<b>1.10.</b>
                    
                    图像分类算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="图神经网络.html" id="chapter_id_10">
<a href="图神经网络.html">
<b>1.11.</b>
                    
                    图神经网络.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="数据标注工具.html" id="chapter_id_11">
<a href="数据标注工具.html">
<b>1.12.</b>
                    
                    数据标注工具.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="深度学习核心之优化器.html" id="chapter_id_12">
<a href="深度学习核心之优化器.html">
<b>1.13.</b>
                    
                    深度学习核心之优化器.md
            
                </a>
</li>
<li class="chapter" data-level="1.14" data-path="深度学习核心之损失函数.html" id="chapter_id_13">
<a href="深度学习核心之损失函数.html">
<b>1.14.</b>
                    
                    深度学习核心之损失函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="深度学习核心之激活函数.html" id="chapter_id_14">
<a href="深度学习核心之激活函数.html">
<b>1.15.</b>
                    
                    深度学习核心之激活函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="深度学习核心基础知识点.html" id="chapter_id_15">
<a href="深度学习核心基础知识点.html">
<b>1.16.</b>
                    
                    深度学习核心基础知识点.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="深度学习模型压缩技术.html" id="chapter_id_16">
<a href="深度学习模型压缩技术.html">
<b>1.17.</b>
                    
                    深度学习模型压缩技术.md
            
                </a>
</li>
<li class="chapter" data-level="1.18" data-path="目标检测与跟踪算法.html" id="chapter_id_17">
<a href="目标检测与跟踪算法.html">
<b>1.18.</b>
                    
                    目标检测与跟踪算法.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">pytorch学习.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#torch">1 torch</a></li><ul><li><span class="title-icon"></span><a href="#基本操作">1.1 基本操作</a></li><li><span class="title-icon"></span><a href="#创建操作-creation-ops">1.2 创建操作 Creation Ops</a></li><li><span class="title-icon"></span><a href="#索引切片连接换位">1.3 索引|切片|连接|换位</a></li><li><span class="title-icon"></span><a href="#随机抽样">1.4 随机抽样</a></li><li><span class="title-icon"></span><a href="#序列化serialization">1.5 序列化Serialization</a></li><li><span class="title-icon"></span><a href="#并行化-parallelism">1.6 并行化 Parallelism</a></li><li><span class="title-icon"></span><a href="#数学操作math-operations">1.7 数学操作Math operations</a></li><li><span class="title-icon"></span><a href="#reduction-ops">1.8 Reduction Ops</a></li><li><span class="title-icon"></span><a href="#比较操作-comparison-ops">1.9 比较操作 Comparison Ops</a></li><li><span class="title-icon"></span><a href="#其它操作-other-operations">1.10 其它操作 Other Operations</a></li><li><span class="title-icon"></span><a href="#blas-and-lapack-operations">1.11 BLAS and LAPACK Operations</a></li></ul><li><span class="title-icon"></span><a href="#tensor">2 Tensor</a></li><ul><li><span class="title-icon"></span><a href="#storage">2.1 storage</a></li></ul><li><span class="title-icon"></span><a href="#实例">3 实例</a></li><ul><li><span class="title-icon"></span><a href="#pytorch加载数据">3.1 Pytorch加载数据</a></li><li><span class="title-icon"></span><a href="#tensorboard">3.2 Tensorboard</a></li><li><span class="title-icon"></span><a href="#transforms">3.3 Transforms</a></li><li><span class="title-icon"></span><a href="#torchvision数据集">3.4 torchvision数据集</a></li><li><span class="title-icon"></span><a href="#损失函数">3.5 损失函数</a></li><li><span class="title-icon"></span><a href="#优化器">3.6 优化器</a></li><li><span class="title-icon"></span><a href="#网络模型使用及修改">3.7 网络模型使用及修改</a></li><li><span class="title-icon"></span><a href="#网络模型保存与读取">3.8 网络模型保存与读取</a></li><li><span class="title-icon"></span><a href="#固定模型参数">3.9 固定模型参数</a></li><li><span class="title-icon"></span><a href="#训练流程">3.10 训练流程</a></li></ul></ul></div><a href="#torch" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><hr/>
<blockquote>
<p><a href="https://www.pytorchtutorial.com/docs/package_references/torch/" target="_blank">pytorch中文文档</a></p>
</blockquote>
<h1 id="torch">1 torch</h1>
<h2 id="基本操作">1.1 基本操作</h2>
<blockquote>
<p><strong>torch.is_tensor</strong>: 如果obj是一个pytorch张量，则返回True</p>
</blockquote>
<pre><code class="lang-python">x=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
torch.is_tensor(x)

Out[<span class="hljs-number">0</span>]: <span class="hljs-keyword">True</span>
</code></pre>
<blockquote>
<p><strong>torch.is_storage</strong>: 如何obj是一个pytorch storage对象，则返回True</p>
</blockquote>
<pre><code class="lang-python">x=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
torch.is_storage(x)

Out[<span class="hljs-number">0</span>]: <span class="hljs-keyword">False</span>
</code></pre>
<blockquote>
<p><strong>torch.numel</strong>: 返回<code>input</code> 张量中的元素个数</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)
torch.numel(a)
Out[<span class="hljs-number">0</span>]: <span class="hljs-number">120</span>

a = torch.zeros(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>)
torch.numel(a)
Out[<span class="hljs-number">1</span>]: <span class="hljs-number">16</span>
</code></pre>
<blockquote>
<p><strong>torch.set_printoptions</strong>: 设置打印选项</p>
<p>参数:</p>
<ul>
<li>precision – 浮点数输出的精度位数 (默认为8 )</li>
<li>threshold – 阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）</li>
<li>edgeitems – 汇总显示中，每维（轴）两端显示的项数（默认值为3）</li>
<li>linewidth – 用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.</li>
<li>profile – pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)</li>
</ul>
</blockquote>
<h2 id="创建操作-creation-ops">1.2 创建操作 Creation Ops</h2>
<blockquote>
<p><strong>torch.eye</strong>: 返回一个2维张量，对角线位置全1，其它位置全0</p>
<p>参数:</p>
<ul>
<li>n (<a href="https://docs.python.org/2/library/functions.html#int" target="_blank">int</a> ) – 行数</li>
<li>m (<a href="https://docs.python.org/2/library/functions.html#int" target="_blank">int</a>, <em>optional</em>) – 列数.如果为None,则默认为<em>n</em></li>
<li>out (<a href="http://pytorch.org/docs/tensors.html#torch.Tensor" target="_blank"><em>Tensor</em></a>, <em>optinal</em>) - Output tensor</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.eye(<span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]:  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>  <span class="hljs-number">0</span>
 <span class="hljs-number">0</span>  <span class="hljs-number">1</span>  <span class="hljs-number">0</span>
 <span class="hljs-number">0</span>  <span class="hljs-number">0</span>  <span class="hljs-number">1</span>

[torch.FloatTensor of size <span class="hljs-number">3</span>x3]
</code></pre>
<blockquote>
<p><strong>from_numpy</strong>: 将<code>numpy.ndarray</code> 转换为pytorch的 <code>Tensor</code>。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小</p>
</blockquote>
<pre><code class="lang-python">a = numpy.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
t = torch.from_numpy(a)
Out[<span class="hljs-number">0</span>]: torch.LongTensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])

t[<span class="hljs-number">0</span>] = <span class="hljs-number">-1</span>
a
Out[<span class="hljs-number">1</span>]: array([<span class="hljs-number">-1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>])
</code></pre>
<blockquote>
<p><strong>torch.linspace</strong>: 返回一个1维张量，包含在区间<code>start</code> 和 <code>end</code> 上均匀间隔的<code>steps</code>个点。 输出1维张量的长度为<code>steps</code></p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的最终值</li>
<li>steps (int) – 在<code>start</code> 和 <code>end</code>间生成的样本数</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.linspace(<span class="hljs-number">3</span>, <span class="hljs-number">10</span>, steps=<span class="hljs-number">5</span>)

Out[<span class="hljs-number">0</span>]: 
  <span class="hljs-number">3.0000</span>
  <span class="hljs-number">4.7500</span>
  <span class="hljs-number">6.5000</span>
  <span class="hljs-number">8.2500</span>
 <span class="hljs-number">10.0000</span>
[torch.FloatTensor of size <span class="hljs-number">5</span>]
</code></pre>
<blockquote>
<p><strong>torch.logspace</strong>: </p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的最终值</li>
<li>steps (int) – 在<code>开始</code>和<code>结束</code>之间要采样的点数。默认值：100</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.logspace(start=<span class="hljs-number">0.1</span>, end=<span class="hljs-number">1.0</span>, steps=<span class="hljs-number">5</span>)

Out[<span class="hljs-number">0</span>]: 
  <span class="hljs-number">1.2589</span>
  <span class="hljs-number">2.1135</span>
  <span class="hljs-number">3.5481</span>
  <span class="hljs-number">5.9566</span>
 <span class="hljs-number">10.0000</span>
[torch.FloatTensor of size <span class="hljs-number">5</span>]
</code></pre>
<blockquote>
<p><strong>torch.ones</strong>: 返回一个全为1 的张量，形状由可变参数<code>sizes</code>定义</p>
</blockquote>
<pre><code class="lang-python">torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)

Out[<span class="hljs-number">0</span>]: 
 <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>
 <span class="hljs-number">1</span>  <span class="hljs-number">1</span>  <span class="hljs-number">1</span>
[torch.FloatTensor of size <span class="hljs-number">2</span>x3]
</code></pre>
<blockquote>
<p><strong>torch.rand</strong>: 返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数<code>sizes</code> 定义</p>
</blockquote>
<pre><code class="lang-python">torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)

Out[<span class="hljs-number">0</span>]: 
 <span class="hljs-number">0.5010</span>  <span class="hljs-number">0.5140</span>  <span class="hljs-number">0.0719</span>
 <span class="hljs-number">0.1435</span>  <span class="hljs-number">0.5636</span>  <span class="hljs-number">0.0538</span>
[torch.FloatTensor of size <span class="hljs-number">2</span>x3]
</code></pre>
<blockquote>
<p><strong>torch.randn</strong>: 返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数</p>
</blockquote>
<pre><code class="lang-python">torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)

Out[<span class="hljs-number">0</span>]: 
 <span class="hljs-number">1.4339</span>  <span class="hljs-number">0.3351</span> <span class="hljs-number">-1.0999</span>
 <span class="hljs-number">1.5458</span> <span class="hljs-number">-0.9643</span> <span class="hljs-number">-0.3558</span>
[torch.FloatTensor of size <span class="hljs-number">2</span>x3]
</code></pre>
<blockquote>
<p><strong>torch.randperm</strong>: 给定参数<code>n</code>，返回一个从<code>0</code> 到<code>n -1</code> 的随机整数排列</p>
</blockquote>
<pre><code class="lang-python">torch.randperm(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])

torch.randperm(<span class="hljs-number">6</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>])
</code></pre>
<blockquote>
<p><strong>torch.arange</strong>: torch.arange(start, end, step=1, out=None) → Tensor</p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的终止点</li>
<li>step (float) – 相邻点的间隔大小</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">0.5</span>)

Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.0000</span>, <span class="hljs-number">1.5000</span>, <span class="hljs-number">2.0000</span>])
</code></pre>
<blockquote>
<p><strong>torch.zeros</strong>: 返回一个全为标量 0 的张量</p>
</blockquote>
<pre><code class="lang-python">torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)

Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])
</code></pre>
<h2 id="索引切片连接换位">1.3 索引|切片|连接|换位</h2>
<blockquote>
<p><strong>torch.cat</strong>: 给定维度上对输入的张量序列<code>seq</code> 进行连接操作</p>
<p><code>torch.cat()</code>可以看做 <code>torch.split()</code> 和 <code>torch.chunk()</code>的反操作</p>
<p>参数:</p>
<ul>
<li>inputs (<em>sequence of Tensors</em>) – 可以是任意相同Tensor 类型的python 序列</li>
<li>dimension (<em>int</em>, <em>optional</em>) – 沿着此维连接张量序列</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)

torch.cat((x, x, x), <span class="hljs-number">0</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[ <span class="hljs-number">0.6030</span>, <span class="hljs-number">-0.0292</span>],
        [ <span class="hljs-number">0.6030</span>, <span class="hljs-number">-0.0292</span>],
        [ <span class="hljs-number">0.6030</span>, <span class="hljs-number">-0.0292</span>]])

torch.cat((x, x, x), <span class="hljs-number">1</span>)
Out[<span class="hljs-number">1</span>]: tensor([[ <span class="hljs-number">0.6030</span>, <span class="hljs-number">-0.0292</span>,  <span class="hljs-number">0.6030</span>, <span class="hljs-number">-0.0292</span>,  <span class="hljs-number">0.6030</span>, <span class="hljs-number">-0.0292</span>]])
</code></pre>
<blockquote>
<p><strong>torch.chunk</strong>: torch.chunk(tensor, chunks, dim=0)</p>
<p>在给定维度(轴)上将输入张量进行分块儿</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 待分块的输入张量</li>
<li>chunks (int) – 分块的个数</li>
<li>dim (int) – 沿着此维度进行分块</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.arange(<span class="hljs-number">11</span>).chunk(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: (tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]), tensor([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]), tensor([<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>]), tensor([ <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]))

torch.rand((<span class="hljs-number">3</span>,<span class="hljs-number">2</span>)).chunk(<span class="hljs-number">3</span>,dim=<span class="hljs-number">0</span>)
Out[<span class="hljs-number">1</span>]: 
(tensor([[<span class="hljs-number">0.4479</span>, <span class="hljs-number">0.8420</span>]]),
 tensor([[<span class="hljs-number">0.2951</span>, <span class="hljs-number">0.9858</span>]]),
 tensor([[<span class="hljs-number">0.2795</span>, <span class="hljs-number">0.7413</span>]]))
</code></pre>
<blockquote>
<p><strong>torch.gather</strong>: torch.gather(input, dim, index, out=None) → Tensor</p>
<p>沿给定轴<code>dim</code>，将输入索引张量<code>index</code>指定位置的值进行聚合</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 源张量</li>
<li>dim (int) – 索引的轴</li>
<li>index (LongTensor) – 聚合元素的下标</li>
<li>out (Tensor, optional) – 目标张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">t = torch.Tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
torch.gather(t, <span class="hljs-number">1</span>, torch.LongTensor([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]]))

Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>]])

t = torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])
torch.gather(t, <span class="hljs-number">1</span>, torch.LongTensor([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]]))
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>]])
</code></pre>
<blockquote>
<p><strong>torch.index_select</strong>: torch.index_select(input, dim, index, out=None) → Tensor</p>
<p>沿着指定维度对输入进行切片，取<code>index</code>中指定的相应项(<code>index</code>为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量<em>Tensor</em>有相同的维度(在指定轴上)</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 索引的轴</li>
<li>index (LongTensor) – 包含索引下标的一维张量</li>
<li>out (Tensor, optional) – 目标张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">-0.8335</span>,  <span class="hljs-number">1.2611</span>,  <span class="hljs-number">0.6569</span>, <span class="hljs-number">-0.1598</span>],
        [<span class="hljs-number">-0.1019</span>,  <span class="hljs-number">1.5010</span>, <span class="hljs-number">-1.4486</span>, <span class="hljs-number">-2.2269</span>],
        [<span class="hljs-number">-0.6087</span>, <span class="hljs-number">-0.6940</span>, <span class="hljs-number">-0.2556</span>, <span class="hljs-number">-1.1843</span>]])

indices = torch.LongTensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>])
torch.index_select(x, <span class="hljs-number">0</span>, indices)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">-0.8335</span>,  <span class="hljs-number">1.2611</span>,  <span class="hljs-number">0.6569</span>, <span class="hljs-number">-0.1598</span>],
        [<span class="hljs-number">-0.6087</span>, <span class="hljs-number">-0.6940</span>, <span class="hljs-number">-0.2556</span>, <span class="hljs-number">-1.1843</span>]])

torch.index_select(x, <span class="hljs-number">1</span>, indices)
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-number">-0.8335</span>,  <span class="hljs-number">0.6569</span>],
        [<span class="hljs-number">-0.1019</span>, <span class="hljs-number">-1.4486</span>],
        [<span class="hljs-number">-0.6087</span>, <span class="hljs-number">-0.2556</span>]])
</code></pre>
<blockquote>
<p><strong>torch.masked_select</strong>: torch.masked_select(input, mask, out=None) → Tensor</p>
<p>根据掩码张量<code>mask</code>中的二元值，取输入张量中的指定项( <code>mask</code>为一个 <em>ByteTensor</em>)，将取值返回到一个新的1D张量</p>
<p>张量 <code>mask</code>须跟<code>input</code>张量有相同数量的元素数目，但形状或维度不需要相同</p>
</blockquote>
<pre><code class="lang-python">x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">-0.8335</span>,  <span class="hljs-number">1.2611</span>,  <span class="hljs-number">0.6569</span>, <span class="hljs-number">-0.1598</span>],
        [<span class="hljs-number">-0.1019</span>,  <span class="hljs-number">1.5010</span>, <span class="hljs-number">-1.4486</span>, <span class="hljs-number">-2.2269</span>],
        [<span class="hljs-number">-0.6087</span>, <span class="hljs-number">-0.6940</span>, <span class="hljs-number">-0.2556</span>, <span class="hljs-number">-1.1843</span>]])

mask = x.ge(<span class="hljs-number">0.5</span>)
Out[<span class="hljs-number">1</span>]: 
tensor([[ <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>],
        [<span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>],
        [<span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>]])

torch.masked_select(x, mask)
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">1.2394</span>, <span class="hljs-number">0.5152</span>, <span class="hljs-number">0.5170</span>, <span class="hljs-number">1.8193</span>, <span class="hljs-number">0.7352</span>])
</code></pre>
<blockquote>
<p><strong>torch.nonzero</strong>: 用于<a href="https://so.csdn.net/so/search?q=输出数组&amp;spm=1001.2101.3001.7020" target="_blank">输出数组</a>的非零值的索引，即用来定位数组中非零的元素</p>
<p><code>as_tuple</code>：如果设为<code>False</code>，则返回一个二维张量，其中每一行都是非零值的索引</p>
</blockquote>
<pre><code class="lang-python">torch.nonzero(torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]))
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>],
        [<span class="hljs-number">2</span>],
        [<span class="hljs-number">4</span>]])

torch.nonzero(torch.Tensor([[<span class="hljs-number">0.6</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],
                             [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>],
                             [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1.2</span>, <span class="hljs-number">0.0</span>],
                             [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-0.4</span>]]))
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])
</code></pre>
<blockquote>
<p><strong>torch.split</strong>: torch.split(tensor, split_size, dim=0)</p>
<p>将输入张量分割成相等形状的chunks(如果可分)</p>
<p>如果沿指定维的张量形状大小不能被<code>split_size</code> 整分， 则最后一个分块会小于其它分块</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 待分割张量</li>
<li>split_size (int) – 单个分块的形状大小</li>
<li>dim (int) – 沿着此维进行分割</li>
</ul>
</blockquote>
<pre><code class="lang-python">a = torch.arange(<span class="hljs-number">10</span>).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">2</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>],
        [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>],
        [<span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])

torch.split(a, <span class="hljs-number">2</span>)
Out[<span class="hljs-number">1</span>]: 
(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],
         [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]),
 tensor([[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>],
         [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>]]),
 tensor([[<span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]))

torch.split(a, [<span class="hljs-number">1</span>,<span class="hljs-number">4</span>])
Out[<span class="hljs-number">2</span>]: 
(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]]),
 tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
         [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>],
         [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>],
         [<span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]))
</code></pre>
<blockquote>
<p><strong>torch.squeeze</strong>: torch.squeeze(input, dim=None, out=None)</p>
<p>将输入张量形状中的<code>1</code> 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)</p>
<p>当给定<code>dim</code>时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B)(A×1×B), <code>squeeze(input, 0)</code> 将会保持张量不变，只有用 <code>squeeze(input, 1)</code>，形状会变成 (A×B)(A×B)。</p>
<p>注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int, optional) – 如果给定，则<code>input</code>只会在给定维度挤压</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)
x.size()
Out[<span class="hljs-number">0</span>]: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])

y = torch.squeeze(x)
y.size()
Out[<span class="hljs-number">1</span>]: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])

y = torch.squeeze(x, <span class="hljs-number">0</span>)
y.size()
Out[<span class="hljs-number">2</span>]: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])

y = torch.squeeze(x, <span class="hljs-number">1</span>)
y.size()
Out[<span class="hljs-number">3</span>]: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
</code></pre>
<blockquote>
<p><strong>torch.unsqueeze</strong>: torch.unsqueeze(input, dim, out=None)</p>
<p>返回一个新的张量，对输入的制定位置插入维度 1</p>
<p>注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个</p>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
x.shape
Out[<span class="hljs-number">0</span>]: torch.Size([<span class="hljs-number">4</span>])

torch.unsqueeze(x, <span class="hljs-number">0</span>).shape
Out[<span class="hljs-number">1</span>]: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])

torch.unsqueeze(x, <span class="hljs-number">1</span>).shape
Out[<span class="hljs-number">2</span>]: torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">1</span>])
</code></pre>
<blockquote>
<p><strong>torch.stack</strong>: torch.stack(sequence, dim=0)</p>
<p>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状</p>
</blockquote>
<pre><code class="lang-python">t1 = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
                   [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
                   [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])
t2 = torch.tensor([[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>],
                   [<span class="hljs-number">40</span>, <span class="hljs-number">50</span>, <span class="hljs-number">60</span>],
                   [<span class="hljs-number">70</span>, <span class="hljs-number">80</span>, <span class="hljs-number">90</span>]])
t1.shape, t2.shape
Out[<span class="hljs-number">0</span>]: (torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]), torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]))

torch.stack((t1, t2), dim=<span class="hljs-number">0</span>).shape
Out[<span class="hljs-number">1</span>]: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])

torch.stack((t1, t2), dim=<span class="hljs-number">1</span>).shape
Out[<span class="hljs-number">2</span>]: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])

torch.stack((t1, t2), dim=<span class="hljs-number">2</span>).shape
Out[<span class="hljs-number">3</span>]: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])
</code></pre>
<blockquote>
<p><strong>torch.transpose</strong>: torch.transpose(input, dim0, dim1, out=None) → Tensor</p>
<p>返回输入矩阵<code>input</code>的转置。交换维度<code>dim0</code>和<code>dim1</code>。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim0 (int) – 转置的第一维</li>
<li>dim1 (int) – 转置的第二维</li>
</ul>
<p><strong>torch.t</strong>: torch.t(input, out=None) → Tensor</p>
<p>输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数<code>transpose(input, 0, 1)</code>的简写</p>
</blockquote>
<pre><code class="lang-python">x = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
x.shape
Out[<span class="hljs-number">0</span>]: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])

torch.transpose(x, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>).shape
Out[<span class="hljs-number">1</span>]: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])

torch.t(x).shape
Out[<span class="hljs-number">2</span>]: torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])
</code></pre>
<blockquote>
<p><strong>torch.unbind</strong>: torch.unbind(tensor, dim=0)[source]</p>
<p>移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 输入张量</li>
<li>dim (int) – 删除的维度</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.unbind(torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
                           [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
                           [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]))
Out[<span class="hljs-number">0</span>]: (tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]), tensor([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]), tensor([<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]))

torch.unbind(torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
                           [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
                           [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]), dim=<span class="hljs-number">0</span>)
Out[<span class="hljs-number">1</span>]: (tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]), tensor([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]), tensor([<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]))

torch.unbind(torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
                           [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
                           [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]]), dim=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">2</span>]: (tensor([<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>]), tensor([<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>]), tensor([<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]))
</code></pre>
<h2 id="随机抽样">1.4 随机抽样</h2>
<blockquote>
<p><strong>torch.seed</strong>: 设置torch cpu随机数种子</p>
<p><strong>torch.manual_seed</strong>: 设置torch cpu随机数种子，torch.manual_seed(seed)</p>
<p>设定生成随机数的种子，并返回一个torch._C.Generator对象</p>
<p><strong>torch.cuda.manual_seed</strong>: 设置torch cuda随机数种子</p>
</blockquote>
<pre><code class="lang-python">torch.seed()
Out[<span class="hljs-number">0</span>]: <span class="hljs-number">348176808892500</span>
torch.seed()
Out[<span class="hljs-number">1</span>]: <span class="hljs-number">348177652492500</span>

torch.manual_seed(<span class="hljs-number">0</span>)
Out[<span class="hljs-number">2</span>]: &lt;torch._C.Generator at <span class="hljs-number">0x138bdf41a90</span>&gt;

torch.cuda.manual_seed(<span class="hljs-number">0</span>)
</code></pre>
<blockquote>
<p><strong>torch.bernoulli</strong>: torch.bernoulli(input, out=None) → Tensor</p>
<p>从伯努利分布中提取二进制随机数（0或1），输入张量应为包含用于绘制二进制随机数的概率的张量。因此，输入中的所有值都必须在以下范围内(0,1)</p>
<p><strong>torch.poisson</strong>: 泊松分布用于计算一个事件在平均价值率(时间)的一定时间内发生的可能性。泊松分布是一个离散的概率分布</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>).uniform_(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">0.0044</span>, <span class="hljs-number">0.7257</span>, <span class="hljs-number">0.2599</span>],
        [<span class="hljs-number">0.1663</span>, <span class="hljs-number">0.2119</span>, <span class="hljs-number">0.7875</span>],
        [<span class="hljs-number">0.7648</span>, <span class="hljs-number">0.8838</span>, <span class="hljs-number">0.6814</span>]])
torch.bernoulli(a)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])
a = torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
torch.bernoulli(a)
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])
a = torch.zeros(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
torch.bernoulli(a)
Out[<span class="hljs-number">3</span>]: 
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])


a = torch.rand(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>) * <span class="hljs-number">5</span> 
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">0.2542</span>, <span class="hljs-number">1.3148</span>, <span class="hljs-number">4.2023</span>, <span class="hljs-number">2.4838</span>],
        [<span class="hljs-number">1.2574</span>, <span class="hljs-number">0.5842</span>, <span class="hljs-number">0.1604</span>, <span class="hljs-number">0.3900</span>],
        [<span class="hljs-number">1.9929</span>, <span class="hljs-number">3.8710</span>, <span class="hljs-number">3.8516</span>, <span class="hljs-number">0.0889</span>],
        [<span class="hljs-number">4.0595</span>, <span class="hljs-number">0.5437</span>, <span class="hljs-number">1.9715</span>, <span class="hljs-number">1.4863</span>]])
torch.poisson(a)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">2.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>]])
</code></pre>
<blockquote>
<p><strong>torch.multinomial</strong>: torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor</p>
<p>对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 包含概率值的张量</li>
<li>num_samples (int) – 抽取的样本数</li>
<li>replacement (bool, optional) – 布尔值，决定是否能重复抽取</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">weights = torch.Tensor([<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>])

torch.multinomial(weights, <span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>])

<span class="hljs-comment"># replacement=True时 概率为0的没机会被取到</span>
torch.multinomial(weights, <span class="hljs-number">4</span>, replacement=<span class="hljs-keyword">True</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
</code></pre>
<blockquote>
<p><strong>torch.normal</strong>: torch.normal(means, std, out=None)</p>
<p>返回一个张量，包含从给定参数<code>means</code>,<code>std</code>的离散正态分布中抽取随机数</p>
</blockquote>
<pre><code class="lang-python">torch.normal(mean=torch.arange(<span class="hljs-number">1.</span>, <span class="hljs-number">11.</span>), std=torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-0.1</span>))
Out[<span class="hljs-number">0</span>]: 
tensor([ <span class="hljs-number">0.9732</span>,  <span class="hljs-number">2.0833</span>,  <span class="hljs-number">2.5282</span>,  <span class="hljs-number">4.3588</span>,  <span class="hljs-number">5.4837</span>,  <span class="hljs-number">5.1150</span>,  <span class="hljs-number">7.0366</span>,  <span class="hljs-number">7.9774</span>,
         <span class="hljs-number">9.1679</span>, <span class="hljs-number">10.0248</span>])

torch.normal(mean=<span class="hljs-number">0.5</span>, std=torch.arange(<span class="hljs-number">1.</span>, <span class="hljs-number">6.</span>))
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">0.7067</span>,  <span class="hljs-number">2.4856</span>, <span class="hljs-number">-2.1957</span>, <span class="hljs-number">-4.3114</span>, <span class="hljs-number">16.2506</span>])

torch.normal(mean=torch.arange(<span class="hljs-number">1.</span>, <span class="hljs-number">6.</span>))
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">0.7835</span>, <span class="hljs-number">4.6096</span>, <span class="hljs-number">2.7244</span>, <span class="hljs-number">5.2810</span>, <span class="hljs-number">4.8413</span>])
</code></pre>
<h2 id="序列化serialization">1.5 序列化Serialization</h2>
<blockquote>
<p><strong>torch.save</strong>: 保存一个对象到一个硬盘文件上 参考: <a href="http://pytorch.org/docs/notes/serialization.html#recommend-saving-models" target="_blank">Recommended approach for saving a model</a> </p>
<p>torch.save(<em>obj</em>, <em>f</em>, <em>pickle_module=pickle</em>, <em>pickle_protocol=DEFAULT_PROTOCOL</em>, <em>_use_new_zipfile_serialization=True</em>)</p>
<p>参数：</p>
<ul>
<li>obj – 保存对象</li>
<li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li>
<li>pickle_module – 用于pickling元数据和对象的模块</li>
<li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])

<span class="hljs-comment"># Save to file</span>
torch.save(x, <span class="hljs-string">'tensor.pt'</span>)

<span class="hljs-comment"># Save to io.BytesIO buffer</span>
buffer = io.BytesIO()
torch.save(x, buffer)
</code></pre>
<blockquote>
<p><strong>torch.load</strong>: 从磁盘文件中读取一个通过<code>torch.save()</code>保存的对象</p>
<p>torch.load(<em>f</em>, <em>map_location=None</em>, <em>pickle_module=pickle</em>, <strong><em>, </em>weights_only=False<em>, </em></strong>pickle_load_args*)</p>
</blockquote>
<pre><code class="lang-python">torch.load(<span class="hljs-string">'tensors.pt'</span>, encoding=<span class="hljs-string">'ascii'</span>)
torch.load(<span class="hljs-string">'tensors.pt'</span>, map_location=torch.device(<span class="hljs-string">'cpu'</span>))
torch.load(<span class="hljs-string">'tensors.pt'</span>, map_location={<span class="hljs-string">'cuda:1'</span>:<span class="hljs-string">'cuda:0'</span>})

<span class="hljs-comment"># Load from io.BytesIO buffer</span>
<span class="hljs-keyword">with</span> open(<span class="hljs-string">'tensor.pt'</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
    buffer = io.BytesIO(f.read())
torch.load(buffer)
</code></pre>
<h2 id="并行化-parallelism">1.6 并行化 Parallelism</h2>
<blockquote>
<p><strong>torch.get_num_threads</strong>: 获得用于并行化CPU操作的OpenMP线程数</p>
<p><strong>torch.set_num_threads</strong>: 设定用于并行化CPU操作的OpenMP线程数</p>
</blockquote>
<h2 id="数学操作math-operations">1.7 数学操作Math operations</h2>
<blockquote>
<p><strong>torch.abs</strong>: 计算输入张量的每个元素绝对值</p>
</blockquote>
<pre><code class="lang-python">torch.abs(torch.FloatTensor([<span class="hljs-number">-1</span>, <span class="hljs-number">-2</span>, <span class="hljs-number">3</span>]))
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>])
</code></pre>
<blockquote>
<p><strong>torch.acos</strong>: torch.acos(input, out=None) → Tensor</p>
<p>返回一个新张量，包含输入张量每个元素的反余弦</p>
</blockquote>
<pre><code class="lang-python">torch.acos(torch.FloatTensor([<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">3.1416</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">1.5708</span>])
</code></pre>
<blockquote>
<p><strong>torch.add</strong>: torch.add(input, value, out=None)</p>
<p>对输入张量<code>input</code>逐元素加上标量值<code>value</code>，并返回结果到一个新的张量</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.3510</span>, <span class="hljs-number">-0.2226</span>, <span class="hljs-number">-0.7971</span>, <span class="hljs-number">-0.2564</span>])
torch.add(a, <span class="hljs-number">20</span>)

Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">20.3510</span>, <span class="hljs-number">19.7774</span>, <span class="hljs-number">19.2029</span>, <span class="hljs-number">19.7436</span>])
</code></pre>
<blockquote>
<p><strong>torch.addcdiv</strong>: torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor</p>
<p>用<code>tensor2</code>对<code>tensor1</code>逐元素相除，然后乘以标量值<code>value</code> 并加到<code>tensor</code></p>
<p>张量的形状不需要匹配，但元素数量必须一致</p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为被除数(分子)</li>
<li>tensor2 (Tensor) –张量，作为除数(分母)</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
</blockquote>
<p><script type="math/tex; mode=display">
out _{i}=\operatorname{input}_{i}+ value \times \frac{\text { tensor } 1_{i}}{\text { tensor 2}_{i}}
</script></p>
<pre><code class="lang-python">t = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)
t1 = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>)
t2 = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)
t, t1, t2
Out[<span class="hljs-number">0</span>]: 
(tensor([[<span class="hljs-number">-1.2863</span>,  <span class="hljs-number">1.1267</span>, <span class="hljs-number">-1.7120</span>]]),
 tensor([[<span class="hljs-number">-0.4294</span>],
         [<span class="hljs-number">-0.5328</span>],
         [<span class="hljs-number">-0.5373</span>]]),
 tensor([[<span class="hljs-number">-0.0876</span>,  <span class="hljs-number">0.4398</span>,  <span class="hljs-number">1.3583</span>]]))

torch.addcdiv(t, t1, t2, value=<span class="hljs-number">0.1</span>)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">-0.7958</span>,  <span class="hljs-number">1.0291</span>, <span class="hljs-number">-1.7436</span>],
        [<span class="hljs-number">-0.6778</span>,  <span class="hljs-number">1.0056</span>, <span class="hljs-number">-1.7512</span>],
        [<span class="hljs-number">-0.6727</span>,  <span class="hljs-number">1.0046</span>, <span class="hljs-number">-1.7515</span>]])
</code></pre>
<blockquote>
<p><strong>torch.addcmul</strong>: torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor</p>
<p>用<code>tensor2</code>对<code>tensor1</code>逐元素相乘，并对结果乘以标量值<code>value</code>然后加到<code>tensor</code></p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为乘子1</li>
<li>tensor2 (Tensor) –张量，作为乘子2</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
</blockquote>
<p><script type="math/tex; mode=display">
out _{i}= input _{i}+ value \times tensor 1_{i} \times tensor 2_{i}
</script></p>
<pre><code class="lang-python">t = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)
t1 = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>)
t2 = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)
t, t1, t2
Out[<span class="hljs-number">0</span>]: 
(tensor([[<span class="hljs-number">-1.2863</span>,  <span class="hljs-number">1.1267</span>, <span class="hljs-number">-1.7120</span>]]),
 tensor([[<span class="hljs-number">-0.4294</span>],
         [<span class="hljs-number">-0.5328</span>],
         [<span class="hljs-number">-0.5373</span>]]),
 tensor([[<span class="hljs-number">-0.0876</span>,  <span class="hljs-number">0.4398</span>,  <span class="hljs-number">1.3583</span>]]))

torch.addcmul(t, t1, t2, value=<span class="hljs-number">0.1</span>)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">-1.2825</span>,  <span class="hljs-number">1.1078</span>, <span class="hljs-number">-1.7703</span>],
        [<span class="hljs-number">-1.2816</span>,  <span class="hljs-number">1.1033</span>, <span class="hljs-number">-1.7844</span>],
        [<span class="hljs-number">-1.2816</span>,  <span class="hljs-number">1.1031</span>, <span class="hljs-number">-1.7850</span>]])
</code></pre>
<blockquote>
<p><strong>torch.asin</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的反正弦函数</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.2583</span>, <span class="hljs-number">-0.5285</span>,  <span class="hljs-number">0.8979</span>,  <span class="hljs-number">1.0104</span>])

torch.asin(a)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">0.2613</span>, <span class="hljs-number">-0.5569</span>,  <span class="hljs-number">1.1149</span>,     nan])
</code></pre>
<blockquote>
<p><strong>torch.atan</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的反正切函数</p>
<p><strong>torch.atan2</strong>: 返回一个新张量，包含两个输入张量<code>input1</code>和<code>input2</code>的反正切函数</p>
<p>torch.atan2(input1, input2, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.2583</span>, <span class="hljs-number">-0.5285</span>,  <span class="hljs-number">0.8979</span>,  <span class="hljs-number">1.0104</span>])
b = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">0.1100</span>, <span class="hljs-number">1.4311</span>, <span class="hljs-number">1.9536</span>, <span class="hljs-number">0.7652</span>])

torch.atan(a)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">0.2528</span>, <span class="hljs-number">-0.4862</span>,  <span class="hljs-number">0.7316</span>,  <span class="hljs-number">0.7906</span>])

torch.atan2(a, b)
Out[<span class="hljs-number">3</span>]: tensor([ <span class="hljs-number">1.1681</span>, <span class="hljs-number">-0.3538</span>,  <span class="hljs-number">0.4308</span>,  <span class="hljs-number">0.9226</span>])
</code></pre>
<blockquote>
<p><strong>torch.ceil</strong>: 对输入<code>input</code>张量每个元素向上取整, 即取不小于每个元素的最小整数</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.9105</span>, <span class="hljs-number">-0.7277</span>,  <span class="hljs-number">0.9516</span>, <span class="hljs-number">-0.1081</span>])

torch.ceil(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-0.</span>, <span class="hljs-number">-0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">-0.</span>])
</code></pre>
<blockquote>
<p><strong>torch.floor</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的floor，即不小于元素的最大整数</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.5661</span>, <span class="hljs-number">-0.9135</span>,  <span class="hljs-number">1.1018</span>, <span class="hljs-number">-0.2633</span>])

torch.floor(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-1.</span>, <span class="hljs-number">-1.</span>,  <span class="hljs-number">1.</span>, <span class="hljs-number">-1.</span>])
</code></pre>
<blockquote>
<p><strong>torch.fmod</strong>: 计算逐元素余数， 保留正负号</p>
<p><strong>torch.remainder</strong>: 计算逐元素余数， 相当于python 中的 % 操作符</p>
</blockquote>
<pre><code class="lang-python">t = torch.tensor([<span class="hljs-number">10</span>, <span class="hljs-number">-22</span>, <span class="hljs-number">31</span>, <span class="hljs-number">-47</span>])

torch.fmod(t, <span class="hljs-number">5</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0</span>, <span class="hljs-number">-2</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">-2</span>])

torch.remainder(t, <span class="hljs-number">5</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])

np.mod(np.array([<span class="hljs-number">10</span>, <span class="hljs-number">-22</span>, <span class="hljs-number">31</span>, <span class="hljs-number">-47</span>]), <span class="hljs-number">5</span>)
Out[<span class="hljs-number">2</span>]: array([<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>], dtype=int32)
</code></pre>
<blockquote>
<p><strong>torch.clamp</strong>: torch.clamp(input, min, max, out=None) → Tensor</p>
<p>将输入<code>input</code>张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量</p>
<pre><code class="lang-python">      | min, <span class="hljs-keyword">if</span> x_i &lt; min
y_i = | x_i, <span class="hljs-keyword">if</span> min &lt;= x_i &lt;= max
      | max, <span class="hljs-keyword">if</span> x_i &gt; max
</code></pre>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.9105</span>, <span class="hljs-number">-0.7277</span>,  <span class="hljs-number">0.9516</span>, <span class="hljs-number">-0.1081</span>])

torch.clamp(a, min=<span class="hljs-number">-0.5</span>, max=<span class="hljs-number">0.5</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-0.5000</span>, <span class="hljs-number">-0.5000</span>,  <span class="hljs-number">0.5000</span>, <span class="hljs-number">-0.1081</span>])
</code></pre>
<blockquote>
<p><strong>torch.cos</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的余弦</p>
<p><strong>torch.cosh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的双曲余弦</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.9105</span>, <span class="hljs-number">-0.7277</span>,  <span class="hljs-number">0.9516</span>, <span class="hljs-number">-0.1081</span>])

torch.cos(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">0.6133</span>, <span class="hljs-number">0.7467</span>, <span class="hljs-number">0.5804</span>, <span class="hljs-number">0.9942</span>])

torch.cosh(a)
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">1.4439</span>, <span class="hljs-number">1.2766</span>, <span class="hljs-number">1.4880</span>, <span class="hljs-number">1.0058</span>])
</code></pre>
<blockquote>
<p><strong>torch.div()</strong>: 将<code>input</code>逐元素除以标量值<code>value</code>，并返回结果到输出张量<code>out</code></p>
<p>torch.div(input, value, out=None)</p>
<p>两张量<code>input</code>和<code>other</code>逐元素相除，并将结果返回到输出</p>
<p>torch.div(<em>input</em>, <em>other</em>, <em>**, </em>rounding_mode=None<em>, </em>out=None*) → Tensor</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.9105</span>, <span class="hljs-number">-0.7277</span>,  <span class="hljs-number">0.9516</span>, <span class="hljs-number">-0.1081</span>])

torch.div(a, <span class="hljs-number">0.5</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-1.8210</span>, <span class="hljs-number">-1.4554</span>,  <span class="hljs-number">1.9032</span>, <span class="hljs-number">-0.2162</span>])


a = torch.tensor([[<span class="hljs-number">-0.3711</span>, <span class="hljs-number">-1.9353</span>, <span class="hljs-number">-0.4605</span>, <span class="hljs-number">-0.2917</span>],
                  [ <span class="hljs-number">0.1815</span>, <span class="hljs-number">-1.0111</span>,  <span class="hljs-number">0.9805</span>, <span class="hljs-number">-1.5923</span>]])
b = torch.tensor([ <span class="hljs-number">0.8032</span>,  <span class="hljs-number">0.2930</span>, <span class="hljs-number">-0.8113</span>, <span class="hljs-number">-0.2308</span>])

torch.div(a, b, rounding_mode=<span class="hljs-string">'trunc'</span>)
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-number">-0.</span>, <span class="hljs-number">-6.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>],
        [ <span class="hljs-number">0.</span>, <span class="hljs-number">-3.</span>, <span class="hljs-number">-1.</span>,  <span class="hljs-number">6.</span>]])

torch.div(a, b, rounding_mode=<span class="hljs-string">'floor'</span>)
Out[<span class="hljs-number">3</span>]: 
tensor([[<span class="hljs-number">-1.</span>, <span class="hljs-number">-7.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>],
        [ <span class="hljs-number">0.</span>, <span class="hljs-number">-4.</span>, <span class="hljs-number">-2.</span>,  <span class="hljs-number">6.</span>]])
</code></pre>
<blockquote>
<p><strong>torch.exp</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的指数。</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> math
torch.exp(torch.Tensor([<span class="hljs-number">0</span>, math.log(<span class="hljs-number">2</span>)]))

Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>])
</code></pre>
<blockquote>
<p><strong>torch.frac</strong>: 返回每个元素的分数部分</p>
</blockquote>
<pre><code class="lang-python">torch.frac(torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">-3.2</span>]))
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.5000</span>, <span class="hljs-number">-0.2000</span>])
</code></pre>
<blockquote>
<p><strong>torch.lerp</strong>: 对两个张量以<code>start</code>，<code>end</code>做线性插值， 将结果返回到输出张量</p>
<p>torch.lerp(<em>input</em>, <em>end</em>, <em>weight</em>, <em>**, </em>out=None*)</p>
<p>参数：</p>
<ul>
<li>start (Tensor) – 起始点张量</li>
<li>end (Tensor) – 终止点张量</li>
<li>weight (float) – 插值公式的weight</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<p><script type="math/tex; mode=display">
out_i=start_i+weight_i∗(end_i−start_i)
</script></p>
<pre><code class="lang-python">start = torch.arange(<span class="hljs-number">1.</span>, <span class="hljs-number">5.</span>)
end = torch.empty(<span class="hljs-number">4</span>).fill_(<span class="hljs-number">10</span>)
start, end
Out[<span class="hljs-number">0</span>]: (tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>]), tensor([<span class="hljs-number">10.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">10.</span>, <span class="hljs-number">10.</span>]))

torch.lerp(start, end, <span class="hljs-number">0.5</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">5.5000</span>, <span class="hljs-number">6.0000</span>, <span class="hljs-number">6.5000</span>, <span class="hljs-number">7.0000</span>])
torch.lerp(start, end, torch.full_like(start, <span class="hljs-number">0.5</span>))
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">5.5000</span>, <span class="hljs-number">6.0000</span>, <span class="hljs-number">6.5000</span>, <span class="hljs-number">7.0000</span>])
</code></pre>
<blockquote>
<p><strong>torch.log</strong>: 计算<code>input</code> 的自然对数</p>
<p><strong>torch.log1p</strong>: 计算<script type="math/tex; ">input + 1</script>的自然对数<script type="math/tex; ">y_i = log(x_i+1)</script>，对值比较小的输入，此函数比<code>torch.log()</code>更准确</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">5</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.3466</span>,  <span class="hljs-number">2.3803</span>, <span class="hljs-number">-0.0423</span>, <span class="hljs-number">-0.9744</span>,  <span class="hljs-number">0.4976</span>])

torch.log(a)
Out[<span class="hljs-number">1</span>]: tensor([    nan,  <span class="hljs-number">0.8672</span>,     nan,     nan, <span class="hljs-number">-0.6980</span>])

torch.log1p(a)
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">-0.4256</span>,  <span class="hljs-number">1.2180</span>, <span class="hljs-number">-0.0432</span>, <span class="hljs-number">-3.6633</span>,  <span class="hljs-number">0.4039</span>])
</code></pre>
<blockquote>
<p><strong>torch.mul</strong>: 用标量值<code>value</code>乘以输入<code>input</code>的每个元素，并返回一个新的结果张量</p>
<p>torch.mul(input, value, out=None)</p>
<p>两个张量<code>input</code>,<code>other</code>按元素进行相乘，并返回到输出张量</p>
<p>torch.mul(input, other, out=None)</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.0603</span>, <span class="hljs-number">-0.5258</span>, <span class="hljs-number">-0.3810</span>])
b = torch.randn(<span class="hljs-number">3</span>)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">1.2408</span>, <span class="hljs-number">-1.3506</span>,  <span class="hljs-number">0.9296</span>])

torch.mul(a, <span class="hljs-number">100</span>)
Out[<span class="hljs-number">2</span>]: tensor([  <span class="hljs-number">6.0299</span>, <span class="hljs-number">-52.5785</span>, <span class="hljs-number">-38.0989</span>])

torch.mul(a, b)
Out[<span class="hljs-number">3</span>]: tensor([ <span class="hljs-number">0.0748</span>,  <span class="hljs-number">0.7101</span>, <span class="hljs-number">-0.3542</span>])
</code></pre>
<blockquote>
<p><strong>torch.neg</strong>: 返回一个新张量，包含输入<code>input</code> 张量按元素取负</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.0603</span>, <span class="hljs-number">-0.5258</span>, <span class="hljs-number">-0.3810</span>])

torch.neg(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-0.0603</span>,  <span class="hljs-number">0.5258</span>,  <span class="hljs-number">0.3810</span>])
</code></pre>
<blockquote>
<p><strong>torch.pow</strong>: torch.pow(input, exponent, out=None)</p>
<p>对输入<code>input</code>的按元素求<code>exponent</code>次幂值，并返回结果张量。 幂值<code>exponent</code> 可以为单一 <code>float</code> 数或者与<code>input</code>相同元素数的张量</p>
</blockquote>
<pre><code class="lang-python">a = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
exp = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])

torch.pow(a, <span class="hljs-number">2</span>)
Out[<span class="hljs-number">2</span>]: tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">16</span>])
torch.pow(a, exp)
Out[<span class="hljs-number">3</span>]: tensor([  <span class="hljs-number">1</span>,   <span class="hljs-number">4</span>,  <span class="hljs-number">27</span>, <span class="hljs-number">256</span>])
torch.pow(<span class="hljs-number">2</span>, exp)
Out[<span class="hljs-number">4</span>]: tensor([ <span class="hljs-number">2</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">8</span>, <span class="hljs-number">16</span>])
</code></pre>
<blockquote>
<p><strong>torch.reciprocal</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的倒数，即 1.0/x</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.reciprocal(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1.0000</span>, <span class="hljs-number">0.5000</span>, <span class="hljs-number">0.3333</span>, <span class="hljs-number">0.2500</span>])
</code></pre>
<blockquote>
<p><strong>torch.round</strong>: 返回一个新张量，将输入<code>input</code>张量每个元素舍入到最近的整数</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([ <span class="hljs-number">0.7995</span>, <span class="hljs-number">-2.0975</span>,  <span class="hljs-number">0.7273</span>,  <span class="hljs-number">0.7539</span>])

torch.round(a)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">1.</span>, <span class="hljs-number">-2.</span>,  <span class="hljs-number">1.</span>,  <span class="hljs-number">1.</span>])
</code></pre>
<blockquote>
<p><strong>torch.rsqrt</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的平方根倒数</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.rsqrt(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1.0000</span>, <span class="hljs-number">0.7071</span>, <span class="hljs-number">0.5774</span>, <span class="hljs-number">0.5000</span>])
</code></pre>
<blockquote>
<p><strong>torch.sigmoid</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的sigmoid值</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.sigmoid(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">0.7311</span>, <span class="hljs-number">0.8808</span>, <span class="hljs-number">0.9526</span>, <span class="hljs-number">0.9820</span>])
</code></pre>
<blockquote>
<p><strong>torch.sign</strong>: 符号函数：返回一个新张量，包含输入<code>input</code>张量每个元素的正负</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.sign(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])
</code></pre>
<blockquote>
<p><strong>torch.sin</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的正弦</p>
<p><strong>torch.sinh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的双曲正弦</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)

torch.sin(a)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.9215</span>,  <span class="hljs-number">0.2650</span>,  <span class="hljs-number">0.8285</span>,  <span class="hljs-number">0.5914</span>])
torch.sinh(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-1.4591</span>,  <span class="hljs-number">0.2714</span>,  <span class="hljs-number">1.1392</span>,  <span class="hljs-number">0.6759</span>])
</code></pre>
<blockquote>
<p><strong>torch.sqrt</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的平方根</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.sqrt(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1.0000</span>, <span class="hljs-number">1.4142</span>, <span class="hljs-number">1.7321</span>, <span class="hljs-number">2.0000</span>])
</code></pre>
<blockquote>
<p><strong>torch.tan</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的正切</p>
<p><strong>torch.tanh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的双曲正切</p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.tan(a)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">1.5574</span>, <span class="hljs-number">-2.1850</span>, <span class="hljs-number">-0.1425</span>,  <span class="hljs-number">1.1578</span>])
torch.tanh(a)
Out[<span class="hljs-number">2</span>]: tensor([<span class="hljs-number">0.7616</span>, <span class="hljs-number">0.9640</span>, <span class="hljs-number">0.9951</span>, <span class="hljs-number">0.9993</span>])
</code></pre>
<blockquote>
<p><strong>torch.trunc</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的截断值(标量x的截断值是最接近其的整数)</p>
<p>简而言之，有符号数的小数部分被舍弃</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-2.1647</span>, <span class="hljs-number">-0.2294</span>,  <span class="hljs-number">0.4943</span>,  <span class="hljs-number">1.5146</span>])

torch.trunc(a)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">-2.</span>, <span class="hljs-number">-0.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">1.</span>])
</code></pre>
<h2 id="reduction-ops">1.8 Reduction Ops</h2>
<blockquote>
<p><strong>torch.cumprod</strong>: torch.cumprod(input, dim, out=None) → Tensor</p>
<p>返回输入沿指定维度的累积，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，<script type="math/tex; ">y_i= \prod _{i}{x_i}</script></p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.cumprod(a, dim=<span class="hljs-number">0</span>)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">6.</span>, <span class="hljs-number">24.</span>])
</code></pre>
<blockquote>
<p><strong>torch.cumsum</strong>: torch.cumsum(input, dim, out=None) → Tensor</p>
<p>返回输入沿指定维度的累加，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，<script type="math/tex; ">y_i= \sum _{i}{x_i}</script></p>
</blockquote>
<pre><code class="lang-python">a = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])

torch.cumsum(a, dim=<span class="hljs-number">0</span>)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">1.</span>,  <span class="hljs-number">3.</span>,  <span class="hljs-number">6.</span>, <span class="hljs-number">10.</span>])
</code></pre>
<blockquote>
<p><strong>torch.dist</strong>: 返回 (<code>input</code> - <code>other</code>) 的 <code>p范数</code></p>
<p>torch.dist(input, other, p=2, out=None) → Tensor</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>other (Tensor) – 右侧输入张量</li>
<li>p (float, optional) – 所计算的范数</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p><script type="math/tex; mode=display">
> ||x||_p = (\sum _{i=1}^{n}{|x_i|^p})^{\frac {1}{p}}
> </script></p>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
y = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>])

torch.dist(x, y, <span class="hljs-number">3.5</span>)
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">4.0000</span>)
torch.dist(x, y, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">1</span>]: tensor(<span class="hljs-number">4.</span>)
</code></pre>
<blockquote>
<p><strong>torch.norm</strong>: 返回输入张量<code>input</code> 的 p 范数</p>
<p>torch.norm(input, p=2) → float</p>
<p>返回输入张量给定维<code>dim</code> 上每行的p 范数</p>
<p>torch.norm(input, p, dim, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: tensor([[ <span class="hljs-number">0.7848</span>, <span class="hljs-number">-0.3629</span>,  <span class="hljs-number">0.4028</span>]])
torch.norm(a, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">1</span>]: tensor(<span class="hljs-number">0.8418</span>)

a = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)
Out[<span class="hljs-number">2</span>]: 
tensor([[ <span class="hljs-number">1.0718</span>,  <span class="hljs-number">3.1510</span>],
        [<span class="hljs-number">-0.3178</span>, <span class="hljs-number">-0.9579</span>],
        [ <span class="hljs-number">0.4065</span>,  <span class="hljs-number">0.4106</span>]])
torch.norm(a, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">3</span>]: tensor([<span class="hljs-number">3.3283</span>, <span class="hljs-number">1.0092</span>, <span class="hljs-number">0.5778</span>])
</code></pre>
<blockquote>
<p><strong>torch.mean</strong>: 返回输入张量给定维度<code>dim</code>上每行的均值</p>
<p><strong>torch.median</strong>: 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的<code>LongTensor</code></p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>values (Tensor, optional) – 结果张量</li>
<li>indices (Tensor, optional) – 返回的索引结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])

torch.mean(x)
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">2.5000</span>)

torch.median(x)
Out[<span class="hljs-number">1</span>]: tensor(<span class="hljs-number">2.</span>)
</code></pre>
<blockquote>
<p><strong>torch.mode</strong>: 返回给定维<code>dim</code>上，每行的<code>众数值</code>， 同时返回一个<code>LongTensor</code>，包含众数职的索引</p>
<p>torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)</p>
<p>返回输入张量给定维度上每行的积</p>
<p>torch.prod(input, dim, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">-0.1038</span>,  <span class="hljs-number">0.8983</span>, <span class="hljs-number">-0.7463</span>],
        [<span class="hljs-number">-0.6661</span>, <span class="hljs-number">-0.5061</span>,  <span class="hljs-number">0.2043</span>]])

torch.mode(a, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">1</span>]: 
torch.return_types.mode(
values=tensor([<span class="hljs-number">-0.7463</span>, <span class="hljs-number">-0.6661</span>]),
indices=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>]))
</code></pre>
<blockquote>
<p><strong>torch.prod</strong>: 返回输入张量<code>input</code> 所有元素的积</p>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
torch.prod(x)
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">24.</span>)

y = torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])
torch.prod(y, dim=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">1</span>]: tensor([ <span class="hljs-number">2.</span>, <span class="hljs-number">12.</span>])
</code></pre>
<blockquote>
<p><strong>torch.std</strong>: 返回输入张量<code>input</code> 所有元素的标准差</p>
<p>torch.std(input) → float</p>
<p>返回输入张量给定维度上每行的标准差</p>
<p>torch.std(input, dim, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
y = torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])

torch.std(x)
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">1.2910</span>)

torch.std(y, dim=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">0.7071</span>, <span class="hljs-number">0.7071</span>])
</code></pre>
<blockquote>
<p><strong>torch.sum</strong>: 返回输入张量所有元素的和</p>
<p>torch.sum(input) → float</p>
<p>返回输入张量给定维度上每行的和</p>
<p>torch.sum(input, dim, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
y = torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])

torch.var(x)
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">10.0</span>)

torch.var(y, dim=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">3.</span>, <span class="hljs-number">7.</span>])
</code></pre>
<blockquote>
<p><strong>torch.var</strong>: 返回输入张量所有元素的方差</p>
<p>torch.var(input) → float</p>
<p>返回输入张量给定维度上每行的方差</p>
<p>torch.var(input, dim, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">x = torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
y = torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])

torch.var(x)
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">1.6667</span>)

torch.var(y, dim=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">0.5000</span>, <span class="hljs-number">0.5000</span>])
</code></pre>
<h2 id="比较操作-comparison-ops">1.9 比较操作 Comparison Ops</h2>
<blockquote>
<p><strong>torch.eq</strong>: torch.eq(input, other, out=None) → Tensor</p>
<p>比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量</p>
<p><strong>torch.ge</strong>: torch.ge(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code>，即是否 input&gt;=other</p>
<p><strong>torch.gt</strong>: torch.gt(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否input&gt;otherinput&gt;other </p>
<p><strong>torch.le</strong>: torch.le(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否input&lt;=other</p>
<p><strong>torch.lt</strong>: torch.lt(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否 input&lt;other</p>
<p><strong>torch.ne</strong>: torch.ne(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code>，即是否 input!=other</p>
</blockquote>
<pre><code class="lang-python">torch.eq(torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]))
Out[<span class="hljs-number">0</span>]: 
tensor([[ <span class="hljs-keyword">True</span>, <span class="hljs-keyword">False</span>],
        [<span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>]])

torch.ge(torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]))
Out[<span class="hljs-number">1</span>]: 
tensor([[ <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>],
        [<span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>]])

torch.gt(torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]))
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>],
        [<span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>]])

torch.le(torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]))
Out[<span class="hljs-number">3</span>]: 
tensor([[ <span class="hljs-keyword">True</span>, <span class="hljs-keyword">False</span>],
        [ <span class="hljs-keyword">True</span>,  <span class="hljs-keyword">True</span>]])

torch.lt(torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]))
Out[<span class="hljs-number">4</span>]: 
tensor([[<span class="hljs-keyword">False</span>, <span class="hljs-keyword">False</span>],
        [ <span class="hljs-keyword">True</span>, <span class="hljs-keyword">False</span>]])

torch.ne(torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]))
Out[<span class="hljs-number">5</span>]: 
tensor([[<span class="hljs-keyword">False</span>,  <span class="hljs-keyword">True</span>],
        [ <span class="hljs-keyword">True</span>, <span class="hljs-keyword">False</span>]])
</code></pre>
<blockquote>
<p><strong>torch.equal</strong>: torch.equal(tensor1, tensor2) → bool</p>
<p>如果两个张量有相同的形状和元素值，则返回<code>True</code> ，否则 <code>False</code></p>
</blockquote>
<pre><code class="lang-python">torch.equal(torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]), torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]))
Out[<span class="hljs-number">0</span>]: <span class="hljs-keyword">True</span>
</code></pre>
<blockquote>
<p><strong>torch.kthvalue</strong>: torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)</p>
<p>取输入张量<code>input</code>指定维上第k 个最小值。如果不指定<code>dim</code>，则默认为<code>input</code>的最后一维</p>
<p><strong>torch.topk</strong>: 沿给定<code>dim</code>维度返回输入张量<code>input</code>中 <code>k</code> 个最大值。 如果不指定<code>dim</code>，则默认为<code>input</code>的最后一维。 如果为<code>largest</code>为 <code>False</code> ，则返回最小的 <code>k</code> 个值</p>
<p>返回一个元组 <em>(values,indices)</em>，其中<code>indices</code>是原始输入张量<code>input</code>中测元素下标。 如果设定布尔值<code>sorted</code> 为<em>True</em>，将会确保返回的 <code>k</code> 个值被排序</p>
<p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>k (int) – “top-k”中的<code>k</code></li>
<li>dim (int, optional) – 排序的维</li>
<li>largest (bool, optional) – 布尔值，控制返回最大或最小值</li>
<li>sorted (bool, optional) – 布尔值，控制返回值是否排序</li>
<li>out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)

<span class="hljs-comment"># torch.kthvalue</span>
torch.kthvalue(x, <span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]:
torch.return_types.kthvalue(
values=tensor(<span class="hljs-number">4</span>),
indices=tensor(<span class="hljs-number">3</span>))

<span class="hljs-comment"># torch.topk</span>
x = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])

torch.topk(x, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">2</span>]: 
torch.return_types.topk(
values=tensor([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>]),
indices=tensor([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>]))

torch.topk(x, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, largest=<span class="hljs-keyword">False</span>)
Out[<span class="hljs-number">3</span>]: 
torch.return_types.topk(
values=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]),
indices=tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]))
</code></pre>
<blockquote>
<p><strong>torch.max</strong>: 返回输入张量所有元素的最大值</p>
<p>torch.max()</p>
<p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引</p>
<p>torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)</p>
<p><code>input</code>中<strong>逐元素</strong>与<code>other</code>相应位置的元素对比，返回最大值到输出张量</p>
<p>torch.max(input, other, out=None) → Tensor</p>
<p><strong>torch.min</strong>: 返回输入张量所有元素的最小值</p>
<p>torch.min(input) → float</p>
<p>返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引</p>
<p>torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)</p>
<p><code>input</code>中<strong>逐元素</strong>与<code>other</code>相应位置的元素对比，返回最小值到输出张量</p>
<p>torch.min(input, other, out=None) → Tensor</p>
</blockquote>
<pre><code class="lang-python">a = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">-0.1204</span>, <span class="hljs-number">-0.5016</span>],
        [ <span class="hljs-number">1.2717</span>,  <span class="hljs-number">0.7351</span>]])
b = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">-1.4497</span>,  <span class="hljs-number">0.7534</span>],
        [ <span class="hljs-number">0.5994</span>, <span class="hljs-number">-0.1490</span>]])

<span class="hljs-comment"># 最大值</span>
torch.max(torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>))
Out[<span class="hljs-number">2</span>]: tensor(<span class="hljs-number">4</span>)
torch.max(a, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">3</span>]: 
torch.return_types.max(
values=tensor([<span class="hljs-number">-0.1204</span>,  <span class="hljs-number">1.2717</span>]),
indices=tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))
torch.max(a, b)
Out[<span class="hljs-number">4</span>]: 
tensor([[<span class="hljs-number">-0.1204</span>,  <span class="hljs-number">0.7534</span>],
        [ <span class="hljs-number">1.2717</span>,  <span class="hljs-number">0.7351</span>]])

<span class="hljs-comment"># 最小值</span>
torch.min(torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>))
Out[<span class="hljs-number">5</span>]: tensor(<span class="hljs-number">1</span>)
torch.min(a, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">6</span>]: 
torch.return_types.min(
values=tensor([<span class="hljs-number">-0.5016</span>,  <span class="hljs-number">0.7351</span>]),
indices=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]))
torch.min(a, b)
Out[<span class="hljs-number">7</span>]: 
tensor([[<span class="hljs-number">-1.4497</span>, <span class="hljs-number">-0.5016</span>],
        [ <span class="hljs-number">0.5994</span>, <span class="hljs-number">-0.1490</span>]])
</code></pre>
<blockquote>
<p><strong>torch.sort</strong>: torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)</p>
<p>对输入张量<code>input</code>沿着指定维按升序排序。如果不给定<code>dim</code>，则默认为输入的最后一维。如果指定参数<code>descending</code>为<code>True</code>，则按降序排序</p>
<p>返回元组 (sorted_tensor, sorted_indices) ， <code>sorted_indices</code> 为原始输入中的下标</p>
</blockquote>
<pre><code class="lang-python">x = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">-2.3460</span>,  <span class="hljs-number">1.3734</span>,  <span class="hljs-number">1.1444</span>, <span class="hljs-number">-0.4736</span>],
        [<span class="hljs-number">-1.1785</span>,  <span class="hljs-number">0.8436</span>, <span class="hljs-number">-1.4403</span>, <span class="hljs-number">-0.1073</span>],
        [<span class="hljs-number">-0.1198</span>,  <span class="hljs-number">0.7067</span>, <span class="hljs-number">-0.0734</span>, <span class="hljs-number">-1.6181</span>]])

sorted, indices = torch.sort(x)
sorted, indices
Out[<span class="hljs-number">1</span>]: 
(tensor([[<span class="hljs-number">-2.3460</span>, <span class="hljs-number">-0.4736</span>,  <span class="hljs-number">1.1444</span>,  <span class="hljs-number">1.3734</span>],
         [<span class="hljs-number">-1.4403</span>, <span class="hljs-number">-1.1785</span>, <span class="hljs-number">-0.1073</span>,  <span class="hljs-number">0.8436</span>],
         [<span class="hljs-number">-1.6181</span>, <span class="hljs-number">-0.1198</span>, <span class="hljs-number">-0.0734</span>,  <span class="hljs-number">0.7067</span>]]),
 tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],
         [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],
         [<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]]))
</code></pre>
<h2 id="其它操作-other-operations">1.10 其它操作 Other Operations</h2>
<blockquote>
<p><strong>torch.cross</strong>: 返回沿着维度<code>dim</code>上，两个张量<code>input</code>和<code>other</code>的向量积（叉积）。 <code>input</code>和<code>other</code> 必须有相同的形状，且指定的<code>dim</code>维上size必须为<code>3</code></p>
<p>如果不指定<code>dim</code>，则默认为第一个尺度为<code>3</code>的维</p>
<p>torch.cross(input, other, dim=-1, out=None) → Tensor</p>
</blockquote>
<p><script type="math/tex; mode=display">
\left[\begin{array}{l}a_{1} \\ a_{2} \\ a_{3}\end{array}\right] \times\left[\begin{array}{l}b_{1} \\ b_{2} \\ b_{3}\end{array}\right]=\left[\begin{array}{c}a_{2} b_{3}-a_{3} b_{2} \\ a_{3} b_{1}-a_{1} b_{3} \\ a_{1} b_{2}-a_{2} b_{1}\end{array}\right]
</script></p>
<pre><code class="lang-python">a = torch.randint(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])
b = torch.randint(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>]])

torch.cross(a, a)
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])

torch.cross(a, b)
Out[<span class="hljs-number">3</span>]: 
tensor([[ <span class="hljs-number">3</span>, <span class="hljs-number">-5</span>,  <span class="hljs-number">1</span>],
        [<span class="hljs-number">-8</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">10</span>]])
</code></pre>
<blockquote>
<p><strong>torch.diag</strong>: torch.diag(input, diagonal=0, out=None) → Tensor</p>
<ul>
<li>如果输入是一个向量(1D 张量)，则返回一个以<code>input</code>为对角线元素的2D方阵</li>
<li>如果输入是一个矩阵(2D 张量)，则返回一个包含<code>input</code>对角线元素的1D张量</li>
</ul>
<p>参数<code>diagonal</code>指定对角线:</p>
<ul>
<li><code>diagonal</code> = 0, 主对角线</li>
<li><code>diagonal</code> &gt; 0, 主对角线之上</li>
<li><code>diagonal</code> &lt; 0, 主对角线之下</li>
</ul>
</blockquote>
<pre><code class="lang-python"><span class="hljs-comment"># 如果输入是一个向量(1D 张量)，则返回一个以`input`为对角线元素的2D方阵</span>
a = torch.randn(<span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">-0.3509</span>,  <span class="hljs-number">0.6176</span>, <span class="hljs-number">-1.4976</span>])
torch.diag(a)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">-0.3509</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
        [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.6176</span>,  <span class="hljs-number">0.0000</span>],
        [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>, <span class="hljs-number">-1.4976</span>]])
torch.diag(a, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">2</span>]: 
tensor([[ <span class="hljs-number">0.0000</span>, <span class="hljs-number">-0.3509</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>],
        [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.6176</span>,  <span class="hljs-number">0.0000</span>],
        [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>, <span class="hljs-number">-1.4976</span>],
        [ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.0000</span>]])

<span class="hljs-comment"># 如果输入是一个矩阵(2D 张量)，则返回一个包含`input`对角线元素的1D张量</span>
<span class="hljs-comment"># 取得给定矩阵第k个对角线:</span>
a = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">3</span>]: 
tensor([[ <span class="hljs-number">0.8224</span>,  <span class="hljs-number">0.7792</span>,  <span class="hljs-number">0.2605</span>],
        [<span class="hljs-number">-0.8646</span>,  <span class="hljs-number">0.2568</span>, <span class="hljs-number">-0.8189</span>],
        [ <span class="hljs-number">1.1693</span>,  <span class="hljs-number">0.8108</span>, <span class="hljs-number">-1.9662</span>]])
torch.diag(a, <span class="hljs-number">0</span>)
Out[<span class="hljs-number">4</span>]: tensor([ <span class="hljs-number">0.8224</span>,  <span class="hljs-number">0.2568</span>, <span class="hljs-number">-1.9662</span>])
torch.diag(a, <span class="hljs-number">1</span>)
Out[<span class="hljs-number">5</span>]: tensor([ <span class="hljs-number">0.7792</span>, <span class="hljs-number">-0.8189</span>])
</code></pre>
<blockquote>
<p><strong>torch.histc</strong>: torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor</p>
<p>计算输入张量的直方图。以<code>min</code>和<code>max</code>为range边界，将其均分成<code>bins</code>个直条，然后将排序好的数据划分到各个直条(bins)中</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>bins (int) – 直方图 bins(直条)的个数(默认100个)</li>
<li>min (int) – range的下边界(包含)</li>
<li>max (int) – range的上边界(包含)</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">torch.histc(torch.FloatTensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]), bins=<span class="hljs-number">4</span>, min=<span class="hljs-number">0</span>, max=<span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>])

torch.histc(torch.FloatTensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]), bins=<span class="hljs-number">4</span>, min=<span class="hljs-number">0</span>, max=<span class="hljs-number">3</span>)
Out[<span class="hljs-number">1</span>]: tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>])
</code></pre>
<blockquote>
<p><strong>torch.renorm</strong>: torch.renorm(input, p, dim, maxnorm, out=None) → Tensor</p>
<p>返回一个张量，包含规范化后的各个子张量，使得沿着<code>dim</code>维划分的各子张量的p范数小于<code>maxnorm</code></p>
<p>如果p范数的值小于<code>maxnorm</code>，则当前子张量不需要修改</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>p (float) – 范数的p</li>
<li>dim (int) – 沿着此维切片，得到张量子集</li>
<li>maxnorm (float) – 每个子张量的范数的最大值</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<pre><code class="lang-python">x = torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
x[<span class="hljs-number">1</span>].fill_(<span class="hljs-number">2</span>)
x[<span class="hljs-number">2</span>].fill_(<span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>]])

torch.renorm(x, p=<span class="hljs-number">1</span>, dim=<span class="hljs-number">0</span>, maxnorm=<span class="hljs-number">5</span>)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1.0000</span>, <span class="hljs-number">1.0000</span>, <span class="hljs-number">1.0000</span>],
        [<span class="hljs-number">1.6667</span>, <span class="hljs-number">1.6667</span>, <span class="hljs-number">1.6667</span>],
        [<span class="hljs-number">1.6667</span>, <span class="hljs-number">1.6667</span>, <span class="hljs-number">1.6667</span>]])
</code></pre>
<blockquote>
<p><strong>torch.trace</strong>: 返回输入2维矩阵对角线元素的和(迹)</p>
</blockquote>
<pre><code class="lang-python">x = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>).view(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])

torch.trace(x)
Out[<span class="hljs-number">1</span>]: tensor(<span class="hljs-number">15</span>)
</code></pre>
<blockquote>
<p><strong>torch.tril</strong>: torch.tril(input, diagonal=0, out=None) → Tensor</p>
<p>返回一个张量<code>out</code>，包含输入矩阵(2D张量)的下三角部分，<code>out</code>其余部分被设为<code>0</code></p>
</blockquote>
<pre><code class="lang-python">x = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>).view(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])

torch.tril(x)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])

torch.tril(x, diagonal=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])
</code></pre>
<blockquote>
<p><strong>torch.triu</strong>: torch.triu(input, k=0, out=None) → Tensor</p>
<p>返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为<code>0</code>。这里所说的上三角部分为矩阵指定对角线<code>diagonal</code>之上的元素。</p>
<p>参数<code>k</code>控制对角线: - <code>k</code> = 0, 主对角线 - <code>k</code> &gt; 0, 主对角线之上 - <code>k</code> &lt; 0, 主对角线之下</p>
</blockquote>
<pre><code class="lang-python">x = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>).view(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>]])

torch.triu(x)
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">9</span>]])

torch.triu(x, diagonal=<span class="hljs-number">1</span>)
Out[<span class="hljs-number">2</span>]: 
tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
</code></pre>
<h2 id="blas-and-lapack-operations">1.11 BLAS and LAPACK Operations</h2>
<blockquote>
<p>torch.addbmm</p>
<p>torch.addmm</p>
<p>torch.addmv</p>
<p>torch.addr</p>
<p>torch.baddbmm</p>
<p>torch.bmm</p>
<p>torch.btrifact</p>
<p>torch.btrisolve</p>
<p><strong>torch.dot</strong>: 计算两个张量的点乘(内乘),两个张量都为1-D 向量</p>
<p>torch.dot(tensor1, tensor2) → float</p>
</blockquote>
<pre><code class="lang-python">torch.dot(torch.Tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]), torch.Tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>]))
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">7.</span>)
</code></pre>
<blockquote>
<p><strong>torch.linalg.eig</strong>: 计算实方阵<code>a</code> 的特征值和特征向量</p>
<p>torch.linalg.eig(A, * , out=None)</p>
</blockquote>
<pre><code class="lang-python">A = torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, dtype=torch.complex128)
Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">-0.2029</span><span class="hljs-number">-0.0673j</span>, <span class="hljs-number">-0.5188</span><span class="hljs-number">-0.6723j</span>],
        [<span class="hljs-number">-1.1984</span>+<span class="hljs-number">0.0585j</span>,  <span class="hljs-number">0.5786</span><span class="hljs-number">-0.1849j</span>]], dtype=torch.complex128)

L, V = torch.linalg.eig(A)
Out[<span class="hljs-number">1</span>]: 
(tensor([<span class="hljs-number">-0.7870</span><span class="hljs-number">-0.5003j</span>,  <span class="hljs-number">1.1626</span>+<span class="hljs-number">0.2481j</span>], dtype=torch.complex128),
 tensor([[ <span class="hljs-number">0.7596</span>+<span class="hljs-number">0.0000j</span>, <span class="hljs-number">-0.4008</span><span class="hljs-number">-0.3285j</span>],
         [ <span class="hljs-number">0.6258</span><span class="hljs-number">-0.1770j</span>,  <span class="hljs-number">0.8552</span>+<span class="hljs-number">0.0000j</span>]], dtype=torch.complex128))
</code></pre>
<h1 id="tensor">2 Tensor</h1>
<p><code>torch.Tensor</code>是一种包含单一数据类型元素的多维矩阵</p>
<p>Torch定义了10种CPU tensor类型和GPU tensor类型：</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.float32</code> or <code>torch.float</code></td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.float64</code> or <code>torch.double</code></td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a href="https://pytorch.org/docs/stable/tensors.html#id4" target="_blank">[1]</a></td>
<td><code>torch.float16</code> or <code>torch.half</code></td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a href="https://pytorch.org/docs/stable/tensors.html#id5" target="_blank">[2]</a></td>
<td><code>torch.bfloat16</code></td>
<td><code>torch.BFloat16Tensor</code></td>
<td><code>torch.cuda.BFloat16Tensor</code></td>
</tr>
<tr>
<td>32-bit complex</td>
<td><code>torch.complex32</code> or <code>torch.chalf</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>64-bit complex</td>
<td><code>torch.complex64</code> or <code>torch.cfloat</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>128-bit complex</td>
<td><code>torch.complex128</code> or <code>torch.cdouble</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.int8</code></td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.int16</code> or <code>torch.short</code></td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.int32</code> or <code>torch.int</code></td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.int64</code> or <code>torch.long</code></td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
<tr>
<td>Boolean</td>
<td><code>torch.bool</code></td>
<td><code>torch.BoolTensor</code></td>
<td><code>torch.cuda.BoolTensor</code></td>
</tr>
<tr>
<td>quantized 8-bit integer (unsigned)</td>
<td><code>torch.quint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 8-bit integer (signed)</td>
<td><code>torch.qint8</code></td>
<td><code>torch.CharTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 32-bit integer (signed)</td>
<td><code>torch.qint32</code></td>
<td><code>torch.IntTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 4-bit integer (unsigned) <a href="https://pytorch.org/docs/stable/tensors.html#id6" target="_blank">[3]</a></td>
<td><code>torch.quint4x2</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
</tbody>
</table>
<blockquote>
<p>创建</p>
</blockquote>
<p>一个张量tensor可以从Python的<code>list</code>或序列构建</p>
<pre><code class="lang-python">torch.FloatTensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])

Out[<span class="hljs-number">0</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])
</code></pre>
<p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<code>numpy.ndarray</code>,<code>torch.Tensor</code>或<code>torch.Storage</code>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor</p>
<pre><code class="lang-python"># 接口 一个空张量tensor可以通过规定其大小来构建
class torch.Tensor
class torch.Tensor(*sizes)
class torch.Tensor(size)
class torch.Tensor(sequence)
class torch.Tensor(ndarray)
class torch.Tensor(tensor)
class torch.Tensor(storage)

# 实例化
torch.IntTensor(2, 4).zero_()
</code></pre>
<p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p>
<pre><code class="lang-python">x = torch.FloatTensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])
x[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>]
Out[<span class="hljs-number">0</span>]: tensor(<span class="hljs-number">6.</span>)

x[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] = <span class="hljs-number">8</span>
x
Out[<span class="hljs-number">1</span>]: 
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]])
</code></pre>
<p>每一个张量tensor都有一个相应的<code>torch.Storage</code>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算</p>
<p><strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，<code>torch.FloatTensor.abs_()</code>会在原地计算绝对值，并返回改变后的tensor，而<code>tensor.FloatTensor.abs()</code>将会在一个新的tensor中计算结果</p>
<blockquote>
<p>关键属性和方法</p>
</blockquote>
<table>
<thead>
<tr>
<th><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor" target="_blank"><code>Tensor.new_tensor</code></a></th>
<th>Returns a new Tensor with <code>data</code> as the tensor data.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_full.html#torch.Tensor.new_full" target="_blank"><code>Tensor.new_full</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>fill_value</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty" target="_blank"><code>Tensor.new_empty</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with uninitialized data.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones" target="_blank"><code>Tensor.new_ones</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>1</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros" target="_blank"><code>Tensor.new_zeros</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>0</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda" target="_blank"><code>Tensor.is_cuda</code></a></td>
<td>Is <code>True</code> if the Tensor is stored on the GPU, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized" target="_blank"><code>Tensor.is_quantized</code></a></td>
<td>Is <code>True</code> if the Tensor is quantized, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta" target="_blank"><code>Tensor.is_meta</code></a></td>
<td>Is <code>True</code> if the Tensor is a meta tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.device.html#torch.Tensor.device" target="_blank"><code>Tensor.device</code></a></td>
<td>Is the <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device" target="_blank"><code>torch.device</code></a> where this Tensor is.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html#torch.Tensor.grad" target="_blank"><code>Tensor.grad</code></a></td>
<td>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <code>backward()</code> computes gradients for <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html#torch.Tensor.ndim" target="_blank"><code>Tensor.ndim</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim" target="_blank"><code>dim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.real.html#torch.Tensor.real" target="_blank"><code>Tensor.real</code></a></td>
<td>Returns a new tensor containing real values of the <code>self</code> tensor for a complex-valued input tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.imag.html#torch.Tensor.imag" target="_blank"><code>Tensor.imag</code></a></td>
<td>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs" target="_blank"><code>Tensor.abs</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs" target="_blank"><code>torch.abs()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs_.html#torch.Tensor.abs_" target="_blank"><code>Tensor.abs_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs" target="_blank"><code>abs()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute" target="_blank"><code>Tensor.absolute</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs" target="_blank"><code>abs()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_" target="_blank"><code>Tensor.absolute_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute" target="_blank"><code>absolute()</code></a> Alias for <code>abs_()</code></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos" target="_blank"><code>Tensor.acos</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos" target="_blank"><code>torch.acos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos_.html#torch.Tensor.acos_" target="_blank"><code>Tensor.acos_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos" target="_blank"><code>acos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos" target="_blank"><code>Tensor.arccos</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos" target="_blank"><code>torch.arccos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_" target="_blank"><code>Tensor.arccos_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos" target="_blank"><code>arccos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add" target="_blank"><code>Tensor.add</code></a></td>
<td>Add a scalar or tensor to <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.add_.html#torch.Tensor.add_" target="_blank"><code>Tensor.add_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add" target="_blank"><code>add()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" target="_blank"><code>Tensor.addbmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm" target="_blank"><code>torch.addbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_" target="_blank"><code>Tensor.addbmm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm" target="_blank"><code>addbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" target="_blank"><code>Tensor.addcdiv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv" target="_blank"><code>torch.addcdiv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_" target="_blank"><code>Tensor.addcdiv_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv" target="_blank"><code>addcdiv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" target="_blank"><code>Tensor.addcmul</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul" target="_blank"><code>torch.addcmul()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_" target="_blank"><code>Tensor.addcmul_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul" target="_blank"><code>addcmul()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm" target="_blank"><code>Tensor.addmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm" target="_blank"><code>torch.addmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_" target="_blank"><code>Tensor.addmm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm" target="_blank"><code>addmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm" target="_blank"><code>Tensor.sspaddmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sspaddmm.html#torch.sspaddmm" target="_blank"><code>torch.sspaddmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv" target="_blank"><code>Tensor.addmv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv" target="_blank"><code>torch.addmv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_" target="_blank"><code>Tensor.addmv_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv" target="_blank"><code>addmv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr" target="_blank"><code>Tensor.addr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr" target="_blank"><code>torch.addr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr_.html#torch.Tensor.addr_" target="_blank"><code>Tensor.addr_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr" target="_blank"><code>addr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint" target="_blank"><code>Tensor.adjoint</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.adjoint.html#torch.adjoint" target="_blank"><code>adjoint()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.allclose.html#torch.Tensor.allclose" target="_blank"><code>Tensor.allclose</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose" target="_blank"><code>torch.allclose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.amax.html#torch.Tensor.amax" target="_blank"><code>Tensor.amax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax" target="_blank"><code>torch.amax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.amin.html#torch.Tensor.amin" target="_blank"><code>Tensor.amin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin" target="_blank"><code>torch.amin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax" target="_blank"><code>Tensor.aminmax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.aminmax.html#torch.aminmax" target="_blank"><code>torch.aminmax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.angle.html#torch.Tensor.angle" target="_blank"><code>Tensor.angle</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle" target="_blank"><code>torch.angle()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.apply_.html#torch.Tensor.apply_" target="_blank"><code>Tensor.apply_</code></a></td>
<td>Applies the function <code>callable</code> to each element in the tensor, replacing each element with the value returned by <code>callable</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html#torch.Tensor.argmax" target="_blank"><code>Tensor.argmax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax" target="_blank"><code>torch.argmax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmin.html#torch.Tensor.argmin" target="_blank"><code>Tensor.argmin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin" target="_blank"><code>torch.argmin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argsort.html#torch.Tensor.argsort" target="_blank"><code>Tensor.argsort</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort" target="_blank"><code>torch.argsort()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere" target="_blank"><code>Tensor.argwhere</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.argwhere.html#torch.argwhere" target="_blank"><code>torch.argwhere()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin" target="_blank"><code>Tensor.asin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin" target="_blank"><code>torch.asin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin_.html#torch.Tensor.asin_" target="_blank"><code>Tensor.asin_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin" target="_blank"><code>asin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" target="_blank"><code>Tensor.arcsin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin" target="_blank"><code>torch.arcsin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_" target="_blank"><code>Tensor.arcsin_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin" target="_blank"><code>arcsin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" target="_blank"><code>Tensor.as_strided</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided" target="_blank"><code>torch.as_strided()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan" target="_blank"><code>Tensor.atan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan" target="_blank"><code>torch.atan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan_.html#torch.Tensor.atan_" target="_blank"><code>Tensor.atan_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan" target="_blank"><code>atan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan" target="_blank"><code>Tensor.arctan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan" target="_blank"><code>torch.arctan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_" target="_blank"><code>Tensor.arctan_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan" target="_blank"><code>arctan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2" target="_blank"><code>Tensor.atan2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2" target="_blank"><code>torch.atan2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_" target="_blank"><code>Tensor.atan2_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2" target="_blank"><code>atan2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2" target="_blank"><code>Tensor.arctan2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arctan2.html#torch.arctan2" target="_blank"><code>torch.arctan2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_" target="_blank"><code>Tensor.arctan2_</code></a></td>
<td>atan2_(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.all.html#torch.Tensor.all" target="_blank"><code>Tensor.all</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.all.html#torch.all" target="_blank"><code>torch.all()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.any.html#torch.Tensor.any" target="_blank"><code>Tensor.any</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.any.html#torch.any" target="_blank"><code>torch.any()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" target="_blank"><code>Tensor.backward</code></a></td>
<td>Computes the gradient of current tensor w.r.t.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" target="_blank"><code>Tensor.baddbmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm" target="_blank"><code>torch.baddbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_" target="_blank"><code>Tensor.baddbmm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm" target="_blank"><code>baddbmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli" target="_blank"><code>Tensor.bernoulli</code></a></td>
<td>Returns a result tensor where each \texttt{result[i]}result[i] is independently sampled from \text{Bernoulli}(\texttt{self[i]})Bernoulli(self[i]).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_" target="_blank"><code>Tensor.bernoulli_</code></a></td>
<td>Fills each location of <code>self</code> with an independent sample from \text{Bernoulli}(\texttt{p})Bernoulli(p).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16" target="_blank"><code>Tensor.bfloat16</code></a></td>
<td><code>self.bfloat16()</code> is equivalent to <code>self.to(torch.bfloat16)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bincount.html#torch.Tensor.bincount" target="_blank"><code>Tensor.bincount</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount" target="_blank"><code>torch.bincount()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" target="_blank"><code>Tensor.bitwise_not</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not" target="_blank"><code>torch.bitwise_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_" target="_blank"><code>Tensor.bitwise_not_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not" target="_blank"><code>bitwise_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" target="_blank"><code>Tensor.bitwise_and</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and" target="_blank"><code>torch.bitwise_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_" target="_blank"><code>Tensor.bitwise_and_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and" target="_blank"><code>bitwise_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" target="_blank"><code>Tensor.bitwise_or</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or" target="_blank"><code>torch.bitwise_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_" target="_blank"><code>Tensor.bitwise_or_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or" target="_blank"><code>bitwise_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" target="_blank"><code>Tensor.bitwise_xor</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor" target="_blank"><code>torch.bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_" target="_blank"><code>Tensor.bitwise_xor_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor" target="_blank"><code>bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" target="_blank"><code>Tensor.bitwise_left_shift</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift" target="_blank"><code>torch.bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_" target="_blank"><code>Tensor.bitwise_left_shift_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift" target="_blank"><code>bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" target="_blank"><code>Tensor.bitwise_right_shift</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift" target="_blank"><code>torch.bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_" target="_blank"><code>Tensor.bitwise_right_shift_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift" target="_blank"><code>bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bmm.html#torch.Tensor.bmm" target="_blank"><code>Tensor.bmm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm" target="_blank"><code>torch.bmm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.bool.html#torch.Tensor.bool" target="_blank"><code>Tensor.bool</code></a></td>
<td><code>self.bool()</code> is equivalent to <code>self.to(torch.bool)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.byte.html#torch.Tensor.byte" target="_blank"><code>Tensor.byte</code></a></td>
<td><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to" target="_blank"><code>Tensor.broadcast_to</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to" target="_blank"><code>torch.broadcast_to()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_" target="_blank"><code>Tensor.cauchy_</code></a></td>
<td>Fills the tensor with numbers drawn from the Cauchy distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil" target="_blank"><code>Tensor.ceil</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil" target="_blank"><code>torch.ceil()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_" target="_blank"><code>Tensor.ceil_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil" target="_blank"><code>ceil()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.char.html#torch.Tensor.char" target="_blank"><code>Tensor.char</code></a></td>
<td><code>self.char()</code> is equivalent to <code>self.to(torch.int8)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky" target="_blank"><code>Tensor.cholesky</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky" target="_blank"><code>torch.cholesky()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse" target="_blank"><code>Tensor.cholesky_inverse</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse" target="_blank"><code>torch.cholesky_inverse()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve" target="_blank"><code>Tensor.cholesky_solve</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve" target="_blank"><code>torch.cholesky_solve()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.chunk.html#torch.Tensor.chunk" target="_blank"><code>Tensor.chunk</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk" target="_blank"><code>torch.chunk()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp" target="_blank"><code>Tensor.clamp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp" target="_blank"><code>torch.clamp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" target="_blank"><code>Tensor.clamp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp" target="_blank"><code>clamp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip.html#torch.Tensor.clip" target="_blank"><code>Tensor.clip</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp" target="_blank"><code>clamp()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip_.html#torch.Tensor.clip_" target="_blank"><code>Tensor.clip_</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_" target="_blank"><code>clamp_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html#torch.Tensor.clone" target="_blank"><code>Tensor.clone</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone" target="_blank"><code>torch.clone()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous" target="_blank"><code>Tensor.contiguous</code></a></td>
<td>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html#torch.Tensor.copy_" target="_blank"><code>Tensor.copy_</code></a></td>
<td>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj.html#torch.Tensor.conj" target="_blank"><code>Tensor.conj</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj" target="_blank"><code>torch.conj()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" target="_blank"><code>Tensor.conj_physical</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.conj_physical.html#torch.conj_physical" target="_blank"><code>torch.conj_physical()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_" target="_blank"><code>Tensor.conj_physical_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical" target="_blank"><code>conj_physical()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj" target="_blank"><code>Tensor.resolve_conj</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.resolve_conj.html#torch.resolve_conj" target="_blank"><code>torch.resolve_conj()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg" target="_blank"><code>Tensor.resolve_neg</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.resolve_neg.html#torch.resolve_neg" target="_blank"><code>torch.resolve_neg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign" target="_blank"><code>Tensor.copysign</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign" target="_blank"><code>torch.copysign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_" target="_blank"><code>Tensor.copysign_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign" target="_blank"><code>copysign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos" target="_blank"><code>Tensor.cos</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos" target="_blank"><code>torch.cos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos_.html#torch.Tensor.cos_" target="_blank"><code>Tensor.cos_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos" target="_blank"><code>cos()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh" target="_blank"><code>Tensor.cosh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh" target="_blank"><code>torch.cosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_" target="_blank"><code>Tensor.cosh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh" target="_blank"><code>cosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef" target="_blank"><code>Tensor.corrcoef</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.corrcoef.html#torch.corrcoef" target="_blank"><code>torch.corrcoef()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero" target="_blank"><code>Tensor.count_nonzero</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero" target="_blank"><code>torch.count_nonzero()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cov.html#torch.Tensor.cov" target="_blank"><code>Tensor.cov</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cov.html#torch.cov" target="_blank"><code>torch.cov()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh" target="_blank"><code>Tensor.acosh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh" target="_blank"><code>torch.acosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_" target="_blank"><code>Tensor.acosh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh" target="_blank"><code>acosh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh" target="_blank"><code>Tensor.arccosh</code></a></td>
<td>acosh() -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_" target="_blank"><code>Tensor.arccosh_</code></a></td>
<td>acosh_() -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html#torch.Tensor.cpu" target="_blank"><code>Tensor.cpu</code></a></td>
<td>Returns a copy of this object in CPU memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cross.html#torch.Tensor.cross" target="_blank"><code>Tensor.cross</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross" target="_blank"><code>torch.cross()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda" target="_blank"><code>Tensor.cuda</code></a></td>
<td>Returns a copy of this object in CUDA memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp" target="_blank"><code>Tensor.logcumsumexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp" target="_blank"><code>torch.logcumsumexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummax.html#torch.Tensor.cummax" target="_blank"><code>Tensor.cummax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax" target="_blank"><code>torch.cummax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummin.html#torch.Tensor.cummin" target="_blank"><code>Tensor.cummin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin" target="_blank"><code>torch.cummin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" target="_blank"><code>Tensor.cumprod</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod" target="_blank"><code>torch.cumprod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_" target="_blank"><code>Tensor.cumprod_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod" target="_blank"><code>cumprod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" target="_blank"><code>Tensor.cumsum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum" target="_blank"><code>torch.cumsum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_" target="_blank"><code>Tensor.cumsum_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum" target="_blank"><code>cumsum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.chalf.html#torch.Tensor.chalf" target="_blank"><code>Tensor.chalf</code></a></td>
<td><code>self.chalf()</code> is equivalent to <code>self.to(torch.complex32)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat" target="_blank"><code>Tensor.cfloat</code></a></td>
<td><code>self.cfloat()</code> is equivalent to <code>self.to(torch.complex64)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble" target="_blank"><code>Tensor.cdouble</code></a></td>
<td><code>self.cdouble()</code> is equivalent to <code>self.to(torch.complex128)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr" target="_blank"><code>Tensor.data_ptr</code></a></td>
<td>Returns the address of the first element of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad" target="_blank"><code>Tensor.deg2rad</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad" target="_blank"><code>torch.deg2rad()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize" target="_blank"><code>Tensor.dequantize</code></a></td>
<td>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.det.html#torch.Tensor.det" target="_blank"><code>Tensor.det</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.det.html#torch.det" target="_blank"><code>torch.det()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim" target="_blank"><code>Tensor.dense_dim</code></a></td>
<td>Return the number of dense dimensions in a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" target="_blank">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach" target="_blank"><code>Tensor.detach</code></a></td>
<td>Returns a new Tensor, detached from the current graph.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach_.html#torch.Tensor.detach_" target="_blank"><code>Tensor.detach_</code></a></td>
<td>Detaches the Tensor from the graph that created it, making it a leaf.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag.html#torch.Tensor.diag" target="_blank"><code>Tensor.diag</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag" target="_blank"><code>torch.diag()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed" target="_blank"><code>Tensor.diag_embed</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed" target="_blank"><code>torch.diag_embed()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat" target="_blank"><code>Tensor.diagflat</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat" target="_blank"><code>torch.diagflat()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal" target="_blank"><code>Tensor.diagonal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal" target="_blank"><code>torch.diagonal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter" target="_blank"><code>Tensor.diagonal_scatter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter" target="_blank"><code>torch.diagonal_scatter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_" target="_blank"><code>Tensor.fill_diagonal_</code></a></td>
<td>Fill the main diagonal of a tensor that has at least 2-dimensions.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmax.html#torch.Tensor.fmax" target="_blank"><code>Tensor.fmax</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax" target="_blank"><code>torch.fmax()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmin.html#torch.Tensor.fmin" target="_blank"><code>Tensor.fmin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin" target="_blank"><code>torch.fmin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.diff.html#torch.Tensor.diff" target="_blank"><code>Tensor.diff</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff" target="_blank"><code>torch.diff()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma" target="_blank"><code>Tensor.digamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma" target="_blank"><code>torch.digamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_" target="_blank"><code>Tensor.digamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma" target="_blank"><code>digamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim" target="_blank"><code>Tensor.dim</code></a></td>
<td>Returns the number of dimensions of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dist.html#torch.Tensor.dist" target="_blank"><code>Tensor.dist</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist" target="_blank"><code>torch.dist()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div" target="_blank"><code>Tensor.div</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.div.html#torch.div" target="_blank"><code>torch.div()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.div_.html#torch.Tensor.div_" target="_blank"><code>Tensor.div_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div" target="_blank"><code>div()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide" target="_blank"><code>Tensor.divide</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide" target="_blank"><code>torch.divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide_.html#torch.Tensor.divide_" target="_blank"><code>Tensor.divide_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide" target="_blank"><code>divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html#torch.Tensor.dot" target="_blank"><code>Tensor.dot</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot" target="_blank"><code>torch.dot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.double.html#torch.Tensor.double" target="_blank"><code>Tensor.double</code></a></td>
<td><code>self.double()</code> is equivalent to <code>self.to(torch.float64)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit" target="_blank"><code>Tensor.dsplit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit" target="_blank"><code>torch.dsplit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.element_size.html#torch.Tensor.element_size" target="_blank"><code>Tensor.element_size</code></a></td>
<td>Returns the size in bytes of an individual element.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq" target="_blank"><code>Tensor.eq</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq" target="_blank"><code>torch.eq()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq_.html#torch.Tensor.eq_" target="_blank"><code>Tensor.eq_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq" target="_blank"><code>eq()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.equal.html#torch.Tensor.equal" target="_blank"><code>Tensor.equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal" target="_blank"><code>torch.equal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf" target="_blank"><code>Tensor.erf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf" target="_blank"><code>torch.erf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf_.html#torch.Tensor.erf_" target="_blank"><code>Tensor.erf_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf" target="_blank"><code>erf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc" target="_blank"><code>Tensor.erfc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc" target="_blank"><code>torch.erfc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_" target="_blank"><code>Tensor.erfc_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc" target="_blank"><code>erfc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" target="_blank"><code>Tensor.erfinv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv" target="_blank"><code>torch.erfinv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_" target="_blank"><code>Tensor.erfinv_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv" target="_blank"><code>erfinv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp" target="_blank"><code>Tensor.exp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp" target="_blank"><code>torch.exp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp_.html#torch.Tensor.exp_" target="_blank"><code>Tensor.exp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp" target="_blank"><code>exp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1" target="_blank"><code>Tensor.expm1</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1" target="_blank"><code>torch.expm1()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_" target="_blank"><code>Tensor.expm1_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1" target="_blank"><code>expm1()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand" target="_blank"><code>Tensor.expand</code></a></td>
<td>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as" target="_blank"><code>Tensor.expand_as</code></a></td>
<td>Expand this tensor to the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_" target="_blank"><code>Tensor.exponential_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the exponential distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix" target="_blank"><code>Tensor.fix</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix" target="_blank"><code>torch.fix()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix_.html#torch.Tensor.fix_" target="_blank"><code>Tensor.fix_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix" target="_blank"><code>fix()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_.html#torch.Tensor.fill_" target="_blank"><code>Tensor.fill_</code></a></td>
<td>Fills <code>self</code> tensor with the specified value.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.flatten.html#torch.Tensor.flatten" target="_blank"><code>Tensor.flatten</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten" target="_blank"><code>torch.flatten()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.flip.html#torch.Tensor.flip" target="_blank"><code>Tensor.flip</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip" target="_blank"><code>torch.flip()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr" target="_blank"><code>Tensor.fliplr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr" target="_blank"><code>torch.fliplr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.flipud.html#torch.Tensor.flipud" target="_blank"><code>Tensor.flipud</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud" target="_blank"><code>torch.flipud()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float" target="_blank"><code>Tensor.float</code></a></td>
<td><code>self.float()</code> is equivalent to <code>self.to(torch.float32)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power" target="_blank"><code>Tensor.float_power</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power" target="_blank"><code>torch.float_power()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_" target="_blank"><code>Tensor.float_power_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power" target="_blank"><code>float_power()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor" target="_blank"><code>Tensor.floor</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor" target="_blank"><code>torch.floor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_.html#torch.Tensor.floor_" target="_blank"><code>Tensor.floor_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor" target="_blank"><code>floor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" target="_blank"><code>Tensor.floor_divide</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide" target="_blank"><code>torch.floor_divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_" target="_blank"><code>Tensor.floor_divide_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide" target="_blank"><code>floor_divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod" target="_blank"><code>Tensor.fmod</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod" target="_blank"><code>torch.fmod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_" target="_blank"><code>Tensor.fmod_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod" target="_blank"><code>fmod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac" target="_blank"><code>Tensor.frac</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac" target="_blank"><code>torch.frac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac_.html#torch.Tensor.frac_" target="_blank"><code>Tensor.frac_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac" target="_blank"><code>frac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.frexp.html#torch.Tensor.frexp" target="_blank"><code>Tensor.frexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp" target="_blank"><code>torch.frexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html#torch.Tensor.gather" target="_blank"><code>Tensor.gather</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather" target="_blank"><code>torch.gather()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd" target="_blank"><code>Tensor.gcd</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd" target="_blank"><code>torch.gcd()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_" target="_blank"><code>Tensor.gcd_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd" target="_blank"><code>gcd()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge" target="_blank"><code>Tensor.ge</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge" target="_blank"><code>torch.ge()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge_.html#torch.Tensor.ge_" target="_blank"><code>Tensor.ge_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge" target="_blank"><code>ge()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" target="_blank"><code>Tensor.greater_equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal" target="_blank"><code>torch.greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_" target="_blank"><code>Tensor.greater_equal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal" target="_blank"><code>greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_" target="_blank"><code>Tensor.geometric_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the geometric distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf" target="_blank"><code>Tensor.geqrf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf" target="_blank"><code>torch.geqrf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ger.html#torch.Tensor.ger" target="_blank"><code>Tensor.ger</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger" target="_blank"><code>torch.ger()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html#torch.Tensor.get_device" target="_blank"><code>Tensor.get_device</code></a></td>
<td>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt" target="_blank"><code>Tensor.gt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt" target="_blank"><code>torch.gt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt_.html#torch.Tensor.gt_" target="_blank"><code>Tensor.gt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt" target="_blank"><code>gt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater" target="_blank"><code>Tensor.greater</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater" target="_blank"><code>torch.greater()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_.html#torch.Tensor.greater_" target="_blank"><code>Tensor.greater_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater" target="_blank"><code>greater()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html#torch.Tensor.half" target="_blank"><code>Tensor.half</code></a></td>
<td><code>self.half()</code> is equivalent to <code>self.to(torch.float16)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink" target="_blank"><code>Tensor.hardshrink</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink" target="_blank"><code>torch.nn.functional.hardshrink()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside" target="_blank"><code>Tensor.heaviside</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside" target="_blank"><code>torch.heaviside()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.histc.html#torch.Tensor.histc" target="_blank"><code>Tensor.histc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc" target="_blank"><code>torch.histc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.histogram.html#torch.Tensor.histogram" target="_blank"><code>Tensor.histogram</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.histogram.html#torch.histogram" target="_blank"><code>torch.histogram()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit" target="_blank"><code>Tensor.hsplit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit" target="_blank"><code>torch.hsplit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot" target="_blank"><code>Tensor.hypot</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot" target="_blank"><code>torch.hypot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_" target="_blank"><code>Tensor.hypot_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot" target="_blank"><code>hypot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0" target="_blank"><code>Tensor.i0</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0" target="_blank"><code>torch.i0()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0_.html#torch.Tensor.i0_" target="_blank"><code>Tensor.i0_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0" target="_blank"><code>i0()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma" target="_blank"><code>Tensor.igamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma" target="_blank"><code>torch.igamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_" target="_blank"><code>Tensor.igamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma" target="_blank"><code>igamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac" target="_blank"><code>Tensor.igammac</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac" target="_blank"><code>torch.igammac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_" target="_blank"><code>Tensor.igammac_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac" target="_blank"><code>igammac()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" target="_blank"><code>Tensor.index_add_</code></a></td>
<td>Accumulate the elements of <code>alpha</code> times <code>source</code> into the <code>self</code> tensor by adding to the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add.html#torch.Tensor.index_add" target="_blank"><code>Tensor.index_add</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_" target="_blank"><code>torch.Tensor.index_add_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" target="_blank"><code>Tensor.index_copy_</code></a></td>
<td>Copies the elements of <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank"><code>tensor</code></a> into the <code>self</code> tensor by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy" target="_blank"><code>Tensor.index_copy</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_" target="_blank"><code>torch.Tensor.index_copy_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" target="_blank"><code>Tensor.index_fill_</code></a></td>
<td>Fills the elements of the <code>self</code> tensor with value <code>value</code> by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill" target="_blank"><code>Tensor.index_fill</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_" target="_blank"><code>torch.Tensor.index_fill_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" target="_blank"><code>Tensor.index_put_</code></a></td>
<td>Puts values from the tensor <code>values</code> into the tensor <code>self</code> using the indices specified in <code>indices</code> (which is a tuple of Tensors).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put.html#torch.Tensor.index_put" target="_blank"><code>Tensor.index_put</code></a></td>
<td>Out-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_" target="_blank"><code>index_put_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_" target="_blank"><code>Tensor.index_reduce_</code></a></td>
<td>Accumulate the elements of <code>source</code> into the <code>self</code> tensor by accumulating to the indices in the order given in <code>index</code> using the reduction given by the <code>reduce</code> argument.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce" target="_blank"><code>Tensor.index_reduce</code></a></td>
<td></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_select.html#torch.Tensor.index_select" target="_blank"><code>Tensor.index_select</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select" target="_blank"><code>torch.index_select()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.indices.html#torch.Tensor.indices" target="_blank"><code>Tensor.indices</code></a></td>
<td>Return the indices tensor of a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" target="_blank">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.inner.html#torch.Tensor.inner" target="_blank"><code>Tensor.inner</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner" target="_blank"><code>torch.inner()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.int.html#torch.Tensor.int" target="_blank"><code>Tensor.int</code></a></td>
<td><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr" target="_blank"><code>Tensor.int_repr</code></a></td>
<td>Given a quantized Tensor, <code>self.int_repr()</code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.inverse.html#torch.Tensor.inverse" target="_blank"><code>Tensor.inverse</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse" target="_blank"><code>torch.inverse()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isclose.html#torch.Tensor.isclose" target="_blank"><code>Tensor.isclose</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose" target="_blank"><code>torch.isclose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite" target="_blank"><code>Tensor.isfinite</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite" target="_blank"><code>torch.isfinite()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isinf.html#torch.Tensor.isinf" target="_blank"><code>Tensor.isinf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf" target="_blank"><code>torch.isinf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf" target="_blank"><code>Tensor.isposinf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf" target="_blank"><code>torch.isposinf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf" target="_blank"><code>Tensor.isneginf</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf" target="_blank"><code>torch.isneginf()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isnan.html#torch.Tensor.isnan" target="_blank"><code>Tensor.isnan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan" target="_blank"><code>torch.isnan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous" target="_blank"><code>Tensor.is_contiguous</code></a></td>
<td>Returns True if <code>self</code> tensor is contiguous in memory in the order specified by memory format.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex" target="_blank"><code>Tensor.is_complex</code></a></td>
<td>Returns True if the data type of <code>self</code> is a complex data type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj" target="_blank"><code>Tensor.is_conj</code></a></td>
<td>Returns True if the conjugate bit of <code>self</code> is set to true.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point" target="_blank"><code>Tensor.is_floating_point</code></a></td>
<td>Returns True if the data type of <code>self</code> is a floating point data type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference" target="_blank"><code>Tensor.is_inference</code></a></td>
<td>See <code>torch.is_inference()</code></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf" target="_blank"><code>Tensor.is_leaf</code></a></td>
<td>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned" target="_blank"><code>Tensor.is_pinned</code></a></td>
<td>Returns true if this tensor resides in pinned memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to" target="_blank"><code>Tensor.is_set_to</code></a></td>
<td>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared" target="_blank"><code>Tensor.is_shared</code></a></td>
<td>Checks if tensor is in shared memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed" target="_blank"><code>Tensor.is_signed</code></a></td>
<td>Returns True if the data type of <code>self</code> is a signed data type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse" target="_blank"><code>Tensor.is_sparse</code></a></td>
<td>Is <code>True</code> if the Tensor uses sparse storage layout, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.istft.html#torch.Tensor.istft" target="_blank"><code>Tensor.istft</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft" target="_blank"><code>torch.istft()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.isreal.html#torch.Tensor.isreal" target="_blank"><code>Tensor.isreal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal" target="_blank"><code>torch.isreal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item" target="_blank"><code>Tensor.item</code></a></td>
<td>Returns the value of this tensor as a standard Python number.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue" target="_blank"><code>Tensor.kthvalue</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue" target="_blank"><code>torch.kthvalue()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm" target="_blank"><code>Tensor.lcm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm" target="_blank"><code>torch.lcm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_" target="_blank"><code>Tensor.lcm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm" target="_blank"><code>lcm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" target="_blank"><code>Tensor.ldexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp" target="_blank"><code>torch.ldexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_" target="_blank"><code>Tensor.ldexp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp" target="_blank"><code>ldexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le" target="_blank"><code>Tensor.le</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.le.html#torch.le" target="_blank"><code>torch.le()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.le_.html#torch.Tensor.le_" target="_blank"><code>Tensor.le_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le" target="_blank"><code>le()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" target="_blank"><code>Tensor.less_equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal" target="_blank"><code>torch.less_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_" target="_blank"><code>Tensor.less_equal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal" target="_blank"><code>less_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp" target="_blank"><code>Tensor.lerp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp" target="_blank"><code>torch.lerp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_" target="_blank"><code>Tensor.lerp_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp" target="_blank"><code>lerp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" target="_blank"><code>Tensor.lgamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma" target="_blank"><code>torch.lgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_" target="_blank"><code>Tensor.lgamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma" target="_blank"><code>lgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log" target="_blank"><code>Tensor.log</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log.html#torch.log" target="_blank"><code>torch.log()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_.html#torch.Tensor.log_" target="_blank"><code>Tensor.log_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log" target="_blank"><code>log()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logdet.html#torch.Tensor.logdet" target="_blank"><code>Tensor.logdet</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet" target="_blank"><code>torch.logdet()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10" target="_blank"><code>Tensor.log10</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10" target="_blank"><code>torch.log10()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10_.html#torch.Tensor.log10_" target="_blank"><code>Tensor.log10_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10" target="_blank"><code>log10()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p" target="_blank"><code>Tensor.log1p</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p" target="_blank"><code>torch.log1p()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_" target="_blank"><code>Tensor.log1p_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p" target="_blank"><code>log1p()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2" target="_blank"><code>Tensor.log2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2" target="_blank"><code>torch.log2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2_.html#torch.Tensor.log2_" target="_blank"><code>Tensor.log2_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2" target="_blank"><code>log2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_" target="_blank"><code>Tensor.log_normal_</code></a></td>
<td>Fills <code>self</code> tensor with numbers samples from the log-normal distribution parameterized by the given mean \mu<em>μ</em> and standard deviation \sigma<em>σ</em>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp" target="_blank"><code>Tensor.logaddexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp" target="_blank"><code>torch.logaddexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2" target="_blank"><code>Tensor.logaddexp2</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2" target="_blank"><code>torch.logaddexp2()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp" target="_blank"><code>Tensor.logsumexp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp" target="_blank"><code>torch.logsumexp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" target="_blank"><code>Tensor.logical_and</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and" target="_blank"><code>torch.logical_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_" target="_blank"><code>Tensor.logical_and_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and" target="_blank"><code>logical_and()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" target="_blank"><code>Tensor.logical_not</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not" target="_blank"><code>torch.logical_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_" target="_blank"><code>Tensor.logical_not_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not" target="_blank"><code>logical_not()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" target="_blank"><code>Tensor.logical_or</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or" target="_blank"><code>torch.logical_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_" target="_blank"><code>Tensor.logical_or_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or" target="_blank"><code>logical_or()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" target="_blank"><code>Tensor.logical_xor</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor" target="_blank"><code>torch.logical_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_" target="_blank"><code>Tensor.logical_xor_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor" target="_blank"><code>logical_xor()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit" target="_blank"><code>Tensor.logit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit" target="_blank"><code>torch.logit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit_.html#torch.Tensor.logit_" target="_blank"><code>Tensor.logit_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit" target="_blank"><code>logit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.long.html#torch.Tensor.long" target="_blank"><code>Tensor.long</code></a></td>
<td><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt" target="_blank"><code>Tensor.lt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt" target="_blank"><code>torch.lt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt_.html#torch.Tensor.lt_" target="_blank"><code>Tensor.lt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt" target="_blank"><code>lt()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less" target="_blank"><code>Tensor.less</code></a></td>
<td>lt(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_.html#torch.Tensor.less_" target="_blank"><code>Tensor.less_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less" target="_blank"><code>less()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu.html#torch.Tensor.lu" target="_blank"><code>Tensor.lu</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu" target="_blank"><code>torch.lu()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve" target="_blank"><code>Tensor.lu_solve</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve" target="_blank"><code>torch.lu_solve()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass" target="_blank"><code>Tensor.as_subclass</code></a></td>
<td>Makes a <code>cls</code> instance with the same data pointer as <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.map_.html#torch.Tensor.map_" target="_blank"><code>Tensor.map_</code></a></td>
<td>Applies <code>callable</code> for each element in <code>self</code> tensor and the given <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank"><code>tensor</code></a> and stores the results in <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" target="_blank"><code>Tensor.masked_scatter_</code></a></td>
<td>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is True.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter" target="_blank"><code>Tensor.masked_scatter</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_" target="_blank"><code>torch.Tensor.masked_scatter_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" target="_blank"><code>Tensor.masked_fill_</code></a></td>
<td>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is True.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill" target="_blank"><code>Tensor.masked_fill</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_" target="_blank"><code>torch.Tensor.masked_fill_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select" target="_blank"><code>Tensor.masked_select</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select" target="_blank"><code>torch.masked_select()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matmul.html#torch.Tensor.matmul" target="_blank"><code>Tensor.matmul</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul" target="_blank"><code>torch.matmul()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power" target="_blank"><code>Tensor.matrix_power</code></a></td>
<td>NOTE<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power" target="_blank"><code>matrix_power()</code></a> is deprecated, use <a href="https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power" target="_blank"><code>torch.linalg.matrix_power()</code></a> instead.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp" target="_blank"><code>Tensor.matrix_exp</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp" target="_blank"><code>torch.matrix_exp()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.max.html#torch.Tensor.max" target="_blank"><code>Tensor.max</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.max.html#torch.max" target="_blank"><code>torch.max()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.maximum.html#torch.Tensor.maximum" target="_blank"><code>Tensor.maximum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum" target="_blank"><code>torch.maximum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean" target="_blank"><code>Tensor.mean</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean" target="_blank"><code>torch.mean()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean" target="_blank"><code>Tensor.nanmean</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nanmean.html#torch.nanmean" target="_blank"><code>torch.nanmean()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.median.html#torch.Tensor.median" target="_blank"><code>Tensor.median</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.median.html#torch.median" target="_blank"><code>torch.median()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian" target="_blank"><code>Tensor.nanmedian</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian" target="_blank"><code>torch.nanmedian()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.min.html#torch.Tensor.min" target="_blank"><code>Tensor.min</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.min.html#torch.min" target="_blank"><code>torch.min()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.minimum.html#torch.Tensor.minimum" target="_blank"><code>Tensor.minimum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum" target="_blank"><code>torch.minimum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mm.html#torch.Tensor.mm" target="_blank"><code>Tensor.mm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm" target="_blank"><code>torch.mm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.smm.html#torch.Tensor.smm" target="_blank"><code>Tensor.smm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.smm.html#torch.smm" target="_blank"><code>torch.smm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mode.html#torch.Tensor.mode" target="_blank"><code>Tensor.mode</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode" target="_blank"><code>torch.mode()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.movedim.html#torch.Tensor.movedim" target="_blank"><code>Tensor.movedim</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim" target="_blank"><code>torch.movedim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis" target="_blank"><code>Tensor.moveaxis</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis" target="_blank"><code>torch.moveaxis()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.msort.html#torch.Tensor.msort" target="_blank"><code>Tensor.msort</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort" target="_blank"><code>torch.msort()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul" target="_blank"><code>Tensor.mul</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul" target="_blank"><code>torch.mul()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul_.html#torch.Tensor.mul_" target="_blank"><code>Tensor.mul_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul" target="_blank"><code>mul()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply" target="_blank"><code>Tensor.multiply</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply" target="_blank"><code>torch.multiply()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_" target="_blank"><code>Tensor.multiply_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply" target="_blank"><code>multiply()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial" target="_blank"><code>Tensor.multinomial</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial" target="_blank"><code>torch.multinomial()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mv.html#torch.Tensor.mv" target="_blank"><code>Tensor.mv</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv" target="_blank"><code>torch.mv()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" target="_blank"><code>Tensor.mvlgamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma" target="_blank"><code>torch.mvlgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_" target="_blank"><code>Tensor.mvlgamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma" target="_blank"><code>mvlgamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nansum.html#torch.Tensor.nansum" target="_blank"><code>Tensor.nansum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum" target="_blank"><code>torch.nansum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow.html#torch.Tensor.narrow" target="_blank"><code>Tensor.narrow</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow" target="_blank"><code>torch.narrow()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy" target="_blank"><code>Tensor.narrow_copy</code></a></td>
<td>See <code>torch.narrow_copy()</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension" target="_blank"><code>Tensor.ndimension</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim" target="_blank"><code>dim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" target="_blank"><code>Tensor.nan_to_num</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num" target="_blank"><code>torch.nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_" target="_blank"><code>Tensor.nan_to_num_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num" target="_blank"><code>nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne" target="_blank"><code>Tensor.ne</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne" target="_blank"><code>torch.ne()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne_.html#torch.Tensor.ne_" target="_blank"><code>Tensor.ne_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne" target="_blank"><code>ne()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" target="_blank"><code>Tensor.not_equal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal" target="_blank"><code>torch.not_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_" target="_blank"><code>Tensor.not_equal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal" target="_blank"><code>not_equal()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg" target="_blank"><code>Tensor.neg</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg" target="_blank"><code>torch.neg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg_.html#torch.Tensor.neg_" target="_blank"><code>Tensor.neg_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg" target="_blank"><code>neg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative" target="_blank"><code>Tensor.negative</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative" target="_blank"><code>torch.negative()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative_.html#torch.Tensor.negative_" target="_blank"><code>Tensor.negative_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative" target="_blank"><code>negative()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nelement.html#torch.Tensor.nelement" target="_blank"><code>Tensor.nelement</code></a></td>
<td>Alias for <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel" target="_blank"><code>numel()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" target="_blank"><code>Tensor.nextafter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter" target="_blank"><code>torch.nextafter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_" target="_blank"><code>Tensor.nextafter_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter" target="_blank"><code>nextafter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero" target="_blank"><code>Tensor.nonzero</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero" target="_blank"><code>torch.nonzero()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm" target="_blank"><code>Tensor.norm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm" target="_blank"><code>torch.norm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_" target="_blank"><code>Tensor.normal_</code></a></td>
<td>Fills <code>self</code> tensor with elements samples from the normal distribution parameterized by <a href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean" target="_blank"><code>mean</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std" target="_blank"><code>std</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel" target="_blank"><code>Tensor.numel</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel" target="_blank"><code>torch.numel()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy" target="_blank"><code>Tensor.numpy</code></a></td>
<td>Returns the tensor as a NumPy <code>ndarray</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr" target="_blank"><code>Tensor.orgqr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr" target="_blank"><code>torch.orgqr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr" target="_blank"><code>Tensor.ormqr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr" target="_blank"><code>torch.ormqr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.outer.html#torch.Tensor.outer" target="_blank"><code>Tensor.outer</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer" target="_blank"><code>torch.outer()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html#torch.Tensor.permute" target="_blank"><code>Tensor.permute</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute" target="_blank"><code>torch.permute()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory" target="_blank"><code>Tensor.pin_memory</code></a></td>
<td>Copies the tensor to pinned memory, if it's not already pinned.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse" target="_blank"><code>Tensor.pinverse</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse" target="_blank"><code>torch.pinverse()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" target="_blank"><code>Tensor.polygamma</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma" target="_blank"><code>torch.polygamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_" target="_blank"><code>Tensor.polygamma_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma" target="_blank"><code>polygamma()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.positive.html#torch.Tensor.positive" target="_blank"><code>Tensor.positive</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive" target="_blank"><code>torch.positive()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow" target="_blank"><code>Tensor.pow</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow" target="_blank"><code>torch.pow()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_" target="_blank"><code>Tensor.pow_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow" target="_blank"><code>pow()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.prod.html#torch.Tensor.prod" target="_blank"><code>Tensor.prod</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod" target="_blank"><code>torch.prod()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_" target="_blank"><code>Tensor.put_</code></a></td>
<td>Copies the elements from <code>source</code> into the positions specified by <code>index</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.qr.html#torch.Tensor.qr" target="_blank"><code>Tensor.qr</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr" target="_blank"><code>torch.qr()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme" target="_blank"><code>Tensor.qscheme</code></a></td>
<td>Returns the quantization scheme of a given QTensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.quantile.html#torch.Tensor.quantile" target="_blank"><code>Tensor.quantile</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile" target="_blank"><code>torch.quantile()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile" target="_blank"><code>Tensor.nanquantile</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile" target="_blank"><code>torch.nanquantile()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale" target="_blank"><code>Tensor.q_scale</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point" target="_blank"><code>Tensor.q_zero_point</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales" target="_blank"><code>Tensor.q_per_channel_scales</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points" target="_blank"><code>Tensor.q_per_channel_zero_points</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis" target="_blank"><code>Tensor.q_per_channel_axis</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg" target="_blank"><code>Tensor.rad2deg</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg" target="_blank"><code>torch.rad2deg()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_" target="_blank"><code>Tensor.random_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the discrete uniform distribution over <code>[from, to - 1]</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.ravel.html#torch.Tensor.ravel" target="_blank"><code>Tensor.ravel</code></a></td>
<td>see <a href="https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel" target="_blank"><code>torch.ravel()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" target="_blank"><code>Tensor.reciprocal</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal" target="_blank"><code>torch.reciprocal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_" target="_blank"><code>Tensor.reciprocal_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal" target="_blank"><code>reciprocal()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream" target="_blank"><code>Tensor.record_stream</code></a></td>
<td>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook" target="_blank"><code>Tensor.register_hook</code></a></td>
<td>Registers a backward hook.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder" target="_blank"><code>Tensor.remainder</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder" target="_blank"><code>torch.remainder()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_" target="_blank"><code>Tensor.remainder_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder" target="_blank"><code>remainder()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm" target="_blank"><code>Tensor.renorm</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm" target="_blank"><code>torch.renorm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_" target="_blank"><code>Tensor.renorm_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm" target="_blank"><code>renorm()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat" target="_blank"><code>Tensor.repeat</code></a></td>
<td>Repeats this tensor along the specified dimensions.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave" target="_blank"><code>Tensor.repeat_interleave</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave" target="_blank"><code>torch.repeat_interleave()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad" target="_blank"><code>Tensor.requires_grad</code></a></td>
<td>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_" target="_blank"><code>Tensor.requires_grad_</code></a></td>
<td>Change if autograd should record operations on this tensor: sets this tensor's <code>requires_grad</code> attribute in-place.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape" target="_blank"><code>Tensor.reshape</code></a></td>
<td>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as" target="_blank"><code>Tensor.reshape_as</code></a></td>
<td>Returns this tensor as the same shape as <code>other</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_" target="_blank"><code>Tensor.resize_</code></a></td>
<td>Resizes <code>self</code> tensor to the specified size.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_" target="_blank"><code>Tensor.resize_as_</code></a></td>
<td>Resizes the <code>self</code> tensor to be the same size as the specified <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank"><code>tensor</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad" target="_blank"><code>Tensor.retain_grad</code></a></td>
<td>Enables this Tensor to have their <code>grad</code> populated during <code>backward()</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad" target="_blank"><code>Tensor.retains_grad</code></a></td>
<td>Is <code>True</code> if this Tensor is non-leaf and its <code>grad</code> is enabled to be populated during <code>backward()</code>, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.roll.html#torch.Tensor.roll" target="_blank"><code>Tensor.roll</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll" target="_blank"><code>torch.roll()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rot90.html#torch.Tensor.rot90" target="_blank"><code>Tensor.rot90</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90" target="_blank"><code>torch.rot90()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round" target="_blank"><code>Tensor.round</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round" target="_blank"><code>torch.round()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round_.html#torch.Tensor.round_" target="_blank"><code>Tensor.round_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round" target="_blank"><code>round()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" target="_blank"><code>Tensor.rsqrt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt" target="_blank"><code>torch.rsqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_" target="_blank"><code>Tensor.rsqrt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt" target="_blank"><code>rsqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter.html#torch.Tensor.scatter" target="_blank"><code>Tensor.scatter</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" target="_blank"><code>torch.Tensor.scatter_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" target="_blank"><code>Tensor.scatter_</code></a></td>
<td>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" target="_blank"><code>Tensor.scatter_add_</code></a></td>
<td>Adds all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor in a similar fashion as <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_" target="_blank"><code>scatter_()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add" target="_blank"><code>Tensor.scatter_add</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_" target="_blank"><code>torch.Tensor.scatter_add_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" target="_blank"><code>Tensor.scatter_reduce_</code></a></td>
<td>Reduces all values from the <code>src</code> tensor to the indices specified in the <code>index</code> tensor in the <code>self</code> tensor using the applied reduction defined via the <code>reduce</code> argument (<code>"sum"</code>, <code>"prod"</code>, <code>"mean"</code>, <code>"amax"</code>, <code>"amin"</code>).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce" target="_blank"><code>Tensor.scatter_reduce</code></a></td>
<td>Out-of-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_" target="_blank"><code>torch.Tensor.scatter_reduce_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.select.html#torch.Tensor.select" target="_blank"><code>Tensor.select</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.select.html#torch.select" target="_blank"><code>torch.select()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter" target="_blank"><code>Tensor.select_scatter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.select_scatter.html#torch.select_scatter" target="_blank"><code>torch.select_scatter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html#torch.Tensor.set_" target="_blank"><code>Tensor.set_</code></a></td>
<td>Sets the underlying storage, size, and strides.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_" target="_blank"><code>Tensor.share_memory_</code></a></td>
<td>Moves the underlying storage to shared memory.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.short.html#torch.Tensor.short" target="_blank"><code>Tensor.short</code></a></td>
<td><code>self.short()</code> is equivalent to <code>self.to(torch.int16)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" target="_blank"><code>Tensor.sigmoid</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid" target="_blank"><code>torch.sigmoid()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_" target="_blank"><code>Tensor.sigmoid_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid" target="_blank"><code>sigmoid()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign" target="_blank"><code>Tensor.sign</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign" target="_blank"><code>torch.sign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign_.html#torch.Tensor.sign_" target="_blank"><code>Tensor.sign_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign" target="_blank"><code>sign()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.signbit.html#torch.Tensor.signbit" target="_blank"><code>Tensor.signbit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit" target="_blank"><code>torch.signbit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn" target="_blank"><code>Tensor.sgn</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn" target="_blank"><code>torch.sgn()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_" target="_blank"><code>Tensor.sgn_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn" target="_blank"><code>sgn()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin" target="_blank"><code>Tensor.sin</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin" target="_blank"><code>torch.sin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin_.html#torch.Tensor.sin_" target="_blank"><code>Tensor.sin_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin" target="_blank"><code>sin()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc" target="_blank"><code>Tensor.sinc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc" target="_blank"><code>torch.sinc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_" target="_blank"><code>Tensor.sinc_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc" target="_blank"><code>sinc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh" target="_blank"><code>Tensor.sinh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh" target="_blank"><code>torch.sinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_" target="_blank"><code>Tensor.sinh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh" target="_blank"><code>sinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh" target="_blank"><code>Tensor.asinh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh" target="_blank"><code>torch.asinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_" target="_blank"><code>Tensor.asinh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh" target="_blank"><code>asinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" target="_blank"><code>Tensor.arcsinh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh" target="_blank"><code>torch.arcsinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_" target="_blank"><code>Tensor.arcsinh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh" target="_blank"><code>arcsinh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.size.html#torch.Tensor.size" target="_blank"><code>Tensor.size</code></a></td>
<td>Returns the size of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet" target="_blank"><code>Tensor.slogdet</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet" target="_blank"><code>torch.slogdet()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter" target="_blank"><code>Tensor.slice_scatter</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.slice_scatter.html#torch.slice_scatter" target="_blank"><code>torch.slice_scatter()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sort.html#torch.Tensor.sort" target="_blank"><code>Tensor.sort</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort" target="_blank"><code>torch.sort()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.split.html#torch.Tensor.split" target="_blank"><code>Tensor.split</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split" target="_blank"><code>torch.split()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask" target="_blank"><code>Tensor.sparse_mask</code></a></td>
<td>Returns a new <a href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" target="_blank">sparse tensor</a> with values from a strided tensor <code>self</code> filtered by the indices of the sparse tensor <code>mask</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim" target="_blank"><code>Tensor.sparse_dim</code></a></td>
<td>Return the number of sparse dimensions in a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-docs" target="_blank">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" target="_blank"><code>Tensor.sqrt</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt" target="_blank"><code>torch.sqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_" target="_blank"><code>Tensor.sqrt_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt" target="_blank"><code>sqrt()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square" target="_blank"><code>Tensor.square</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.square.html#torch.square" target="_blank"><code>torch.square()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.square_.html#torch.Tensor.square_" target="_blank"><code>Tensor.square_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square" target="_blank"><code>square()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" target="_blank"><code>Tensor.squeeze</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze" target="_blank"><code>torch.squeeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_" target="_blank"><code>Tensor.squeeze_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze" target="_blank"><code>squeeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.std.html#torch.Tensor.std" target="_blank"><code>Tensor.std</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std" target="_blank"><code>torch.std()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.stft.html#torch.Tensor.stft" target="_blank"><code>Tensor.stft</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft" target="_blank"><code>torch.stft()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage.html#torch.Tensor.storage" target="_blank"><code>Tensor.storage</code></a></td>
<td>Returns the underlying storage.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset" target="_blank"><code>Tensor.storage_offset</code></a></td>
<td>Returns <code>self</code> tensor's offset in the underlying storage in terms of number of storage elements (not bytes).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type" target="_blank"><code>Tensor.storage_type</code></a></td>
<td>Returns the type of the underlying storage.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html#torch.Tensor.stride" target="_blank"><code>Tensor.stride</code></a></td>
<td>Returns the stride of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub" target="_blank"><code>Tensor.sub</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub" target="_blank"><code>torch.sub()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub_.html#torch.Tensor.sub_" target="_blank"><code>Tensor.sub_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub" target="_blank"><code>sub()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract" target="_blank"><code>Tensor.subtract</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract" target="_blank"><code>torch.subtract()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_" target="_blank"><code>Tensor.subtract_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract" target="_blank"><code>subtract()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum" target="_blank"><code>Tensor.sum</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum" target="_blank"><code>torch.sum()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size" target="_blank"><code>Tensor.sum_to_size</code></a></td>
<td>Sum <code>this</code> tensor to <code>size</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.svd.html#torch.Tensor.svd" target="_blank"><code>Tensor.svd</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd" target="_blank"><code>torch.svd()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes" target="_blank"><code>Tensor.swapaxes</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes" target="_blank"><code>torch.swapaxes()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims" target="_blank"><code>Tensor.swapdims</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims" target="_blank"><code>torch.swapdims()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.symeig.html#torch.Tensor.symeig" target="_blank"><code>Tensor.symeig</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig" target="_blank"><code>torch.symeig()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t" target="_blank"><code>Tensor.t</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.t.html#torch.t" target="_blank"><code>torch.t()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.t_.html#torch.Tensor.t_" target="_blank"><code>Tensor.t_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t" target="_blank"><code>t()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split" target="_blank"><code>Tensor.tensor_split</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split" target="_blank"><code>torch.tensor_split()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tile.html#torch.Tensor.tile" target="_blank"><code>Tensor.tile</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile" target="_blank"><code>torch.tile()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to" target="_blank"><code>Tensor.to</code></a></td>
<td>Performs Tensor dtype and/or device conversion.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn" target="_blank"><code>Tensor.to_mkldnn</code></a></td>
<td>Returns a copy of the tensor in <code>torch.mkldnn</code> layout.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.take.html#torch.Tensor.take" target="_blank"><code>Tensor.take</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.take.html#torch.take" target="_blank"><code>torch.take()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim" target="_blank"><code>Tensor.take_along_dim</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim" target="_blank"><code>torch.take_along_dim()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan" target="_blank"><code>Tensor.tan</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan" target="_blank"><code>torch.tan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan_.html#torch.Tensor.tan_" target="_blank"><code>Tensor.tan_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan" target="_blank"><code>tan()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh" target="_blank"><code>Tensor.tanh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh" target="_blank"><code>torch.tanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_" target="_blank"><code>Tensor.tanh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh" target="_blank"><code>tanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh" target="_blank"><code>Tensor.atanh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh" target="_blank"><code>torch.atanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_" target="_blank"><code>Tensor.atanh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh" target="_blank"><code>atanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" target="_blank"><code>Tensor.arctanh</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh" target="_blank"><code>torch.arctanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_" target="_blank"><code>Tensor.arctanh_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh" target="_blank"><code>arctanh()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist" target="_blank"><code>Tensor.tolist</code></a></td>
<td>Returns the tensor as a (nested) list.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html#torch.Tensor.topk" target="_blank"><code>Tensor.topk</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk" target="_blank"><code>torch.topk()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense" target="_blank"><code>Tensor.to_dense</code></a></td>
<td>Creates a strided copy of <code>self</code> if <code>self</code> is not a strided tensor, otherwise returns <code>self</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse" target="_blank"><code>Tensor.to_sparse</code></a></td>
<td>Returns a sparse copy of the tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr" target="_blank"><code>Tensor.to_sparse_csr</code></a></td>
<td>Convert a tensor to compressed row storage format (CSR).</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc" target="_blank"><code>Tensor.to_sparse_csc</code></a></td>
<td>Convert a tensor to compressed column storage (CSC) format.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr" target="_blank"><code>Tensor.to_sparse_bsr</code></a></td>
<td>Convert a CSR tensor to a block sparse row (BSR) storage format of given blocksize.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc" target="_blank"><code>Tensor.to_sparse_bsc</code></a></td>
<td>Convert a CSR tensor to a block sparse column (BSC) storage format of given blocksize.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trace.html#torch.Tensor.trace" target="_blank"><code>Tensor.trace</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace" target="_blank"><code>torch.trace()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose" target="_blank"><code>Tensor.transpose</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose" target="_blank"><code>torch.transpose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_" target="_blank"><code>Tensor.transpose_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose" target="_blank"><code>transpose()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve" target="_blank"><code>Tensor.triangular_solve</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve" target="_blank"><code>torch.triangular_solve()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril" target="_blank"><code>Tensor.tril</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril" target="_blank"><code>torch.tril()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril_.html#torch.Tensor.tril_" target="_blank"><code>Tensor.tril_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril" target="_blank"><code>tril()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu" target="_blank"><code>Tensor.triu</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu" target="_blank"><code>torch.triu()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu_.html#torch.Tensor.triu_" target="_blank"><code>Tensor.triu_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu" target="_blank"><code>triu()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide" target="_blank"><code>Tensor.true_divide</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide" target="_blank"><code>torch.true_divide()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" target="_blank"><code>Tensor.true_divide_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_" target="_blank"><code>true_divide_()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc" target="_blank"><code>Tensor.trunc</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc" target="_blank"><code>torch.trunc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_" target="_blank"><code>Tensor.trunc_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc" target="_blank"><code>trunc()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.type.html#torch.Tensor.type" target="_blank"><code>Tensor.type</code></a></td>
<td>Returns the type if dtype is not provided, else casts this object to the specified type.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.type_as.html#torch.Tensor.type_as" target="_blank"><code>Tensor.type_as</code></a></td>
<td>Returns this tensor cast to the type of the given tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unbind.html#torch.Tensor.unbind" target="_blank"><code>Tensor.unbind</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind" target="_blank"><code>torch.unbind()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten" target="_blank"><code>Tensor.unflatten</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten" target="_blank"><code>torch.unflatten()</code></a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html#torch.Tensor.unfold" target="_blank"><code>Tensor.unfold</code></a></td>
<td>Returns a view of the original tensor which contains all slices of size <code>size</code> from <code>self</code> tensor in the dimension <code>dimension</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_" target="_blank"><code>Tensor.uniform_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the continuous uniform distribution:</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique.html#torch.Tensor.unique" target="_blank"><code>Tensor.unique</code></a></td>
<td>Returns the unique elements of the input tensor.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive" target="_blank"><code>Tensor.unique_consecutive</code></a></td>
<td>Eliminates all but the first element from every consecutive group of equivalent elements.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" target="_blank"><code>Tensor.unsqueeze</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze" target="_blank"><code>torch.unsqueeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_" target="_blank"><code>Tensor.unsqueeze_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze" target="_blank"><code>unsqueeze()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.values.html#torch.Tensor.values" target="_blank"><code>Tensor.values</code></a></td>
<td>Return the values tensor of a <a href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs" target="_blank">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.var.html#torch.Tensor.var" target="_blank"><code>Tensor.var</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.var.html#torch.var" target="_blank"><code>torch.var()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.vdot.html#torch.Tensor.vdot" target="_blank"><code>Tensor.vdot</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot" target="_blank"><code>torch.vdot()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view" target="_blank"><code>Tensor.view</code></a></td>
<td>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html#torch.Tensor.view_as" target="_blank"><code>Tensor.view_as</code></a></td>
<td>View this tensor as the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit" target="_blank"><code>Tensor.vsplit</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit" target="_blank"><code>torch.vsplit()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.where.html#torch.Tensor.where" target="_blank"><code>Tensor.where</code></a></td>
<td><code>self.where(condition, y)</code> is equivalent to <code>torch.where(condition, self, y)</code>.</td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" target="_blank"><code>Tensor.xlogy</code></a></td>
<td>See <a href="https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy" target="_blank"><code>torch.xlogy()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_" target="_blank"><code>Tensor.xlogy_</code></a></td>
<td>In-place version of <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy" target="_blank"><code>xlogy()</code></a></td>
</tr>
<tr>
<td><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_" target="_blank"><code>Tensor.zero_</code></a></td>
<td>Fills <code>self</code> tensor with zeros.</td>
</tr>
</tbody>
</table>
<h2 id="storage">2.1 storage</h2>
<blockquote>
<p><a href="https://www.jianshu.com/p/ebd7f6395bf4" target="_blank">tensor的数据结构、storage()、stride()、storage_offset()</a></p>
</blockquote>
<p>pytorch中一个tensor对象分为<strong>头信息区（Tensor）</strong>和<strong>存储区（Storage）</strong>两部分</p>
<p><a data-lightbox="80cdc78a-f152-42a0-8750-133343833e70" data-title="tensor结构" href="https://upload-images.jianshu.io/upload_images/14029140-4ea52cc5277ae598.png?imageMogr2/auto-orient/strip|imageView2/2/w/719/format/webp" target="_blank"><img alt="tensor结构" src="https://upload-images.jianshu.io/upload_images/14029140-4ea52cc5277ae598.png?imageMogr2/auto-orient/strip|imageView2/2/w/719/format/webp"/></a>
头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据）则以<strong>连续一维数组</strong>的形式放在存储区，由torch.Storage实例管理着</p>
<p><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p>
<blockquote>
<p>获取tensor的storage</p>
</blockquote>
<pre><code class="lang-python">a = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">4.0</span>],[<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>],[<span class="hljs-number">3.0</span>, <span class="hljs-number">5.0</span>]])
a.storage()
Out[<span class="hljs-number">0</span>]: 
 <span class="hljs-number">1.0</span>
 <span class="hljs-number">4.0</span>
 <span class="hljs-number">2.0</span>
 <span class="hljs-number">1.0</span>
 <span class="hljs-number">3.0</span>
 <span class="hljs-number">5.0</span>
[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size <span class="hljs-number">6</span>]

a.storage()[<span class="hljs-number">2</span>] = <span class="hljs-number">9</span>

id(a.storage())
Out[<span class="hljs-number">1</span>]: <span class="hljs-number">1343354913168</span>
</code></pre>
<h1 id="实例">3 实例</h1>
<blockquote>
<p><a href="https://github.com/AccumulateMore/CV" target="_blank">小土堆+李沐课程笔记</a></p>
<p><a href="https://www.bilibili.com/video/BV1hE411t7RN/?p=11&amp;vd_source=d741c08a55ba6a6a780b28e90920def0" target="_blank">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】</a></p>
</blockquote>
<h2 id="pytorch加载数据">3.1 Pytorch加载数据</h2>
<p>Pytorch中加载数据需要Dataset、Dataloader。</p>
<ul>
<li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li>
<li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li>
</ul>
<h2 id="tensorboard">3.2 Tensorboard</h2>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-comment"># 准备的测试数据集</span>
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor())               
<span class="hljs-comment"># batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      </span>
test_loader = DataLoader(dataset=test_data,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-keyword">True</span>,num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-keyword">False</span>)      
<span class="hljs-comment"># 用for循环取出DataLoader打包好的四个数据</span>
writer = SummaryWriter(<span class="hljs-string">"logs"</span>)
step = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:
    imgs, targets = data <span class="hljs-comment"># 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span>
    writer.add_images(<span class="hljs-string">"test_data"</span>,imgs,step)
    step = step + <span class="hljs-number">1</span>

writer.close()
</code></pre>
<h2 id="transforms">3.3 Transforms</h2>
<p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。</p>
<p>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

img_path = <span class="hljs-string">"Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg"</span>
img = Image.open(img_path)  

tensor_trans = transforms.ToTensor()  <span class="hljs-comment"># 创建 transforms.ToTensor类 的实例化对象</span>
tensor_img = tensor_trans(img)  <span class="hljs-comment"># 调用 transforms.ToTensor类 的__call__的魔术方法   </span>
print(tensor_img)
</code></pre>
<h2 id="torchvision数据集">3.4 torchvision数据集</h2>
<p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。</p>
<p>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
train_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,download=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># root为存放数据集的相对路线</span>
test_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,download=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># train=True是训练集，train=False是测试集  </span>

print(test_set[<span class="hljs-number">0</span>])       <span class="hljs-comment"># 输出的3是target </span>
print(test_set.classes)  <span class="hljs-comment"># 测试数据集中有多少种</span>

img, target = test_set[<span class="hljs-number">0</span>] <span class="hljs-comment"># 分别获得图片、target</span>
print(img)
print(target)

print(test_set.classes[target]) <span class="hljs-comment"># 3号target对应的种类</span>
img.show()
</code></pre>
<h2 id="损失函数">3.5 损失函数</h2>
<p>① Loss损失函数一方面计算实际输出和目标之间的差距。</p>
<p>② Loss损失函数另一方面为我们更新输出提供一定的依据</p>
<blockquote>
<p>L1loss损失函数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss
inputs = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)
targets = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)
inputs = torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
targets = torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
loss = L1Loss()  <span class="hljs-comment"># 默认为 maen</span>
result = loss(inputs,targets)
print(result)
</code></pre>
<blockquote>
<p>MSE损失函数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
inputs = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype=torch.float32)
targets = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>],dtype=torch.float32)
inputs = torch.reshape(inputs,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
targets = torch.reshape(targets,(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))
loss_mse = nn.MSELoss()
result_mse = loss_mse(inputs,targets)
print(result_mse)
</code></pre>
<blockquote>
<p>交叉熵损失函数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> L1Loss
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

x = torch.tensor([<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>])
y = torch.tensor([<span class="hljs-number">1</span>])
x = torch.reshape(x,(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)) <span class="hljs-comment"># 1的 batch_size，有三类</span>
loss_cross = nn.CrossEntropyLoss()
result_cross = loss_cross(x,y)
print(result_cross)
</code></pre>
<h2 id="优化器">3.6 优化器</h2>
<p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。</p>
<p>② 梯度要清零，如果梯度不清零会导致梯度累加</p>
<pre><code class="lang-python">loss = nn.CrossEntropyLoss() <span class="hljs-comment"># 交叉熵    </span>
tudui = Tudui()
optim = torch.optim.SGD(tudui.parameters(),lr=<span class="hljs-number">0.01</span>)   <span class="hljs-comment"># 随机梯度下降优化器</span>
<span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
    imgs, targets = data
    outputs = tudui(imgs)
    result_loss = loss(outputs, targets) <span class="hljs-comment"># 计算实际输出与目标输出的差距</span>
    optim.zero_grad()  <span class="hljs-comment"># 梯度清零</span>
    result_loss.backward() <span class="hljs-comment"># 反向传播，计算损失函数的梯度</span>
    optim.step()   <span class="hljs-comment"># 根据梯度，对网络的参数进行调优</span>
    print(result_loss) <span class="hljs-comment"># 对数据只看了一遍，只看了一轮，所以loss下降不大</span>
</code></pre>
<blockquote>
<p>神经网络学习率优化</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn 
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">64</span>,drop_last=<span class="hljs-keyword">True</span>)

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tudui</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Tudui, self).__init__()        
        self.model1 = Sequential(
            Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),
            MaxPool2d(<span class="hljs-number">2</span>),
            Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),
            MaxPool2d(<span class="hljs-number">2</span>),
            Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>,padding=<span class="hljs-number">2</span>),
            MaxPool2d(<span class="hljs-number">2</span>),
            Flatten(),
            Linear(<span class="hljs-number">1024</span>,<span class="hljs-number">64</span>),
            Linear(<span class="hljs-number">64</span>,<span class="hljs-number">10</span>)
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x

loss = nn.CrossEntropyLoss() <span class="hljs-comment"># 交叉熵    </span>
tudui = Tudui()
optim = torch.optim.SGD(tudui.parameters(),lr=<span class="hljs-number">0.01</span>)   <span class="hljs-comment"># 随机梯度下降优化器</span>
scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=<span class="hljs-number">5</span>, gamma=<span class="hljs-number">0.1</span>) <span class="hljs-comment"># 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">20</span>):
    running_loss = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:
        imgs, targets = data
        outputs = tudui(imgs)
        result_loss = loss(outputs, targets) <span class="hljs-comment"># 计算实际输出与目标输出的差距</span>
        optim.zero_grad()  <span class="hljs-comment"># 梯度清零</span>
        result_loss.backward() <span class="hljs-comment"># 反向传播，计算损失函数的梯度</span>
        optim.step()   <span class="hljs-comment"># 根据梯度，对网络的参数进行调优</span>
        scheduler.step() <span class="hljs-comment"># 学习率太小了，所以20个轮次后，相当于没走多少</span>
        running_loss = running_loss + result_loss
    print(running_loss) <span class="hljs-comment"># 对这一轮所有误差的总和</span>
</code></pre>
<h2 id="网络模型使用及修改">3.7 网络模型使用及修改</h2>
<blockquote>
<p>网络模型添加</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

dataset = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
vgg16_true = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">True</span>) <span class="hljs-comment"># 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span>
vgg16_true.add_module(<span class="hljs-string">'add_linear'</span>,nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>)) <span class="hljs-comment"># 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span>

print(vgg16_true)
</code></pre>
<blockquote>
<p>网络模型修改</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

vgg16_false = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">False</span>) <span class="hljs-comment"># 没有预训练的参数     </span>
print(vgg16_false)
vgg16_false.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>,<span class="hljs-number">10</span>)
print(vgg16_false)
</code></pre>
<h2 id="网络模型保存与读取">3.8 网络模型保存与读取</h2>
<blockquote>
<p>模型结构 + 模型参数</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">False</span>)
torch.save(vgg16,<span class="hljs-string">"./model/vgg16_method1.pth"</span>) <span class="hljs-comment"># 保存方式一：模型结构 + 模型参数      </span>
print(vgg16)

model = torch.load(<span class="hljs-string">"./model/vgg16_method1.pth"</span>) <span class="hljs-comment"># 保存方式一对应的加载模型    </span>
print(model)
</code></pre>
<blockquote>
<p>模型参数（官方推荐），不保存网络模型结构</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-keyword">False</span>)
torch.save(vgg16.state_dict(),<span class="hljs-string">"./model/vgg16_method2.pth"</span>) <span class="hljs-comment"># 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span>
print(vgg16)

model = torch.load(<span class="hljs-string">"./model/vgg16_method2.pth"</span>) <span class="hljs-comment"># 导入模型参数   </span>
print(model)
</code></pre>
<h2 id="固定模型参数">3.9 固定模型参数</h2>
<p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p>
<ol>
<li>一个是设置不要更新参数的<a href="https://so.csdn.net/so/search?q=网络层&amp;spm=1001.2101.3001.7020" target="_blank">网络层</a>为false</li>
<li>另一个就是在定义优化器时只传入要更新的参数</li>
</ol>
<p>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim


<span class="hljs-comment"># 定义一个简单的网络</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">net</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_class=<span class="hljs-number">3</span>)</span>:</span>
        super(net, self).__init__()
        self.fc1 = nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>)
        self.fc2 = nn.Linear(<span class="hljs-number">4</span>, num_class)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-keyword">return</span> self.fc2(self.fc1(x))


model = net()

<span class="hljs-comment"># 冻结fc1层的参数</span>
<span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():
    <span class="hljs-keyword">if</span> <span class="hljs-string">"fc1"</span> <span class="hljs-keyword">in</span> name:
        param.requires_grad = <span class="hljs-keyword">False</span>

loss_fn = nn.CrossEntropyLoss()

<span class="hljs-comment"># 只传入requires_grad = True的参数</span>
optimizer = optim.SGD(filter(<span class="hljs-keyword">lambda</span> p: p.requires_grad, net.parameters(), lr=<span class="hljs-number">1e-2</span>)
print(<span class="hljs-string">"model.fc1.weight"</span>, model.fc1.weight)
print(<span class="hljs-string">"model.fc2.weight"</span>, model.fc2.weight)

model.train()
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
    x = torch.randn((<span class="hljs-number">3</span>, <span class="hljs-number">8</span>))
    label = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, [<span class="hljs-number">3</span>]).long()
    output = model(x)

    loss = loss_fn(output, label)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(<span class="hljs-string">"model.fc1.weight"</span>, model.fc1.weight)
print(<span class="hljs-string">"model.fc2.weight"</span>, model.fc2.weight)
</code></pre>
<h2 id="训练流程">3.10 训练流程</h2>
<blockquote>
<p>DataLoader加载数据集</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       

<span class="hljs-comment"># length 长度</span>
train_data_size = len(train_data)
test_data_size = len(test_data)
<span class="hljs-comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span>
print(<span class="hljs-string">"训练数据集的长度：{}"</span>.format(train_data_size))
print(<span class="hljs-string">"测试数据集的长度：{}"</span>.format(test_data_size))

<span class="hljs-comment"># 利用 Dataloader 来加载数据集</span>
train_dataloader = DataLoader(train_data_size, batch_size=<span class="hljs-number">64</span>)        
test_dataloader = DataLoader(test_data_size, batch_size=<span class="hljs-number">64</span>)
</code></pre>
<blockquote>
<p>测试网络正确</p>
</blockquote>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-comment"># 搭建神经网络</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tudui</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Tudui, self).__init__()        
        self.model1 = nn.Sequential(
            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),  <span class="hljs-comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span>
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Flatten(),  <span class="hljs-comment"># 展平后变成 64*4*4 了</span>
            nn.Linear(<span class="hljs-number">64</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>,<span class="hljs-number">64</span>),
            nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">10</span>)
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    tudui = Tudui()
    input = torch.ones((<span class="hljs-number">64</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>))
    output = tudui(input)
    print(output.shape)  <span class="hljs-comment"># 测试输出的尺寸是不是我们想要的</span>
</code></pre>
<blockquote>
<p>网络训练数据</p>
</blockquote>
<p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。</p>
<p>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p>
<p>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p>
<p>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。</p>
<p>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter

<span class="hljs-comment"># from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tudui</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Tudui, self).__init__()        
        self.model1 = nn.Sequential(
            nn.Conv2d(<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),  <span class="hljs-comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span>
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Conv2d(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),
            nn.MaxPool2d(<span class="hljs-number">2</span>),
            nn.Flatten(),  <span class="hljs-comment"># 展平后变成 64*4*4 了</span>
            nn.Linear(<span class="hljs-number">64</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>,<span class="hljs-number">64</span>),
            nn.Linear(<span class="hljs-number">64</span>,<span class="hljs-number">10</span>)
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = self.model1(x)
        <span class="hljs-keyword">return</span> x

<span class="hljs-comment"># 准备数据集</span>
train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       
test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">"./dataset"</span>,train=<span class="hljs-keyword">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="hljs-keyword">True</span>)       

<span class="hljs-comment"># length 长度</span>
train_data_size = len(train_data)
test_data_size = len(test_data)
<span class="hljs-comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span>
print(<span class="hljs-string">"训练数据集的长度：{}"</span>.format(train_data_size))
print(<span class="hljs-string">"测试数据集的长度：{}"</span>.format(test_data_size))

<span class="hljs-comment"># 利用 Dataloader 来加载数据集</span>
train_dataloader = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)        
test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)

<span class="hljs-comment"># 创建网络模型</span>
tudui = Tudui() 

<span class="hljs-comment"># 损失函数</span>
loss_fn = nn.CrossEntropyLoss() <span class="hljs-comment"># 交叉熵，fn 是 fuction 的缩写</span>

<span class="hljs-comment"># 优化器</span>
learning = <span class="hljs-number">0.01</span>  <span class="hljs-comment"># 1e-2 就是 0.01 的意思</span>
optimizer = torch.optim.SGD(tudui.parameters(),learning)   <span class="hljs-comment"># 随机梯度下降优化器  </span>

<span class="hljs-comment"># 设置网络的一些参数</span>
<span class="hljs-comment"># 记录训练的次数</span>
total_train_step = <span class="hljs-number">0</span>
<span class="hljs-comment"># 记录测试的次数</span>
total_test_step = <span class="hljs-number">0</span>

<span class="hljs-comment"># 训练的轮次</span>
epoch = <span class="hljs-number">10</span>

<span class="hljs-comment"># 添加 tensorboard</span>
writer = SummaryWriter(<span class="hljs-string">"logs"</span>)

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(epoch):
    print(<span class="hljs-string">"-----第 {} 轮训练开始-----"</span>.format(i+<span class="hljs-number">1</span>))

    <span class="hljs-comment"># 训练步骤开始</span>
    tudui.train() <span class="hljs-comment"># 当网络中有dropout层、batchnorm层时，这些层能起作用</span>
    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_dataloader:
        imgs, targets = data
        outputs = tudui(imgs)
        loss = loss_fn(outputs, targets) <span class="hljs-comment"># 计算实际输出与目标输出的差距</span>

        <span class="hljs-comment"># 优化器对模型调优</span>
        optimizer.zero_grad()  <span class="hljs-comment"># 梯度清零</span>
        loss.backward() <span class="hljs-comment"># 反向传播，计算损失函数的梯度</span>
        optimizer.step()   <span class="hljs-comment"># 根据梯度，对网络的参数进行调优</span>

        total_train_step = total_train_step + <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            print(<span class="hljs-string">"训练次数：{}，Loss：{}"</span>.format(total_train_step,loss.item()))  <span class="hljs-comment"># 方式二：获得loss值</span>
            writer.add_scalar(<span class="hljs-string">"train_loss"</span>,loss.item(),total_train_step)

    <span class="hljs-comment"># 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span>
    tudui.eval()  <span class="hljs-comment"># 当网络中有dropout层、batchnorm层时，这些层不能起作用</span>
    total_test_loss = <span class="hljs-number">0</span>
    total_accuracy = <span class="hljs-number">0</span>
    <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 没有梯度了</span>
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader: <span class="hljs-comment"># 测试数据集提取数据</span>
            imgs, targets = data
            outputs = tudui(imgs)
            loss = loss_fn(outputs, targets) <span class="hljs-comment"># 仅data数据在网络模型上的损失</span>
            total_test_loss = total_test_loss + loss.item() <span class="hljs-comment"># 所有loss</span>
            accuracy = (outputs.argmax(<span class="hljs-number">1</span>) == targets).sum()
            total_accuracy = total_accuracy + accuracy

    print(<span class="hljs-string">"整体测试集上的Loss：{}"</span>.format(total_test_loss))
    print(<span class="hljs-string">"整体测试集上的正确率：{}"</span>.format(total_accuracy/test_data_size))
    writer.add_scalar(<span class="hljs-string">"test_loss"</span>,total_test_loss,total_test_step)
    writer.add_scalar(<span class="hljs-string">"test_accuracy"</span>,total_accuracy/test_data_size,total_test_step)  
    total_test_step = total_test_step + <span class="hljs-number">1</span>

    torch.save(tudui, <span class="hljs-string">"./model/tudui_{}.pth"</span>.format(i)) <span class="hljs-comment"># 保存每一轮训练后的结果</span>
    <span class="hljs-comment">#torch.save(tudui.state_dict(),"tudui_{}.path".format(i)) # 保存方式二         </span>
    print(<span class="hljs-string">"模型已保存"</span>)

writer.close()
</code></pre>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2023-09-08 03:10:39
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: nlp关键词和摘要提取技术整理.md" class="navigation navigation-prev" href="nlp关键词和摘要提取技术整理.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: transformer.md" class="navigation navigation-next" href="transformer.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":53039,"date":"2023/04/16 16:39:14","cover":"https://pic.hycbook.com/i/hexo/post_cover/蕾姆2.webp","title":"pytorch学习.md","tags":["pytorch"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/蕾姆2.webp","mathjax":true,"categories":["deep_learning"],"description":"pytorch学习记录","level":"1.7","depth":1,"next":{"title":"transformer.md","level":"1.8","depth":1,"path":"chapters/transformer.md","ref":"chapters/transformer.md","articles":[]},"previous":{"title":"nlp关键词和摘要提取技术整理.md","level":"1.6","depth":1,"path":"chapters/nlp关键词和摘要提取技术整理.md","ref":"chapters/nlp关键词和摘要提取技术整理.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2021","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipay打赏","button":"欢迎打赏","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechat打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"深度学习知识驿站","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"欢迎留下评论交流~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"深度学习相关学习记录","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"书籍主页":"https://study.hycbook.com"}},"gitbook":"*","description":"记录 深度学习 的学习和一些技巧的使用"},"file":{"path":"chapters/pytorch学习.md","mtime":"2023-09-08T03:10:39.215Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-09-08T03:11:00.633Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
