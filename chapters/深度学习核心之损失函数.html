<!DOCTYPE HTML>
<html lang="zh-hans">
<head>
<meta charset="utf-8"/>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<title>深度学习核心之损失函数.md · 深度学习相关学习记录</title>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="深度学习核心之损失函数" name="description"/>
<meta content="GitBook 3.2.3" name="generator"/>
<meta content="narutohyc" name="author"/>
<link href="../gitbook/style.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-splitter/splitter.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchors/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-donate/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-anchor-navigation-ex/style/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-tbfed-pagefooter/footer.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-code/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-search-plus/search.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-lightbox/css/lightbox.min.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-pageview-count/plugin.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-highlight/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-fontsettings/website.css" rel="stylesheet"/>
<link href="../gitbook/gitbook-plugin-theme-comscore/test.css" rel="stylesheet"/>
<meta content="true" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black" name="apple-mobile-web-app-status-bar-style"/>
<link href="../gitbook/images/apple-touch-icon-precomposed-152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<link href="../gitbook/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="深度学习核心之激活函数.html" rel="next"/>
<link href="深度学习核心之优化器.html" rel="prev"/>
<link href="./chapters/res/other/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="./chapters/res/other/favicon.ico" rel="apple-touch-icon-precomposed" sizes="152x152"/>
<style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
<script>
        window["gitbook-plugin-github-buttons"] = {"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]};
    </script>
</head>
<body>
          <div class="mountain_a"></div>
          <div class="mountain_b"></div>
          <div class="house right">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="house left">
            <div class="fence"></div>
            <div class="wall"></div>
            <div class="roof left"></div>
            <div class="roof right"></div>
            <div class="door"></div>
          </div>
          <div class="tree_back"></div>
          <div class="tree"></div>
          <div class="postbox_a">
            <div class="hole"></div>
          </div>
          <div class="postbox_b">
            <div class="hole"></div>
          </div>
          <div class="windmill">
            <div class="tower"></div>
            <div class="t1"></div>
            <div class="t2"></div>
            <div class="blade">
              <div class="windblade"></div>
              <div class="windblade windblade2"></div>
              <div class="windblade windblade3"></div>
              <div class="windblade windblade4"></div>
            </div>
          </div>
          <div class="allsnows">
            <div class="snow1"></div>
            <div class="snow2"></div>
            <div class="snow3"></div>
            <div class="snow4"></div>
            <div class="snow5"></div>
            <div class="snow6"></div>
            <div class="snow7"></div>
            <div class="snow8"></div>
            <div class="snow9"></div>
            <div class="snow10"></div>
            <div class="snow11"></div>
            <div class="snow12"></div>
            <div class="snow13"></div>
            <div class="snow14"></div>
            <div class="snow15"></div>
            <div class="snow16"></div>
            <div class="snow17"></div>
            <div class="snow18"></div>
            <div class="snow19"></div>
            <div class="snow20"></div>
          </div>
          <div class="ground">
            <div class="g1"></div>
            <div class="g2"></div>
            <div class="g3"></div>
            <div class="ice">
              <div class="glare"></div>
              <div class="ice_shadow"></div>
            </div>

          </div>
    
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input placeholder="输入并搜索" type="text"/>
</div>
<nav role="navigation">
<ul class="summary">
<li>
<a class="custom-link" href="https://study.hycbook.com" target="_blank">书籍主页</a>
</li>
<li class="divider"></li>
<li class="chapter" data-level="1.1" data-path="../" id="chapter_id_0">
<a href="../">
<b>1.1.</b>
                    
                    Introduction
            
                </a>
</li>
<li class="chapter" data-level="1.2" data-path="LLM模型微调系列.html" id="chapter_id_1">
<a href="LLM模型微调系列.html">
<b>1.2.</b>
                    
                    LLM模型微调系列.md
            
                </a>
</li>
<li class="chapter" data-level="1.3" data-path="LLM模型部署调试推理.html" id="chapter_id_2">
<a href="LLM模型部署调试推理.html">
<b>1.3.</b>
                    
                    LLM模型部署调试推理.md
            
                </a>
</li>
<li class="chapter" data-level="1.4" data-path="dl_in_vision_field.html" id="chapter_id_3">
<a href="dl_in_vision_field.html">
<b>1.4.</b>
                    
                    dl_in_vision_field.md
            
                </a>
</li>
<li class="chapter" data-level="1.5" data-path="huggingface基本使用教程.html" id="chapter_id_4">
<a href="huggingface基本使用教程.html">
<b>1.5.</b>
                    
                    huggingface基本使用教程.md
            
                </a>
</li>
<li class="chapter" data-level="1.6" data-path="nlp关键词和摘要提取技术整理.html" id="chapter_id_5">
<a href="nlp关键词和摘要提取技术整理.html">
<b>1.6.</b>
                    
                    nlp关键词和摘要提取技术整理.md
            
                </a>
</li>
<li class="chapter" data-level="1.7" data-path="pytorch学习.html" id="chapter_id_6">
<a href="pytorch学习.html">
<b>1.7.</b>
                    
                    pytorch学习.md
            
                </a>
</li>
<li class="chapter" data-level="1.8" data-path="transformer.html" id="chapter_id_7">
<a href="transformer.html">
<b>1.8.</b>
                    
                    transformer.md
            
                </a>
</li>
<li class="chapter" data-level="1.9" data-path="图像分割算法.html" id="chapter_id_8">
<a href="图像分割算法.html">
<b>1.9.</b>
                    
                    图像分割算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.10" data-path="图像分类算法.html" id="chapter_id_9">
<a href="图像分类算法.html">
<b>1.10.</b>
                    
                    图像分类算法.md
            
                </a>
</li>
<li class="chapter" data-level="1.11" data-path="图神经网络.html" id="chapter_id_10">
<a href="图神经网络.html">
<b>1.11.</b>
                    
                    图神经网络.md
            
                </a>
</li>
<li class="chapter" data-level="1.12" data-path="数据标注工具.html" id="chapter_id_11">
<a href="数据标注工具.html">
<b>1.12.</b>
                    
                    数据标注工具.md
            
                </a>
</li>
<li class="chapter" data-level="1.13" data-path="深度学习核心之优化器.html" id="chapter_id_12">
<a href="深度学习核心之优化器.html">
<b>1.13.</b>
                    
                    深度学习核心之优化器.md
            
                </a>
</li>
<li class="chapter active" data-level="1.14" data-path="深度学习核心之损失函数.html" id="chapter_id_13">
<a href="深度学习核心之损失函数.html">
<b>1.14.</b>
                    
                    深度学习核心之损失函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.15" data-path="深度学习核心之激活函数.html" id="chapter_id_14">
<a href="深度学习核心之激活函数.html">
<b>1.15.</b>
                    
                    深度学习核心之激活函数.md
            
                </a>
</li>
<li class="chapter" data-level="1.16" data-path="深度学习核心基础知识点.html" id="chapter_id_15">
<a href="深度学习核心基础知识点.html">
<b>1.16.</b>
                    
                    深度学习核心基础知识点.md
            
                </a>
</li>
<li class="chapter" data-level="1.17" data-path="深度学习模型压缩技术.html" id="chapter_id_16">
<a href="深度学习模型压缩技术.html">
<b>1.17.</b>
                    
                    深度学习模型压缩技术.md
            
                </a>
</li>
<li class="chapter" data-level="1.18" data-path="目标检测与跟踪算法.html" id="chapter_id_17">
<a href="目标检测与跟踪算法.html">
<b>1.18.</b>
                    
                    目标检测与跟踪算法.md
            
                </a>
</li>
<li class="divider"></li>
<li>
<a class="gitbook-link" href="https://www.gitbook.com" target="blank">
            本书使用 GitBook 发布
        </a>
</li>
</ul>
</nav>
</div>
<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">
<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="..">深度学习核心之损失函数.md</a>
</h1>
</div>
<div class="page-wrapper" role="main" tabindex="-1">
<div class="page-inner">
<div class="search-plus" id="book-search-results">
<div class="search-noresults">
<section class="normal markdown-section">
<div id="anchor-navigation-ex-navbar"><i class="fa fa-navicon"></i><ul><li><span class="title-icon"></span><a href="#概述">1 概述</a></li><li><span class="title-icon"></span><a href="#回归损失函数">2 回归损失函数</a></li><ul><li><span class="title-icon"></span><a href="#l1-loss">2.1 L1 Loss</a></li><li><span class="title-icon"></span><a href="#l2-loss">2.2 L2 Loss</a></li><li><span class="title-icon"></span><a href="#smooth-l1-loss">2.3 Smooth L1 Loss</a></li><li><span class="title-icon"></span><a href="#huber-loss">2.4 Huber Loss</a></li><li><span class="title-icon"></span><a href="#quantile-loss">2.5 Quantile Loss</a></li><li><span class="title-icon"></span><a href="#iou-loss">2.6 IoU Loss</a></li></ul><li><span class="title-icon"></span><a href="#分类损失函数">3 分类损失函数</a></li><ul><li><span class="title-icon"></span><a href="#binary-cross-entropy">3.1 Binary Cross Entropy</a></li><li><span class="title-icon"></span><a href="#cross-entropy-loss">3.2 Cross Entropy Loss</a></li><ul><li><span class="title-icon"></span><a href="#最大似然角度">3.2.1 最大似然角度</a></li><li><span class="title-icon"></span><a href="#信息论角度">3.2.2 信息论角度</a></li></ul><li><span class="title-icon"></span><a href="#hinge-loss">3.3 Hinge Loss</a></li><li><span class="title-icon"></span><a href="#focal-loss">3.4 Focal Loss</a></li></ul><li><span class="title-icon"></span><a href="#基于概率的损失">4 基于概率的损失</a></li><ul><li><span class="title-icon"></span><a href="#kl散度">4.1 KL散度</a></li></ul><li><span class="title-icon"></span><a href="#正则化技术">5 正则化技术</a></li><ul><li><span class="title-icon"></span><a href="#l1正则化">5.1 L1正则化</a></li><li><span class="title-icon"></span><a href="#l2正则化">5.2 L2正则化</a></li><li><span class="title-icon"></span><a href="#弹性网正则化">5.3 弹性网正则化</a></li><li><span class="title-icon"></span><a href="#其他正则">5.4 其他正则</a></li></ul></ul></div><a href="#概述" id="anchorNavigationExGoTop"><i class="fa fa-arrow-up"></i></a><hr/>
<h1 id="概述">1 概述</h1>
<blockquote>
<p><a href="https://www.cvmart.net/community/detail/4879" target="_blank">一文看尽深度学习中的各种损失函数</a></p>
<p><a href="https://blog.csdn.net/caip12999203000/article/details/127307395" target="_blank">常用的损失函数合集</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247530581&amp;idx=3&amp;sn=08304b4b0f161a0a81538e1bfea27a2c&amp;chksm=fb3add5ecc4d5448881026bd45798c60c6e2094899043a95830809996cc0611b6a2c70c53200&amp;scene=27" target="_blank">深度学习常用损失函数的基本形式、原理及特点</a></p>
<p><a href="https://calvinfeng.gitbook.io/machine-learning-notebook/supervised-learning/basic-overview/loss_function_overview" target="_blank">Loss Functions</a></p>
</blockquote>
<p><code>损失函数(Loss Function)</code>是用来衡量模型预测值与真实值之间差异的函数，它是深度学习中的一个重要组成部分，用于评估模型的性能并指导模型的优化过程</p>
<p>损失函数、代价函数、目标函数的关系</p>
<ul>
<li><strong>损失函数(Loss Function)</strong>：损失函数是用来衡量模型在<strong>单个样本上的预测结果与真实标签之间的差异</strong>。它是一个标量值，表示模型预测的误差或损失程度。损失函数通常是针对单个样本计算的，例如均方误差(MSE)、交叉熵损失等。在训练过程中，通过最小化损失函数来优化模型参数，使模型的预测结果与真实标签更接近</li>
<li><strong>代价函数(Cost Function)</strong>：代价函数是指<strong>整个训练集上的平均损失或误差函数</strong>。代价函数是损失函数的求和或平均，用于衡量模型在整个训练集上的预测结果与真实标签之间的总体差异。代价函数通常是在训练过程中使用的，用于计算梯度并更新模型参数</li>
<li><strong>目标函数(Objective Function)</strong>：<strong>目标函数是在训练过程中要最小化或最大化的函数，可以是损失函数或代价函数</strong>。目标函数是模型训练的目标，通过优化目标函数来调整模型参数，使得模型在训练集上的性能达到最优</li>
</ul>
<table>
<thead>
<tr>
<th>定义</th>
<th>损失函数(Loss Function)</th>
<th>代价函数(Cost Function)</th>
<th>目标函数(Objective Function)</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集</td>
<td>单个样本</td>
<td>整个训练集</td>
<td>训练要优化函数</td>
</tr>
</tbody>
</table>
<p>在实际应用中，损失函数、代价函数和目标函数这些术语有时会被混用，但它们都涉及到衡量模型的预测结果与真实标签之间的差异，并在训练过程中用于优化模型</p>
<p><strong>损失函数大致可分为两种</strong>：<code>回归损失</code>(针对<strong>连续型</strong>变量)和<code>分类损失</code>(针对<strong>离散型</strong>变量)</p>
<p>具体使用哪个术语取决于上下文和个人偏好，但它们都指向类似的概念</p>
<p><a data-lightbox="d272d765-4fbe-47c8-9a83-9faea5956045" data-title="损失函数结构图" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/损失函数结构图.svg" target="_blank"><img alt="损失函数结构图" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/损失函数结构图.svg"/></a></p>
<h1 id="回归损失函数">2 回归损失函数</h1>
<blockquote>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247528906&amp;idx=2&amp;sn=7bdd966af599e2b8a336560d5dc3fdea&amp;chksm=fb3ad4c1cc4d5dd7bc868de3c75a7d52d91d98c6561462757f2a68fe04d497f4b7b6e79ac620&amp;scene=27" target="_blank">深度学习常用损失函数总览：基本形式、原理、特点</a></p>
</blockquote>
<h2 id="l1-loss">2.1 L1 Loss</h2>
<p><code>L1 Loss</code>也称为<code>Mean Absolute Error</code>，即<strong>平均绝对误差(MAE)</strong>，公式定义为
<script type="math/tex; mode=display">
J_{M A E}=\frac{1}{N} \sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|
</script>
<strong>优点</strong>: 对离群点(<strong>Outliers</strong>)或者异常值更具有鲁棒性</p>
<p><strong>缺点</strong>: 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习</p>
<p><a data-lightbox="454ffb9c-432e-4965-9596-f90958d8798a" data-title="MAE损失函数" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/MAE损失函数.webp" target="_blank"><img alt="MAE损失函数" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/MAE损失函数.webp"/></a></p>
<blockquote>
<p><strong>模型预测与真实值之间的误差服从拉普拉斯分布Laplace distribution</strong></p>
</blockquote>
<p>可以在一定的假设下通过最大化似然可以得到MAE损失的形式，假设模型预测与真实值之间的误差服从<code>拉普拉斯分布</code>Laplace distribution <script type="math/tex; ">(\mu=0, b=1)</script>，则给定一个输入<script type="math/tex; ">x_{i}</script>，模型输出真实值<script type="math/tex; ">y_{i}</script>的概率为
<script type="math/tex; mode=display">
p\left(y_{i} \mid x_{i}\right)=\frac{1}{2} \exp \left(-\left|y_{i}-\hat{y}_{i}\right|\right)
</script>
对其求对数可以得到的负对数似然实际上就是MAE损失的形式
<script type="math/tex; mode=display">
\begin{array}{l}

L(x, y)=\prod_{i=1}^{N} \frac{1}{2} \exp \left(-\left|y_{i}-\hat{y}_{i}\right|\right) \\ \\

L L(x, y)=-\frac{N}{2}-\sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right| \\ \\

N L L(x, y)=\sum_{i=1}^{N}\left|y_{i}-\hat{y}_{i}\right|

\end{array}
</script></p>
<h2 id="l2-loss">2.2 L2 Loss</h2>
<p><code>L2 Loss</code>也称为<code>Mean Squred Error</code>，即<strong>均方差(MSE)</strong>，它衡量的是预测值与真实值之间距离的平方和
<script type="math/tex; mode=display">
J_{M S E}=\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}
</script>
<strong>优点</strong>: 收敛速度快，能够对梯度给予合适的惩罚权重，而<strong>不是一视同仁</strong>，使梯度更新的方向可以更加精确</p>
<p><strong>缺点</strong>: 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性，假如我们训练数据中存在较大的异常值，此时我们将会有一个巨大的权重更新，这有可能会使模型失去平衡</p>
<p><a data-lightbox="351b3287-60d5-42ae-95bd-d88bf878e67b" data-title="MSE损失函数" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/MSE损失函数.webp" target="_blank"><img alt="MSE损失函数" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/MSE损失函数.webp"/></a></p>
<blockquote>
<p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的</strong></p>
</blockquote>
<p>MSE假设了误差服从<code>高斯分布</code>，在高斯分布假设下，可以使用最大化似然得到均方差损失的形式，假设模型预测与真实值之间的误差服从标准高斯分布<script type="math/tex; ">(\mu=0, \sigma=1)</script>，则给定一个输入<script type="math/tex; ">x_{i}</script>，模型输出真实值<script type="math/tex; ">y_{i}</script>的概率为
<script type="math/tex; mode=display">
p\left(y_{i} \mid x_{i}\right) 

= \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right] 

=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}\right)
</script>
进一步我们假设数据集中<script type="math/tex; ">\mathrm{N}</script>个样本点之间相互独立，则给定所有<script type="math/tex; ">x</script>输出所有真实值<script type="math/tex; ">y</script>的概率，即似然Likelihood为所有<script type="math/tex; ">p\left(y_{i} \mid x_{i}\right)</script>的累乘
<script type="math/tex; mode=display">
L(x, y)=\prod_{i=1}^{N} \frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{\left(y_{i}-\hat{y}_{i}\right)^{2}}{2}\right)
</script>
通常为了计算方便，我们通常最大化对数似然Log-Likelihood
<script type="math/tex; mode=display">
L L(x, y)=\log (L(x, y))=-\frac{N}{2} \log 2 \pi-\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}
</script>
去掉与<script type="math/tex; ">\hat{y}_{i}</script>无关的第一项，然后转化为最小化负对数似然Negative Log-Likelihood
<script type="math/tex; mode=display">
N L L(x, y)=\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}
</script>
可以看到这个实际上就是均方差损失的形式，也就是说在模型输出与真实值的<strong>误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的</strong></p>
<p>因此在这个假设能被满足的场景中(比如回归)，均方差损失是一个很好的损失函数选择</p>
<p>当这个假设没能被满足的场景中(比如分类)，均方差损失不是一个好的选择</p>
<blockquote>
<p>MAE和MSE作为损失函数的主要区别是</p>
</blockquote>
<ol>
<li><p>MSE损失相比MAE通常可以更快地收敛</p>
<p>当使用梯度下降算法时，MSE的梯度为<script type="math/tex; ">-\hat{y_{i}}</script>，而MAE损失的梯度为<script type="math/tex; ">\pm 1 </script>，即MSE的梯度的scale会随误差大小变化，而MAE的梯度的scale则一直保持为1，即便在绝对误差<script type="math/tex; ">\left|y_{i}-\hat{y_{i}}\right|</script>很小的时候，MAE的梯度scale也同样为1，这实际上是非常不利于模型的训练的</p>
<p>当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了MSE在大部分时候比MAE收敛地更快，这也是MSE更为流行的原因</p>
</li>
<li><p>MAE损失对于outlier更加健壮，即更加不易受到outlier影响，当误差非常大的时候，MSE损失会远远大于MAE损失</p>
</li>
<li><p>MSE假设了误差服从<code>高斯分布</code>，MAE假设了误差服从<code>拉普拉斯分布</code>，拉普拉斯分布本身对于outlier更加robust</p>
</li>
</ol>
<blockquote>
<p>适用场景</p>
</blockquote>
<h2 id="smooth-l1-loss">2.3 Smooth L1 Loss</h2>
<blockquote>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss" target="_blank">pytorch SMOOTHL LOSS</a></p>
</blockquote>
<p><code>Smooth L1 Loss</code>即<code>平滑的L1损失(SLL)</code>，出自<a href="https://arxiv.org/abs/1504.08083" target="_blank">Fast RCNN</a>，也称为SLL，Smooth L1 loss也具备了L1 loss和L2 loss各自的优点，本质就是L1和L2的组合
<script type="math/tex; mode=display">
J_{SLL} =\left\{

\begin{array}{ll}0.5\left(y_{i}-\hat{y}_{i}\right)^{2} / \beta , & \text { if }\left|y_{i}-\hat{y}_{i}\right|< \beta \\ 

\left|y_{i}-\hat{y}_{i}\right|-0.5 * \beta,  & \text { otherwise }

\end{array}\right.
</script></p>
<h2 id="huber-loss">2.4 Huber Loss</h2>
<blockquote>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html#torch.nn.HuberLoss" target="_blank">pytorch Huber LOSS</a></p>
</blockquote>
<p><code>Huber Loss</code>是一种类似于Smooth L1 Loss的损失函数，它也能够<strong>平衡L2范数和L1范数之间的权衡</strong></p>
<p>Huber loss和Smooth L1 loss具有相同的曲线走势，当Huber loss中的δ等于1时，Huber loss等价于Smooth L1 loss
<script type="math/tex; mode=display">
J_{HL} =\left\{

\begin{array}{ll}0.5\left(y_{i}-\hat{y}_{i}\right)^{2}  , & \text { if }\left|y_{i}-\hat{y}_{i}\right|< \delta \\ 

\delta * \left|y_{i}-\hat{y}_{i}\right|-0.5 * \delta,  & \text { otherwise }

\end{array}\right.
</script>
对于Huber损失来说，<script type="math/tex; ">\delta</script>的选择十分重要，它决定了模型处理局外点的行为。当残差大于<script type="math/tex; ">\delta</script>时使用L1损失，很小时则使用更为合适的L2损失来进行优化</p>
<p><a data-lightbox="45b5e0d7-caff-43b9-a963-d16bd339eda6" data-title="Huber损失函数" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Huber损失函数.webp" target="_blank"><img alt="Huber损失函数" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Huber损失函数.webp"/></a></p>
<blockquote>
<p>优点</p>
</blockquote>
<ul>
<li><p><strong>零点导数连续</strong>: Huber损失函数克服了MAE和MSE的缺点，不仅可以保持损失函数具有<strong>连续的导数</strong></p>
</li>
<li><p><strong>解决离群点梯度爆炸的问题</strong>: 利用MSE梯度随误差减小的特性来得到更精确的最小值，也对局外点具有<strong>更好的鲁棒性</strong></p>
</li>
</ul>
<p>但Huber损失函数的良好表现得益于精心训练的超参数<script type="math/tex; ">\delta</script>，当<script type="math/tex; ">\delta</script>趋向于0时它就退化成了MAE，而当<script type="math/tex; ">\delta</script>趋向于无穷时则退化为了MSE</p>
<h2 id="quantile-loss">2.5 Quantile Loss</h2>
<p><code>分位数回归Quantile Regression</code>是一类在实际应用中非常有用的回归算法，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数</p>
<h2 id="iou-loss">2.6 IoU Loss</h2>
<blockquote>
<p><a href="https://arxiv.org/pdf/1608.01471.pdf" target="_blank">UnitBox: An Advanced Object Detection Network 2016</a></p>
</blockquote>
<p><a data-lightbox="f465faa1-d361-4677-9a51-2e6dbbe370bf" data-title="IOU loss公式可视化" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/IOU loss公式可视化.webp" target="_blank"><img alt="IOU loss公式可视化" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/IOU loss公式可视化.webp"/></a></p>
<h1 id="分类损失函数">3 分类损失函数</h1>
<h2 id="binary-cross-entropy">3.1 Binary Cross Entropy</h2>
<blockquote>
<p><a href="https://www.scholat.com/teamwork/showPostMessage.html?id=8939&amp;teamId=1263" target="_blank">简单的交叉熵损失函数，你真的懂了吗</a></p>
</blockquote>
<p>对于分类问题，最常用的损失函数是交叉熵损失函数<code>Cross Entropy Loss</code></p>
<p>考虑二分类，在二分类中我们通常使用Sigmoid函数将模型的输出压缩到<script type="math/tex; ">(0,1)</script>区间内<script type="math/tex; ">\hat{y_{i}} \in(0,1)</script>，用来代表给定输入<script type="math/tex; ">x_{i}</script>，模型判断为正类的概率</p>
<p>由于只有正负两类， 因此同时也得到了负类的概率
<script type="math/tex; mode=display">
\begin{array}{l}

p\left(y_{i}=1 \mid x_{i}\right)=\hat{y_{i}} \\

p\left(y_{i}=0 \mid x_{i}\right)=1-\hat{y_{i}}

\end{array}
</script>
将两条式子合并成一条
<script type="math/tex; mode=display">
p\left(y_{i} \mid x_{i}\right)=\left(\hat{y}_{i}\right)^{y_{i}}\left(1-\hat{y}_{i}\right)^{1-y_{i}}
</script>
假设数据点之间<strong>独立同分布</strong>，则似然可以表示为
<script type="math/tex; mode=display">
L(x, y)=\prod_{i=1}^{N}\left(\hat{y}_{i}\right)^{y_{i}}\left(1-\hat{y}_{i}\right)^{1-y_{i}}
</script>
对似然取对数，然后加负号变成最小化负对数似然，即为交叉熵损失函数的形式
<script type="math/tex; mode=display">
N L L(x, y)=J_{C E}=-\sum_{i=1}^{N}\left(y_{i} \log \left(\hat{y}_{i}\right)+\left(1-y_{i}\right) \log \left(1-\hat{y_{i}}\right)\right)
</script></p>
<blockquote>
<p>可视化</p>
</blockquote>
<p>下图是对二分类的交叉熵损失函数的可视化，蓝线是目标值为0时输出不同输出的损失，黄线是目标值为1时的损失</p>
<p><a data-lightbox="7ffc6f66-efd1-4f2e-be5a-e725ed13e0a1" data-title="二分类的交叉熵损失函数的可视化" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/二分类的交叉熵损失函数的可视化.webp" target="_blank"><img alt="二分类的交叉熵损失函数的可视化" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/二分类的交叉熵损失函数的可视化.webp"/></a></p>
<p>可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长</p>
<p>图中<code>蓝线</code>是<script type="math/tex; ">y_i=0</script>的图线，此时损失函数变为
<script type="math/tex; mode=display">
J_{CE} = -log(1-\hat{y_{i}})
</script>
图中<code>黄线</code>是<script type="math/tex; ">y_i=1</script>的图线，此时损失函数变为
<script type="math/tex; mode=display">
J_{CE} = -log (\hat{y_{i}})
</script>
<strong>从图形中我们可以发现</strong>：预测输出与<script type="math/tex; ">y</script>差得越多，<script type="math/tex; ">J_{CE}</script>的值越大，也就是说对当前模型的<strong>惩罚</strong>越大，而且是<strong>非线性增大</strong>，是一种类似指数增长的级别</p>
<p>这是由log函数本身的特性所决定的，这样的好处是模型会倾向于让预测输出更接近真实样本标签<script type="math/tex; ">y</script></p>
<h2 id="cross-entropy-loss">3.2 Cross Entropy Loss</h2>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/518626935" target="_blank">交叉熵损失函数（CrossEntropy Loss）</a></p>
</blockquote>
<p>在多分类的任务中，交叉樀损失函数的推导思路和二分类是一样的，变化的地方主要有两个</p>
<ol>
<li><strong>维度变化</strong>: 真实值<script type="math/tex; ">y_{i}</script>现在是一个one-hot向量</li>
<li><strong>激活函数</strong>: 模型输出的最后的激活函数由原来的<strong>Sigmoid函数</strong>换成<strong>Softmax函数</strong></li>
</ol>
<p><a data-lightbox="324909b4-7c58-4732-b61d-4f8c52fd14f5" data-title="Cross Entropy Loss损失函数" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Cross Entropy Loss损失函数.webp" target="_blank"><img alt="Cross Entropy Loss损失函数" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Cross Entropy Loss损失函数.webp"/></a></p>
<blockquote>
<p>为什么分类用交叉熵损失，而不是均方差损失</p>
</blockquote>
<p>均方差损失实际上均方差损失<strong>假设了误差服从高斯分布</strong>，在分类任务下这个假设没办法被满足，因此效果会很差</p>
<p>为什么是交叉熵损失呢? 有两个角度可以解释这个事情，一个角度从<code>最大似然</code>的角度，另一个角度是可以用<code>信息论</code>来解释交叉熵损失</p>
<h3 id="最大似然角度">3.2.1 最大似然角度</h3>
<p>Softmax函数将每个维度的输出范围都限定在<script type="math/tex; ">(0,1)</script>之间，同时所有维度的输出和为1，用于表示一个概率分布
<script type="math/tex; mode=display">
p\left(y_{i} \mid x_{i}\right)=\prod_{k=1}^{K}\left(y_{i}^{k}\right)^{y_{i}^{k}}
</script>
其中<script type="math/tex; ">k \in K</script>表示<script type="math/tex; ">\mathrm{K}</script>个类别中的一类，同样的假设数据点之间独立同分布，可得到负对数似然为
<script type="math/tex; mode=display">
N L L(x, y)=J_{C E}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(y_{i}^{k}\right)
</script>
由于<script type="math/tex; ">y_{i}</script>是一个<strong>one-hot向量</strong>，除了目标类为1之外<strong>其他类别上的输出都为0</strong>，因此上式也可以写为
<script type="math/tex; mode=display">
J_{C E}=-\sum_{i=1}^{N} y_{i}^{c_{i}} \log \left(y_{i}^{\hat{c}_{i}}\right)
</script>
其中<script type="math/tex; ">c_{i}</script>是样本<script type="math/tex; ">x_{i}</script>的目标类。通常这个应用于多分类的交叉樀损失函数也被称为<code>Softmax Loss</code>或者<code>Categorical Cross Entropy Loss</code></p>
<h3 id="信息论角度">3.2.2 信息论角度</h3>
<p>假设对于样本<script type="math/tex; ">x_{i}</script>存在一个最优分布<script type="math/tex; ">y_{i}^{\star}</script>真实地表明了这个样本属于各个类别的概率，那么我们希望模型的输出<script type="math/tex; ">\hat{y}_{i}</script>尽可能地逼近这个最优分布</p>
<p>在信息论中，我们可以使用<code>KL散度(Kullback-Leibler Divergence)</code>来衡量两个分布的相似性</p>
<p>给定分布<script type="math/tex; ">p</script>和分布<script type="math/tex; ">q</script>，两者的KL散度公式如下
<script type="math/tex; mode=display">
K L(p, q)=\sum_{k=1}^{K} p^{k} \log \left(p^{k}\right)-\sum_{k=1}^{K} p^{k} \log \left(q^{k}\right)
</script>
其中第一项为分布<script type="math/tex; ">p</script>的信息熵，第二项为分布<script type="math/tex; ">p</script>和<script type="math/tex; ">q</script>的交叉熵。将最优分布<script type="math/tex; ">y_{i}^{\star}</script>和输出分布<script type="math/tex; ">\hat{y}_{i}</script>带入<script type="math/tex; ">p</script>和<script type="math/tex; ">q</script>得到
<script type="math/tex; mode=display">
K L\left(y_{i}^{\star}, \hat{y_{i}}\right)=\sum_{k=1}^{K} y_{i}^{\star k} \log \left(y_{i}^{\star k}\right)-\sum_{k=1}^{K} y_{i}^{\star k} \log \left(y_{i}^{\hat{k}}\right)
</script>
由于我们希望两个分布尽量相近，因此我们<strong>最小化KL散度</strong>。同时由于上式第一项信息熵仅与最优分布本身相关，因此我们在最小化的过程中可以忽略掉，变成最小化
<script type="math/tex; mode=display">
-\sum_{k=1}^{K} y_{i}^{\star k} \log \left(y_{i}^{\hat{k}}\right)
</script>
我们并不知道最优分布<script type="math/tex; ">y_{i}^{\star}</script>，但训练数据里面的目标值<script type="math/tex; ">y_{i}</script>可以看做是<script type="math/tex; ">y_{i}^{\star}</script>的一个近似分布
<script type="math/tex; mode=display">
-\sum_{k=1}^{K} y_{i}^{k} \log \left(y_{i}^{\hat{k}}\right)
</script>
这个是针对单个训练样本的损失函数，如果考虑整个数据集，则
<script type="math/tex; mode=display">
J_{K L}=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i}^{k} \log \left(y_{i}^{\hat{k}}\right)=-\sum_{i=1}^{N} y_{i}^{c_{i}} \log \left(y_{i}^{\hat{c}_{i}}\right)
</script>
可以看到通过<strong>最小化交叉嫡</strong>的角度推导出来的结果和使用最大化似然得到的结果是一致的</p>
<h2 id="hinge-loss">3.3 Hinge Loss</h2>
<p><code>合页损失Hinge Loss</code>是另外一种二分类损失函数，适用于maximum-margin的分类，支持向量机Support Vector Machine (SVM)模型的损失函数本质上就是Hinge Loss + L2正则化
<script type="math/tex; mode=display">
J_{\text {hinge }}=\sum_{i=1}^{N} \max \left(0,1-\operatorname{sgn}\left(y_{i}\right) \hat{y_{i}}\right)
</script>
下图是<script type="math/tex; ">y</script>为正类，即<script type="math/tex; ">sgn(y)=1</script>时，不同输出的合页损失示意图</p>
<p><a data-lightbox="a75f28fd-e322-4033-b881-f09b86b6a551" data-title="Hinge Loss损失函数" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Hinge Loss损失函数.webp" target="_blank"><img alt="Hinge Loss损失函数" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Hinge Loss损失函数.webp"/></a></p>
<p>可以看到当<script type="math/tex; ">y</script>为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在区间时还会有一个较小的惩罚</p>
<p><strong>即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失</strong></p>
<p>使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类</p>
<h2 id="focal-loss">3.4 Focal Loss</h2>
<blockquote>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank">Focal Loss for Dense Object Detection 2018</a></p>
<p><a href="https://www.csc.kth.se/cvap/cvg/rg/materials/louise_001_slides.pdf" target="_blank">ppt: Focal Loss for Dense Object Detection</a></p>
<p><a href="https://blog.csdn.net/BIgHAo1/article/details/121783011" target="_blank">Focal Loss损失函数(超级详细的解读)</a></p>
</blockquote>
<p><code>Focal loss</code>损失函数是为了解决<strong>one-stage</strong>目标检测中<strong>正负样本极度不平衡的问题</strong>，由<strong>何恺明</strong>(Kaiming He)团队提出</p>
<p>Focal loss是基于<strong>BCE(二分类交叉熵)</strong>的。它是一个动态缩放的交叉熵损失，通过一个动态缩放因子，可以动态降低训练过程中易区分样本的权重，从而将重心快速<strong>聚焦在那些难区分的样本</strong>(有可能是正样本，也有可能是负样本，但都是对训练网络有帮助的样本)</p>
<blockquote>
<p>正负样本不平衡(Class Imbalance)</p>
</blockquote>
<p>在一张图像中能够匹配到目标的候选框(正样本)个数一般只有十几个或几十个，而没有匹配到的候选框(负样本)则有10000~100000个</p>
<p>这么多的负样本不仅对训练网络起不到什么作用，反而会淹没掉少量但有助于训练的样本</p>
<blockquote>
<p>Focal loss是为了解决一阶段目标检测模型，那为什么二阶段不用解决</p>
</blockquote>
<p>在<strong>two-stage</strong>中分了两步，第一步时同样也会生成许多的负样本以及很少的正样本，但到第二步时，它会在第一步的基础上选取特定数量的正负样本去检测，所以正负样本并不会特别不平衡，二阶段模型还可以采用更复杂的<code>采样策略</code>和<code>hard negative mining (难例挖掘)</code>等方法来处理样本不平衡和难易样本的问题，因此对于二阶段目标检测模型来说，Focal Loss的优势可能相对较小</p>
<blockquote>
<p>引出Focal loss</p>
</blockquote>
<p>为了方便接下来的描述，这里先定义<script type="math/tex; ">p_t</script>为
<script type="math/tex; mode=display">
p_{\mathrm{t}}=\left\{\begin{array}{ll}p & \text { if } y=1 \\ 1-p & \text { otherwise }\end{array}\right.
</script>
此时cross entropy可以定义为
<script type="math/tex; mode=display">
J_{FL}(p, y)=\left\{\begin{array}{ll}-\log (p) & \text { if } y=1 \\ -\log (1-p) & \text { otherwise }\end{array}\right.
 
 \longrightarrow
 
J_{FL}(p, y)=J_{FL}\left(p_{\mathrm{t}}\right)=-\log \left(p_{\mathrm{t}}\right)
</script>
解决类别不平衡的常见方法是为类别1引入一个权重因子<script type="math/tex; "> \alpha \in [0, 1]</script>，而对于类别非1引入权重因子<script type="math/tex; ">1-\alpha</script>，这里引出<code>Balanced Cross Entropy(平衡交叉熵)</code>
<script type="math/tex; mode=display">
J_{FL}\left(p_{\mathrm{t}}\right)=-\alpha_{\mathrm{t}} \log \left(p_{\mathrm{t}}\right)
</script>
在论文实验中显示，密集检测器训练过程中遇到的类别不平衡问题使得交叉熵损失失去了效果，<strong>易于分类的负样本占据了大部分损失并主导了梯度</strong></p>
<p>虽然<script type="math/tex; ">\alpha</script>平衡了正样本和负样本的重要性，但它<code>无法区分易于和困难的样本</code></p>
<p>因此，论文提出了新的损失函数以减小易于样本的权重，从而将训练的重点放在困难的负样本上，更具体地说，论文提出在交叉熵损失中添加一个<code>调制因子</code><script type="math/tex; ">\left(1-p_{\mathrm{t}}\right)^{\gamma}</script>，其中<script type="math/tex; ">\gamma</script>是可调的<code>Focal参数</code>。我们将这个损失函数称为<code>Focal loss</code>，定义Focal Loss公式如下
<script type="math/tex; mode=display">
J_{FL}\left(p_{\mathrm{t}}\right) = - \alpha_{t} \left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)
</script>
其中<script type="math/tex; ">\gamma</script>作用是调节难易，较小的<script type="math/tex; ">\gamma</script>值会使得易样本的损失权重下降更慢，而较大的<script type="math/tex; ">\gamma</script>值则会加速易样本的损失权重下降</p>
<p><script type="math/tex; ">\alpha</script>作用是平衡正负样(正负样本数量不均衡)，当<script type="math/tex; ">\alpha</script>接近0时，负样本的损失贡献被放大，从而平衡了正负样本之间的重要性</p>
<p>通过调整<script type="math/tex; ">\alpha</script>和<script type="math/tex; ">\gamma</script>的值，可以根据具体情况调节模型对不同样本的关注程度，提高模型对难样本的学习和训练效果</p>
<blockquote>
<p>可视化</p>
</blockquote>
<p>下图可视化了<script type="math/tex; ">\gamma \in[0,5]</script>的值，可以观察到</p>
<ol>
<li>增加了<strong>分类不准确样本</strong>在损失函数中的权重</li>
<li>增加了<strong>难分样本</strong>在损失函数的权重，使得损失函数倾向于难分的样本，有助于提高难分样本的准确度</li>
</ol>
<p><a data-lightbox="c771e21d-6f9d-4cae-8485-d8977acfacad" data-title="Focal Loss在不同Upsilon下的损失曲线" href="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Focal Loss在不同Upsilon下的损失曲线.webp" target="_blank"><img alt="Focal Loss在不同Upsilon下的损失曲线" src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之损失函数/Focal Loss在不同Upsilon下的损失曲线.webp"/></a></p>
<p><script type="math/tex; ">\gamma</script>作用是调节难易样本对于总loss的权重(正负样本中都有难易，都进行了调节)</p>
<p><script type="math/tex; ">\gamma</script>调节简单样本权重降低的速率，当<script type="math/tex; ">\gamma=0</script>时即为交叉熵损失函数，当<script type="math/tex; ">\gamma</script>增加时，调整因子的影响也在增加。实验发现<script type="math/tex; ">\gamma=2</script>是最优</p>
<h1 id="基于概率的损失">4 基于概率的损失</h1>
<h2 id="kl散度">4.1 KL散度</h2>
<p>KL-散度损失函数的定义如下
<script type="math/tex; mode=display">
J_{KL} = -\sum_{i=0}^{C} y_{i} \log \left(\hat{y}_{i}\right)-y_{i} \log \left(y_{i}\right)=\sum_{i=0}^{C} y_{i}\left(\frac{y_{i}}{\hat{y}_{i}}\right)
</script>
<strong>优点：</strong></p>
<ul>
<li>适用于近似复杂的目标分布，如图像</li>
</ul>
<p>如上所示，KL散度损失是从我们网络预测的交叉熵分布与目标分布的熵之间的差异。它告诉我们模型离期望的分布有多远</p>
<blockquote>
<p>那么我们什么情况下使用它</p>
</blockquote>
<p>如果我们的任务是生成图像，那么目标分布要复杂得多，在这种情况下，使用KL散度损失的效果最好</p>
<h1 id="正则化技术">5 正则化技术</h1>
<h2 id="l1正则化">5.1 L1正则化</h2>
<p>在损失函数中添加模型参数的 L1 范数作为正则项。它促使模型参数稀疏化，即将一些参数压缩为零，从而实现特征选择和模型简化</p>
<h2 id="l2正则化">5.2 L2正则化</h2>
<p>在损失函数中添加模型参数的 L2 范数作为正则项。它对模型参数进行平滑约束，使模型参数值趋向于较小的值，有助于防止过拟合</p>
<h2 id="弹性网正则化">5.3 弹性网正则化</h2>
<p>弹性网正则化是 L1 正则化和 L2 正则化的一种结合。它同时对模型参数使用 L1 和 L2 正则化，从而综合考虑了<code>稀疏性</code>和<code>平滑性</code>的影响</p>
<h2 id="其他正则">5.4 其他正则</h2>
<p>Dropout：Dropout 是一种在训练过程中随机丢弃部分神经元的技术。它可以防止神经网络过拟合，并提高模型的泛化能力。通过在训练过程中以一定概率将部分神经元置零，Dropout 可以强制模型在没有完整神经网络的情况下进行学习</p>
<p>数据增强(Data Augmentation)：数据增强是一种通过对训练数据进行随机变换来扩充数据集的技术。常见的数据增强操作包括随机裁剪、旋转、翻转、平移、缩放等。数据增强可以增加训练数据的多样性，提高模型的泛化能力和鲁棒性</p>
<p>图像增强技术：了解常见的图像增强方法，如对比度调整、亮度调整、色彩平衡、直方图均衡化等，以及它们在图像处理和计算机视觉中的应用</p>
<p></p><footer class="page-footer"><span class="copyright">Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook</span><span class="footer-modification">该文件修订时间：
2023-08-18 00:25:02
</span></footer><hr/><div id="vcomments"></div><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script>new Valine({el: "#vcomments",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false})</script><p></p>
</section>
</div>
<div class="search-results">
<div class="has-results">
<h1 class="search-results-title"><span class="search-results-count"></span> results matching "<span class="search-query"></span>"</h1>
<ul class="search-results-list"></ul>
</div>
<div class="no-results">
<h1 class="search-results-title">No results matching "<span class="search-query"></span>"</h1>
</div>
</div>
</div>
</div>
</div>
</div>
<a aria-label="Previous page: 深度学习核心之优化器.md" class="navigation navigation-prev" href="深度学习核心之优化器.html">
<i class="fa fa-angle-left"></i>
</a>
<a aria-label="Next page: 深度学习核心之激活函数.md" class="navigation navigation-next" href="深度学习核心之激活函数.html">
<i class="fa fa-angle-right"></i>
</a>
<script src="https://cdn.jsdelivr.net/gh/zztongtong/CDN/js/live2d.min.js"></script><div style="position:absolute; bottom:0; left:0; width:200;"><canvas height="350" id="model_1" width="200"></canvas></div></div>
<script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"abbrlink":46832,"date":"2023/05/26 16:25:15","cover":"https://pic.hycbook.com/i/hexo/post_cover/蕾姆8.webp","title":"深度学习核心之损失函数.md","tags":["深度学习","神经网络","损失函数"],"top_img":"https://pic.hycbook.com/i/hexo/post_imgs/蕾姆8.webp","mathjax":true,"categories":["deep_learning"],"description":"深度学习核心之损失函数","level":"1.14","depth":1,"next":{"title":"深度学习核心之激活函数.md","level":"1.15","depth":1,"path":"chapters/深度学习核心之激活函数.md","ref":"chapters/深度学习核心之激活函数.md","articles":[]},"previous":{"title":"深度学习核心之优化器.md","level":"1.13","depth":1,"path":"chapters/深度学习核心之优化器.md","ref":"chapters/深度学习核心之优化器.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-sharing","splitter","expandable-chapters-small","anchors","github","github-buttons","donate","sharing-plus","anchor-navigation-ex","mathjax","mermaid-gb3","tbfed-pagefooter","code","search-plus","-lunr","-search","lightbox","theme-comscore","valine","pageview-count","favicon-absolute","copyright-v"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright © narutohyc.com 2021","modify_label":"该文件修订时间：","modify_format":"YYYY-MM-DD HH:mm:ss"},"github":{"url":"https://github.com/hycBook"},"splitter":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"code":{"copyButtons":true},"donate":{"alipay":"https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png","alipayText":"alipay打赏","button":"欢迎打赏","title":"","wechat":"https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png","wechatText":"wechat打赏"},"favicon-absolute":{"appleTouchIconMore":{},"appleTouchIconPrecomposed152":"./chapters/res/other/favicon.ico","appleTouchIconPrecomposedMore":{},"favicon":"./chapters/res/other/favicon.ico"},"copyright-v":{"copyProtect":false,"enableFooter":false,"site":"https://dl.hycbook.com","author":"narutohyc","website":"深度学习知识驿站","image":"https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png"},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mermaid-gb3":{},"anchor-navigation-ex":{"associatedWithSummary":true,"float":{"floatIcon":"fa fa-navicon","level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"mode":"float","multipleH1":true,"pageTop":{"level1Icon":"","level2Icon":"","level3Icon":"","showLevelIcon":false},"printLog":false,"showGoTop":true,"showLevel":false},"lightbox":{"jquery":true,"sameUuid":false},"theme-comscore":{},"pageview-count":{},"github-buttons":{"buttons":[{"user":"hycBook","repo":"bk_dl_page","type":"star","size":"small","count":true}]},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"expandable-chapters-small":{},"sharing":{"qq":true,"all":["google","facebook","weibo","twitter","qq","qzone","linkedin","pocket"],"douban":true,"facebook":true,"weibo":true,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":false,"google":true,"viber":false,"stumbleupon":false,"qzone":true,"linkedin":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":true},"anchors":{},"valine":{"avatar":"wavatar","lang":"zh-CN","pageSize":15,"placeholder":"欢迎留下评论交流~","recordIP":false,"appId":"evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz","appKey":"utUrzoiqNaDEGlgr09JL1pXB"},"search-plus":{}},"theme":"default","author":"narutohyc","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"深度学习相关学习记录","language":"zh-hans","mathjax":{"forceSVG":true},"links":{"sidebar":{"书籍主页":"https://study.hycbook.com"}},"gitbook":"*","description":"记录 深度学习 的学习和一些技巧的使用"},"file":{"path":"chapters/深度学习核心之损失函数.md","mtime":"2023-08-18T00:25:02.611Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-08-18T00:26:15.683Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
<canvas class="fireworks"></canvas><script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script></div>
<script src="../gitbook/gitbook.js"></script>
<script src="../gitbook/theme.js"></script>
<script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
<script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
<script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-donate/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
<script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/book/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/jquery.mark.min.js"></script>
<script src="../gitbook/gitbook-plugin-search-plus/search.js"></script>
<script src="../gitbook/gitbook-plugin-lightbox/js/lightbox.min.js"></script>
<script src="../gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-copyright-v/plugin.js"></script>
<script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
<script src="../gitbook/gitbook-plugin-theme-comscore/test.js"></script>
<script src="../gitbook/gitbook-plugin-mermaid-gb3/mermaid/mermaid.min.js"></script>
</body>
</html>
