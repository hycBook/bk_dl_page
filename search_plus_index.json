{"./":{"url":"./","title":"Introduction","keywords":"","body":" &#x1F40D; 深度学习学习记录 其中有些来自一些博客论坛，能加上原文地址的都已在文中尽可能加上 如有侵权，欢迎联系作者~ 1832044043@qq.com 个人论文主页链接 gitbook使用教程: gitbook使用教程 markdwon高阶语法: Markdown进阶（更改字体、颜色、大小，设置文字背景色，调整图片大小设置居中） Cmd Markdown 简明语法手册 EMOJI CHEAT SHEET python官方参考资料: Python 3.8.3 文档 NumPy 参考手册 Pandas: 强大的 Python 数据分析支持库 scikit-learn (sklearn) 官方文档中文版 Matplotlib 教程 https://pyecharts.org/#/ python其他参考资料: Python 标准库 Python 语言参考 Python文档内容 Python 标准库 python索引 PEP索引 python青南-炫技 背景、鼠标特效、2d动漫角色等参考鲸之声demo进行了自定义的修改 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/LLM Tokenizer分词系列.html":{"url":"chapters/LLM Tokenizer分词系列.html","title":"LLM Tokenizer分词系列.md","summary":"LLM Tokenizer分词系列","keywords":"","body":"tokenizer概述分词例子分词粒度子词分词Byte-Pair Encoding (BPE)Byte-level BPEWordPieceUnigramSentencePiece训练分词器 tokenizer hugging face Tokenizer文档 huggingface的分词器的摘要 【LLM系列之Tokenizer】如何科学地训练一个LLM分词器 概述 文本分词的过程涉及将文本拆分成多个单词或子单词。接着，这些单词或子单词会被映射到特定的ID，转换过程涉及一个查找表，这是一种简单的对应关系 因此，我们的主要关注点在于解析文本为一系列的单词或子单词 更具体地说，我们将探讨&#x1F917; Transformers库中常用的三种主要分词器类型：Byte-Pair Encoding (BPE)、WordPiece和SentencePiece，并且我们将提供实例说明哪种模型采用了哪种分词器 要了解特定预训练模型使用了哪种分词器，你可以参考每个模型主页上的文档说明，例如BertTokenizer，你会发现模型采用的是WordPiece分词器 分词例子 将一段文本分词到小块是一个比它看起来更加困难的任务，并且有很多方式来实现分词，举个例子，让我们看看这个句子 \"Don't you love &#x1F917; Transformers? We sure do.\" 对这段文本分词的一个简单方式，就是使用空格来分词，得到的结果是： [\"Don't\", \"you\", \"love\", \"&#x1F917;\", \"Transformers?\", \"We\", \"sure\", \"do.\"] 上面的分词是一个明智的开始，但是如果我们查看token \"Transformers?\" 和 \"do.\"，我们可以观察到标点符号附在单词\"Transformer\" 和 \"do\"的后面，这并不是最理想的情况 我们应该将标点符号考虑进来，这样一个模型就没必要学习一个单词和每个可能跟在后面的 标点符号的不同的组合，这么组合的话，模型需要学习的组合的数量会急剧上升。将标点符号也考虑进来，对范例文本进行分词的结果就是： [\"Don\", \"'\", \"t\", \"you\", \"love\", \"&#x1F917;\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"] 分词的结果更好了，然而，这么做也是不好的，分词怎么处理单词\"Don't\"，\"Don't\"的含义是\"do not\"，所以这么分词[\"Do\", \"n't\"] 会更好 现在开始事情就开始变得复杂起来了，部分的原因是每个模型都有它自己的分词类型 依赖于我们应用在文本分词上的规则， 相同的文本会产生不同的分词输出 用在训练数据上的分词规则，被用来对输入做分词操作，一个预训练模型才会正确的执行 spaCy and Moses 是两个受欢迎的基于规则的分词器，将这两个分词器应用在示例文本上，spaCy 和 Moses会输出类似下面的结果： [\"Do\", \"n't\", \"you\", \"love\", \"&#x1F917;\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"] 可见上面的分词使用到了空格和标点符号的分词方式，以及基于规则的分词方式 空格和标点符号分词以及基于规则的分词都是单词分词的例子，不那么严格的来说，单词分词的定义就是将句子分割到很多单词 然而将文本分割到更小的块是符合直觉的，当处理大型文本语料库时，上面的 分词方法会导致很多问题 在这种情况下，空格和标点符号分词通常会产生一个非常大的词典（使用到的所有不重复的单词和tokens的集合） 像：Transformer XL使用空格和标点符号分词，结果会产生一个大小是267,735的词典 这么大的一个词典容量，迫使模型有着一个巨大的embedding矩阵，以及巨大的输入和输出层，这会增加内存使用量，也会提高时间复杂度 通常情况下，transformers模型几乎没有词典容量大于50,000的，特别是只在一种语言上预训练的模型 所以如果简单的空格和标点符号分词让人不满意，为什么不简单的对字符分词 尽管字符分词是非常简单的，并且能极大的减少内存使用，降低时间复杂度，但是这样做会让模型很难学到有意义的输入表达 像： 比起学到单词\"today\"的一个有意义的上下文独立的表达，学到字母\"t\"的一个有意义的上下文独立的表达是相当困难的 因此，字符分词经常会伴随着性能的下降。所以为了获得最好的结果，transformers模型在单词级别分词和字符级别分词之间使用了一个折中的方案被称作子词分词 分词粒度 NLP中Tokenizers总结（BPE、WordPiece、Unigram和SentencePiece） 在NLP中，模型如Bert、GPT）的输入通常需要先进行tokenize，其目的是将输入的文本流，切分为一个个子串，每个子串都有完整的语义，便于学习embedding表达和后续模型的使用。tokenize有三种粒度：word/subword/char word/词：词是最自然的语言单元，对于英文来说其天然存在空格进行，切分相对容易，常用的分词器有spaCy和Moses 中文不具备这样的分割符，所以相对困难一些，不过目前也有Jieba、HanLP、LTP等分词器，这些分词器基于规则与模型，可以取得良好的分词效果 使用词时会有2个问题，通常情况下词表大小不超过5w： 词表通常是基于语料进行分词获得，但遇到新的语料时可能会出现OOV的情况 词表过于庞大，对于模型来说大部分参数都集中在输入输出层，不利于模型学习，且容易爆内存（显存） char/字符：字符是一种语言最基本的组成单元，如英文中的'a'、'b'、'c'或中文中的‘你’、‘我’、‘他’等，使用字符有如下问题： 字符数量是有限的通常数量较少，这样在学习每个字符的embedding向量时，每个字符中包含非常多的语义，学习起来比较困难 以字符分割，会造成序列长度过长，对后续应用造成较大限制 subword/子词：它介于char和word之间，可以很好的平衡词汇量和语义独立性，它的切分准则是常用的词不被切分，而不常见的词切分为子词 子词分词 子词分词原则 子词分词算法依赖这样的原则： 频繁使用的单词不应该被分割成更小的子词 很少使用的单词应该被分解到有意义的子词 举个例子： \"annoyingly\"能被看作一个很少使用的单词，能被分解成\"annoying\"和`\"ly\"`` `\"annoying\"和\"ly\"作为独立地子词，出现的次数都很频繁，而且与此同时单词\"annoyingly\"的含义可以通过组合\"annoying\"和\"ly\"的含义来获得 在粘合和胶水语言上，像Turkish语言，这么做是相当有用的，在这样的语言里，通过线性组合子词，大多数情况下你能形成任意长的复杂的单词 子词分词允许模型有一个合理的词典大小，而且能学到有意义的上下文独立地表达 除此以外，子词分词可以让模型处理以前从来没见过的单词， 方式是通过分解这些单词到已知的子词，举个例子：BertTokenizer对句子\"I have a new GPU!\"分词的结果如下： >>> from transformers import BertTokenizer >>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") >>> tokenizer.tokenize(\"I have a new GPU!\") [\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"] 因为我们正在考虑不区分大小写的模型，句子首先被转换成小写字母形式 我们可以见到单词[\"i\", \"have\", \"a\", \"new\"]在分词器的词典内，但是这个单词\"gpu\"不在词典内 所以，分词器将\"gpu\"分割成已知的子词[\"gp\" and \"##u\"] \"##\"意味着剩下的 token应该附着在前面那个token的后面，不带空格的附着（分词的解码或者反向） 另外一个例子，XLNetTokenizer对前面的文本例子分词结果如下： from transformers import XLNetTokenizer tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\") tokenizer.tokenize(\"Don't you love &#x1F917; Transformers? We sure do.\") >>> [\"▁Don\", \"'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"&#x1F917;\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"] 当我们查看SentencePiece时会回过头来解释这些\"▁\"符号的含义。正如你能见到的，很少使用的单词 \"Transformers\"能被分割到更加频繁使用的子词\"Transform\"和\"ers\" 现在让我们来看看不同的子词分割算法是怎么工作的，注意到所有的这些分词算法依赖于某些训练的方式，这些训练通常在语料库上完成， 相应的模型也是在这个语料库上训练的 Byte-Pair Encoding (BPE) BPE-Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015) Byte Pair Encoding BPE依赖于一个预分词器，这个预分词器会将训练数据分割成单词。预分词可以是简单的 空格分词，像：GPT-2，RoBERTa 更加先进的预分词方式包括了基于规则的分词，像：XLM，FlauBERT，FlauBERT在大多数语言使用了Moses，或者GPT，GPT使用了Spacy和ftfy，统计了训练语料库中每个单词的频次 在预分词以后，生成了单词的集合，也确定了训练数据中每个单词出现的频次 下一步，BPE产生了一个基础词典，包含了集合中所有的符号，BPE学习融合的规则-组合基础词典中的两个符号来形成一个新的符号 BPE会一直学习直到词典的大小满足了期望的词典大小的要求。注意到 期望的词典大小是一个超参数，在训练这个分词器以前就需要人为指定 举个例子，让我们假设在预分词以后，下面的单词集合以及他们的频次都已经确定好了： (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) 所以，基础的词典是[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]，将所有单词分割成基础词典内的符号，就可以获得： (\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5) BPE接着会统计每个可能的符号对的频次，然后挑出出现最频繁的的符号对，在上面的例子中，\"h\"跟了\"u\"出现了10 + 5 = 15次 （10次是出现了10次\"hug\"，5次是出现了5次\"hugs\"） 然而，最频繁的符号对是\"u\"后面跟了个\"g\"，总共出现了10 + 5 + 5 = 20次 因此，分词器学到的第一个融合规则是组合所有的\"u\"后面跟了个\"g\"符号 下一步，\"ug\"被加入到了词典内。单词的集合就变成了： (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5) BPE接着会统计出下一个最普遍的出现频次最大的符号对，也就是\"u\"后面跟了个\"n\"，出现了16次，\"u\"，\"n\"被融合成了\"un\"。 也被加入到了词典中，再下一个出现频次最大的符号对是\"h\"后面跟了个\"ug\"，出现了15次 又一次这个符号对被融合成了\"hug\"， 也被加入到了词典中 在当前这步，词典是[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]，我们的单词集合则是： (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5) 假设，the Byte-Pair Encoding在这个时候停止训练，学到的融合规则并应用到其他新的单词上（只要这些新单词不包括不在基础词典内的符号 就行） 举个例子，单词\"bug\"会被分词到[\"b\", \"ug\"]，但是\"mug\"会被分词到[\"\", \"ug\"]，因为符号\"m\"不在基础词典内 通常来看的话，单个字母像\"m\"不会被\"\"符号替换掉，因为训练数据通常包括了每个字母，每个字母至少出现了一次，但是在特殊的符号 中也可能发生像emojis 就像之前提到的那样，词典的大小，举个例子，基础词典的大小 + 融合的数量，是一个需要配置的超参数 举个例子：GPT 的词典大小是40,478，因为GPT有着478个基础词典内的字符，在40,000次融合以后选择了停止训练 Byte-level BPE 一个包含了所有可能的基础字符的基础字典可能会非常大，如果考虑将所有的unicode字符作为基础字符 为了拥有一个更好的基础词典，GPT-2使用了字节 作为基础词典，这是一个非常聪明的技巧，迫使基础词典是256大小，而且确保了所有基础字符包含在这个词典内。使用了其他的规则来处理标点符号，这个GPT2的分词器能对每个文本进行分词，不需要使用到符号。GPT-2有一个大小是50,257 的词典，对应到256字节的基础tokens，一个特殊的文本结束token，这些符号经过了50,000次融合学习 WordPiece Japanese and Korean Voice Search (Schuster et al., 2012) WordPiece是子词分词算法，被用在BERT，DistilBERT，和Electra，和BPE非常相似 WordPiece首先初始化一个词典，这个词典包含了出现在训练数据中的每个字符，然后递进的学习一个给定数量的融合规则 和BPE相比较， WordPiece不会选择出现频次最大的符号对，而是选择了加入到字典以后能最大化训练数据似然值的符号对 所以这到底意味着什么？参考前面的例子，最大化训练数据的似然值，等价于找到一个符号对，它们的概率除以这个符号对中第一个符号的概率，接着除以第二个符号的概率，在所有的符号对中商最大 像：如果\"ug\"的概率除以\"u\"除以\"g\"的概率的商，比其他任何符号对更大， 这个时候才能融合\"u\"和\"g\" 直觉上，WordPiece，和BPE有点点不同，WordPiece是评估融合两个符号会失去的量，来确保这么做是值得的 Unigram Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018) Unigram是一个子词分词器算法，和BPE或者WordPiece相比较 ，Unigram使用大量的符号来初始化它的基础字典，然后逐渐的精简每个符号来获得一个更小的词典。举例来看基础词典能够对应所有的预分词 的单词以及最常见的子字符串。Unigram没有直接用在任何transformers的任何模型中，但是和SentencePiece一起联合使用。 在每个训练的步骤，Unigram算法在当前词典的训练数据上定义了一个损失函数（经常定义为log似然函数的），还定义了一个unigram语言模型。 然后，对词典内的每个符号，算法会计算如果这个符号从词典内移除，总的损失会升高多少 Unigram然后会移除百分之p的符号，这些符号的loss 升高是最低的（p通常是10%或者20%），像：这些在训练数据上对总的损失影响最小的符号 重复这个过程，直到词典已经达到了期望的大小。 为了任何单词都能被分词，Unigram算法总是保留基础的字符 因为Unigram不是基于融合规则（和BPE以及WordPiece相比较），在训练以后算法有几种方式来分词，如果一个训练好的Unigram分词器 的词典是这个： [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"], \"hugs\"可以被分词成[\"hug\", \"s\"], [\"h\", \"ug\", \"s\"]或者[\"h\", \"u\", \"g\", \"s\"] 所以选择哪一个呢？Unigram在保存词典的时候还会保存训练语料库内每个token的概率，所以在训练以后可以计算每个可能的分词结果的概率 实际上算法简单的选择概率最大的那个分词结果，但是也会提供概率来根据分词结果的概率来采样一个可能的分词结果 分词器在损失函数上训练，这些损失函数定义了这些概率 假设训练数据包含了这些单词 x*{1}, \\dots, x*{N}，一个单词x*{i}的所有可能的分词结果的集合定义为S(x*{i})，然后总的损失就可以定义为： L = - \\sum _{i=1}^{N}{log ( \\sum _{x \\in S(x_i)}{p(x)})} SentencePiece SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018) 目前为止描述的所有分词算法都有相同的问题：它们都假设输入的文本使用空格来分开单词，然而，不是所有的语言都使用空格来分开单词 一个可能的解决方案是使用某种语言特定的预分词器。像：XLM使用了一个特定的中文、日语和Thai的预分词器 为了更加广泛的解决这个问题，SentencePiece将输入文本看作一个原始的输入流，因此使用的符合集合中也包括了空格 SentencePiece然后会使用BPE或者unigram算法来产生合适的词典 举例来说，XLNetTokenizer使用了SentencePiece，这也是为什么上面的例子中\"▁\"符号包含在词典内 SentencePiece解码是非常容易的，因为所有的tokens能被concatenate起来，然后将\"▁\"替换成空格 库内所有使用了SentencePiece的transformers模型，会和unigram组合起来使用，像：使用了SentencePiece的模型是ALBERT, XLNet，Marian，和T5 训练分词器 大模型基础知识系列：从头训练一个自己的Tokenizer 当前，预训练语言模型已成为NLP算法工程师的工具箱中的常客。在实际应用中，几乎所有的NLP模型都依赖于分词器（Tokenizer）来处理文本数据 虽然通常我们会倾向于使用现成的分词器，但有时候创建一个定制化的分词器也是必要的 对于分词器的构建，通常可以选择使用sentencepiece或者huggingface的tokenizers库，我们可以采用tokenizers库来训练我们自己的分词器，确保tokenizers库已经安装在你的系统上 pip install tokenizers 有了tokenizers库，我们可以开始构建我们的Tokenizer。这个过程包括配置多个组件以自定Tokenizer的行为，包括但不限于： 模型（Models）：这是Tokenizer的核心，负责实际的分词操作。可选的模型包括WordLevel、BPE、Unigram和WordPiece 规范化器（Normalizers）：规范化器用于预处理输入文本，将其转换为标准化的格式，如进行Unicode规范化或转换为小写，同时跟踪与原始文本的对齐关系 预分词器（PreTokenizers）：预分词器按照一定规则拆分输入文本，以确保底层模型按照这些预设边界构建令牌 后处理器（PostProcessors）：在分词流程完成后，后处理器负责在标记化后的字符串中插入特殊标记，比如说为模型提供标准格式的字符串 解码器（Decoders）：解码器能够将分词器生成的ID转换回人类可读的文本 这些组件的组合使得Tokenizer不仅能够执行基本的分词任务，还能为特定的NLP问题提供定制化的解决方案 在调用 Tokenizer.encode 或 Tokenizer.encode_batch 时，输入文本将经过以下流程： 规范化 预分词 模型处理 后处理 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: train_tokenizer.py @Description: @time: 2024/2/15 9:42 \"\"\" from datasets import load_dataset from tokenizers.models import BPE from tokenizers.normalizers import NFD, StripAccents from tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits, ByteLevel from tokenizers.trainers import BpeTrainer from tokenizers import Tokenizer, normalizers, pre_tokenizers, decoders from tokenizers import tokenizers from tokenizers.processors import TemplateProcessing def batch_iterator(batch_size=10000): dataset = load_dataset(\"TurboPascal/tokenizers_example_zh_en\", cache_dir='./cache/') print(dataset) for i in range(0, len(dataset), batch_size): yield dataset['train'][i: i + batch_size][\"text\"] def train_tokenizer(): # 自定数据集 data_files = [r\".\\data\\dataset_hyc.txt\"] # 定义tokenizer tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\")) # 定义一个归一化对象 normalizer = normalizers.Sequence([NFD(), StripAccents()]) tokenizer.normalizer = normalizer # 我们主要使用四类分割，空白、标点符号、数字、Bytelevel # pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True), ByteLevel()]) # tokenizer.pre_tokenizer = pre_tokenizer # 使用空白分割 tokenizer.pre_tokenizer = Whitespace() # 解码器 tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, use_regex=True) # 字节级 BPE 可能在生成的令牌中包括空白。如果您不希望偏移量包含这些空格，那么必须使用这个 PostProcessor。 tokenizer.post_processor = tokenizers.processors.ByteLevel() # 定义一个BpeTrainer trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]) # 开始训练 # 方式一 tokenizer.train(data_files, trainer) # 方式二 # tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset['train'])) # tokenizer保存 tokenizer.save(\"data/tokenizer-wiki.json\") # tokenizer加载 tokenizer = Tokenizer.from_file(\"data/tokenizer-wiki.json\") sentence = \"我尝试了很多的方法\" output = tokenizer.encode(sentence) print(output.tokens) # ['我', '[UNK]', '[UNK]', '了', '很', '多的', '方', '法'] print(output.ids) # [376, 0, 0, 44, 339, 1561, 438, 524] print(output.offsets[5]) # (5, 7) print(sentence[output.offsets[5][0]:output.offsets[5][1]]) # '多的' tokenizer.token_to_id(\"[SEP]\") # 2 # 后续处理 tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\", pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\", special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), ], ) output = tokenizer.encode_batch([sentence]) print(output) output = tokenizer.encode_batch([[\"我尝试了许多的方法\", \"却始终没有成功\"], [\"自己说过的话\", \"就必须要努力去践行\"]]) print(output) tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\") output = tokenizer.encode_batch([\"我尝试了许多的方法\", \"却始终没有成功\"]) print(output[0].tokens, output[1].tokens) # ['[CLS]', '我', '[UNK]', '[UNK]', '了', '许', '多的', '方', '法', '[SEP]'] ['[CLS]', '[UNK]', '[UNK]', '[UNK]', '没有', '成', '功', '[SEP]', '[PAD]', '[PAD]'] print(output[1].attention_mask) # [1, 1, 1, 1, 1, 1, 1, 1, 0, 0] if __name__ == '__main__': train_tokenizer() Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/LLM模型微调系列.html":{"url":"chapters/LLM模型微调系列.html","title":"LLM模型微调系列.md","summary":"LLM模型微调系列","keywords":"","body":"LLM模型概述微调发展脉络Adapter TuningPrefix/Prompt-TuningPrefix-TuningPrompt-TuningP-TuningP-Tuning V1P-Tuning V2LORA系列LORA(转)AdaLoRA(转)QLORARLHFFlash_Atten(转)概述核心要点提出问题解决方案ForwardStandard AttentionFlashAttention(Tiling)IO复杂度分析Standard AttentionFlashAttentionBackward理论基础实验验证LLM模型 什么是LLM（大语音模型） 概述 Large Language Model(LLM)，也称为大型语言模型，是一种基于机器学习和自然语言处理技术的模型，它通过对大量的文本数据进行训练，来学习服务人类语言理解和生成的能力 LLM的核心思想是通过大规模的无监督训练来学习自然语言的模式和语言结构，这在一定程度上能够模拟人类的语言认知和生成过程 与传统的NLP模型相比，LLM能够更好地理解和生成自然文本，同时还能够表现出一定的逻辑思维和推理能力 近年来，LLM得到了广泛的应用，其中最具代表性的是谷歌的BERT和OpenAI的GPT系列。这些模型在多个自然语言处理领域已经取得了显著的成果，包括文本分类、命名实体识别、情感分析、机器翻译、自动问答等 然而，在实际应用中，LLM面临着更多的挑战 首先，LLM需要大量的计算资源和大规模的数据集来训练，这对于一般的企业和个人来说十分困难 其次，由于LLM模型的复杂性和计算量较大，对于实时的语言处理应用来说，LLM在应用效率和响应速度上还存在一定的局限性 因此，如何解决模型训练和应用过程中的计算性能和效率问题，是LLM面临的主要挑战之一 微调 LLM大模型低资源微调p tuning v2和lora区别 prefix, p-tuningv2, lora finetune该怎么选择? 让天下没有难Tuning的大模型：PEFT技术简介 2023-04 大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2 大模型高效微调综述下： DiffPruning、BitFit、LoRa、AdaLoRA、MAM Adapters、UniPELT 微调(Fine-tuning)是一种常用的技术，用于将预训练的语言模型适应于特定的任务或领域。微调的目的是通过在特定任务上进行有监督的训练，调整模型参数以提高其性能和适应性 以下是微调在适应语言模型中的有效性的几个原因： 迁移学习：预训练的语言模型在大规模文本数据上进行了无监督的学习，从中学习到了通用的语言表示。通过微调，我们可以将这些通用的语言表示迁移到特定任务或领域上，因此可以利用模型在预训练阶段学到的知识 少样本学习：微调通常只需要在特定任务的相对较小的标注数据集上进行训练，而不是从头开始训练一个全新的模型。这对于许多任务来说是非常有益的，因为获得大规模标注数据可能是昂贵或困难的。通过利用预训练模型的泛化能力，微调可以在少量标注样本上实现较好的性能 领域自适应：通过微调，可以将语言模型从通用领域适应到特定领域。通过在特定领域的数据上微调，模型可以学习到该领域的特定语言模式、词汇和上下文，从而提高在该领域任务上的性能 模型个性化：微调还可以用于个性化模型，以适应特定用户或特定应用场景的需求。通过微调模型，可以根据个体用户的偏好、行为或数据特点进行定制，提供更准确和个性化的预测和推荐 微调语言模型是一种有效的方法，可以通过迁移学习、少样本学习、领域自适应和模型个性化等方式，利用预训练模型的优势和泛化能力，提高模型在特定任务或领域上的性能和适应性 为什么需要微调 高效训练，减少训练成本 共享基础大模型，在上面叠加自己的新模型 发展脉络 Adapter系列 AdapterFusion: Non-Destructive Task Composition for Transfer Learning 2021 Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter 2021 LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention 2023 LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model 2023 github LLaMA-Adapter: Efficient Fine-tuning of LLaMA p-tunning系列 Prefix-Tuning: Optimizing Continuous Prompts for Generation 2021 The Power of Scale for Parameter-Efficient Prompt Tuning 2021 P-Tuning - GPT Understands, Too 2021 P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks 2022 lora系列 LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 2021 AdaLoRA Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning 2023 QLORA: Efficient Finetuning of Quantized LLM 2023 另外huggingface很贴心的把常见的fine-Tuning方法都做了集成，只用几行代码就可添加和修改，十分方便，还有微软提供的加速库 huggingface官网实现的fine-Tuning方法 microsoft/DeepSpeed 加速 解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning & Prompt-Tuning & P-Tuning 微调LM和全部冻结的prompt模板相比，微调Prompt范式最大的区别就是prompt模板都是连续型(Embedding)，而非和Token对应的离散型模板 核心在于我们并不关心prompt本身是否是自然语言，只关心prompt作为探针能否引导出预训练模型在下游任务上的特定能力 固定LM微调Prompt的范式有以下几个优点 性价比高: 微调参数少，冻结LM只微调prompt部分的参数 无人工参与: 无需人工设计prompt模板，依赖模型微调即可 多任务共享模型: 因为LM被冻结，只需训练针对不同任务的prompt即可。因此可以固定预训练模型，拔插式加入Prompt用于不同下游任务 Adapter Tuning 预训练模型微调 | 一文带你了解Adapter Tuning Adapter Tuning 随着计算机硬件性能的提高，预训练模型参数量越来越多，在训练下游任务时进行全模型微调变得昂贵且耗时，Adapter 的出现缓解了这个问题。Adapter在预训练模型每层中插入用于下游任务的参数，在微调时将模型主体冻结，仅训练特定于任务的参数，减少训练时算力开销 Adapter模块设计方法 2019年，Houlsby N等人将Adapter引入NLP领域，作为全模型微调的一种替代方案。Adapter主体架构下图所示 Prefix/Prompt-Tuning hugging face参数高效微调peft源码解析 P系列关系： Prefix-Tuning(软提示/连续提示) Prompt-Tuning(软提示/连续提示)(可看做是Prefix-Tuning的简化版本) P-Tuning(软提示/连续提示) P-Tuning V2(软提示/连续提示)(可看做是Prefix-Tuning的优化版本) Prefix Tuning和PTuning V2在实现上基本上是一样的，其实就是一样的 下面是peft作者回复的关于Prefix Tuning和PTuning V2在实现上的关系(How to switch to P-Tuning v2) Hello, those are implemented together. P-Tuning v2 introduced optional parameterization of prompt tokens which you can specify via prefix_projection of PrefixTuningConfig. The other contribution was the ability of work without verbalizers using the linear classification head for NLU tasks whereas Prefix-Tuning paper which focused on NLG didn't focus on this. So, they are supported via the same PrefixEncoder PEFT method 另外在peft/peft_model.p的代码中有这样一段(大概1106行) if peft_config.peft_type == PeftType.PREFIX_TUNING: # PREFIX_TUNING、P_TUNING_V2 past_key_values = self.get_prompt(batch_size) return self.base_model( input_ids=input_ids, inputs_embeds=inputs_embeds, past_key_values=past_key_values, **kwargs ) else: # PROMPT_TUNING、P_TUNING if inputs_embeds is None: inputs_embeds = self.word_embeddings(input_ids) # concat prompt labels if labels is not None: prefix_labels = torch.full((batch_size, peft_config.num_virtual_tokens), -100).to(labels.device) kwargs[\"labels\"] = torch.cat((prefix_labels, labels), dim=1) prompts = self.get_prompt(batch_size=batch_size, task_ids=task_ids) prompts = prompts.to(inputs_embeds.dtype) inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1) return self.base_model(inputs_embeds=inputs_embeds, **kwargs) 可以看出PREFIX_TUNING生效是通过past_key_values传播的，下面是通过拼接到inputs_embeds上实现的 Prefix-Tuning 等待... Prefix-Tuning可以理解是CTRL[1]模型的连续化升级版，为了生成不同领域和话题的文本，CTRL是在预训练阶段在输入文本前加入了control code，例如好评前面加'Reviews Rating:5.0',差评前面加'Reviews Rating:1.0', 政治评论前面加‘Politics Title:’，把语言模型的生成概率，优化成了基于文本主题的条件概率 Prefix-Tuning进一步把control code优化成了虚拟Token，每个NLP任务对应多个虚拟Token的Embedding（prefix），对于Decoder-Only的GPT，prefix只加在句首，对于Encoder-Decoder的BART，不同的prefix同时加在编码器和解码器的开头。在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新。不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新 Prompt-Tuning https://github.com/google-research/prompt-tuning 等待... Prompt-Tunning是以上prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果 对比Prefix-Tunning，prompt-tuning的主要差异如下: 论文使用100个prefix token作为默认参数，大于以上prefix-tuning默认的10个token，不过差异在于prompt-Tunning只对输入层(Embedding)进行微调，而Prefix是对虚拟Token对应的上游layer全部进行微调。因此Prompt-Tunning的微调参数量级要更小，且不需要修改原始模型结构，这是“简化”的来源。相同的prefix长度，Prompt-Tunning( P-Tuning P-Tuning V1 github THUDM/P-tuning 手动尝试最优的提示无异于大海捞针，于是便有了自动离散提示搜索的方法(左图)，但提示是离散的，神经网络是连续的，所以寻找的最优提示可能是次优的。p-tuning依然是固定LLM参数，利用多层感知机和LSTM对prompt进行编码，编码之后与其他向量进行拼接之后正常输入LLM。注意，训练之后只保留prompt编码之后的向量即可，无需保留编码器 动机 一个刻板印象是GPT不适合理解类任务，这篇就是去思考这种刻板印象是否正确 GPT-3采用人工构造的模版来做in context learning，人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置啥的都会造成比较大的变化（这里作者做了一个简单的验证实验，具体看论文）。近来的自动化搜索模版工作成本也比较高，同时以前这种离散化的token的搜索出来的结果可能并不是最优的 和prefix-tuning差不多，反正是基于这两点去设计了一种连续可微的模版 相比prefix-tuning，这里加了可微的virtual token，但是仅限于输入，没有在每层加；另外virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token P-Tuning V2 github THUDM/P-tuning-v2 P-tuning V2论文和代码实现详解 chatGLM的浅薄解析 P-tuning V2 大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2 可以简单的将P-Tuning认为是针对Prompt Tuning的改进，P-Tuning v2认为是针对Prefix Tuning的改进 概述 代码示例 PrefixEncoder类，为了获得连续prompt，设计的模块 import torch class PrefixEncoder(torch.nn.Module): r''' The torch.nn model to encode the prefix Input shape: (batch-size, prefix-length) Output shape: (batch-size, prefix-length, 2*layers*hidden) ''' def __init__(self, config): super().__init__() self.prefix_projection = config.prefix_projection if self.prefix_projection: # Use a two-layer MLP to encode the prefix self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size) # 初始化重参数化的编码器 self.trans = torch.nn.Sequential( torch.nn.Linear(config.hidden_size, config.prefix_hidden_size), torch.nn.Tanh(), torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size) ) else: self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size) def forward(self, prefix: torch.Tensor): if self.prefix_projection: prefix_tokens = self.embedding(prefix) past_key_values = self.trans(prefix_tokens) else: past_key_values = self.embedding(prefix) return past_key_values 源码也可以看到 Prefix Tuning 与 P-Tuning v2 最主要的差别就是是否进行重新参数化编码 class BertPrefixForTokenClassification(BertPreTrainedModel): def __init__(self, config): super().__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config, add_pooling_layer=False) self.dropout = torch.nn.Dropout(config.hidden_dropout_prob) self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels) from_pretrained = False if from_pretrained: self.classifier.load_state_dict(torch.load('model/checkpoint.pkl')) for param in self.bert.parameters(): param.requires_grad = False self.pre_seq_len = config.pre_seq_len self.n_layer = config.num_hidden_layers self.n_head = config.num_attention_heads self.n_embd = config.hidden_size // config.num_attention_heads self.prefix_tokens = torch.arange(self.pre_seq_len).long() self.prefix_encoder = PrefixEncoder(config) bert_param = 0 for name, param in self.bert.named_parameters(): bert_param += param.numel() all_param = 0 for name, param in self.named_parameters(): all_param += param.numel() total_param = all_param - bert_param print('total param is {}'.format(total_param)) # 9860105 def get_prompt(self, batch_size): prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device) # 得到连续Prompt past_key_values = self.prefix_encoder(prefix_tokens) # bsz, seqlen, _ = past_key_values.shape # 改变形状 past_key_values = past_key_values.view( batch_size, self.pre_seq_len, self.n_layer * 2, self.n_head, self.n_embd ) past_key_values = self.dropout(past_key_values) # 改变形状，划分成数组。每一个数组元素形状为：(2,batch_size,n_head,seq_len,head_dim) past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2) return past_key_values def forward( self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, ): return_dict = return_dict if return_dict is not None else self.config.use_return_dict batch_size = input_ids.shape[0] past_key_values = self.get_prompt(batch_size=batch_size) prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device) attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1) # 开始传递past_key_values outputs = self.bert( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, past_key_values=past_key_values, ) ... return TokenClassifierOutput( loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions, ) 一次前向计算中，P-tuning v2会通过self.get_prompt(batch_size=batch_size)得到要连续Prompt BertEncoder会执行for循环，把past_key_values拆分到一个个BertLayer self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) ... for i, layer_module in enumerate(self.layer): if output_hidden_states: all_hidden_states = all_hidden_states + (hidden_states,) layer_head_mask = head_mask[i] if head_mask is not None else None past_key_value = past_key_values[i] if past_key_values is not None else None ... # BertLayer layer_module(..., past_key_value, ...) 巧妙的利用past_key_values参数，将past_key_values数组中每一个元素，拼接到BertSelfAttention中Key和Value 代码跟踪链路BertModel -> BertEncoder -> BertLayer -> BertAttention -> BertSelfAttention class BertSelfAttention(nn.Module): ... def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor: # 将张量转换形状，调换维度。这个代码会在seq_length维度进行拼接，其他维度不可动 new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size) x = x.view(new_x_shape) return x.permute(0, 2, 1, 3) def forward( self, hidden_states: torch.Tensor, attention_mask: Optional[torch.FloatTensor] = None, head_mask: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.FloatTensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, output_attentions: Optional[bool] = False, ) -> Tuple[torch.Tensor]: mixed_query_layer = self.query(hidden_states) # If this is instantiated as a cross-attention module, the keys # and values come from an encoder; the attention mask needs to be # such that the encoder's padding tokens are not attended to. is_cross_attention = encoder_hidden_states is not None if is_cross_attention and past_key_value is not None: # reuse k,v, cross_attentions key_layer = past_key_value[0] value_layer = past_key_value[1] attention_mask = encoder_attention_mask elif is_cross_attention: key_layer = self.transpose_for_scores(self.key(encoder_hidden_states)) value_layer = self.transpose_for_scores(self.value(encoder_hidden_states)) attention_mask = encoder_attention_mask elif past_key_value is not None: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) key_layer = torch.cat([past_key_value[0], key_layer], dim=2) value_layer = torch.cat([past_key_value[1], value_layer], dim=2) else: key_layer = self.transpose_for_scores(self.key(hidden_states)) value_layer = self.transpose_for_scores(self.value(hidden_states)) query_layer = self.transpose_for_scores(mixed_query_layer) ... 这里就会把past_key_value拼接到了原始的k、v上面，这样子就相当于给k、v添加了额外需要学习的参数了，再微调时只更新这部分新的参数即可 P-tuning V2连续Prompt代码实现仿真代码 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: p_tuning_test.py @Description: @time: 2023/6/6 15:31 \"\"\" import torch from torch import nn def run(): def transpose_for_scores(x: torch.Tensor) -> torch.Tensor: new_x_shape = x.size()[:-1] + (12, 64) x = x.view(new_x_shape) return x.permute(0, 2, 1, 3) prompt = torch.rand(32, 128, 48, 12, 64) # batch_size, seq_len, num_layer*2, num_head, head_size prompt = prompt.permute([2, 0, 3, 1, 4]) print(f\"P-tuningV2构造的trainable continuous embeddings形状：{prompt.shape}\") past_key_values = prompt.split(2) num_layers = 24 hidden_dim = 768 n_head = 12 head_dim = hidden_dim // n_head all_head_size = n_head * head_dim hidden_states = torch.randn(32, 128, 768) # batch_size, seq_len, hidden_size print(f\"输入的向量形状：{hidden_states.shape}\") for i in range(num_layers): past_key_value = past_key_values[i] print(f\"每一层BertLayer需要加入的prompt形状: {past_key_value.shape}\") self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None # BertSelfAttention query = nn.Linear(hidden_dim, all_head_size) key = nn.Linear(hidden_dim, all_head_size) value = nn.Linear(hidden_dim, all_head_size) # 原始kv的大小 key_layer = transpose_for_scores(key(hidden_states)) old_key_layer_shape = key_layer.shape print(f\"经过transpose_for_scores后的key形状：{old_key_layer_shape}\") value_layer = transpose_for_scores(value(hidden_states)) old_value_layer_shape = value_layer.shape print(f\"经过transpose_for_scores后的value形状：{old_value_layer_shape}\\n\") # 拼接后kv的大小 key_layer = torch.cat([past_key_value[0], key_layer], dim=2) print( f\"past_key_value[0]的形状：{past_key_value[0].shape} 原始key_layer的形状：{old_key_layer_shape} 经过cat后的key_layer形状：{key_layer.shape}\") value_layer = torch.cat([past_key_value[1], value_layer], dim=2) print( f\"past_key_value[1]的形状：{past_key_value[1].shape} 原始value_layer的形状：{old_value_layer_shape} 经过cat后的value_layer形状：{value_layer.shape}\\n\") mixed_query_layer = query(hidden_states) print(f\"hidden_states经过query层后输出的形状：{mixed_query_layer.size()}\") # batch seq len embed query_layer = transpose_for_scores(mixed_query_layer) print(f\"经过transpose_for_scores后的query形状{query_layer.size()}\") # batch print(\"注意力分数开始计算\") attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) print(f\"attention_scores的形状：{attention_scores.size()}\") # batch head seq_len seq_len print(\"开始注意力汇聚计算\") context_layer = torch.matmul(attention_scores, value_layer) print(f\"注意力汇聚后输出矩阵context_layer的形状：{context_layer.size()}\") # batch head seq_len embed/12 print(\"最后，将context_layer的形状恢复成输入hidden_states的形状\") context_layer = context_layer.permute(0, 2, 1, 3).contiguous() new_context_layer_shape = context_layer.size()[:-2] + (768,) context_layer = context_layer.view(new_context_layer_shape) print(f\"context_layer的形状恢复完成，其形状为：{context_layer.size()}\") print(\"一次P-tuningV2的BertLayer计算仿真结束\") break if __name__ == '__main__': run() 测试输出 S:\\Anaconda3\\envs\\torch38\\python.exe Q:\\pyCharmWS\\chatgpts\\P-tuning-v2\\tests\\p_tuning_test.py P-tuningV2构造的trainable continuous embeddings形状：torch.Size([48, 32, 12, 128, 64]) 输入的向量形状：torch.Size([32, 128, 768]) 每一层BertLayer需要加入的prompt形状: torch.Size([2, 32, 12, 128, 64]) 经过transpose_for_scores后的key形状：torch.Size([32, 12, 128, 64]) 经过transpose_for_scores后的value形状：torch.Size([32, 12, 128, 64]) ====================> 核心 past_key_value[0]的形状：torch.Size([32, 12, 128, 64]) 原始key_layer的形状：torch.Size([32, 12, 128, 64]) 经过cat后的key_layer形状：torch.Size([32, 12, 256, 64]) past_key_value[1]的形状：torch.Size([32, 12, 128, 64]) 原始value_layer的形状：torch.Size([32, 12, 128, 64]) 经过cat后的value_layer形状：torch.Size([32, 12, 256, 64]) ====================> 核心 hidden_states经过query层后输出的形状：torch.Size([32, 128, 768]) 经过transpose_for_scores后的query形状torch.Size([32, 12, 128, 64]) 注意力分数开始计算 attention_scores的形状：torch.Size([32, 12, 128, 256]) 开始注意力汇聚计算 注意力汇聚后输出矩阵context_layer的形状：torch.Size([32, 12, 128, 64]) 最后，将context_layer的形状恢复成输入hidden_states的形状 context_layer的形状恢复完成，其形状为：torch.Size([32, 128, 768]) 一次P-tuningV2的BertLayer计算仿真结束 LORA系列 点赞&#x1F44D;B站博主 小杨不努力0v0 + 博主相关的文章链接 LORA(转) LoRA：训练你的GPT【论文粗读·1】 一种通过低秩近似增量矩阵的，经过广泛验证足够Robust的微调方法 摘要 随着自然语言处理(NLP)模型规模的不断增长，由于成本和资源限制，对其进行完全微调以用于下游任务的挑战日益增加 介绍低秩适应(Low-Rank Adaptation)。LoRA通过引入参数矩阵来减少参数，并将GPU内存需求降低了3倍。相比于使用GPT-3进行微调，它将参数减少了10,000倍 尽管可训练参数更少，但LoRA在大多数语言模型上表现优于微调，具有更高的训练吞吐量和无推理延迟 对语言模型适应中的秩缺失进行的实证研究为LoRA的有效性提供了证据，LoRA是开源的 介绍 对于下游任务而言，完全微调大型语言模型是具有挑战性的。 受到[内在维度]的研究启发，我们提出了LoRA，具有以下优势： 低任务切换开销: 一个预训练模型可以被共享并用于构建多个针对不同任务的小型LoRA模块 参数高效: LoRA通过使用自适应优化器，在训练过程中使得训练更高效，并将硬件门槛降低了最多3倍 无推理延迟: LoRA的简单线性设计使得可训练矩阵在部署时可以与冻结权重合并 正交性: LoRA与许多先前的方法是正交的，并且可以与它们结合使用，比如前缀微调(prefix-tuning) 问题描述 模型训练时参数量评估，对于LLM，如果模型的参数时\\Phi的话 全量微调 \\max _{\\Phi} \\sum_{(x, y) \\in \\mathcal{Z}} \\sum_{t=1}^{|y|} \\log \\left(P_{\\Phi}\\left(y_{t} \\mid x, y_{ 使用Adam优化器下的，\\mathrm{n}=8，并且使用混合精度，一个参数需要16个bytes来存储，这16个bytes分别为 权重W需要fp16来存储，激活值A需要fp16，为了更新权重还需存一个复制W_c需要fp32，优化器需要存两个值，分别是M和V(方差)，分别需要fp32 其中fp16占2bytes，fp32占bytes，总共为2+2+4+4+4 = 16bytes 因此，\\Phi个参数就需要 \\Phi * 2nbytes Lora \\max _{\\Theta} \\sum_{(x, y) \\in \\mathcal{Z}} \\sum_{t=1}^{|y|} \\log \\left(p_{\\Phi_{0}+\\Delta \\Phi(\\Theta)}\\left(y_{t} \\mid x, y_{ Lora在训练时，将原来的参数固定下来，只更新新增加的参数，因此Mem Required: (4 \\Phi+\\theta * 2 n) bytes 在GPT-3的175B参数下，这里的\\Theta可以达到原来的0.01 \\% 为什么会提出Lora呢 原来的Adapter方法也是固定模型的参数，只训练MLP参数，但是这样子有几个弊端 增加了网络深度，增加了推理的时间 添加MLP之后，训练出来的最优方案也只能收敛到MLP层，不一定是全局最好的 直接去优化这个Promote并不能保证优化是单调的，也就是不是全局最优，很难优化好 减少了可用于处理下游任务的序列长度，因为新加入的Promote会占用输入的token长度 具体方法 最核心的思路如下公式所示，研究表明\\Delta W通常是一个欠秩的矩阵 W \\Leftarrow W_{0}+\\Delta W \\\\ W_{0}+\\Delta W=W_{0}+B A \\\\ h=W_{0} x+\\Delta W x=W_{0} x+B A x 因此，\\Delta W可以进行低秩分解 训练时，B初始化为全零矩阵，这样子参数量就从d*d变成了d*2*r，这里的r一般是远小于d的 优点 LoRA是对完全微调的一种推广方法 在适应过程中，LoRA不需要对权重矩阵进行完全秩的累积梯度更新，而是可以基于预训练的权重矩阵设置秩 当LoRA应用于所有权重矩阵并且偏置进行训练时，这种方法提供了类似于完全微调的表现能力 没有额外的推理延迟 在生产部署中，LoRA可以计算和存储W=W_0+BA，其中W_0和𝐵𝐴属于ℝ^{𝑑×𝑘}，当切换到另一个下游任务时，可以通过减去𝐵𝐴并加上不同的𝐵'𝐴'来恢复𝑊_0，这是一个快速操作，几乎没有额外的内存开销(潮汐GPU) 为什么低秩矩阵有效 当给定参数数量时，应该调整预训练Transformer中的哪些具体权重矩阵子集以实现最佳的下游性能？ 在给定参数预算的情况下，确定要调整的权重矩阵子集以实现最佳下游性能是一个复杂的问题，并且没有固定的答案 通常，可以考虑根据下游任务的特点和需求进行权衡和选择 一种常见的方法是通过对不同权重矩阵进行实验性微调，并根据性能评估来确定适合特定任务的权重矩阵子集 最优的适应矩阵\\Delta W是否真的是欠秩的吗？如果是，那么在实际情况下推荐的秩是多少？ 最优的适应矩阵\\Delta W是否真的是欠秩的，这取决于具体情况。秩缺失意味着矩阵的秩(矩阵的线性独立列数或行数的最大数量)较低 对于实际目的，建议选择适当的秩以平衡模型性能和计算成本。具体推荐的秩取决于任务的复杂性、数据集的规模以及可用的计算资源等因素 \\Delta W和W之间的关系是什么？\\Delta W和W之间是否存在高相关性？\\Delta W的大小与W相比如何？ \\Delta W表示适应矩阵，用于调整预训练权重矩阵W，\\Delta W和W之间的关系取决于具体的适应方法和优化算法。在某些情况下，\\Delta W可以通过对W的微小调整来获得，而在其他情况下，\\Delta W可能包含更大的变化 \\Delta W和W之间的相关性取决于适应方法的设计和优化过程的细节。它们可以存在一定的相关性，但具体情况可能因模型架构、任务要求和数据集特征而异 \\Delta W的大小与W的大小之间没有固定的比较关系，因为它们的尺度取决于具体的数值范围和调整方法 对于attention参数附加到哪个上更有效 实验在GPT-3 175B模型上设置了一个参数预算为18M(如果以FP16存储，大约为35MB)。这对应于当我们适应一种注意力权重时r=8，或者当我们适应两种类型时r=4，适用于所有96层 需要注意的是，将所有参数放在\\Delta 𝑊_𝑞或\\Delta 𝑊_k中会导致显著降低性能，而注入到𝑊_𝑞和𝑊_v则产生最佳结果。这表明，即使秩为4，\\Delta 𝑊中包含了足够的信息，使得与使用具有较大秩的单一类型的权重相比，使用更多的权重矩阵更可取 回答：将参数设置到q、v上时，r取多少合适 LoRA在非常小的秩(特别是对于𝑊_𝑞，𝑊_𝑣而言)下已经取得了相当的竞争力，这表明更新矩阵\\Delta W可能具有非常小的内在秩，使用低秩矩阵对LLM进行fine tune的时候，可以用一个非常小的低秩矩阵，就可以捕捉到对下游任务的一些特征信息，这为我们提供了一个非常高效的LLM的fine tune的方式，同时提高了下游任务的性能 回答：\\Delta W和W之间的关系是什么 首先对\\Delta W进行奇异值分解，并且把它左奇异值向量和右奇异值向量乘到W_q上，把W_q映射到\\Delta W_q的子空间，并计算F范数，同时还把W_q映射到映射到随机矩阵上 以此来证明\\Delta W_q与W_q有更强的相关性 首先，与随机矩阵相比，\\Delta W_q与W_q具有更强的相关性，表明\\Delta W放大了预训练模型中的W中已经存在的某些特征 其次，\\Delta W不是重复W的奇异值方向，而是只放大在W中没有强调的方向 第三，放大因子相当巨大：当r=4时，21.5≈6.91/0.32，表明\\Delta W只是放大了W中的一些特征，且放大倍数是很大的，相当于是把下游任务需要的特征提取出来并进行放大 AdaLoRA(转) AdaLoRA：更强大的LoRA github cauyxy/YourGPT Lora中的r是一个确定值，但不是对于所有的层，像q、k这样的层，q内在秩比较大，v内在秩比较小，对于不同的矩阵应该使用不同的内在秩 AdaLoRA是在对同样的参数量下，对不同的矩阵使用不同的r，通过奇异值分解，判断r的大小，来取得更好的效果 提出问题 在NLP领域，对于下游任务进行大型预训练语言模型的微调已经成为一种重要的做法。一般而言，我们会采用对原有的预训练模型进行全量微调的方法来适配下游任务，但这种方法存在两个问题 训练阶段: 对于预训练模型进行微调的时候，为了更新权重参数，需要大量的显存来存储参数的梯度和优化器信息，在当今预训练模型的参数变得越来越大的情况下，针对下游任务微调门槛变得越来越高 推理阶段: 由于我们训练的时候是对于模型参数进行全量的更新，所以多个下游任务需要为每个任务维护一个大型模型的独立副本，这样就导致我们在实际应用的时候浪费了不必要的存储 现有方法: 为了解决这些问题，研究者提出了两个主要研究方向，以减少微调参数的数量，同时保持甚至提高预训练语言模型的性能 添加小型网络模块 将小型网络模块添加到PLMs中，保持基础模型保持不变的情况下仅针对每个任务微调这些模块，可以用于所有任务。这样，只需引入和更新少量任务特定的参数，就可以适配下游的任务，大大提高了预训练模型的实用性，方法示例 Adapter tuning：是在基础模型的各层之间插入小型神经模块 Prefix tuning：将可训练的前缀标记附加到基础模型的输入或隐藏层上 Prompt Tuning: 修改模型的输入，在模型输入的前面加一些特定的前缀 可行之处：可以达到与完全微调几乎相当的性能，同时仅更新不到原始模型参数的1％，大大减少了内存消耗。 存在问题： Adapter tuning：引入了推理延迟，最终收敛到适配器层 Prefix or Prompt tuning：直接优化Prefix和Prompt是非单调的，比较难收敛，并且消耗了输入的token 下游任务增量更新 对预训练权重的增量更新进行建模，而无需修改模型架构 W=W^{(0)}+\\Delta 方法示例： Diff pruning：将\\Delta初始化为与W相同的维度，然后根据参数的大小按元素对\\Delta进行剪枝 LoRA：通过两个小得多的矩阵的乘积将\\Delta参数化为低阶矩阵 W=W^{(0)}+\\Delta=W^{(0)}+B A 可行之处：可以达到与完全微调几乎相当的性能 存在问题: Diff pruning： 需要底层实现来加速非结构化稀疏矩阵的计算，不能直接使用现有的框架 训练过程中需要存储完整的\\Delta矩阵，相比于Full finetune并没有降低计算成本 LoRA： 预先指定每个增量矩阵的内在秩r相同，忽略了在微调预训练模型时，权重矩阵的重要性在不同模块和层之间存在显著差异 只训练了self-attention，没有训练feed-forward networks，事实上FFN更重要 问题总结 不能预先指定矩阵的秩，需要动态更新增量矩阵的R 权重矩阵的重要性在不同模块和层之间存在显著差异 需要找到更加重要的矩阵，分配更多的参数，裁剪不重要的矩阵 找到重要的矩阵，提升模型效果 裁剪不重要的矩阵，降低参数计算量，降低模型效果差的风险 解决方案 目标：在类似LoRA的微调过程中动态分配参数预算给权重矩阵 调整增量矩阵的秩来控制预算分配。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低以防止过拟合并节省计算预算 采用参数化矩阵来模拟SVD，并舍弃不重要的奇异值，同时保留奇异向量。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法可以加速计算，同时保留未来恢复的可能性并稳定训练 在训练损失中添加了额外的惩罚项，以规范奇异矩阵P和Q的正交性，从而避免SVD的大量计算并稳定训练 SVD-BASED ADAPTATION 如上所述，我们把增量矩阵\\Delta做一个奇异值分解的近似，即\\Delta=P \\Lambda Q，对矩阵更新的描述则有如下表示 W=W^{(0)}+\\Delta=W^{(0)}+P \\Lambda Q 为了保证\\mathrm{P}和\\mathrm{Q}的正交性，即P^{\\top} P=Q Q^{\\top}=I，我们提出如下所示的正则损失 R(P, Q)=\\left\\|P^{\\top} P-I\\right\\|_{\\mathrm{F}}^{2}+\\left\\|Q Q^{\\top}-I\\right\\|_{\\mathrm{F}}^{2} 为什么不直接在原来的BA上进行修剪？ 当一对奇异向量被认为为不重要时，我们必须修剪它的所有元素。这就导致几乎不可能重新激活修剪过的奇异向量，因为它们的元素都被清零并且不再训练 与之对比，AdaLoRA只是Mask了奇异值 LoRA的A和B不是正交的，这意味着奇异向量可以相互依赖。 与截断最小的奇异值相比，丢弃奇异向量可能会导致原始矩阵发生更大的变化。 因此，在分配完秩的每一步之后，增量矩阵通常会发生更多不可预测的显著变化，这导致训练不稳定，甚至损害模型的效果 IMPORTANCE-AWARE RANK ALLOCATION 我们将基于SVD的秩调整应用于每个权重矩阵，包括每个transformer层的W_{q}, W_{k}, W_{v}, W_{f 1}和W_{f 2}。为了控制参数预算，我们在训练期间根据重要性得分迭代修剪奇异值 为了更好地表示，我们用k来索引增量矩阵\\Delta_{k}=P_{k} \\Lambda_{k} Q_{k} for k=1, \\ldots, n，用\\mathcal{G}_{k, i}=\\left\\{P_{k, * i}, \\lambda_{k, i}, Q_{k, i *}\\right\\}来表示第k个矩阵的奇异值，奇异向量三元组，\\mathcal{S}_{k, i} 来表示这个三元组的重要性 而\\mathcal{C}(\\mathcal{P}, \\mathcal{E}, \\mathcal{Q})作为参数训练的代价，同时加上正则化项，就得出了如下的目标函数: (\\gamma是正则化系数) \\mathcal{L}(\\mathcal{P}, \\mathcal{E}, \\mathcal{Q})=\\mathcal{C}(\\mathcal{P}, \\mathcal{E}, \\mathcal{Q})+\\gamma \\sum_{k=1}^{n} R\\left(P_{k}, Q_{k}\\right) 我们在训练的时候就可以通过梯度下降的方式对P, \\Lambda, Q进行更新，下面是\\Lambda的例子 \\tilde{\\Lambda}_{k}^{(t)}=\\Lambda_{k}^{(t)}-\\eta \\nabla_{\\Lambda_{k}} \\mathcal{L}\\left(\\mathcal{P}^{(t)}, \\mathcal{E}^{(t)}, \\mathcal{Q}^{(t)}\\right) 然后我们再基于\\mathcal{S}_{k, i}对\\Lambda进行裁剪 \\Lambda_{k}^{(t+1)}=\\mathcal{T}\\left(\\tilde{\\Lambda}_{k}^{(t)}, S_{k}^{(t)}\\right), \\text { with } \\mathcal{T}\\left(\\tilde{\\Lambda}_{k}^{(t)}, S_{k}^{(t)}\\right)_{i i}=\\left\\{\\begin{array}{ll} \\tilde{\\Lambda}_{k, i i}^{(t)} & S_{k, i}^{(t)} \\text { is in the top- } b^{(t)} \\text { of } S^{(t)} \\\\ 0 & \\text { otherwise } \\end{array}\\right. 其中S^{(t)}=\\left\\{S_{k, i}^{(t)}\\right\\}_{1 \\leq k \\leq n, 1 \\leq i \\leq r}包含所有三元组的重要性分数。b^{(t)}是第\\mathrm{t}步剩余奇异值的预算 通过这种方式，我们通过修剪不太重要的奇异值，将更多预算留给优先级较高的增量矩阵 Magnitude of singular values 这样的话只有最小的奇异值以及最不重要的奇异向量被去弃。它最大限度地减小了与原始矩阵的偏差，进一步稳定了训练。但是这个度量不能正确量化参数(三元组)对模型性能的贡献 S_{k, i}=\\left|\\lambda_{k, i}\\right| Sensitivity-based importance 之前的工作利用灵敏度来量化单个参数的重要性，并据此对参数进行非结构化修剪。在我们的例子上，我们必须设计一个新的度量标准，因为三元组要被按组丢弃了，所以每一项的敏感性都应该被考虑，并适当地组合起来，以量化三元组对模型性能的整体贡献 我们设计了如下所示的函数来计算importance score S_{k, i}=s\\left(\\lambda_{k, i}\\right)+\\frac{1}{d_{1}} \\sum_{j=1}^{d_{1}} s\\left(P_{k, j i}\\right)+\\frac{1}{d_{2}} \\sum_{j=1}^{d_{2}} s\\left(Q_{k, i j}\\right) 我们可以采用s(\\cdot)的灵敏度，s(\\cdot)定义为梯度权重乘积的大小: I\\left(w_{i j}\\right)=\\left|w_{i j} \\nabla_{w_{i j}} \\mathcal{L}\\right| 本质上近似于参数归零时的损失变化。如果去除一个参数影响较大，则模型对该参数敏感，我们应该保留它 但之前的工作指出，直接计算的敏感性还不是一个可靠的重要指标。这样的分数是在抽样的minibatch上估计的。随机采样和复杂的训练动态导致灵敏度估计的变异性大，不确定性大，这样可能会导致对于参数的重要性的错误估计。提出通过灵敏度平滑和不确定性量化，加入累计灵敏度的影响来解决这一问题: \\begin{array}{l} \\bar{I}^{(t)}\\left(w_{i j}\\right)=\\beta_{1} \\bar{I}^{(t-1)}\\left(w_{i j}\\right)+\\left(1-\\beta_{1}\\right) I^{(t)}\\left(w_{i j}\\right) \\\\ \\bar{U}^{(t)}\\left(w_{i j}\\right)=\\beta_{2} \\bar{U}^{(t-1)}\\left(w_{i j}\\right)+\\left(1-\\beta_{2}\\right)\\left|I^{(t)}\\left(w_{i j}\\right)-\\bar{I}^{(t)}\\left(w_{i j}\\right)\\right| \\end{array} 接下来，我们把s(\\cdot)定义为\\bar{I}^{(t)}和\\bar{U}^{(t)}的乘积 s^{(t)}\\left(w_{i j}\\right)=\\bar{I}^{(t)}\\left(w_{i j}\\right) \\cdot \\bar{U}^{(t)}\\left(w_{i j}\\right) 这样，我们就得到了一个既考虑了三元组所有元素，又考虑了累计灵敏度足够平滑的一个重要性函数 GLOBAL BUDGET SCHEDULER 在低秩自适应的情况下，调整秩自然是为了控制参数预算。因此，我们将预算b^{(t)}定义为所有增量矩阵的总秩，即总奇异值的数量 回想一下，预算分配是在微调期间迭代执行的。为了便于训练，我们提出了一个全局预算调度器。具体来说，我们从略高于目标预算b^{(T)}的初始预算算b^{(0)}开始(例如，b^{(T)}的1.5倍) 我们将每个增量矩阵的初始秩设为r=\\frac{b^{(0)}}{n}。我们对t_{\\text {init }}步进行warmup，然后按照三次计划减少预算b^{(t)}，直到达到b^{(t)} 最后，我们得到的修正完预算分布，并对t_{\\text {final }}步骤的模型进行了微调 这使得AdaLoRA可以先探索参数空间，然后再关注最重要的权重 实验验证 QLORA FineTune -> P_tuning -> P_tuning V2 -> LoRA -> QLoRA BERT Adapter RLHF 入门】大语言模型常用微调框架介绍｜LoRA&Prefix-Tuning&Prompt-Tuning&P-Tuning v2&RLHF微调原理简介 RLHF: Reinforcement Learning from Human。Feedback，即基于人工反馈机制的强化学习。最早与2022年4月，由OpenAI研究团队系统总结并提出.并在GPT模型的对话类任务微调中大放异彩，被称为ChatGPT背后的功臣 RLHF也是目前为止常用的、最为复杂的基于强化学习的大语言模型微调方法，目前最好的端到端RLHF实现是DeepSpeedChat库，由微软开源并维护 基于强化学习的进阶微调方法RLHF方法 论文地址: https://arxiv.org/abs/2203.02155 步骤1: 监督微调 (SFT)-一 使用精选的人类回答来微调预训练的语言模型以应对各种查询 步骤2:奖励模型微调 -- 使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立的 (通常比 SFT 小的) 奖励模型 (RW) 步骤3: RLHF 训练 --利用 Proximal Policy Optimization (PPO) 算法根据 RW 模型的奖励D九天Hector反馈进一步微调 SFT 模型。 Flash_Atten(转) 前置知识 GPU Arch:自顶向下分析 + B站 GPU Arch：自顶向下分析【浅谈底层·1】 随着人工智能特别是以GPT为代表的生成式AI的迅猛发展，GPU已经成为了一种不可或缺的工具，甚至企业都以拥有多少高端GPU作为抓住风口能力的衡量标准。相比之下，CPU虽然在传统计算领域占据主导地位，但在处理AI任务时却不及GPU出色 为什么AI计算通常选择GPU而不是CPU，分析GPU在AI计算中的优势，同时，从底层原理探讨从Volta到最新的Hopper四代NVIDIA GPU架构的演进，展示其不断提升的性能和功能 GPU主要由计算单元ALU组成。CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下，计算能力只是CPU很小的一部分 通过上面自顶向下的分析，我们知道，对于GPU中的存储部分访问速度由快到慢，计算部分从大到小排列为 \\begin{array}{c} \\text{Mem Speed:(L1 Cache/SMEM)>L2 Cache>HBM} \\\\ \\text{Compute Unit:GPC>TPC>SM>(TensorCore, SFU, INT32, FP32..)} \\end{array} NVLink是什么？为什么需要他？ 大模型通常具有巨大的参数数量和复杂的结构，需要处理大量的数据。分布式训练将这些大型模型分割成多个部分，由多个GPU或计算节点并行处理，每个部分处理自己的数据子集。然后通过全局通信，参数同步等方式进行梯度传播，此时GPU之间的通信带宽就变的越来越重要 在NVLink出现之前，GPU与GPU之间的数据交互通过PCIe（Peripheral Component Interconnect Express）总线进行。但PCIe存在两个问题，一是PCIe总线的带宽相对有限，其中PCIe 4.0x16的最大带宽也就64GB/s，二是PCIe总线的延迟相对较高，在GPU之间传输数据时，每次数据传输都需要通过CPU和主机内存来完成。这种传输路径会导致额外的延迟，并降低数据传输的效率。然而，深度学习应用中需要更高的带宽和更低的延迟，PCIe显然是无法满足当下的神经网络训练需求 NVLink利用高带宽、低延迟的通信通道，直接将多个GPU连接在一起，实现快速、高效的数据传输和共享。通过NVLink，GPU之间的数据交互可以直接在GPU之间进行，而无需通过CPU和主机内存。这种直接内存访问（DMA）的方式大大减少了数据传输的复制和延迟，提高了数据共享的效率。此外，NVLink还提供了一致的内存空间，使得多个GPU能够共享同一份内存，简化了程序设计和数据管理的复杂性 概述 FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Paper 2022 FlashAttention: 更快训练更长上下文的GPT Transformer作为GPT类模型的基础架构提供了强大的特征处理能力，但是处理更长上下文仍然是一个挑战，因为核心的自注意力模块在序列长度上具有O(N^2)的时间和内存复杂度&#x1F613; 这篇Flash Attention的工作深入硬件，新提出了一种具有IO感知的，快速的⚡️，节省内存的&#x1F9E0;，精确的&#x1F3AF;注意力算法。目前，Flash Attention已经集成至torch2.0，并且社区也提供了多种实现 78s看懂FlashAttention【有点意思·1】 核心要点 ⚡️为什么加快了计算？Fast 降低了耗时的HBM访问次数。采用Tiling技术分块从HBM加载数据到SRAM进行融合计算 &#x1F9E0;为什么节省了内存？Memory-Efficient 不再对中间矩阵S，P进行存储。在反向的时候通过Recomputation重新计算来计算梯度 &#x1F3AF;为什么是精准注意力？Exact Attention 算法流程只是分块计算，无近似操作 提出问题 Transformer结构已成为自然语言处理和图像分类等应用中最常用的架构。尽管Transformer在规模上不断增大和加深，但处理更长上下文仍然是一个挑战，因为核心的自注意力模块在序列长度上具有二次方的时间和内存复杂度。这导致在处理长序列时速度变慢且内存需求巨大。因此，我们需要一些优化算法来提高注意力模块的计算速度和内存利用率 解决方案 Forward Standard Attention 在注意力的一般实现中，对\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d} 三个输入执行以下算法得到输出\\mathbf{O}，其中softmax行级别执行 \\mathbf{S}=\\mathbf{Q} \\mathbf{K}^{\\top} \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{P}=\\operatorname{softmax}(\\mathbf{S}) \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{O}=\\mathbf{P V} \\in \\mathbb{R}^{N \\times d} 在这个算法中，\\mathbf{S}，\\mathbf{P}矩阵都是很大，需要在HBM中实例化来进行存储，这样就会带来很多HBM的访问次数， 最终体现到算法时间端到端较长的延迟 FlashAttention(Tiling) 理论基础 在传统算法中，一种方式是将Mask和SoftMax部分融合，以减少访存次数。然而，FlashAttention则更加激进，它将从输入\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}到输出\\mathbf{O}的整个过程进行融合，以避免\\mathbf{S}， \\mathbf{P}矩阵的存储开销，实现端到端的延迟缩减。然而，由于输入的长度N通常很长，无法完全将完整的\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}, \\mathbf{O}及中间计算结果存储在SRAM中。因此，需要依 赖HBM进行访存操作，与原始计算延迟相比没有太大差异，甚至会变慢(没具体测) 为了让计算过程的结果完全在SRAM中，摆脱对HBM的依赖，可以采用分片操作，每次进行部分计算，确保这些计算结果能在SRAM内进行交互，待得到对应的结果后再进行输出 这个过程中，有一点需要注意的是，之前对于softmax的计算是以行为单位的，如下所示: m(x):=\\max _{i} x_{i}, \\quad f(x):=\\left[\\begin{array}{lll} e^{x_{1}-m(x)} & \\ldots & e^{x_{B}-m(x)} \\end{array}\\right], \\quad \\ell(x):=\\sum_{i} f(x)_{i}, \\quad \\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)} 当我们将输入进行分片后，无法对完整的行数据执行Softmax操作。这是因为Softmax函数在计算时需要考虑整个行的数据 然而，我们可以通过如下所示方法来获得与完整行Softmax相同的结果，而无需使用近似操作 \\begin{array}{l} m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right), \\quad f(x)=\\left[\\begin{array}{ll} e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) & \\left.e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] \\end{array}\\right. \\\\ \\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right), \\quad \\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)} \\end{array} 具体的分块softmax代码演示 import torch q = torch.tensor([1,2]).float() v = torch.tensor([1,2]).float() q_sm = torch.softmax(q, 0) print(q_sm) # tensor([0.2689, 0.7311]) torch.dot(q_sm, v) # tensor(1.7311) m_pre = float(\"-inf\") l_pre = 0 cur_sum = 0 block1 = torch.tensor([1]).float() # get cur max value m_cur = max(torch.max(block1), m_pre) # scale pre log value by max exp l_pre *= torch.exp(m_pre - m_cur) # calculate current log sum p = torch.exp(block1 - m_cur) l_cur = torch.sum(p) + l_pre # scale pre result by log sum cur_sum = cur_sum * l_pre / l_cur p = p / l_cur cur_sum = 1 * p[0] l_pre = l_cur m_pre = m_cur print(cur_sum) # tensor(1.) block2 = torch.tensor([2]).float() m_cur = max(torch.max(block2), m_pre) l_pre *= torch.exp(m_pre - m_cur) p = torch.exp(block2 - m_cur) l_cur = torch.sum(p) + l_pre cur_sum = cur_sum * l_pre / l_cur p = p / l_cur cur_sum += 2 * p[0] print(cur_sum) # tensor(1.7311) 代码实现 @triton.jit def _fwd_kernel( Q, K, V, sm_scale, L, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, ): start_m = tl.program_id(0) off_hz = tl.program_id(1) # initialize offsets offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) offs_n = tl.arange(0, BLOCK_N) offs_d = tl.arange(0, BLOCK_DMODEL) off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk # Initialize pointers to Q, K, V q_ptrs = Q + off_q k_ptrs = K + off_k v_ptrs = V + off_v # initialize pointer to m and l m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\") l_prev = tl.zeros([BLOCK_M], dtype=tl.float32) acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32) # load q: it will stay in SRAM throughout q = tl.load(q_ptrs) # loop over k, v and update accumulator for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N): # -- compute qk ---- k = tl.load(k_ptrs) qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) qk += tl.dot(q, k) qk *= sm_scale qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\")) # compute new m m_curr = tl.maximum(tl.max(qk, 1), m_prev) # correct old l l_prev *= tl.exp(m_prev - m_curr) # attention weights p = tl.exp(qk - m_curr[:, None]) l_curr = tl.sum(p, 1) + l_prev # rescale operands of matmuls l_rcp = 1. / l_curr p *= l_rcp[:, None] acc *= (l_prev * l_rcp)[:, None] # update acc p = p.to(Q.dtype.element_ty) v = tl.load(v_ptrs) acc += tl.dot(p, v) # update m_i and l_i l_prev = l_curr m_prev = m_curr # update pointers k_ptrs += BLOCK_N * stride_kn v_ptrs += BLOCK_N * stride_vk # rematerialize offsets to save registers start_m = tl.program_id(0) offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) # write back l and m l_ptrs = L + off_hz * N_CTX + offs_m m_ptrs = M + off_hz * N_CTX + offs_m tl.store(l_ptrs, l_prev) tl.store(m_ptrs, m_prev) # initialize pointers to output offs_n = tl.arange(0, BLOCK_DMODEL) off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on out_ptrs = Out + off_o tl.store(out_ptrs, acc) IO复杂度分析 Standard Attention 对于标准注意力实现，初期我们需要把输入 \\mathbf{Q}, \\mathbf{K}, \\mathbf{V}从HBM中读取，并计算完毕后把输出\\mathbf{O}写入到HBM中 第一步把\\mathbf{Q}, \\mathbf{K}读取出来计算出\\mathbf{S}=\\mathbf{Q} \\mathbf{K}^{\\top}，然后把\\mathbf{S}存回去，内存访问复杂度\\Theta\\left(N d+N^{2}\\right) 第二步把\\mathbf{S}读取出来计算出\\mathbf{P}=\\operatorname{softmax}(\\mathbf{S})，然后把\\mathbf{P}存回去，内存访问复杂度\\Theta\\left(N^{2}\\right) 第三步把\\mathbf{V}, \\mathbf{P}读取出来计算出\\mathbf{O}=\\mathbf{P V}，然后计算出结果\\mathbf{O}，内存访问复杂度\\Theta\\left(N d+N^{2}\\right) 综上所述，整体的内存访问复杂度为\\Theta\\left(N d+N^{2}\\right) FlashAttention 对于FlashAttention，我们设置一个分块大小B_{c}来把\\mathbf{K}, \\mathbf{V}分成T_{c}块，对于\\mathbf{Q}, \\mathbf{O}的每一块都要把\\mathbf{K}, \\mathbf{V}部分的全部元素Load一遍，这样则有FlashAttention的内存访问复杂度为\\Theta\\left(N d+N d T_{c}\\right)=\\Theta\\left(N d T_{c}\\right) 在这里，我们需要两个分块大小，\\mathbf{Q}, \\mathbf{O}的分块大小B_{r}，\\mathbf{K}, \\mathbf{V}的分块大小B_{c}，我们设定SRAM的大小为M，为了能把分块后的\\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B_{c} \\times d}放进SRAM，那么则有一下限制: B_{c} d=O(M) \\Leftrightarrow B_{c}=O\\left(\\frac{M}{d}\\right) 相应的，\\mathbf{Q}, \\mathbf{O} \\in \\mathbb{R}^{B_{r} \\times d}有如下限制: B_{r} d=O(M) \\Leftrightarrow B_{r}=O\\left(\\frac{M}{d}\\right) 最终，还有一个中间态\\mathbf{S}=\\mathbf{Q K}^{\\top} \\in \\mathbb{R}^{B_{r} \\times B_{c}}需要存储，则有如下限制: B_{r} B_{c}=O(M) 综上，限制如下 B_{c}=\\Theta\\left(\\frac{M}{d}\\right), \\quad B_{r}=\\Theta\\left(\\min \\left(\\frac{M}{d}, \\frac{M}{B_{c}}\\right)\\right)=\\Theta\\left(\\min \\left(\\frac{M}{d}, d\\right)\\right) 进而推出 T_{c}=\\frac{N}{B_{c}}=\\Theta\\left(\\frac{N d}{M}\\right) 那么在M=\\Theta(N d) 的前提下，则有FlashAttention的HBM内存访问复杂度为： \\Theta\\left(N d T_{c}\\right)=\\Theta\\left(\\frac{N^{2} d^{2}}{M}\\right)=\\Theta(N d) 在语言建模中，通常有d \\lll N，则有\\Theta_{\\text {stand }}\\left(N d+N^{2}\\right)>\\Theta_{f l a s h}(N d)。这样，在前向的过程中，我们采用分块计算的方式，避免了\\mathbf{S}, \\mathbf{P}矩阵的存储开销，整体的运算都在SRAM内进行，降低了HBM访问次数，大大提升了计算的速度，减少了对存储的消耗 Backward 理论基础 在上面前向的时候我们为了减少HBM访存次数，降低内存消耗量，我们并没有对\\mathbf{S}, \\mathbf{P}矩阵进行存储，而这个在反向传播计算梯度的时候确实需要的一个信息 之前有通过Gradient checkpointing的方式来实现梯度实现在前向的时候更加节省内存 我们这里则采用重新计算的方式来计算对应的梯度。在上面前向计算的时候我们不会存储\\mathbf{S}, \\mathbf{P}矩阵，但是我们会存储对应的指数项之和L来进行梯度的计算 我们在反向的过程中最重要的事情就是就是Loss函数\\phi对\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}, \\mathbf{O}对应的梯度 其中$\\mathbf{O}对应的梯度最好计算\\mathbf{d} \\mathbf{O}=\\frac{\\partial \\phi}{\\partial \\mathbf{O}}，其中\\mathbf{O}是现成的 而\\mathbf{V}对应的梯度也很好计算，由于\\mathbf{O}=\\mathbf{P V}，根据链式求导法则和矩阵求导法则则有\\mathbf{d V}=\\mathbf{P}^{T} \\mathbf{d} \\mathbf{O}，更详细如下所示: d v{j}=\\sum{i} P{i j} d o{i}=\\sum{i} \\frac{e^{q{i}^{T}} k{j}}{L{i}} d o{i} 而\\mathbf{Q}, \\mathbf{K}对应的梯度算起来就比较复杂一点。这两个经过的计算逻辑步骤更多，我们可以一步一步的来进行计算。我们可以先计算\\mathbf{d P}，\\mathbf{d S}。由于\\mathbf{O}=\\mathbf{P V} ，则有\\mathbf{d P}如下表示 d P{i j}=d o{i}^{T} v{j} Fact: y=\\operatorname{softmax}(x)的雅各比矩阵为\\operatorname{diag}(y)-y y^{T}，具体推导见 [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1) 由于P{i:}=\\operatorname{softmax}\\left(S{i:}\\right)， 根据上述定理则有: d S{i:}=\\left(\\operatorname{diag}\\left(P{i:}\\right)-P{i:} P{i:}^{T}\\right) d P{i:}=P{i:} \\circ d P{i:}-\\left(P{i:}^{T} d P{i:}\\right) P{i:} 接下来我们定义如下表示: D{i}=P{i:}^{T} d P{i:}=\\sum \\frac{e^{q{i} \\kappa{j}}}{L{i}} d o{i}^{T} v{j}=d o{i}^{T} \\sum \\frac{e^{q{i} \\kappa{j}}}{L{i}} v{j}=d o{i}^{T} o{i} 根据上述定义简化上上式则有如下表示: d S{i:}=P{i:} \\circ d P{i:}-D{i} P{i:} 相应的d \\mathbf{S}可表示为如下形式: d S{i j}=P{i j} d P{i j}-D{i} P{i j}=P{i j}\\left(d P{i j}-D{i}\\right) 又因为S{i j}=q{i}^{T} k{j}，结合上述推导利用链式求导法则\\mathbf{Q}, \\mathbf{K}对应的梯度有如下表示： \\begin{array}{l} d q{i}=\\sum{j} d S{i j} k{j}=\\sum{j} P{i j}\\left(d P{i j}-D{i}\\right) k{j}=\\sum{j} \\frac{e^{q{i}^{T} k{j}}}{L{i}}\\left(d o{i}^{T} v{j}-D{i}\\right) k{j} \\ d k{j}=\\sum{i} d S{i j} q{i}=\\sum{i} P{i j}\\left(d P{i j}-D{i}\\right) q{i}=\\sum{i} \\frac{e^{q{i}^{T} k{j}}}{L{i}}\\left(d o{i}^{T} v{j}-D{i}\\right) q{i} \\end{array} 至此，我们得到了一个完整的包含前向和反向的，降低了HBM访问次数的，新的Attention算子 ### 代码实现 ```python @triton.jit def _bwd_kernel( Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX, num_block, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, ): off_hz = tl.program_id(0) off_z = off_hz // H off_h = off_hz % H # offset pointers for batch/head Q += off_z * stride_qz + off_h * stride_qh K += off_z * stride_qz + off_h * stride_qh V += off_z * stride_qz + off_h * stride_qh DO += off_z * stride_qz + off_h * stride_qh DQ += off_z * stride_qz + off_h * stride_qh DK += off_z * stride_qz + off_h * stride_qh DV += off_z * stride_qz + off_h * stride_qh for start_n in range(0, num_block): lo = start_n * BLOCK_M # initialize row/col offsets offs_qm = lo + tl.arange(0, BLOCK_M) offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M) offs_m = tl.arange(0, BLOCK_N) offs_k = tl.arange(0, BLOCK_DMODEL) # initialize pointers to value-like data q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk) k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk) v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk) do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk) dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk) # pointer to row-wise quantities in value-like data D_ptrs = D + off_hz * N_CTX m_ptrs = M + off_hz * N_CTX # initialize dv amd dk dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32) dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32) # k and v stay in SRAM throughout k = tl.load(k_ptrs) v = tl.load(v_ptrs) # loop over rows for start_m in range(lo, num_block * BLOCK_M, BLOCK_M): offs_m_curr = start_m + offs_m # load q, k, v, do on-chip q = tl.load(q_ptrs) # recompute p = softmax(qk, dim=-1).T # NOTE: `do` is pre-divided by `l`; no normalization here qk = tl.dot(q, tl.trans(k)) qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\")) m = tl.load(m_ptrs + offs_m_curr) p = tl.exp(qk * sm_scale - m[:, None]) # compute dv do = tl.load(do_ptrs) dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do) # compute dp = dot(v, do) Di = tl.load(D_ptrs + offs_m_curr) dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None] dp += tl.dot(do, tl.trans(v)) # compute ds = p * (dp - delta[:, None]) ds = p * dp * sm_scale # compute dk = dot(ds.T, q) dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q) # compute dq dq = tl.load(dq_ptrs) dq += tl.dot(ds.to(Q.dtype.element_ty), k) tl.store(dq_ptrs, dq) # increment pointers dq_ptrs += BLOCK_M * stride_qm q_ptrs += BLOCK_M * stride_qm do_ptrs += BLOCK_M * stride_qm # write-back dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk) dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk) tl.store(dv_ptrs, dv) tl.store(dk_ptrs, dk) ``` ### Block-Sparse 相比于上面的全量计算，块稀疏的FlashAttention需要额外提供一个Mask矩阵\\tilde{\\mathbf{M}} \\in{0,1}^{N \\times N}用于将一些元 素置零来保证块稀疏加速计算 本章对于块稀疏的一个计算只是一个简单的尝试，没有进行太深入的探索，所以这里我们先一笔带过，后面我们可以讲一篇对FlashAttention进行块稀疏优化的工作SCFA \\mathbf{S}=\\mathbf{Q K} \\mathbf{K}^{\\top} \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{P}=\\operatorname{softmax}\\left(\\mathbf{S} \\odot 1{\\overline{\\mathbf{M}}}\\right) \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{O}=\\mathbf{P V} \\in \\mathbb{R}^{N \\times d} $$ 实验验证 通过实验验证发现，FlashAttention在速度和内存占用方面都表现出明显的优势，并取得了良好的效果 目前，FlashAttention已经经过广泛验证, torch2.0中已提供flashattention的实现 正如标题《Fast and Memory-Efficient Exact Attention with IO-Awareness》所示，FlashAttention的优点在于充分考虑了在计算任务中IO的重要性，并通过分块计算的方式开发了一种快速、节省显存、精确无近似的注意力实现方法。这使得我们更便于训练具有更长上下文的Transformer模型，并且为后续注意力算法的优化提供了一个基准 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/LLM模型部署调试推理.html":{"url":"chapters/LLM模型部署调试推理.html","title":"LLM模型部署调试推理.md","summary":"LLM模型部署调试推理","keywords":"","body":"LLM模型概述主要应用GPU环境阿里云AutoDL本地Langchain-ChatGLM准备工作推理修改配置启动web服务pycharm远程配置相关技术localGPT LLM模型 什么是LLM（大语音模型） 概述 Large Language Model(LLM)，也称为大型语言模型，是一种基于机器学习和自然语言处理技术的模型，它通过对大量的文本数据进行训练，来学习服务人类语言理解和生成的能力 LLM的核心思想是通过大规模的无监督训练来学习自然语言的模式和语言结构，这在一定程度上能够模拟人类的语言认知和生成过程 与传统的NLP模型相比，LLM能够更好地理解和生成自然文本，同时还能够表现出一定的逻辑思维和推理能力 近年来，LLM得到了广泛的应用，其中最具代表性的是谷歌的BERT和OpenAI的GPT系列。这些模型在多个自然语言处理领域已经取得了显著的成果，包括文本分类、命名实体识别、情感分析、机器翻译、自动问答等 然而，在实际应用中，LLM面临着更多的挑战 首先，LLM需要大量的计算资源和大规模的数据集来训练，这对于一般的企业和个人来说十分困难 其次，由于LLM模型的复杂性和计算量较大，对于实时的语言处理应用来说，LLM在应用效率和响应速度上还存在一定的局限性 因此，如何解决模型训练和应用过程中的计算性能和效率问题，是LLM面临的主要挑战之一 主要应用 LLM在实际应用中有多种形式，以下是一些具体的示例： 自然语言理解：通过语法词汇、句法语义、语境等相互作用，使计算机能够理解人类语言。如通过语音合成功能，实现文字转语音的技术。另外，基于LLM技术的英语学习App可以针对不同的用户让机器进行自适应教学，帮助用户更加高效地学习英语 机器翻译：LLM可以利用大量的文本数据进行翻译学习，提高翻译准确度和语音翻译度，同时加速翻译速度，例如谷歌翻译等体验都有明显提升 情感分析：LLM技术可以通过对大量数据进行分析，实现对人们在社交媒体上对商品、服务和品牌等的评价，从而帮助企业了解消费者的心态，并根据情感信息来优化其营销策略 机器学习语音识别：在语音识别领域，LLM技术可以通过深度学习算法等技术改进，从而在分辨率、声音重叠、噪音和复杂语音语言中实现更好的语音识别效果。同时，在语音交互上，也可以进行一些基于LLM的定制，例如智能音箱、语音接待员和客户服务机器人等 文本生成：LLM技术在文本生成方面的应用可以在文本摘要、翻译和自动化写作等方面进行适用。例如，LLM模型可以从短篇小说的背景信息中自动生成描述、对话等内容，同时还可以通过对不同风格的文本进行学习，并从中学习创造性的文本生成 GPU环境 腾讯云GPU价格表 阿里云GPU价格表 阿里云 购买一台自己的GPU服服务器 购买阿里云服务器可以通过以下步骤进行： 访问阿里云官网并注册一个账户 登录您的阿里云账户，选择\"产品与服务\"，然后选择\"云服务器 ECS\" 在云服务器ECS页面上，您可以选择不同的实例类型和配置，根据您的需求选择适合的服务器规格。这些规格包括CPU、内存、存储空间等 配置网络和安全组，您可以设置网络类型、公网带宽、IP地址等 根据您的需求选择购买时长，可以选择包年包月或按小时计费，这里如果只是为了测试或玩一玩，可以选择最便宜的方式 配置系统和软件，您可以选择操作系统和其他软件预装选项 查看订单和价格，确认无误后，点击\"立即购买\" 在支付页面选择合适的支付方式完成购买 完成支付后，您将收到购买服务器的确认信息和服务器登录凭证 端口开放 安装基础的python环境 首先复制公网ip，使用ssh工具连接自己的服务器，用户名默认是root，密码为自定义密码，登陆进来首先会系统初始化GPU环境，等待完成后会自动重启 自动重启后，可以用nvidia-smi查看nvidia驱动配置 安装git，后面很多项目或者模型下载会用到，包括git lfs # 安装git yum install git -y # 安装git lfs（大文件下载） curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash yum install git-lfs git lfs install 安装anaconda，并创建自己的python环境 # 安装anaconda环境 wget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.sh chmod +x Anaconda3-2023.03-1-Linux-x86_64.sh ./Anaconda3-2023.03-1-Linux-x86_64.sh # 创建自己的python环境 conda create -n gpt310 python=3.10 anaconda 安装pytorch，这里的cuda选cu117，因为上面的服务器的cuda是11.4的，这里的cu117是可以向下兼容的 # 切换到gpt310 source activate gpt310 # 安装gpu版的pytorch, 需要在GPU环境下安装，否则安装是cpu版本 pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117 在Python中导入PyTorch并检查CUDA是否可用 import torch # 检查PyTorch是否使用了CUDA if torch.cuda.is_available(): print(\"CUDA可用\") else: print(\"CUDA不可用\") 还可以使用torch.cuda.get_device_capability()函数来获取计算机上支持的CUDA设备的计算能力。这可以帮助您确定所安装的CUDA版本是否与您的显卡兼容 用完机器记得关机，不然会一直扣费 AutoDL AutoDL平台简介 AutoDL算力平台是指为自动化深度学习(AutoDL)任务提供计算资源和基础设施的平台。由于深度学习任务通常需要大量的计算资源和存储空间，为了有效地执行AutoDL任务，需要具备相应的算力平台 一个优秀的AutoDL算力平台通常具备以下特点： 高性能计算：提供强大的计算能力，包括高性能的CPU、GPU或专用的AI芯片等。这些计算资源可以支持AutoDL任务的模型训练、超参数搜索和架构搜索等计算密集型任务 分布式计算：支持分布式计算和训练，使得AutoDL任务可以在多个计算节点上并行执行，从而加快任务的完成时间。这对于大规模数据和复杂模型的AutoDL任务尤为重要 数据存储和管理：提供高效的数据存储和管理系统，以支持大规模数据集的存储和访问。这可以确保AutoDL任务能够快速、可靠地访问所需的训练数据和验证数据 任务调度和资源管理：具备任务调度和资源管理功能，可以有效地管理多个AutoDL任务的执行，包括资源分配、优先级管理和任务调度等，以保证任务的顺利进行 易用性和灵活性：提供友好的用户界面和工具，使得用户可以方便地配置和管理AutoDL任务。同时，提供灵活的配置选项，以满足不同任务和需求的定制化要求 AutoDL算力平台的设计和功能旨在提供高效、可扩展和易用的计算资源，以支持AutoDL任务的快速迭代和大规模应用。它们可以帮助研究人员和开发者更加便捷地进行AutoDL实验和模型优化，从而推动自动化深度学习领域的发展 购买算力 选择计费方式和显卡 选择是否需要扩展数据盘，以及基础镜像，这里选Miniconda环境 启动机器，进入终端可以看到 +--------------------------------------------------AutoDL--------------------------------------------------------+ 目录说明: ╔═════════════════╦════════╦════╦═════════════════════════════════════════════════════════════════════════╗ ║目录 ║名称 ║速度║说明 ║ ╠═════════════════╬════════╬════╬═════════════════════════════════════════════════════════════════════════╣ ║/ ║系 统 盘║一般║实例关机数据不会丢失，可存放代码等。会随保存镜像一起保存。 ║ ║/root/autodl-tmp ║数 据 盘║ 快 ║实例关机数据不会丢失，可存放读写IO要求高的数据。但不会随保存镜像一起保存 ║ ╚═════════════════╩════════╩════╩═════════════════════════════════════════════════════════════════════════╝ CPU ：0.5 核心 内存：2 GB GPU ：No devices were found 存储： 系 统 盘/ ：46% 12G/25G 数 据 盘/root/autodl-tmp：65% 33G/50G +----------------------------------------------------------------------------------------------------------------+ *注意: 1.系统盘较小请将大的数据存放于数据盘或网盘中，重置系统时数据盘和网盘中的数据不受影响 2.清理系统盘请参考：https://www.autodl.com/docs/qa/ 安装git-lfs以及ssl等依赖 curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash sudo apt-get install git-lfs git lfs install sudo apt-get install openssl sudo apt-get install libssl-dev 安装pytorch环境 # 创建自己的python环境 conda create -n gpt310 python=3.10 anaconda # 切换到gpt310 source activate gpt310 # 安装gpu版的pytorch, 需要在GPU环境下安装，否则安装是cpu版本 pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117 在Python中导入PyTorch并检查CUDA是否可用 import torch # 检查PyTorch是否使用了CUDA if torch.cuda.is_available(): print(\"CUDA可用\") else: print(\"CUDA不可用\") 其它事项 控制台下容器实例就是租用的机器，自定义服务是将容器内的6006端口映射到公网 /root/autodl-tmp可以放一些数据或模型，开关机不会丢失，但不随镜像一起保存 无卡开机模型不带GPU，很便宜，一小时0.01元 本地 &#x1F47B;没有显卡，&#x1F915;伤不起&#x1F915; Langchain-ChatGLM github imClumsyPanda/langchain-ChatGLM &#x1F916;️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案 &#x1F4A1; 受 GanymedeNil 的项目 document.ai 和 AlexZhangji 创建的 ChatGLM-6B Pull Request 启发，建立了全流程可使用开源模型实现的本地知识库问答应用。现已支持使用 ChatGLM-6B 等大语言模型直接接入，或通过 fastchat api 形式接入 Vicuna, Alpaca, LLaMA, Koala, RWKV 等模型 ✅ 本项目中 Embedding 默认选用的是 GanymedeNil/text2vec-large-chinese，LLM 默认选用的是 ChatGLM-6B。依托上述模型，本项目可实现全部使用开源模型离线私有部署 ⛓️ 本项目实现原理如下图所示，过程包括加载文件 -> 读取文本 -> 文本分割 -> 文本向量化 -> 问句向量化 -> 在文本向量中匹配出与问句向量最相似的top k个 -> 匹配出的文本作为上下文和问题一起添加到prompt中 -> 提交给LLM生成回答 从文档处理角度来看，实现流程如下： ChatGLM-6B模型硬件需求 注：如未将模型下载至本地，请执行前检查$HOME/.cache/huggingface/文件夹剩余空间，模型文件下载至本地需要15GB存储空间。注：一些其它的可选启动项见项目启动选项模型下载方法可参考常见问题中Q8 量化等级 最低 GPU 显存（推理） 最低 GPU 显存（高效参数微调） FP16（无量化） 13 GB 14 GB INT8 8 GB 9 GB INT4 6 GB 7 GB 准备工作 项目环境准备 先在/home目录下创建huangyc文件夹，进入huangyc目录下后，拉取项目代码 git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git 安装必要库 # centos系统下 yum install libX11 -y yum install libXext -y pip uninstall detectron2 # ubuntu系统不需要 进入langchain-ChatGLM文件夹，安装依赖库，AutoDl平台下好像需要加-i参数 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple 模型准备 从huggingface开放模型下找到自己的需要的模型，用git lfs下载，如果在AutoDl平台，可以把模型下载到/root/autodl-tmp git clone https://huggingface.co/THUDM/chatglm-6b-int4.git 可以用ls -lh .命令查看下载的文件大小是否正常 下载完的chatglm-6b-int4模型文件夹，放到langchain-ChatGLM项目的model文件夹下，没有model文件夹的话，自己创建下 如果下载失败的话，可以试下用wget下载，例如 wget --no-check-certificate https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/pytorch_model.bin 推理 修改配置 打开langchain-ChatGLM/config/model_config.py，修改如下配置 # supported LLM models # llm_model_dict 处理了loader的一些预设行为，如加载位置，模型名称，模型处理器实例 llm_model_dict = { ...... \"chatglm-6b-int4\": { \"name\": \"chatglm-6b-int4\", - \"pretrained_model_name\": \"THUDM/chatglm-6b-int4\", + \"pretrained_model_name\": \"model/chatglm-6b-int4\", \"local_model_path\": None, \"provides\": \"ChatGLM\" } ...... } # LLM 名称 LLM_MODEL = \"chatglm-6b-int4\" # 如果你需要加载本地的model，指定这个参数 ` --no-remote-model`，或者下方参数修改为 `True` NO_REMOTE_MODEL = False 这样模型会从本地加载 修改webui.py，用于启动Web 交互 (demo .queue(concurrency_count=3) - .launch(server_name='0.0.0.0', + .launch(server_name='localhost', server_port=7860, # AutoDl平台用的是6006 show_api=False, - share=False, + share=True, inbrowser=False)) 阿里云服务器那边需要开放7860端口 启动web服务 进入到langchain-ChatGLM项目下，执行webui.py python webui.py 如果发现端口被占用了，可以用netstat -atunlp | grep 7860命令查看并杀死占用该端口的进程(自己判断) 打开网页，你的ip:7860，就可以看到如下界面，默认是对话界面 可以切换到知识库测试，在这里可以上传自己的文档到知识库 后台看下当前显存占用，模型用的是chatglm-6b-int4模型 之后测试上传了《流畅的Python》高清官方中文版.pdf，后台一直在更新知识库，等了十几分钟，还没结束，我就切换到普通对话了，又测了十几轮对话，突然爆了显存不够，不知道是不是上传知识库引起的，还是随着对话增加，显存一直没释放 pycharm远程配置 PyCharm远程配置解释器和项目提供了以下几个重要的用途和优势： 远程开发：您可以在本地使用PyCharm编写代码，并将代码部署和运行在远程服务器上。这意味着您不需要在本地配置和安装与远程服务器环境相同的软件和依赖项。您可以利用服务器上的计算资源和环境来进行开发和测试，减轻了本地机器的负担 统一开发环境：通过远程配置解释器和项目，您可以在本地使用PyCharm的功能和工具来开发和调试代码。您可以享受PyCharm提供的智能代码补全、调试器、版本控制集成等功能，无需切换到其他编辑器或远程终端 协作与团队开发：远程配置解释器和项目使团队成员可以共享相同的开发环境。无论是在本地还是远程服务器上，团队成员可以使用相同的配置和依赖项来开发和测试代码。这有助于确保代码在不同环境下的一致性，并促进团队协作和开发效率 远程调试和错误排查：通过配置远程解释器，您可以在本地使用PyCharm的调试器来调试远程服务器上的代码。这样，您可以逐步执行代码、观察变量和监控程序状态，以便更轻松地进行错误排查和修复 远程部署和管理：通过远程配置项目，您可以将本地的代码同步到远程服务器上，并在服务器上运行和管理项目。这简化了部署过程，并使您能够直接在远程服务器上操作项目文件和资源 PyCharm远程配置解释器和项目提供了一种方便而高效的方式，让您可以在本地使用PyCharm进行开发和调试，同时利用远程服务器的优势来运行和部署代码。这对于需要在远程环境下进行开发和协作的场景非常有用，如远程服务器上的Web开发、数据处理和云计算等任务 pycharm远程配置参考本站的Spark集群搭建章节下的配置远程解释器 相关技术 langchain: langchain、LangChain：为你定制一个专属的GPT LangChain是一个用于开发基于语言模型的应用程序开发框架，用户可以利用LangChain的模块来改善大语言模型的使用，通过输入自己的知识库来定制化自己的大语言模型 faiss: Faiss Documentation、[python] 向量检索库Faiss使用指北 所谓相似性搜索是指通过比较多维空间中数据之间的相似性来搜索与输入数据最相似的目标数据。例如人脸识别中，通过比较人脸向量之前的距离来识别当前人脸与哪张人脸相似。因此，该技术被广泛应用于信息检索、计算机视觉、数据分析等领域。如果要检索的数据很多时，那么就需要一个向量检索库来加速检索 Faiss包含多种相似性搜索方法，并提供cpu和gpu版本支持。Faiss的优势在于通过较小的精度损失提高向量相似度的检索速度和减少内存使用量 ChatGLM-6B: chatglm、github THUDM/ChatGLM-6B ChatGLM-6B是一个开源的、支持中英双语的对话语言模型，基于General Language Model (GLM)架构，具有62亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署(INT4量化级别下最低只需6GB显存)。 ChatGLM-6B使用了和ChatGPT相似的技术，针对中文问答和对话进行了优化。经过约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答 Hugging Face: Hugging Face Hugging Face是一个知名的开源社区和公司，专注于自然语言处理(NLP)和机器学习(ML)领域。他们开发了许多流行的开源工具和库，使得构建和应用NLP模型更加便捷 localGPT github PromtEngineer/localGPT 等待 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/dl_in_vision_field.html":{"url":"chapters/dl_in_vision_field.html","title":"dl_in_vision_field.md","summary":null,"keywords":"","body":"模型介绍图像分类目标检测和跟踪图像分割图像处理图像生成动作识别风格转换人脸识别图像描述OCR 必读！计算机视觉四大基本任务(分类、定位、检测、分割) 模型介绍 模型可视化netron工具库 pytorch模型转onnx模型的方法详解 图像分类 Stable Diffusion总共包含三个主要的组件 人工智能Ai画画——stable diffusion 原理和使用方法详解！ 1）Clip Text用于文本编码。输入：文本输出：77个token嵌入向量，其中每个向量包含768个维度 2）UNet + Scheduler在信息（潜）空间中逐步处理/扩散信息。输入：文本嵌入和一个由噪声组成的初始多维数组（结构化的数字列表，也叫张量tensor）。输出：一个经过处理的信息阵列 3）自编码解码器（Autoencoder Decoder），使用处理过的信息矩阵绘制最终图像的解码器。 Clip Text 是一种自然语言处理模型，由 OpenAI 开发。它基于 CLIP（Contrastive Language-Image Pretraining）模型，旨在将文本和图像联系起来。Clip Text 模型可以理解和处理文本数据，以便进行各种任务，例如文本分类、情感分析、命名实体识别等。通过训练 Clip Text 模型，可以使其具备对文本的理解能力，从而支持在自然语言处理领域进行各种应用和研究。 UNet 是一种用于图像分割的卷积神经网络模型。它最初由 Olaf Ronneberger 等人在 2015 年提出，旨在解决医学图像分割任务中的问题。UNet 的设计灵感来自于生物学中的神经元结构，它具有一个特殊的 U 形结构，因此得名 UNet。 UNet 的特点是具有对称的编码器-解码器结构，其中编码器部分由多个卷积和池化层组成，用于逐步提取图像的特征。解码器部分则通过上采样和卷积操作逐步将特征映射恢复到原始图像的尺寸，用于生成分割结果。此外，UNet 还通过跳跃连接（skip connections）在编码器和解码器之间建立了直接连接，以便保留和利用不同层级的特征信息。 UNet 在医学图像分割任务中取得了很好的效果，并且在其他领域的图像分割任务中也得到了广泛应用。它的网络结构简单、易于实现和训练，并且能够处理不同尺度和形状的目标物体，因此成为图像分割领域的重要模型之一。 目标检测和跟踪 图像分割 语义分割 实例分割 图像处理 超分辨率 除雾 图像生成 动作识别 风格转换 人脸识别 图像描述 OCR 深入了解视觉语言模型 BEiT: BERT Pre-Training of Image Transformers 2022 【深度学习】详解 BEiT 《BEIT》-基于图像重建进行预训练！微软提出BEIT，Top-1准确率达86.3%！代码已开源！ 与其他模型不同，VisionEncoderDecoderModel 是一个标准化的模型，可用于初始化任意图像转文本模型，这类模型可以使用任何预训练的基于Transformer的视觉模型作为编码器(例如ViT(子监督训练)、BEiT、DeiT、Swin)以及任何预训练的语言模型作为解码器(例如RoBERTa、GPT2、BERT、DistilBERT)。事实上，TrOCR是这个标准类的一个实例 TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models 2022 微软亚洲研究院的研究员们展开了深入研究，提出了首个利用预训练模型的端到端基于Transformer的文本识别OCR模型：TrOCR。该模型简单有效，可以使用大规模合成数据进行预训练，并且能够在人工标注的数据上进行微调。实验证明，TrOCR在打印数据和手写数据上均超过了当前最先进的模型 一般的光学字符识别包含两个部分：文本检测和文本识别 文本检测: 用于在文本图像中定位文本块，粒度可以是单词级别或是文本行级别 目前的解决方案大多是将该任务视为物体检测问题，并采用了如YoLOv5和DBNet的传统物体检测模型 文本识别: 致力于理解文本图像并将视觉信号转换为自然语言符号，该任务通常使用编码器-解码器架构 现有方法采用了基于CNN网络的编码器进行图像理解，以及基于RNN网络的解码器进行文本生成 为了更有效的训练TrOCR模型，研究员们使用了ViT模式的预训练模型和BERT模式的预训练模型，来分别初始化编码器和解码器 TrOCRForCausalLM huggingface TrOCRForCausalLM 翻译：The ViTFeatureExtractor class is responsible for preprocessing the input image and RobertaTokenizer decodes the generated target tokens to the target string. The TrOCRProcessor wraps ViTFeatureExtractor and RobertaTokenizer into a single instance to both extract the input features and decode the predicted token ids. ViTFeatureExtractor类负责预处理输入图像，而RobertaTokenizer则将生成的目标标记解码为目标字符串。TrOCRProcessor将ViTFeatureExtractor和RobertaTokenizer封装为单个实例，既可以提取输入特征，又可以解码预测的标记ID 翻译：The VisionEncoderDecoderModel can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model as the encoder (e.g. ViT, BEiT, DeiT) and any pretrained language model as the decoder (e.g. RoBERTa, GPT2, BERT). The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for example) TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei. An example of how to use a VisionEncoderDecoderModel for inference can be seen in TrOCR. VisionEncoderDecoderModel可以用于初始化一个图像到文本序列模型，其中编码器可以是任何预训练的图像自编码模型（例如ViT、BEiT、DeiT），解码器可以是任何预训练的语言模型（例如RoBERTa、GPT2、BERT） 初始化图像到文本序列模型时使用预训练的检查点已经被证明是有效的，例如在《TrOCR：基于预训练模型的基于Transformer的光学字符识别》一文中，作者Minghao Li、Tengchao Lv、Lei Cui、Yijuan Lu、Dinei Florencio、Cha Zhang、Zhoujun Li、Furu Wei展示了这一点 关于如何使用VisionEncoderDecoderModel进行推断的示例可以在TrOCR中找到 图像特定的归纳偏差 图像特定的归纳偏差是指在处理图像数据时，机器学习算法或人类大脑对图像的处理和理解中存在的偏好或倾向性。这种偏差可能导致算法或人类在处理图像时出现系统性的错误或误解。 图像特定的归纳偏差可能源自以下几个方面： 形状偏差：人类和机器学习算法在处理图像时，可能更容易关注物体的形状和轮廓。这可能导致对于形状特征较强的物体更敏感，而对于纹理、颜色等其他特征的重要性较低。 颜色偏差：颜色是图像中的重要特征之一，但人类和机器学习算法可能对不同颜色的感知和辨别能力存在差异。某些颜色的辨别可能更容易，而对于其他颜色的辨别可能相对较差。 尺度偏差：图像中的物体尺度可能对于算法或人类的视觉处理产生影响。例如，较大尺度的物体可能更容易被注意到和理解，而较小尺度的物体可能容易被忽略或误解。 视角偏差：视角对于理解图像中的物体和场景也具有重要作用。不同的视角可能导致物体的外观发生变化，从而影响算法或人类对物体的认知和解释。 这些图像特定的归纳偏差可能会对计算机视觉任务（如物体检测、图像分类等）的性能产生影响，并导致一些常见的误分类或误解。为了减少这些偏差的影响，研究人员和开发者通常会采取一系列的方法，例如使用更丰富的特征表示、进行数据增强、多角度训练等，以提高算法在处理图像数据时的准确性和鲁棒性。 基于TrOCR 提一些面试题 当涉及到TrOCR（Transformer-based Optical Character Recognition）的面试问题时，可以考虑以下几个方面的问题： 什么是TrOCR？它与传统的OCR方法有何不同？ TrOCR中使用的是哪种Transformer模型？请描述其结构和主要组成部分。 在TrOCR中，图像是如何被处理和输入到Transformer模型中的？ TrOCR如何实现对文本的自回归生成？它采用了哪些技术和策略？ TrOCR中的预训练和微调是如何进行的？使用了哪些数据集和训练方法？ TrOCR在哪些任务和数据集上取得了优秀的性能？可以谈谈其在印刷体和手写体识别任务上的表现。 TrOCR的优势和局限性是什么？相对于传统的OCR方法，它有哪些潜在的应用前景和挑战？ 在TrOCR中，是否存在一些常见的问题或挑战？可以谈谈对应的解决方法或改进思路。 TrOCR在实际应用中可能面临的计算资源和效率方面的问题有哪些？如何解决或优化这些问题？ 除了TrOCR，还有哪些基于Transformer的图像处理任务和应用？可以举例说明。 这些问题将涵盖TrOCR的基本概念、模型结构、训练和应用等方面，有助于评估面试者对TrOCR的理解和熟悉程度。根据具体情况，你还可以进一步展开讨论相关的深入问题。 回答下你提的十个问题 当涉及到TrOCR（Transformer-based Optical Character Recognition）的面试问题时，以下是对你提出的十个问题的简要回答： TrOCR是一种基于Transformer的光学字符识别方法，与传统的OCR方法相比，它利用Transformer模型实现了端到端的文本生成和理解，具有更好的建模能力和上下文理解能力。 TrOCR使用的是Transformer模型，它由编码器和解码器组成。编码器采用自注意力机制来处理输入图像的特征表示，解码器采用自回归生成的方式逐步生成输出文本。 在TrOCR中，图像经过预处理步骤，如分割为固定大小的图像块，并进行线性嵌入，然后添加位置编码。处理后的图像表示被输入到Transformer编码器中进行处理。 TrOCR采用自回归生成的方式对文本进行生成，即逐个生成每个字符或词汇。它使用了类似于语言建模的技术，通过上文的信息来预测下一个字符或词汇。 TrOCR的预训练阶段通常使用大规模的合成数据进行训练，以学习图像和文本之间的对应关系。然后，可以通过微调阶段使用真实标注的数据来优化模型的性能。 TrOCR在印刷体和手写体识别任务上取得了优秀的性能。它在大规模的OCR数据集上进行了评估，并在准确性和泛化能力方面超过了传统的OCR方法。 TrOCR的优势包括能够对长文本进行建模、上下文理解能力强、适应不同语种和字体样式等。然而，TrOCR可能对计算资源要求较高，并且在处理一些特定的复杂场景（如低质量图像或特殊字体）时可能面临挑战。 在TrOCR中，一些常见的问题包括文本重复生成、模糊图像的处理、长文本的生成和训练数据的标注质量。解决这些问题的方法可能涉及使用更复杂的解码策略、改进的图像预处理和增强技术以及更好的数据标注和清洗方法。 TrOCR在实际应用中可能面临的计算资源和效率方面的问题包括模型大小和推理速度。为了解决这些问题，可以采用模型压缩和加速技术，如剪枝、量化和模型并行化等。 除了TrOCR，还有许多基于Transformer的图像处理任务和应用。例如，图像分类、目标检测 OCR识别手写文字(中文+英文) 参考文献： TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models 2022 该论文提出了一种名为TrOCR的基于Transformer的光学字符识别(OCR)模型，通过利用预训练模型进行特征提取和文本生成，实现端到端的文本识别任务 TrOCR模型的关键创新点包括以下几个方面： 基于Transformer的架构：TrOCR采用了Transformer架构作为其基础模型，其中包括编码器和解码器。编码器用于提取图像特征，解码器用于生成识别的文本序列 预训练模型的应用：TrOCR利用预训练的图像和文本模型作为编码器和解码器，如ViT、RoBERTa等。这些预训练模型能够提供丰富的视觉和语言表示能力，有助于提高OCR的准确性 大规模合成数据集：为了进行预训练和微调，TrOCR使用了大规模的合成数据集，包括数百万张打印文本图像和手写文本图像。这样可以增加模型在不同领域和样式的文本上的泛化能力 实验证明，TrOCR模型在打印文本、手写文本和场景文本识别任务上取得了优异的性能表现，超过了当前的state-of-the-art模型。该论文的贡献在于将Transformer应用于OCR任务，并且通过预训练模型的利用提高了OCR的准确性和泛化能力 架构： model(VisionEncoderDecoderModel) = encoder(DeiT)+decoder(TrOCRForCausalLM) DeiTModel( (embeddings): DeiTEmbeddings( (patch_embeddings): DeiTPatchEmbeddings( (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16)) ) (dropout): Dropout(p=0.0, inplace=False) ) (encoder): DeiTEncoder( (layer): ModuleList( (0): DeiTLayer( (attention): DeiTAttention( (attention): DeiTSelfAttention( (query): Linear(in_features=384, out_features=384, bias=True) (key): Linear(in_features=384, out_features=384, bias=True) (value): Linear(in_features=384, out_features=384, bias=True) (dropout): Dropout(p=0.0, inplace=False) ) (output): DeiTSelfOutput( (dense): Linear(in_features=384, out_features=384, bias=True) (dropout): Dropout(p=0.0, inplace=False) ) ) (intermediate): DeiTIntermediate( (dense): Linear(in_features=384, out_features=1536, bias=True) (intermediate_act_fn): GELUActivation() ) (output): DeiTOutput( (dense): Linear(in_features=1536, out_features=384, bias=True) (dropout): Dropout(p=0.0, inplace=False) ) (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True) (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True) ) * 12 ) ) (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True) (pooler): DeiTPooler( (dense): Linear(in_features=384, out_features=384, bias=True) (activation): Tanh() ) ) # 通过线性层将编码器和解码器连接到了一起 enc_to_dec_proj = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.hidden_size) TrOCRForCausalLM( (model): TrOCRDecoderWrapper( (decoder): TrOCRDecoder( (embed_tokens): Embedding(11318, 256, padding_idx=1) (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256) (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (layers): ModuleList( (0): TrOCRDecoderLayer( (self_attn): TrOCRAttention( (k_proj): Linear(in_features=256, out_features=256, bias=True) (v_proj): Linear(in_features=256, out_features=256, bias=True) (q_proj): Linear(in_features=256, out_features=256, bias=True) (out_proj): Linear(in_features=256, out_features=256, bias=True) ) (activation_fn): ReLU() (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (encoder_attn): TrOCRAttention( (k_proj): Linear(in_features=384, out_features=256, bias=True) (v_proj): Linear(in_features=384, out_features=256, bias=True) (q_proj): Linear(in_features=256, out_features=256, bias=True) (out_proj): Linear(in_features=256, out_features=256, bias=True) ) (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (fc1): Linear(in_features=256, out_features=1024, bias=True) (fc2): Linear(in_features=1024, out_features=256, bias=True) (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True) ) * 6 ) ) ) (output_projection): Linear(in_features=256, out_features=11318, bias=False) ) 实现细节: 数据集: Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/huggingface基本使用教程.html":{"url":"chapters/huggingface基本使用教程.html","title":"huggingface基本使用教程.md","summary":"huggingface基本使用教程","keywords":"","body":"huggingface概述安装模型下载加速git clonehuggingface-cli多线程下载器镜像网站快速开始datasets安装快速开始视觉nlp概述DataCollator类加载数据集进阶加载数据集探索数据集Preprocess处理构建数据集分享数据集评估指标安装快速开始指标种类指标加载指标计算结果存储可视化选择合适指标transformers概述安装快速开始PipelineAutoClassAutoConfigtokenizerTrainer教程模型训练分布式加速示例代码PEFT模块其他模块核心类ModelOutputPreTrainedModelAutoTrainGradioDiffusersAccelerate huggingface 概述 Hugging Face 官网任务分类和示例 LLM高效调参_PEFT库简介及使用 Hugging Face是一个知名的开源社区和公司，专注于自然语言处理(NLP)和机器学习(ML)领域。他们开发了许多流行的开源工具和库，使得构建和应用NLP模型更加便捷 Hugging face起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，10,000个数据集，变成了机器学习界的github 在这里主要有以下大家需要的资源 Datasets：数据集，以及数据集的下载地址 Models：包括各种处理CV和NLP等任务的模型，上面模型都是可以免费获得 主要包括计算机视觉、自然语言处理、语音处理、多模态、表格处理、强化学习 course：免费的nlp课程 docs：文档 展开细节 Computer Vision(计算机视觉任务)：包括lmage Classification(图像分类)，lmage Segmentation(图像分割)、zero-Shot lmage Classification(零样本图像分类)、lmage-to-Image(图像到图像的任务)、Unconditional lmage Generation(无条件图像生成)、Object Detection(目标检测)、Video Classification(视频分类)、Depth Estimation(深度估计，估计拍摄者距离图像各处的距离) Natural Language Processing(自然语言处理)：包括Translation(机器翻译)、Fill-Mask(填充掩码，预测句子中被遮掩的词)、Token Classification(词分类)、Sentence Similarity(句子相似度)、Question Answering(问答系统)，Summarization(总结，缩句)、Zero-Shot Classification (零样本分类)、Text Classification(文本分类)、Text2Text(文本到文本的生成)、Text Generation(文本生成)、Conversational(聊天)、Table Question Answer(表问答，1.预测表格中被遮掩单词2.数字推理，判断句子是否被表格数据支持) Audio(语音)：Automatic Speech Recognition(语音识别)、Audio Classification(语音分类)、Text-to-Speech(文本到语音的生成)、Audio-to-Audio(语音到语音的生成)、Voice Activity Detection(声音检测、检测识别出需要的声音部分) Multimodal(多模态)：Feature Extraction(特征提取)、Text-to-Image(文本到图像)、Visual Question Answering(视觉问答)、Image2Text(图像到文本)、Document Question Answering(文档问答) Tabular(表格)：Tabular Classification(表分类)、Tabular Regression(表回归) Reinforcement Learning(强化学习)：Reinforcement Learning(强化学习)、Robotics(机器人) 安装 安装transformers库 pip install transformers 模型下载加速 如何快速下载huggingface模型——全方法总结 git clone 官方提供了 git clone repo_url 的方式下载，这种方法相当简单，然而却是最不推荐直接用的方法，缺点有二： 不支持断点续传，断了重头再来 clone 会下载历史版本占用磁盘空间，即使没有历史版本 huggingface-cli hf的模型下载工具: download-files-from-the-hub huggingface-cli 隶属于 huggingface_hub 库，不仅可以下载模型、数据，还可以可以登录huggingface、上传模型、数据等 huggingface-cli 属于官方工具，其长期支持肯定是最好的。优先推荐！ 安装依赖 pip install -U huggingface_hub 注意：huggingface_hub 依赖于 Python>=3.8，此外需要安装 0.17.0 及以上的版本，推荐0.19.0+ 基本用法 huggingface-cli download --resume-download bigscience/bloom-560m --local-dir bloom-560m 下载数据集 huggingface-cli download --resume-download --repo-type dataset lavita/medical-qa-shared-task-v1-toy 值得注意的是，有个--local-dir-use-symlinks False 参数可选，因为huggingface的工具链默认会使用符号链接来存储下载的文件，导致--local-dir指定的目录中都是一些链接文件，真实模型则存储在~/.cache/huggingface下，如果不喜欢这个可以用 --local-dir-use-symlinks False取消这个逻辑 多线程下载器 多线程加速是一种有效、显著提高下载速度的方法 经典多线程工具推荐两个：IDM、Aria2。 IDM 适用于 Windows、aria2 适用于 Linux。本文头图就是 IDM 工具。因此获取URL后，可以利用这些多线程工具来下载。以我的一次实测为例，单线程700KB/s，IDM 8线程 6MB/s。千兆宽带下，利用IDM能跑到80MB/s+ 手动获取仓库中所有 URL 比较麻烦，作者写了一个命令行脚本 hdf.sh（Gitst链接），结合自动获取 url 以及 aria2 多线程下载，适合于 Linux #!/usr/bin/env bash # Color definitions RED='\\033[0;31m' GREEN='\\033[0;32m' YELLOW='\\033[1;33m' NC='\\033[0m' # No Color trap 'printf \"${YELLOW}\\nDownload interrupted. If you re-run the command, you can resume the download from the breakpoint.\\n${NC}\"; exit 1' INT display_help() { cat [--include include_pattern] [--exclude exclude_pattern] [--hf_username username] [--hf_token token] [--tool wget|aria2c] [-x threads] [--dataset] Description: Downloads a model or dataset from Hugging Face using the provided model ID. Parameters: model_id The Hugging Face model ID in the format 'repo/model_name'. --include (Optional) Flag to specify a string pattern to include files for downloading. --exclude (Optional) Flag to specify a string pattern to exclude files from downloading. exclude_pattern The pattern to match against filenames for exclusion. --hf_username (Optional) Hugging Face username for authentication. --hf_token (Optional) Hugging Face token for authentication. --tool (Optional) Download tool to use. Can be wget (default) or aria2c. -x (Optional) Number of download threads for aria2c. --dataset (Optional) Flag to indicate downloading a dataset. Example: hfd bigscience/bloom-560m --exclude safetensors hfd meta-llama/Llama-2-7b --hf_username myuser --hf_token mytoken --tool aria2c -x 8 hfd lavita/medical-qa-shared-task-v1-toy --dataset EOF exit 1 } MODEL_ID=$1 shift # Default values TOOL=\"wget\" THREADS=1 HF_ENDPOINT=${HF_ENDPOINT:-\"https://huggingface.co\"} while [[ $# -gt 0 ]]; do case $1 in --include) INCLUDE_PATTERN=\"$2\"; shift 2 ;; --exclude) EXCLUDE_PATTERN=\"$2\"; shift 2 ;; --hf_username) HF_USERNAME=\"$2\"; shift 2 ;; --hf_token) HF_TOKEN=\"$2\"; shift 2 ;; --tool) TOOL=\"$2\"; shift 2 ;; -x) THREADS=\"$2\"; shift 2 ;; --dataset) DATASET=1; shift ;; *) shift ;; esac done # Check if aria2, wget, curl, git, and git-lfs are installed check_command() { if ! command -v $1 &>/dev/null; then echo -e \"${RED}$1 is not installed. Please install it first.${NC}\" exit 1 fi } [[ \"$TOOL\" == \"aria2c\" ]] && check_command aria2c [[ \"$TOOL\" == \"wget\" ]] && check_command wget check_command curl; check_command git; check_command git-lfs [[ -z \"$MODEL_ID\" || \"$MODEL_ID\" =~ ^-h ]] && display_help MODEL_DIR=\"${MODEL_ID#*/}\" if [[ \"$DATASET\" == 1 ]]; then MODEL_ID=\"datasets/$MODEL_ID\" fi echo \"Downloading to ./$MODEL_DIR\" if [ -d \"$MODEL_DIR/.git\" ]; then printf \"${YELLOW}%s exists, Skip Clone.\\n${NC}\" \"$MODEL_DIR\" cd \"$MODEL_DIR\" && GIT_LFS_SKIP_SMUDGE=1 git pull || { printf \"Git pull failed.\\n\"; exit 1; } else REPO_URL=\"$HF_ENDPOINT/$MODEL_ID\" GIT_REFS_URL=\"${REPO_URL}/info/refs?service=git-upload-pack\" echo \"Test GIT_REFS_URL: $GIT_REFS_URL\" response=$(curl -s -o /dev/null -w \"%{http_code}\" \"$GIT_REFS_URL\") if [ \"$response\" == \"401\" ] || [ \"$response\" == \"403\" ]; then if [[ -z \"$HF_USERNAME\" || -z \"$HF_TOKEN\" ]]; then printf \"${RED}HTTP Status Code: $response.\\nThe repository requires authentication, but --hf_username and --hf_token is not passed. Please get token from https://huggingface.co/settings/tokens.\\nExiting.\\n${NC}\" exit 1 fi REPO_URL=\"https://$HF_USERNAME:$HF_TOKEN@${HF_ENDPOINT#https://}/$MODEL_ID\" elif [ \"$response\" != \"200\" ]; then echo -e \"${RED}Unexpected HTTP Status Code: $response.\\nExiting.\\n${NC}\"; exit 1 fi echo \"git clone $REPO_URL\" GIT_LFS_SKIP_SMUDGE=1 git clone \"$REPO_URL\" && cd \"$MODEL_DIR\" || { printf \"${RED}Git clone failed.\\n${NC}\"; exit 1; } for file in $(git lfs ls-files | awk '{print $3}'); do truncate -s 0 \"$file\" done fi printf \"\\nStart Downloading lfs files, bash script:\\n\" files=$(git lfs ls-files | awk '{print $3}') declare -a urls for file in $files; do url=\"$HF_ENDPOINT/$MODEL_ID/resolve/main/$file\" file_dir=$(dirname \"$file\") mkdir -p \"$file_dir\" if [[ \"$TOOL\" == \"wget\" ]]; then download_cmd=\"wget -c \\\"$url\\\" -O \\\"$file\\\"\" [[ -n \"$HF_TOKEN\" ]] && download_cmd=\"wget --header=\\\"Authorization: Bearer ${HF_TOKEN}\\\" -c \\\"$url\\\" -O \\\"$file\\\"\" else download_cmd=\"aria2c -x $THREADS -s $THREADS -k 1M -c \\\"$url\\\" -d \\\"$file_dir\\\" -o \\\"$(basename \"$file\")\\\"\" [[ -n \"$HF_TOKEN\" ]] && download_cmd=\"aria2c --header=\\\"Authorization: Bearer ${HF_TOKEN}\\\" -x $THREADS -s $THREADS -k 1M -c \\\"$url\\\" -d \\\"$file_dir\\\" -o \\\"$(basename \"$file\")\\\"\" fi [[ -n \"$INCLUDE_PATTERN\" && $file != *\"$INCLUDE_PATTERN\"* ]] && printf \"# %s\\n\" \"$download_cmd\" && continue [[ -n \"$EXCLUDE_PATTERN\" && $file == *\"$EXCLUDE_PATTERN\"* ]] && printf \"# %s\\n\" \"$download_cmd\" && continue printf \"%s\\n\" \"$download_cmd\" urls+=(\"$url|$file\") done for url_file in \"${urls[@]}\"; do IFS='|' read -r url file 该工具同样支持设置镜像端点的环境变量: export HF_ENDPOINT=\"https://hf-mirror.com\" 基本命令： ./hdf.sh bigscience/bloom-560m --tool aria2c -x 4 如果没有安装 aria2，则可以默认用 wget： ./hdf.sh bigscience/bloom-560m 镜像网站 Huggingface-镜像网站 可下载模型和数据集，解决Huggingface无法访问问题 使用以下py脚本可以快速生成下载模型等文件的sh脚本 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: download_hf_models.py @Description: @time: 2024/1/18 11:08 \"\"\" import contextlib import sys from typing import List from urllib.parse import unquote, urlparse import requests from basic_support.logger.logger_config import logger @contextlib.contextmanager def print_to_file(file: str, mode: str = 'w', encoding='utf-8', errors=None, newline=None, closefd=True): \"\"\" 将print重定向输出到文件 :param file: 文件名 :param mode: 读写模式 :param encoding: 文件编码 :param errors: :param newline: :param closefd: :return: \"\"\" f = open(file=file, mode=mode, encoding=encoding, errors=errors, newline=newline, closefd=closefd) # 保存原来的sys.stdout original_stdout = sys.stdout # 将sys.stdout重定向到文件流 sys.stdout = f yield # 恢复原来的sys.stdout sys.stdout = original_stdout f.close() def extract_main_domain(url): parsed_url = urlparse(url) main_domain = f\"{parsed_url.scheme}://{parsed_url.hostname}\" return main_domain def gen_download_hf_models_script(model_url: str, file_name: str, filter_types: List[str] = None): \"\"\" 产生下载hf模型的脚本 :param model_url: 支持 https://hf-mirror.com 和 https://huggingface.co/models 如: https://hf-mirror.com/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0 :param file_name: 输出文件名字, 如nohup_download_baichuan2.sh :param filter_types: 需要过滤的文件类型[暂时没实现] :return: \"\"\" from bs4 import BeautifulSoup # 获取主要域名 main_domain = extract_main_domain(model_url) # 输出主要域名 logger.info(f\"解析到域名为：{main_domain}\") # 发送HTTP GET请求获取网页内容 logger.info(\"开始解析下载\") response = requests.get(model_url) logger.info(\"网页下载完成, 准备解析下载地址\") html_content = response.text # 使用BeautifulSoup对HTML内容进行解析 soup = BeautifulSoup(html_content, 'html.parser') download_files = soup.findAll('a', {'title': 'Download file'}) with print_to_file(file_name): print('echo \"开始下载模型等文件\"\\n') for idx, download_file in enumerate(download_files): href = unquote(download_file.get('href')) print('date +\"当前时间为: %Y-%m-%d %H:%M:%S\"') url = f\"{main_domain}{href}\" file_name =os.path.basename(href).split('?')[0] print(f'wget -O \"{file_name}\" \"{url}\"') print(f'echo \"下载完成\"\\n') print('date +\"当前时间为: %Y-%m-%d %H:%M:%S\"') print('echo \"Download completed successfully.\"') logger.info(\"下载地址解析完毕\") logger.info(f\"脚本输出路径为: {file_name}\") if __name__ == '__main__': test_url = r\"https://hf-mirror.com/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0\" gen_download_hf_models_script(model_url=test_url, file_name='noup_download_baichuan2.sh') 快速开始 hf快速开始教程 下图是huggingface模块关系图 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: noup_huggingface.py @Description: @time: 2024/2/3 9:49 \"\"\" from datasets import load_dataset from transformers import AutoModelForSequenceClassification from transformers import AutoTokenizer from transformers import DataCollatorWithPadding from transformers import Trainer from transformers import TrainingArguments def run(): model_name = \"distilbert-base-uncased\" output_dir = \"path/to/save/folder/\" # 加载预训练模型 model = AutoModelForSequenceClassification.from_pretrained(model_name) # 加载分词器 tokenizer = AutoTokenizer.from_pretrained(model_name) # 加载数据集 dataset = load_dataset(\"rotten_tomatoes\") def tokenize_dataset(p_dataset): \"\"\" 定义数据处理函数 @param p_dataset: @return: \"\"\" return tokenizer(p_dataset[\"text\"]) # 对数据集调用处理函数(这里主要做分词) dataset = dataset.map(tokenize_dataset, batched=True) # 有些还需要做标签对齐 # label2id = {\"contradiction\": 0, \"neutral\": 1, \"entailment\": 2} # mnli = load_dataset(\"glue\", \"mnli\", split=\"train\") # mnli_aligned = mnli.align_labels_with_mapping(label2id, \"label\") # 定义一个数据收集器 data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # 配置训练参数 training_args = TrainingArguments(output_dir, learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=2, ) # 定义一个trainer trainer = Trainer(model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], tokenizer=tokenizer, data_collator=data_collator, ) # 开始训练 trainer.train() if __name__ == '__main__': run() 对于使用序列到序列模型（Seq2Seq）的任务，如翻译或摘要，应该使用Seq2SeqTrainer和Seq2SeqTrainingArguments类 您可以通过继承Trainer内部的方法来自定义训练循环的行为。这使您能够自定义特征，如损失函数、优化器和调度器。查看Trainer参考以了解哪些方法可以被继承 自定义训练循环的另一种方式是使用回调（Callbacks）。您可以使用回调与其他库集成以及检查训练循环，以报告进度或提前停止训练 回调不会修改训练循环本身的任何内容。若要自定义像损失函数这样的内容，您需要继承Trainer datasets HuggingFace datasets库总结 安装 下面三个命令都用于安装Hugging Face的datasets库的不同配置 pip install datasets：这个命令安装的是datasets库的基本配置，它提供了对常见的自然语言处理(NLP)任务和数据集的支持，例如文本分类、命名实体识别、问答系统等。如果您只需要处理文本数据或进行常见的NLP任务，这个基本配置就足够了 pip install datasets[audio]：这个命令安装的是datasets库的\"audio\"配置。它包含了对声音和音频数据集的支持，例如自动语音识别(ASR)和音频分类任务。如果您需要处理声音和音频数据，比如进行语音识别或音频分类，安装这个配置会提供相应的功能和数据集支持 pip install datasets[vision]：这个命令安装的是datasets库的\"vision\"配置。它包含了对图像和计算机视觉任务的支持，例如图像分类、目标检测和分割等。如果您需要处理图像数据或进行计算机视觉任务，安装这个配置会提供相应的功能和数据集支持 通过安装不同的配置，您可以选择仅安装您需要的功能和支持的任务类型，以减少库的安装和存储空间。根据您的具体需求，选择适合的配置进行安装即可 # 安装基础版 pip install datasets # 安装for声音 pip install datasets[audio] # 安装for图像 pip install datasets[vision] 快速开始 视觉 from datasets import load_dataset, Image from torch.utils.data import DataLoader from torchvision.transforms import Compose, ColorJitter, ToTensor # 加载数据集 dataset = load_dataset(\"beans\", split=\"train\") jitter = Compose( [ColorJitter(brightness=0.5, hue=0.5), ToTensor()] ) def transforms(examples): examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]] return examples dataset = dataset.with_transform(transforms) def collate_fn(examples): images = [] labels = [] for example in examples: images.append((example[\"pixel_values\"])) labels.append(example[\"labels\"]) pixel_values = torch.stack(images) labels = torch.tensor(labels) return {\"pixel_values\": pixel_values, \"labels\": labels} # 定义DataLoader dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4) nlp 使用 Hugging Face 提供的datasets库加载了GLUE(General Language Understanding Evaluation)数据集中的MRPC(Microsoft Research Paraphrase Corpus)部分的训练集。这个数据集用于句子对的相似性判断任务 from datasets import load_dataset from transformers import AutoModelForSequenceClassification, AutoTokenizer import torch dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\") # load a pretrained BERT model and its corresponding tokenizer from the &#x1F917; Transformers library. model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\") tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") def encode(examples): return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\") dataset = dataset.map(encode, batched=True) dataset[0] {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0, 'input_ids': array([ 101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 11336, 6732, 3384, 1106, 1140, 1112, 1178, 107, 1103, 7737, 107, 117, 7277, 2180, 5303, 4806, 1117, 1711, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102]), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} # Rename the label column to labels, which is the expected input name in BertForSequenceClassification dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True) dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"]) dataloader = torch.utils.data.DataLoader(dataset, batch_size=32) 概述 datasets库中的Dataset对象通常用来处理和存储数据。当数据需要载入模型进行训练或评估时，DataLoader被用来创建数据的迭代器，允许批量处理和并行加载 DataCollator则用于将这些批次的数据整理成模型需要的格式，进行适当的填充或其他预处理步骤 简单来说，你可以这样想象它们的工作流： Dataset负责数据的存储和预处理 DataLoader负责从Dataset中抽取数据，组成批次，并可选择并行加载数据 DataCollator负责将DataLoader提供的批次数据进行填充和整理，以确保模型可以正确处理 DataCollator类 huggingface的DataCollator和pytorch的collate_fn的关系 在PyTorch中，collate_fn 是 DataLoader 的一个参数，用于指定如何将多个数据样本组合成一个批次 这个函数接受一个样本列表作为输入，然后返回一个批次，通常是通过堆叠（stacking）或填充（padding）样本来实现 collate_fn 在处理长度不一致的数据时特别有用，例如文本数据或时间序列数据 Hugging Face的 DataCollator 基本上是 collate_fn 的一个扩展或包装器。它通常是一个类，实现了一个 __call__ 方法，该方法的功能与 collate_fn 相同 在Hugging Face的Transformers库中，有预先定义的 DataCollator 类，它们被设计用来处理特定类型的数据和模型需求，如填充到相同长度，或者为了语言模型训练而进行数据掩蔽的任务 下面是一个示例，展示了在PyTorch和Hugging Face的Transformers中如何使用 collate_fn 和 DataCollator： 在PyTorch中使用自定义 collate_fn： from torch.utils.data import DataLoader def custom_collate_fn(batch): # 自定义的堆叠、填充逻辑 pass data_loader = DataLoader(dataset, batch_size=32, collate_fn=custom_collate_fn) 在Hugging Face的Transformers中使用 DataCollator： from transformers import DataCollatorWithPadding from torch.utils.data import DataLoader # 对于一些特定的任务，Transformers库提供了预定义的DataCollator data_collator = DataCollatorWithPadding(tokenizer) data_loader = DataLoader(dataset, batch_size=32, collate_fn=data_collator) 在这个例子中，DataCollatorWithPadding 是Hugging Face提供的一个类，它使用给定的tokenizer来自动处理批次的填充 当创建 DataLoader 实例时，你可以直接将 data_collator 作为 collate_fn 的值传入，这是因为 DataCollatorWithPadding 类的实例是可调用的，这样 DataLoader 在每个批次准备数据时会调用 data_collator 总的来说，Hugging Face的 DataCollator 提供了一个更高级别、更方便的接口，尤其是为了与 Transformers 库中的NLP模型和任务配合使用，而PyTorch的 collate_fn 是这个接口的底层机制，提供了自定义数据组合逻辑的基础功能 小结 可以查看huggingface的Trainer类，很容易发现他们之间的关系：Hugging Face的 DataCollator 基本上是 collate_fn 的一个扩展或包装器。它通常是一个类，实现了一个 __call__ 方法，该方法的功能与 collate_fn 相同 from torch.utils.data import DataLoader class Trainer: def __init__( self, model: Union[PreTrainedModel, nn.Module] = None, args: TrainingArguments = None, data_collator: Optional[DataCollator] = None, train_dataset: Optional[Dataset] = None, eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None, tokenizer: Optional[PreTrainedTokenizerBase] = None, model_init: Optional[Callable[[], PreTrainedModel]] = None, compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None, callbacks: Optional[List[TrainerCallback]] = None, optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, ): ... default_collator = default_data_collator if tokenizer is None else DataCollatorWithPadding(tokenizer) self.data_collator = data_collator if data_collator is not None else default_collator ... def get_train_dataloader(self) -> DataLoader: \"\"\" Returns the training [`~torch.utils.data.DataLoader`]. Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed training if necessary) otherwise. Subclass and override this method if you want to inject some custom behavior. \"\"\" if self.train_dataset is None: raise ValueError(\"Trainer: training requires a train_dataset.\") train_dataset = self.train_dataset data_collator = self.data_collator if is_datasets_available() and isinstance(train_dataset, datasets.Dataset): train_dataset = self._remove_unused_columns(train_dataset, description=\"training\") else: data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\") if isinstance(train_dataset, torch.utils.data.IterableDataset): if self.args.world_size > 1: train_dataset = IterableDatasetShard( train_dataset, batch_size=self._train_batch_size, drop_last=self.args.dataloader_drop_last, num_processes=self.args.world_size, process_index=self.args.process_index, ) return DataLoader( train_dataset, batch_size=self._train_batch_size, collate_fn=data_collator, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, ) train_sampler = self._get_train_sampler() return DataLoader( train_dataset, batch_size=self._train_batch_size, sampler=train_sampler, collate_fn=data_collator, drop_last=self.args.dataloader_drop_last, num_workers=self.args.dataloader_num_workers, pin_memory=self.args.dataloader_pin_memory, worker_init_fn=seed_worker, ) 加载数据集 查看数据集描述 from datasets import load_dataset_builder ds_builder = load_dataset_builder(\"rotten_tomatoes\") ds_builder.info.description Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005. ds_builder.info.features {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None), 'text': Value(dtype='string', id=None)} 加载数据集 from datasets import load_dataset dataset = load_dataset(\"rotten_tomatoes\", split=\"train\") 当一个数据集由多个文件(我们称之为分片)组成时，可以显著加快数据集的下载和准备步骤 您可以使用num_proc参数选择并行准备数据集时要使用的进程数。在这种情况下，每个进程被分配了一部分分片来进行准备 from datasets import load_dataset oscar_afrikaans = load_dataset(\"oscar-corpus/OSCAR-2201\", \"af\", num_proc=8) imagenet = load_dataset(\"imagenet-1k\", num_proc=8) ml_librispeech_spanish = load_dataset(\"facebook/multilingual_librispeech\", \"spanish\", num_proc=8) 查看数据集的分片名称，并加载指定的分片名称 from datasets import get_dataset_split_names from datasets import load_dataset get_dataset_split_names(\"rotten_tomatoes\") ['train', 'validation', 'test'] # 加载指定分片 dataset = load_dataset(\"rotten_tomatoes\", split=\"train\") Dataset({ features: ['text', 'label'], num_rows: 8530 }) # 还可以这么写： train_test_ds = datasets.load_dataset(\"bookcorpus\", split=\"train+test\") train_10_20_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[10:20]\") train_10pct_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[:10%]\") train_10_80pct_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[:10%]+train[-80%:]\") val_ds = datasets.load_dataset(\"bookcorpus\", split=[f\"train[{k}%:{k+10}%]\" for k in range(0, 100, 10)]) train_ds = datasets.load_dataset(\"bookcorpus\", split=[f\"train[:{k}%]+train[{k+10}%:]\" for k in range(0, 100, 10)]) train_50_52_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[50%:52%]\") train_52_54_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[52%:54%]\") # 18 records, from 450 (included) to 468 (excluded). train_50_52pct1_ds = datasets.load_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\", from_=50, to=52, unit=\"%\", rounding=\"pct1_dropremainder\")) # 18 records, from 468 (included) to 486 (excluded). train_52_54pct1_ds = datasets.load_dataset(\"bookcorpus\", split=datasets.ReadInstruction(\"train\",from_=52, to=54, unit=\"%\", rounding=\"pct1_dropremainder\")) # Or equivalently: train_50_52pct1_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[50%:52%](pct1_dropremainder)\") train_52_54pct1_ds = datasets.load_dataset(\"bookcorpus\", split=\"train[52%:54%](pct1_dropremainder)\") # 加载全部数据 dataset = load_dataset(\"rotten_tomatoes\") DatasetDict({ train: Dataset({ features: ['text', 'label'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label'], num_rows: 1066 }) }) 查看数据集子集，一个数据下可能还有很多子数据集 from datasets import get_dataset_config_names configs = get_dataset_config_names(\"PolyAI/minds14\") print(configs) ['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN', 'all'] 加载指定子数据集 from datasets import load_dataset mindsFR = load_dataset(\"PolyAI/minds14\", \"fr-FR\", split=\"train\") # 指定子数据集是fr-FR 指定数据集的文件, 避免load过多的数据 data_files = {\"validation\": \"en/c4-validation.*.json.gz\"} c4_validation = load_dataset(\"allenai/c4\", data_files=data_files, split=\"validation\") load本地的json、csv文件等，可以load远程文件、sql等 #{\"version\": \"0.1.0\", # \"data\": [{\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false}, # {\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true}] #} from datasets import load_dataset dataset = load_dataset(\"json\", data_files=\"my_file.json\", field=\"data\") 通过python对象来创建dataset from datasets import Dataset import pandas as pd # 字典方式 my_dict = {\"a\": [1, 2, 3]} dataset = Dataset.from_dict(my_dict) # list方式 my_list = [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}] dataset = Dataset.from_list(my_list) # pandas方式 df = pd.DataFrame({\"a\": [1, 2, 3]}) dataset = Dataset.from_pandas(df) load多个文本文件: 文本必须一行就是一条样本 from datasets import load_dataset dataset = load_dataset(\"text\", data_files={\"train\": [\"my_text_1.txt\", \"my_text_2.txt\"], \"test\": \"my_test_file.txt\"}) # Load from a directory dataset = load_dataset(\"text\", data_dir=\"path/to/text/dataset\") 离线load: 将环境变量HF_DATASETS_OFFLINE设置为1以启用完全离线模式 进阶加载数据集 从脚本加载数据集 您可能在本地计算机上有一个&#x1F917;Datasets的加载脚本。在这种情况下，通过将以下路径之一传递给load_dataset()来加载数据集： 加载脚本文件的本地路径。 包含加载脚本文件的目录的本地路径(仅当脚本文件与目录具有相同的名称时) dataset = load_dataset(\"path/to/local/loading_script/loading_script.py\", split=\"train\") # equivalent because the file has the same name as the directory dataset = load_dataset(\"path/to/local/loading_script\", split=\"train\") 可以从Hub上下载加载脚本，并对其进行编辑以添加自己的修改。将数据集仓库下载到本地，以便加载脚本中相对路径引用的任何数据文件都可以被加载 git clone https://huggingface.co/datasets/eli5 在加载脚本上进行编辑后，通过将其本地路径传递给load_dataset()来加载它 from datasets import load_dataset eli5 = load_dataset(\"path/to/local/eli5\") csv+json方式 数据集可以从存储在计算机上的本地文件和远程文件中加载。这些数据集很可能以csv、json、txt或parquet文件的形式存储。load_dataset()函数可以加载这些文件类型的数据集 from datasets import load_dataset # csv方式 dataset = load_dataset(\"csv\", data_files=\"my_file.csv\") # json方式 # {\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false} # {\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true} dataset = load_dataset(\"json\", data_files=\"my_file.json\") # {\"version\": \"0.1.0\", # \"data\": [{\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false}, # {\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true}] # } dataset = load_dataset(\"json\", data_files=\"my_file.json\", field=\"data\") # 从http方式加载csv base_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\" dataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\") # Parquet方式 dataset = load_dataset(\"parquet\", data_files={'train': 'train.parquet', 'test': 'test.parquet'}) base_url = \"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/\" data_files = {\"train\": base_url + \"wikipedia-train.parquet\"} wiki = load_dataset(\"parquet\", data_files=data_files, split=\"train\") sql方式 使用from_sql()方法可以通过指定连接到数据库的URI来读取数据库内容。您可以读取表名或执行查询操作 from datasets import Dataset dataset = Dataset.from_sql(\"data_table_name\", con=\"sqlite:///sqlite_file.db\") dataset = Dataset.from_sql(\"SELECT text FROM table WHERE length(text) > 100 LIMIT 10\", con=\"sqlite:///sqlite_file.db\") For more details, check out the how to load tabular datasets from SQL databases guide. 探索数据集 下标 # 第一个样本 dataset[0] #{'label': 1, # 'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'} # 最后一个样本 dataset[-1] # 只取text列 dataset[\"text\"] # 返回a list of 样本列 # 第一个样本text列 dataset[0][\"text\"] # 性能：dataset[0]['text']比dataset['text'][0]快2倍。 数据切片 # Get the first three rows dataset[:3] # Get rows between three and six dataset[3:6] 迭代方式，streaming=True from datasets import load_dataset iterable_dataset = load_dataset(\"food101\", split=\"train\", streaming=True) for example in iterable_dataset: print(example) break {'image': , 'label': 6} # Get first three examples list(iterable_dataset.take(3)) [{'image': , 'label': 6}, {'image': , 'label': 6}, {'image': , 'label': 6}] 排序+shuffle+选择+filter+切分数据集+分片 # sort: 按某一列排序 dataset.sort(\"label\") # 打乱 shuffled_dataset = sorted_dataset.shuffle(seed=42) # 选择 small_dataset = dataset.select([0, 10, 20, 30, 40, 50]) # 匹配查找 start_with_ar = dataset.filter(lambda example: example[\"sentence1\"].startswith(\"Ar\")) len(start_with_ar) start_with_ar[\"sentence1\"] # 匹配查找：根据下标 even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True) # 切分 dataset.train_test_split(test_size=0.1) # 分片 # 数据集支持分片，将非常大的数据集划分为预定义数量的块。 在 shard() 中指定 num_shards 参数以确定要将数据集拆分成的分片数。 您还需要使用 index 参数提供要返回的分片。 from datasets import load_dataset datasets = load_dataset(\"imdb\", split=\"train\") print(dataset) dataset.shard(num_shards=4, index=0) # 四分之一 列重命名+移除列+转换格式+flatten from datasets import ClassLabel, Value from datasets import load_dataset # 列重命名 dataset = dataset.rename_column(\"sentence1\", \"sentenceA\") # 去掉某一列 dataset = dataset.remove_columns([\"sentence1\", \"sentence2\"]) # 转换格式：一列或者多列 new_features = dataset.features.copy() new_features[\"label\"] = ClassLabel(names=[\"negative\", \"positive\"]) new_features[\"idx\"] = Value(\"int64\") dataset = dataset.cast(new_features) # 转换格式：一列 dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000)) # 将某一列的key\\value拉平 dataset = load_dataset(\"squad\", split=\"train\") # ??? map转换 from multiprocess import set_start_method from datasets import load_dataset import torch import os set_start_method(\"spawn\") # remove_columns 转换的同时去掉某一列 updated_dataset = dataset.map(lambda example: {\"new_sentence\": example[\"sentence1\"]}, remove_columns=[\"sentence1\"]) updated_dataset.column_names # with_indices: 对下标处理 updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, with_indices=True) updated_dataset[\"sentence2\"][:5] #如果您设置with_rank=True，map()也适用于进程的等级。 这类似于with_indices参数。 映射函数中的with_rank参数位于索引1之后(如果它已经存在) def gpu_computation(example, rank): os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank % torch.cuda.device_count()) # Your big GPU call goes here return examples updated_dataset = dataset.map(gpu_computation, with_rank=True) # 多线程 updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, num_proc=4) # batched chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names) # 数据增强 def augment_data(examples): outputs = [] for sentence in examples[\"sentence1\"]: words = sentence.split(' ') K = randint(1, len(words)-1) masked_sentence = \" \".join(words[:K] + [mask_token] + words[K+1:]) predictions = fillmask(masked_sentence) augmented_sequences = [predictions[i][\"sequence\"] for i in range(3)] outputs += [sentence] + augmented_sequences return {\"data\": outputs} augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8) augmented_dataset[:9][\"data\"] # 处理多split dataset = load_dataset('glue', 'mrpc') encoded_dataset = dataset.map(lambda examples: tokenizer(examples[\"sentence1\"]), batched=True) encoded_dataset[\"train\"][0] 合并+拼接数据集 from datasets import concatenate_datasets, load_dataset from datasets import Dataset # 加载数据集 bookcorpus = load_dataset(\"bookcorpus\", split=\"train\") wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\") wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"]) # only keep the 'text' column assert bookcorpus.features.type == wiki.features.type bert_dataset = concatenate_datasets([bookcorpus, wiki]) # 可以换concate的方向 bookcorpus_ids = Dataset.from_dict({\"ids\": list(range(len(bookcorpus)))}) bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1) 相互穿插 import torch # 按概率穿插 seed = 42 probabilities = [0.3, 0.5, 0.2] d1 = Dataset.from_dict({\"a\": [0, 1, 2]}) d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]}) d3 = Dataset.from_dict({\"a\": [20, 21, 22]}) dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed) dataset[\"a\"] # 按所有的样本都出现过一次后，马上停止 d1 = Dataset.from_dict({\"a\": [0, 1, 2]}) d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]}) d3 = Dataset.from_dict({\"a\": [20, 21, 22]}) dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\") dataset[\"a\"] format dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"]) # 返回一个新dataset dataset = dataset.with_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"]) # 查看 dataset.format 保存 from datasets import load_from_disk encoded_dataset.save_to_disk(\"path/of/my/dataset/directory\") # 从本地load上来 reloaded_dataset = load_from_disk(\"path/of/my/dataset/directory\") encoded_dataset.to_csv(\"path/of/my/dataset.csv\") Dataset.to_json() Preprocess处理 文本处理：用transformers的tokenizer from transformers import AutoTokenizer from datasets import load_dataset tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") dataset = load_dataset(\"rotten_tomatoes\", split=\"train\") tokenizer(dataset[0][\"text\"]) {'input_ids': [101, 1103, 2067, 1110, 17348, 1106, 1129, 1103, 6880, 1432, 112, 188, 1207, 107, 14255, 1389, 107, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 170, 11791, 5253, 188, 1732, 7200, 10947, 12606, 2895, 117, 179, 7766, 118, 172, 15554, 1181, 3498, 6961, 3263, 1137, 188, 1566, 7912, 14516, 6997, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} 分词器返回一个包含三个项目的字典： input_ids：表示文本中各个标记的数字 token_type_ids：如果有多个序列，指示一个标记属于哪个序列 attention_mask：指示一个标记是否应该被掩盖(masked) dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"]) 音频信号：重新采样音频信号 from transformers import AutoFeatureExtractor from datasets import load_dataset, Audio feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\") dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\") dataset[0][\"audio\"] {'array': array([ 0. , 0.00024414, -0.00024414, ..., -0.00024414, 0. , 0. ], dtype=float32), 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav', 'sampling_rate': 8000} MInDS-14数据集卡会告诉您采样率为8kHz Wav2Vec2模型卡说它是在16kHz语音音频上采样的。 这意味着您需要对MInDS-14数据集进行上采样以匹配模型的采样率 使用cast_column()函数并在Audio功能中设置sampling_rate参数以对音频信号进行上采样。 当您现在调用音频列时，它会被解码并重新采样到16kHz： dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000)) dataset[0][\"audio\"] # 加速：使用 map() 函数将整个数据集重新采样到16kHz def preprocess_function(examples): audio_arrays = [x[\"array\"] for x in examples[\"audio\"]] inputs = feature_extractor( audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True ) return inputs dataset = dataset.map(preprocess_function, batched=True) 图像增强 在图像数据集中，最常见的预处理操作之一是数据增强(data augmentation)，这是一种在不改变数据含义的情况下对图像引入随机变化的过程 这可以包括改变图像的颜色属性或随机裁剪图像。您可以自由选择任何数据增强库，并且&#x1F917;Datasets将帮助您将数据增强应用到您的数据集中 from transformers import AutoFeatureExtractor from datasets import load_dataset, Image from torchvision.transforms import RandomRotation feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\") dataset = load_dataset(\"beans\", split=\"train\") rotate = RandomRotation(degrees=(0, 90)) def transforms(examples): examples[\"pixel_values\"] = [rotate(image.convert(\"RGB\")) for image in examples[\"image\"]] return examples # 应用图像转换 dataset.set_transform(transforms) dataset[0][\"pixel_values\"] label id对齐 在Transformers库中，label id对齐(label ID alignment)通常指的是将标签与模型输出的预测结果对齐。当使用预训练模型进行分类或回归等任务时，通常需要将标签映射为模型期望的标签ID 具体来说，对于分类任务，常见的做法是将标签映射为整数标签ID。例如，如果有三个类别[\"cat\", \"dog\", \"bird\"]，可以将它们映射为[0, 1, 2]，并将模型的输出标签预测结果与这些标签ID进行对齐 对于回归任务，可能需要将连续值的标签进行离散化或归一化，并将其映射为标签ID。例如，将一个连续的目标值范围映射为一组离散的标签ID 在使用Transformers库进行训练或评估时，您需要确保标签与模型的输出结果具有相同的标签ID对齐，以便正确计算损失、评估指标和解码预测结果 需要注意的是，标签ID对齐的具体实现方式可能因任务和库的使用而有所不同。在具体的代码实现中，您可能需要根据您的数据集和模型设置进行相应的标签ID对齐操作 from datasets import load_dataset label2id = {\"contradiction\": 0, \"neutral\": 1, \"entailment\": 2} mnli = load_dataset(\"glue\", \"mnli\", split=\"train\") mnli_aligned = mnli.align_labels_with_mapping(label2id, \"label\") 构建数据集 如果您使用自己的数据，可能需要创建一个数据集。使用&#x1F917;Datasets创建数据集可以享受到该库的所有优势：快速加载和处理数据、流式处理大型数据集、内存映射等等。您可以使用&#x1F917;Datasets的低代码方法轻松快速地创建数据集，减少启动训练模型所需的时间。在许多情况下，只需将数据文件拖放到Hub上的数据集仓库中，就可以轻松完成 在本教程中，您将学习如何使用&#x1F917;Datasets的低代码方法创建各种类型的数据集： 基于文件夹的构建器(Folder-based builders)，用于快速创建图像或音频数据集 使用from_方法从本地文件创建数据集 基于文件夹的构建器 有两个基于文件夹的构建器：ImageFolder(图像文件夹构建器)和AudioFolder(音频文件夹构建器) 它们是低代码方法，可以快速创建包含数千个示例的图像、语音和音频数据集。它们非常适用于在扩展到更大的数据集之前，快速原型化计算机视觉和语音模型 基于文件夹的构建器会使用您的数据，并自动生成数据集的特征、划分和标签。具体来说： ImageFolder使用Image特征来解码图像文件。它支持许多图像扩展格式，例如jpg和png，还支持其他格式。您可以查看支持的图像扩展格式的完整列表 AudioFolder使用Audio特征来解码音频文件。它支持音频扩展格式，如wav和mp3，您可以查看支持的音频扩展格式的完整列表 例如，如果您的图像数据集(对于音频数据集也是一样)存储如下所示： pokemon/train/grass/bulbasaur.png pokemon/train/fire/charmander.png pokemon/train/water/squirtle.png pokemon/test/grass/ivysaur.png pokemon/test/fire/charmeleon.png pokemon/test/water/wartortle.png from datasets import ImageFolder from datasets import AudioFolder dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/pokemon\") dataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\") 数据集中可以包含有关数据集的其他信息，例如文本标题或转录，可以使用包含在数据集文件夹中的metadata.csv文件来进行存储 metadata文件需要有一个file_name列，将图像或音频文件与其相应的元数据进行关联 file_name, text bulbasaur.png, There is a plant seed on its back right from the day this Pokémon is born. charmander.png, It has a preference for hot things. squirtle.png, When it retracts its long neck into its shell, it squirts out water with vigorous force. To learn more about each of these folder-based builders, check out the and ImageFolder or AudioFolder guides. 基于文件的构建器 使用 from_generator() 方法是从生成器创建数据集的最节省内存的方式，这是由于生成器的迭代行为。这在处理非常大的数据集时特别有用，因为数据集是逐步在磁盘上生成的，然后进行内存映射，这样可以避免将整个数据集加载到内存中 from datasets import Dataset def gen(): yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"} yield {\"pokemon\": \"squirtle\", \"type\": \"water\"} ds = Dataset.from_generator(gen) ds[0] {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"} 基于生成器的IterableDataset需要使用for循环进行迭代，例如： from datasets import IterableDataset ds = IterableDataset.from_generator(gen) for example in ds: print(example) {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"} {\"pokemon\": \"squirtle\", \"type\": \"water\"} 使用from_dict()方法是从字典创建数据集的简单直接的方式： from datasets import Dataset ds = Dataset.from_dict({\"pokemon\": [\"bulbasaur\", \"squirtle\"], \"type\": [\"grass\", \"water\"]}) ds[0] {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"} 分享数据集 点击您的个人资料并选择新的数据集以创建一个新的数据集仓库。 为您的数据集选择一个名称，并选择它是一个公共数据集还是私有数据集。公共数据集对任何人可见，而私有数据集只能由您或您组织的成员查看 一旦您的数据集存储在Hub上，任何人都可以使用load_dataset()函数加载它： from datasets import load_dataset dataset = load_dataset(\"stevhliu/demo\") 使用Python进行上传 喜欢以编程方式上传数据集的用户可以使用huggingface_hub库。该库允许用户从Python中与Hub进行交互 首先安装该库： pip install huggingface_hub 要在Hub上使用Python上传数据集，您需要登录到您的Hugging Face账户： huggingface-cli login 使用push_to_hub()函数帮助您将文件添加、提交和推送到您的仓库： from datasets import load_dataset dataset = load_dataset(\"stevhliu/demo\") # dataset = dataset.map(...) # 在这里进行所有的数据处理 dataset.push_to_hub(\"stevhliu/processed_demo\") 如果要将数据集设置为私有，请将private参数设置为True。该参数仅在首次创建仓库时有效 dataset.push_to_hub(\"stevhliu/private_processed_demo\", private=True) 评估指标 安装 一种用于轻松评估机器学习模型和数据集的库 只需一行代码，您就可以访问数十种不同领域(自然语言处理、计算机视觉、强化学习等)的评估方法 无论是在本地机器上还是在分布式训练环境中，您都可以以一种一致且可重复的方式评估您的模型 安装 pip install evaluate 测试 python -c \"import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))\" {'exact_match': 1.0} 快速开始 指标种类 Evaluate Metric卡片实例 &#x1F917;Evaluate提供了广泛的评估工具。它涵盖了文本、计算机视觉、音频等多种形式，并提供了用于评估模型或数据集的工具。这些工具分为三个类别 评估类型 典型的机器学习流程涉及到不同方面的评估，对于每个方面，&#x1F917; Evaluate都提供了相应的工具： 指标(Metric)：用于评估模型的性能，通常涉及模型的预测结果和一些真实标签。您可以在evaluate-metric中找到所有集成的指标 比较(Comparison)：用于比较两个模型。可以通过将它们的预测结果与真实标签进行比较并计算它们的一致性来进行比较。您可以在evaluate-comparison中找到所有集成的比较方法 测量(Measurement)：数据集和训练在其上的模型同样重要。通过测量可以研究数据集的特性。您可以在evaluate-measurement中找到所有集成的测量方法 每个评估模块都作为一个Space存储在Hugging Face Hub上。它们提供了一个交互式小部件和一个文档卡片，用于记录其使用方法和限制 评估工具之间的关系和区别 Evaluate库中的Metric(指标)、Comparison(比较)和Measurement(测量)是三种不同的评估工具，用于评估机器学习模型和数据集。它们之间的关系和区别如下： Metric(指标)： 用途：用于评估模型的性能 具体含义：指标通过将模型的预测结果与真实标签进行比较来衡量模型的表现 示例：准确率、精确率、召回率、F1分数等 目的：提供了对模型性能的定量评估，帮助衡量模型在特定任务上的表现 Comparison(比较)： 用途：用于比较两个模型之间的差异 具体含义：比较工具将两个模型的预测结果与真实标签进行对比，计算它们之间的一致性或差异程度 示例：一致性指标、相对误差等 目的：帮助评估不同模型之间的性能差异，找到更好的模型或进行模型选择 Measurement(测量)： 用途：用于研究数据集的属性和特性 具体含义：测量工具用于对数据集进行分析，探索数据集的结构、分布、偏差等方面的信息 示例：数据集大小、样本分布、类别不平衡度等 目的：提供对数据集的详细了解，帮助了解数据集的特点和潜在问题 这三种评估工具在Evaluate库中各自独立，用于不同的评估目的。Metric用于衡量模型性能，Comparison用于比较不同模型之间的性能差异，Measurement用于研究和了解数据集的特性。通过使用这些工具，可以全面评估和理解机器学习模型和数据集的表现和特点 指标加载 官方+社区 指标 在使用Hugging Face的Evaluate库加载评估工具时，可以通过显式指定评估的类型来确保加载正确的工具。这可以防止名称冲突或混淆，确保您使用的是期望的评估工具 import evaluate accuracy = evaluate.load(\"accuracy\") # 显式指定评估的类型 word_length = evaluate.load(\"word_length\", module_type=\"measurement\") # 社区指标 element_count = evaluate.load(\"lvwerra/element_count\", module_type=\"measurement\") 查看可用的模块方法 evaluate.list_evaluation_modules( module_type=\"comparison\", include_community=False, with_details=True) [{'name': 'mcnemar', 'type': 'comparison', 'community': False, 'likes': 1}, {'name': 'exact_match', 'type': 'comparison', 'community': False, 'likes': 0}] 指标计算 计算指标 当涉及到计算实际得分时，有两种主要的方法： 一体式计算(All-in-one)：通过一次性将所有必要的输入传递给compute()方法来计算得分 accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1]) {'accuracy': 0.5} 逐步计算(Incremental)：通过使用EvaluationModule.add()或EvaluationModule.add_batch()将必要的输入逐步添加到模块中，然后在最后使用 EvaluationModule.compute()计算得分 # add的方式 for ref, pred in zip([0,1,0,1], [1,0,0,1]): accuracy.add(references=ref, predictions=pred) accuracy.compute() {'accuracy': 0.5} # add_batch的方式 for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]): accuracy.add_batch(references=refs, predictions=preds) accuracy.compute() {'accuracy': 0.5} 在你需要以批量方式从模型中获取预测结果时特别有用： for model_inputs, gold_standards in evaluation_dataset: predictions = model(model_inputs) metric.add_batch(references=gold_standards, predictions=predictions) metric.compute() 分布式指标 在分布式环境中计算指标可能会有些棘手。指标评估是在不同的数据子集上的单独Python进程或节点中执行的 通常情况下，当一个指标得分是可加的( f(A \\cup B) = f(A) + f(B))时，你可以使用分布式的reduce操作来收集每个数据子集的得分。但是当指标是非可加的( f(A \\cup B) \\neq f(A) + f(B))时，情况就不那么简单了。例如，你不能将每个数据子集的F1分数相加作为最终的指标 克服这个问题的常见方法是回退到单进程评估，但指标在单个GPU上进行评估，这会导致效率降低 &#x1F917;Evaluate通过仅在第一个节点上计算最终的指标来解决了这个问题 预测结果和参考结果被分别计算并提供给每个节点的指标，这些结果暂时存储在Apache Arrow表中，避免了GPU或CPU内存的混乱 当你准备使用compute()计算最终指标时，第一个节点能够访问所有其他节点上存储的预测结果和参考结果。一旦它收集到所有的预测结果和参考结果，compute()将进行最终的指标评估 这个解决方案使得&#x1F917;Evaluate能够在分布式设置中执行分布式预测，这对于提高评估速度非常重要。同时，你还可以使用复杂的非可加指标，而不浪费宝贵的GPU或CPU内存 组合评估 通常情况下，我们不仅想评估单个指标，而是想评估一系列不同的指标，以捕捉模型性能的不同方面 例如，对于分类问题，除了准确度外，通常还会计算F1分数、召回率和精确度，以便更好地了解模型的性能。当然，你可以加载一系列指标并依次调用它们。然而，一种更方便的方法是使用combine()函数将它们捆绑在一起： clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"]) clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1]) { 'accuracy': 0.667, 'f1': 0.667, 'precision': 1.0, 'recall': 0.5 } 自动化评估 使用evaluate.evaluator()提供了自动化的评估功能，只需要一个模型、数据集和度量指标，与EvaluationModules中的度量指标相比，它不需要模型的预测结果。因此，使用给定的度量指标在数据集上评估模型更容易，因为推理过程是在内部处理的 为了实现这一点，它使用了transformers库中的pipeline抽象。然而，只要符合pipeline接口，你也可以使用自己的框架 from transformers import pipeline from datasets import load_dataset from evaluate import evaluator import evaluate 为了使用evaluator进行评估，让我们加载一个基于IMDb训练的transformers pipeline（但你也可以传递自己的自定义推理类来适应任何遵循pipeline调用API的框架），并使用IMDb的测试集和准确度度量指标进行评估 pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\", device=0) data = load_dataset(\"imdb\", split=\"test\").shuffle().select(range(1000)) metric = evaluate.load(\"accuracy\") task_evaluator = evaluator(\"text-classification\") results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric, label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1},) {'accuracy': 0.934} 仅仅计算度量指标的值通常还不足以知道一个模型是否显著优于另一个模型。通过使用自助法(bootstrapping)，evaluate计算置信区间和标准误差，这有助于估计分数的稳定性 results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric, label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}, strategy=\"bootstrap\", n_resamples=200) {'accuracy': { 'confidence_interval': (0.906, 0.9406749892841922), 'standard_error': 0.00865213251082787, 'score': 0.923 } } 评估器期望数据输入具有\"text\"和\"label\"列。如果您的数据集不同，可以使用关键字参数input_column=\"text\"和label_column=\"label\"来提供列名 目前只支持\"text-classification\"任务，将来可能会添加更多的任务类型 结果存储 评估结果save和push 保存和分享评估结果是一个重要的步骤。我们提供evaluate.save()函数来方便地保存指标结果。你可以传递一个特定的文件名或目录。在后一种情况下，结果将保存在一个带有自动创建的文件名的文件中 除了目录或文件名，该函数还接受任意的键值对作为输入，并将它们存储在一个JSON文件中 result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1]) hyperparams = {\"model\": \"bert-base-uncased\"} evaluate.save(\"./results/\", experiment=\"run 42\", **result, **hyperparams) PosixPath('results/result-2022_05_30-22_09_11.json') # result-2022_05_30-22_09_11.json { \"experiment\": \"run 42\", \"accuracy\": 0.5, \"model\": \"bert-base-uncased\", \"_timestamp\": \"2022-05-30T22:09:11.959469\", \"_git_commit_hash\": \"123456789abcdefghijkl\", \"_evaluate_version\": \"0.1.0\", \"_python_version\": \"3.9.12 (main, Mar 26 2022, 15:51:15) \\n[Clang 13.1.6 (clang-1316.0.21.2)]\", \"_interpreter_path\": \"/Users/leandro/git/evaluate/env/bin/python\" } 除了指定的字段，它还包含有用的系统信息，用于重现结果，你还应该将它们报告到模型在Hub上的存储库中 evaluate.push_to_hub( model_id=\"huggingface/gpt2-wikitext2\", # model repository on hub metric_value=0.5, # metric value metric_type=\"bleu\", # metric name, e.g. accuracy.name metric_name=\"BLEU\", # pretty name which is displayed dataset_type=\"wikitext\", # dataset name on the hub dataset_name=\"WikiText\", # pretty name dataset_split=\"test\", # dataset split used task_type=\"text-generation\", # task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json task_name=\"Text Generation\" # pretty name for task ) 上传自己的指标Creating and sharing a new evaluation 可视化 当比较多个模型时，仅通过查看它们的得分往往很难发现它们之间的差异。而且通常情况下，并没有一个单一的最佳模型，而是在准确性和延迟等方面存在着权衡，因为较大的模型可能具有更好的性能但也更慢。我们正在逐步添加不同的可视化方法，例如绘图，以便更轻松地选择适合特定用例的最佳模型。 例如，如果您有多个模型的结果列表（以字典形式），您可以将它们传递给radar_plot()函数进行可视化： import evaluate from evaluate.visualization import radar_plot data = [ {\"accuracy\": 0.99, \"precision\": 0.8, \"f1\": 0.95, \"latency_in_seconds\": 33.6}, {\"accuracy\": 0.98, \"precision\": 0.87, \"f1\": 0.91, \"latency_in_seconds\": 11.2}, {\"accuracy\": 0.98, \"precision\": 0.78, \"f1\": 0.88, \"latency_in_seconds\": 87.6}, {\"accuracy\": 0.88, \"precision\": 0.78, \"f1\": 0.81, \"latency_in_seconds\": 101.6} ] model_names = [\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"] plot = radar_plot(data=data, model_names=model_names) plot.show() 选择合适指标 评估指标可以分为三个高级类别： 通用指标：适用于各种情况和数据集的指标，例如精确度和准确度 precision_metric = evaluate.load(\"precision\") results = precision_metric.compute(references=[0, 1], predictions=[0, 1]) print(results) {'precision': 1.0} 任务特定指标：仅适用于特定任务的指标，例如机器翻译(通常使用BLEU或ROUGE指标进行评估)或命名实体识别(通常使用seqeval进行评估) 数据集特定指标：旨在衡量模型在特定基准数据集上的性能，例如GLUE基准测试具有专门的评估指标 transformers 概述 What &#x1F917; Transformers can do &#x1F917; Transformers提供了API和工具，可轻松下载和训练最先进的预训练模型。使用预训练模型可以减少计算成本、碳足迹，并节省从头开始训练模型所需的时间和资源。这些模型支持不同领域的常见任务，包括： &#x1F4DD; 自然语言处理：文本分类、命名实体识别、问答系统、语言建模、摘要生成、翻译、多项选择和文本生成 &#x1F5BC;️ 计算机视觉：图像分类、目标检测和分割 &#x1F5E3;️ 音频：自动语音识别和音频分类 &#x1F419; 多模态：表格问答、光学字符识别、从扫描文档中提取信息、视频分类和视觉问答 &#x1F917; Transformers支持在PyTorch、TensorFlow和JAX之间进行框架互操作。这提供了在模型的不同阶段使用不同框架的灵活性；可以在一个框架中用三行代码训练模型，然后在另一个框架中加载模型进行推理。模型还可以导出为ONNX和TorchScript等格式，以便在生产环境中进行部署 安装 pip install transformers datasets 快速开始 Pipeline pipeline()是使用预训练模型进行推理的最简单和最快捷的方法。您可以直接使用pipeline()进行许多任务的推理，涵盖了不同的模态，下表列出了其中一些任务 Task Description Modality Pipeline identifier Text classification assign a label to a given sequence of text NLP pipeline(task=“sentiment-analysis”) Text generation generate text given a prompt NLP pipeline(task=“text-generation”) Summarization generate a summary of a sequence of text or document NLP pipeline(task=“summarization”) Image classification assign a label to an image CV pipeline(task=“image-classification”) Image segmentation assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) CV pipeline(task=“image-segmentation”) Object detection predict the bounding boxes and classes of objects in an image CV pipeline(task=“object-detection”) Audio classification assign a label to some audio data Audio pipeline(task=“audio-classification”) Automatic speech recognition transcribe speech into text Audio pipeline(task=“automatic-speech-recognition”) Visual question answering answer a question about the image, given an image and a question Multimodal pipeline(task=“vqa”) Document question answering answer a question about a document, given an image and a question Multimodal pipeline(task=“document-question-answering”) Image captioning generate a caption for a given image Multimodal pipeline(task=“image-to-text”) 基本使用 首先，通过创建pipeline()的实例并指定要使用的任务，开始使用它。在本指南中，我们以情感分析的pipeline()为例： from transformers import pipeline classifier = pipeline(\"sentiment-analysis\") pipeline()会下载并缓存用于情感分析的默认预训练模型和分词器。现在，您可以在目标文本上使用分类器了： classifier(\"We are very happy to show you the &#x1F917; Transformers library.\") [{'label': 'POSITIVE', 'score': 0.9998}] 如果您有多个输入，请将输入作为列表传递给pipeline()，以返回一个字典列表 results = classifier([\"We are very happy to show you the &#x1F917; Transformers library.\", \"We hope you don't hate it.\"]) for result in results: print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\") label: POSITIVE, with score: 0.9998 label: NEGATIVE, with score: 0.5309 pipeline()还可以对任何您喜欢的任务迭代整个数据集。在这个例子中，让我们选择自动语音识别作为我们的任务 import torch from transformers import pipeline speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\") 加载您想要迭代的音频数据集(有关更多详细信息，请参阅&#x1F917; Datasets快速入门)。例如，加载MInDS-14数据集： from datasets import load_dataset, Audio dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\") 您需要确保数据集的采样率与facebook/wav2vec2-base-960h 训练时使用的采样率相匹配 dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate)) 调用\"audio\"列时，音频文件会自动加载和重新采样。从前四个样本中提取原始波形数组，并将其作为列表传递给pipeline： result = speech_recognizer(dataset[:4][\"audio\"]) print([d[\"text\"] for d in result]) ['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT'] 对于输入较大的更大数据集(如语音或视觉数据)，您可以将生成器传递给pipeline，而不是将其作为列表加载到内存中 在pipeline中使用其他模型和分词器pipeline()可以适应Hub中的任何模型，这使得对pipeline()进行其他用途的调整变得容易 例如，如果您想要一个能够处理法语文本的模型，请使用Hub上的标签来过滤合适的模型。通过对过滤结果进行排序，您可以获得一个针对法语文本进行情感分析的多语言BERT模型 在pipeline中使用另一个模型和分词器 pipeline()可以适应Hub中的任何模型，这使得将pipeline()适应其他用例变得容易 from transformers import AutoTokenizer, AutoModelForSequenceClassification model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" model = AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer) classifier(\"Nous sommes très heureux de vous présenter la bibliothèque &#x1F917; Transformers.\") AutoClass AutoClass是一种快捷方式，它可以根据模型的名称或路径自动获取预训练模型的架构。您只需要选择与您的任务相匹配的AutoClass和相应的预处理类 AutoTokenizer AutoTokenizer分词器负责将文本预处理为模型输入的数字数组。有多个规则来规定分词的过程，包括如何拆分一个单词以及以何种级别拆分单词 最重要的是，您需要使用相同的模型名称来实例化一个分词器，以确保您使用了与预训练模型相同的分词规则 使用AutoTokenizer加载一个分词器 将return_tensors参数设置为pt以返回适用于PyTorch的张量，或者设置为tf以返回适用于TensorFlow的张量 from transformers import AutoTokenizer model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" tokenizer = AutoTokenizer.from_pretrained(model_name) encoding = tokenizer(\"We are very happy to show you the &#x1F917; Transformers library.\", return_tensors=\"pt\") {'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} tokenizer.decode(encoding[\"input_ids\"]) \"We are very happy to show you the &#x1F917; Transformers library.\" 分词器返回一个包含三个项目的字典： input_ids：表示文本中各个标记的数字 token_type_ids：如果有多个序列，指示一个标记属于哪个序列 attention_mask：指示一个标记是否应该被掩盖(masked) 分词器还可以接受一个输入列表，并对文本进行填充和截断，以返回具有统一长度的批处理数据 pt_batch = tokenizer( [\"We are very happy to show you the &#x1F917; Transformers library.\", \"We hope you don't hate it.\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\") pad + truncation # padding batch_sentences = [ \"But what about second breakfast?\", \"Don't think he knows about second breakfast, Pip.\", \"What about elevensies?\", ] encoded_input = tokenizer(batch_sentences, padding=True) {'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]} # truncation 将truncation参数设置为True，可以将序列截断为模型所能接受的最大长度 batch_sentences = [ \"But what about second breakfast?\", \"Don't think he knows about second breakfast, Pip.\", \"What about elevensies?\", ] encoded_input = tokenizer(batch_sentences, padding=True, truncation=True) {'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]} AutoModel &#x1F917;Transformers提供了一种简单而统一的方法来加载预训练模型实例。这意味着您可以像加载AutoTokenizer一样加载AutoModel 唯一的区别是选择正确的AutoModel来适应任务。对于文本(或序列)分类，您应该加载AutoModelForSequenceClassification from transformers import AutoModelForSequenceClassification model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" pt_model = AutoModelForSequenceClassification.from_pretrained(model_name) pt_outputs = pt_model(**pt_batch) 模型将最终的激活值存储在logits属性中。应用softmax函数到logits上以获取概率值 from torch import nn pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1) tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725], [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=) 在huggingface库中，AutoModel类可以根据给定的checkpoint自动选择并加载适合的模型。它支持各种不同的模型架构，包括： AutoModel: 用于通用的模型加载，根据checkpoint自动选择适合的模型架构 AutoModelForSequenceClassification: 用于序列分类任务的模型，如文本分类 AutoModelForQuestionAnswering: 用于问答任务的模型，如阅读理解 AutoModelForTokenClassification: 用于标记分类任务的模型，如命名实体识别 AutoModelForMaskedLM: 用于遮蔽语言建模任务的模型，如BERT AutoModelForCausalLM: 用于有因果关系的语言建模任务的模型，如GPT AutoModelForImageClassification: 用于图像分类任务的模型，如ResNet AutoModelForImageSegmentation: 用于图像分割任务的模型，如Mask R-CNN 这些仅是AutoModel类的一些示例，实际上还有更多可用的模型架构。您可以根据具体的任务需求选择适合的AutoModel类进行加载和使用 其他的Auto类 AutoImageProcessor 对于视觉任务，图像处理器将图像处理为正确的输入格式 from transformers import AutoImageProcessor image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\") AutoFeatureExtractor 对于音频任务，特征提取器将音频信号处理为正确的输入格式 from transformers import AutoFeatureExtractor feature_extractor = AutoFeatureExtractor.from_pretrained( \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\" ) AutoProcessor 多模态任务需要一个处理器来结合两种类型的预处理工具。例如，LayoutLMV2模型需要一个图像处理器来处理图像，还需要一个分词器来处理文本；处理器将两者结合起来 from transformers import AutoProcessor processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\") 模型保存 一旦您的模型经过微调，您可以使用PreTrainedModel.save_pretrained()将其与其标记器一起保存起来： # 模型+分词器 保存 pt_save_directory = \"./pt_save_pretrained\" tokenizer.save_pretrained(pt_save_directory) pt_model.save_pretrained(pt_save_directory) # 加载 pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory) AutoConfig 您可以修改模型的配置类来更改模型的构建方式。配置类指定了模型的属性，例如隐藏层的数量或注意力头数 当您从自定义配置类初始化模型时，您将从头开始。模型的属性将被随机初始化，您需要在使用模型之前对其进行训练以获得有意义的结果 首先导入AutoConfig，然后加载要修改的预训练模型。在AutoConfig.from_pretrained()中，您可以指定要更改的属性，例如注意力头的数量： from transformers import AutoConfig from transformers import AutoModel my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12) my_model = AutoModel.from_config(my_config) tokenizer huggingface的分词器的摘要 【LLM系列之Tokenizer】如何科学地训练一个LLM分词器 参见：LLM Tokenizer分词系列 Trainer 对于PyTorch，所有模型都是标准的torch.nn.Module，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，但&#x1F917;Transformers提供了Trainer类，其中包含基本的训练循环，并添加了其他功能，如分布式训练、混合精度等 根据您的任务，通常会向Trainer传递以下参数： PreTrainedModel或torch.nn.Module对象 from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\") TrainingArguments包含了可以修改的模型超参数，比如学习率、批大小和训练的轮数。如果你不指定任何训练参数，将使用默认值 from transformers import TrainingArguments training_args = TrainingArguments( output_dir=\"path/to/save/folder/\", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=2, ) Preprocessing类，例如tokenizer(标记器)、image processor(图像处理器)、feature extractor(特征提取器)或processor(处理器) from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") 加载数据集 from datasets import load_dataset dataset = load_dataset(\"rotten_tomatoes\") # doctest: +IGNORE_RESULT 创建一个函数来对数据集进行标记化处理，然后使用map函数将其应用于整个数据集 def tokenize_dataset(dataset): return tokenizer(dataset[\"text\"]) dataset = dataset.map(tokenize_dataset, batched=True) 使用DataCollatorWithPadding来从数据集中创建一个批次的示例 from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) DataCollatorWithPadding是Hugging Face的transformers库中的一个类，用于在训练过程中创建批次数据。它的作用是将不同长度的样本填充到相同长度，以便能够同时进行批处理 具体来说，DataCollatorWithPadding会根据给定的数据集，找到其中最长的样本，并将其他样本填充到相同的长度。填充通常使用特定的填充令牌(token)来完成，这样模型在处理时可以识别出填充部分，并进行相应的处理 使用DataCollatorWithPadding可以确保批次数据的长度一致，从而提高训练效率，并避免由于不同长度样本导致的错误 现在将所有这些类组合在Trainer中 from transformers import Trainer trainer = Trainer( model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], tokenizer=tokenizer, data_collator=data_collator, ) # doctest: +SKIP trainer.train() Trainer类提供了自定义训练循环行为的方法，你可以通过继承Trainer类并重写其中的方法来实现自定义行为。这样你就可以定制诸如损失函数、优化器和学习率调度器等功能。你可以参考Trainer类的文档了解可以重写的方法 另一种定制训练循环的方式是使用回调函数(Callbacks)。你可以使用回调函数与其他库进行集成，监视训练过程并报告进展，或在必要时提前停止训练。回调函数不会修改训练循环本身的行为。如果你需要定制损失函数等内容，你需要继承Trainer类来实现 教程 模型训练 使用预训练模型有很多好处。它可以减少计算成本和碳足迹，并且可以让您使用最先进的模型，而无需从头开始训练 &#x1F917;Transformers提供了对各种任务的数千个预训练模型的访问。当您使用预训练模型时，您可以在特定于您任务的数据集上进行微调训练。这被称为微调，是一种非常强大的训练技术 数据准备 from datasets import load_dataset from transformers import AutoTokenizer # 1. 加载数据集 dataset = load_dataset(\"yelp_review_full\") dataset[\"train\"][100] {'label': 0, 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'} # 可以创建一个较小的数据集子集，用于微调，以减少所需的时间 small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)) small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000)) # 2. 分词器 tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) Train with PyTorch Trainer from transformers import AutoModelForSequenceClassification from transformers import TrainingArguments import numpy as np import evaluate # 1. 加载模型 model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5) # 2. 定义训练参数 在训练参数中指定evaluation_strategy参数，以在每个epoch结束时报告评估指标 training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\") # 3. 加载评估器 metric = evaluate.load(\"accuracy\") # 在计算度量标准的时候调用compute，以计算您的预测的准确率。在将预测结果传递给compute之前，您需要将预测结果转换为logits def compute_metrics(eval_pred): logits, labels = eval_pred predictions = np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) # 4. 定义Trainer trainer = Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics, ) # 5. 开始训练 trainer.train() Train in native PyTorch Trainer负责训练循环，并允许您通过一行代码对模型进行微调。对于喜欢编写自己的训练循环的用户，您也可以在原生PyTorch中对&#x1F917;Transformers模型进行微调 from torch.utils.data import DataLoader from transformers import AutoModelForSequenceClassification from torch.optim import AdamW from transformers import get_scheduler import torch from tqdm.auto import tqdm import evaluate # 1. 数据集预处理 tokenized_datasets = tokenized_datasets.remove_columns([\"text\"]) tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") tokenized_datasets.set_format(\"torch\") # 2. 定义DataLoader train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8) eval_dataloader = DataLoader(small_eval_dataset, batch_size=8) # 3. 加载模型 model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5) # 4. 定义优化器 optimizer = AdamW(model.parameters(), lr=5e-5) # 5. 定义scheduler num_epochs = 3 num_training_steps = num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps ) # 6. 移动模型到指定设备 device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) # 7. 开始训练 progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dataloader: batch = {k: v.to(device) for k, v in batch.items()} outputs = model(**batch) loss = outputs.loss loss.backward() optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) # 8. 验证集评估 metric = evaluate.load(\"accuracy\") model.eval() for batch in eval_dataloader: batch = {k: v.to(device) for k, v in batch.items()} with torch.no_grad(): outputs = model(**batch) logits = outputs.logits predictions = torch.argmax(logits, dim=-1) metric.add_batch(predictions=predictions, references=batch[\"labels\"]) metric.compute() 分布式加速 huggingface的accelerate模块 &#x1F917;Accelerate是Hugging Face提供的用于简化分布式训练的库。它旨在使分布式训练更加容易和高效，支持多种深度学习框架，包括PyTorch和TensorFlow Accelerate提供了以下功能： 数据并行：Accelerate使用accelerator.DataParallel类来实现数据并行，可以在多个GPU上同时训练模型 混合精度训练：Accelerate支持自动混合精度训练，通过将模型参数和梯度转换为半精度浮点数来减少内存占用和计算量 分布式训练：Accelerate使用accelerator.DistributedDataParallel类来实现分布式训练，可以在多个机器上并行训练模型 训练循环的自动管理：Accelerate提供了一个accelerator.Trainer类，它封装了训练循环，自动处理数据加载、前向传播、反向传播、优化器更新等过程 使用Accelerate可以简化分布式训练的配置和管理，使用户能够更轻松地利用多个GPU或多台机器进行训练，并获得更高的训练效率 安装 pip install accelerate 示例代码，以下代码只列出改变的部分代码 只需要在训练循环中添加四行额外的代码即可启用分布式训练 from accelerate import Accelerator # 1. 定义加速器 accelerator = Accelerator() # 2. dataloader包装 train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare( train_dataloader, eval_dataloader, model, optimizer ) # 3. 反向传播 for epoch in range(num_epochs): for batch in train_dataloader: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) 完整代码如下 + from accelerate import Accelerator from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler + accelerator = Accelerator() model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) optimizer = AdamW(model.parameters(), lr=3e-5) - device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") - model.to(device) + train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare( + train_dataloader, eval_dataloader, model, optimizer + ) num_epochs = 3 num_training_steps = num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps ) progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dataloader: - batch = {k: v.to(device) for k, v in batch.items()} outputs = model(**batch) loss = outputs.loss - loss.backward() + accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) 示例代码 包括自然语言处理、语音、计算机视觉和多模态 PEFT模块 huggingface PEFT模块 详见兼一书虫-LLM模型微调系列 &#x1F917;PEFT，即Parameter-Efficient Fine-Tuning(参数高效微调)，是一个用于高效地将预训练语言模型(PLM)适应于各种下游应用的库，而无需对所有模型参数进行微调 PEFT方法只微调少量的(额外的)模型参数，显著降低了计算和存储成本，因为对大规模PLM进行完整微调代价过高。最近的最先进的PEFT技术达到了与完整微调相当的性能 PEFT与&#x1F917;Accelerate库无缝集成，用于利用DeepSpeed和Big Model Inference进行大规模模型微调 Supported methods (截至23-06-15) LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks P-Tuning: GPT Understands, Too Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention 其他模块 核心类 LLM 入门笔记-transformers库简单介绍](https://mp.weixin.qq.com/s/svJMXj9UwI2sd_SAtFExig)) ModelOutput ModelOutput(transformers.utils.ModelOutput)是所有模型输出的基类。简单理解它就是一个字典，在模型的 forward函数里把原本的输出做了一下封装而已，方便用户能直观地知道输出是什么 例如CausalLMOutput顾名思义就是用于像 GPT 这样自回归模型的输出，ModelOutput是所有模型输出的基类 class ModelOutput(OrderedDict): def __init_subclass__(cls) -> None: \"\"\" 这个方法允许对 ModelOutput 的子类进行定制，使得子类在被创建时能够执行特定的操作或注册到某个系统中。 \"\"\" ... def __init__(self, *args, **kwargs): \"\"\" 初始化 ModelOutput 类的实例。 \"\"\" super().__init__(*args, **kwargs) def __post_init__(self): \"\"\" 在初始化 ModelOutput 类的实例之后执行的操作，允许进一步对实例进行处理或设置属性。子类需要用 dataclass 装饰器 \"\"\" ... 基于 ModelOutput，hf 预先定义了 40 多种不同的 sub-class，这些类是 Hugging Face Transformers 库中用于表示不同类型模型输出的基础类，每个类都提供了特定类型模型输出的结构和信息，以便于在实际任务中对模型输出进行处理和使用 每个 sub-class 都需要用装饰器 @dataclass，我们以CausalLMOutputWithPast为例看一下源码 @dataclass class CausalLMOutputWithPast(ModelOutput): loss: Optional[torch.FloatTensor] = None logits: torch.FloatTensor = None past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None hidden_states: Optional[Tuple[torch.FloatTensor]] = None attentions: Optional[Tuple[torch.FloatTensor]] = None 为了保持代码规范，我们需要在模型的forward函数中对输出结果进行封装，示例如下： class MyModel(PretrainedModel): def __init__(self): self.model = ... def forward(self, inputs, labels): output = self.model(**inputs) hidden_states = ... loss = loss_fn(outputs, labels) return CausalLMOutputWithPast( loss=loss, logits=logits, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, ) 这里简单介绍以下几种，更多的可以查看官方文档和源码： BaseModelOutput: 该类是许多基本模型输出的基础，包含模型的一般输出，如 logits、hidden_states 等 BaseModelOutputWithNoAttention: 在模型输出中不包含注意力（attention）信息 BaseModelOutputWithPast: 包含过去隐藏状态的模型输出，适用于能够迭代生成文本的模型，例如语言模型 BaseModelOutputWithCrossAttentions: 在模型输出中包含交叉注意力（cross attentions）信息 通常用于特定任务中需要跨注意力的情况，比如机器翻译 BaseModelOutputWithPastAndCrossAttentions: 同时包含过去隐藏状态和交叉注意力的模型输出 MoEModelOutput: 包含混合专家模型（Mixture of Experts）输出的模型 MoECausalLMOutputWithPast: 混合专家语言模型的输出，包括过去隐藏状态 Seq2SeqModelOutput: 序列到序列模型输出的基类，适用于需要生成序列的模型 CausalLMOutput: 用于生成式语言模型输出的基础类，提供生成文本的基本信息 CausalLMOutputWithPast: 生成式语言模型输出的类，包含过去隐藏状态，用于连续生成文本的模型 PreTrainedModel PreTrainedModel (transformers.modeling_utils.PretrainedModel) 是所有模型的基类 所以你如果看到一个模型取名为LlamaForCausalLM，那你就可以知道这个模型的输出格式大概率就是自回归输出，即前面提到的CausalLMOutput PreTrainedModel 是 Hugging Face Transformers 库中定义预训练模型的基类 它继承了 nn.Module，同时混合了几个不同的 mixin 类，如 ModuleUtilsMixin、GenerationMixin、PushToHubMixin 和 PeftAdapterMixin 这个基类提供了创建和定义预训练模型所需的核心功能和属性 class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin): config_class = None base_model_prefix = \"\" main_input_name = \"input_ids\" _auto_class = None _no_split_modules = None _skip_keys_device_placement = None _keep_in_fp32_modules = None ... def __init__(self, config: PretrainedConfig, *inputs, **kwargs): super().__init__() ... 在这个基类中，我们可以看到一些重要的属性和方法： config_class：指向特定预训练模型类的配置文件，用于定义模型的配置 base_model_prefix：基本模型前缀，在模型的命名中使用，例如在加载预训练模型的权重时使用 main_input_name：指定模型的主要输入名称，通常是 input_ids _init_weights 方法：用于初始化模型权重的方法 在这个基类中，大多数属性都被定义为 None 或空字符串，这些属性在具体的预训练模型类中会被重写或填充 LLama例子 接下来我们将看到如何使用 PretrainedModel 类定义 llama 模型 class LlamaPreTrainedModel(PreTrainedModel): config_class = LlamaConfig base_model_prefix = \"model\" supports_gradient_checkpointing = True _no_split_modules = [\"LlamaDecoderLayer\"] _skip_keys_device_placement = \"past_key_values\" _supports_flash_attn_2 = True def _init_weights(self, module): std = self.config.initializer_range if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=std) if module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=std) if module.padding_idx is not None: module.weight.data[module.padding_idx].zero_() 在这个例子中，首先定义了 LlamaPreTrainedModel 类作为 llama 模型的基类，它继承自 PreTrainedModel。在这个基类中，我们指定了一些 llama 模型特有的属性，比如配置类 LlamaConfig、模型前缀 model、支持梯度检查点（gradient checkpointing）、跳过的模块列表 _no_split_modules 等等。 然后，我们基于这个基类分别定义了 LlamaModel、LlamaForCausalLM 和 LlamaForSequenceClassification。这些模型的逻辑关系如下图所示 LlamaModel是 llama 模型的主体定义类，也就是我们最常见的普pytorch 定义模型的方法、默认的输出格式为BaseModelOutputWithPast class LlamaModel(LlamaPreTrainedModel): def __init__(self, config: LlamaConfig): super().__init__(config) self.padding_idx = config.pad_token_id self.vocab_size = config.vocab_size self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx) self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]) self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) ... def forward(self, ...): ... return BaseModelOutputWithPast(...) LlamaForCausalLM 适用于生成式语言模型的 llama 模型，可以看到 backbone 就是 LlamaModel，增加了lm_head作为分类器，输出长度为词汇表达大小，用来预测下一个单词。输出格式为CausalLMOutputWithPast class LlamaForCausalLM(LlamaPreTrainedModel): # 适用于生成式语言模型的 Llama 模型定义 def __init__(self, config): super().__init__(config) self.model = LlamaModel(config) self.vocab_size = config.vocab_size self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False) ... def forward(self, ...): outputs = self.model(...) ... # 后处理 outputs，以满足输出格式要求 return CausalLMOutputWithPast(...) LlamaForSequenceClassification 适用于序列分类任务的 llama 模型，同样把 LlamaModel作为 backbone， 不过增加了score作为分类器，输出长度为 label 的数量，用来预测类别。输出格式为SequenceClassifierOutputWithPast class LlamaForSequenceClassification(LlamaPreTrainedModel): # 适用于序列分类任务的 Llama 模型定义 def __init__(self, config): super().__init__(config) self.num_labels = config.num_labels self.model = LlamaModel(config) self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False) ... def forward(self, ...): outputs = self.model(...) ... # 后处理 outputs，以满足输出格式要求 return SequenceClassifierOutputWithPast(...) 每个子类根据特定的任务或应用场景进行了定制，以满足不同任务的需求 另外可以看到 hf 定义的模型都是由传入的 config参数定义的，所以不同模型对应不同的配置啦，这也是为什么我们经常能看到有像 BertConfig，GPTConfig这些预先定义好的类 例如我们可以很方便地通过指定的字符串或者文件获取和修改不同的参数配置 config = BertConfig.from_pretrained(\"bert-base-uncased\") # Download configuration from huggingface.co and cache. config = BertConfig.from_pretrained(\"./test/saved_model/\") # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')* config = BertConfig.from_pretrained(\"./test/saved_model/my_configuration.json\") config = BertConfig.from_pretrained(\"bert-base-uncased\", output_attentions=True, foo=False) hf 为了造福懒人，提供了更加简便的 API，即 Auto 系列 API。至于有多简便，看看下面的 demo 就知道了 from transformers import AutoConfig, AutoModel # Download configuration from huggingface.co and cache. config = AutoConfig.from_pretrained(\"bert-base-cased\") model = AutoModel.from_config(config) AutoTrain AutoTrain是一个用于自动化训练的库，旨在简化模型训练的过程。它提供了一种简单的方法来定义和训练深度学习模型，自动处理数据加载、批处理、优化器、损失函数等训练过程中的细节。通过使用AutoTrain，你可以更快速地搭建和训练模型，减少样板代码的编写，并且能够轻松地进行超参数搜索和模型选择 Gradio Gradio是一个用于构建交互式界面的库，使你能够轻松地为你的深度学习模型创建Web应用程序。Gradio提供了一个简单而强大的API，可以将模型与用户界面组件(如文本框、滑块、图像上传器等)相连接，从而实现模型的实时推理和可视化。通过Gradio，你可以快速构建一个交互式的演示或部署你的模型到Web上，无需编写复杂的前端代码 Diffusers Diffusers是一个用于生成图像、音频甚至分子的三维结构的最新预训练扩散模型的库。无论您是寻找一个简单的推理解决方案，还是想要训练自己的扩散模型，&#x1F917;Diffusers都是一个支持两者的模块化工具箱。我们的库着重于易用性而非性能，简洁而非复杂，可定制性而非抽象性，该库主要包含以下三个组件： 最新的扩散推理流程，只需几行代码即可实现 可互换的噪声调度器，用于在生成速度和质量之间平衡权衡 可用作构建块的预训练模型，可以与调度器结合使用，创建您自己的端到端扩散系统 Accelerate Hugging Face的Accelerate是一个旨在简化和加速深度学习模型训练和推理过程的库 它提供了一个高级API，抽象了分布式训练、混合精度和梯度累积等复杂性，使用户能够轻松地充分利用硬件资源的潜力 Accelerate兼容PyTorch和TensorFlow，并提供了一套工具和实用程序，以实现跨多个GPU或多台机器的高效分布式训练。它包括以下功能： 分布式训练：Accelerate提供了简单易用的接口，使用户能够将训练过程分布到多个GPU或多台机器上。它支持常见的分布式训练策略，如数据并行和模型并行，并自动处理数据的分发和梯度的聚合，使用户无需手动编写复杂的分布式训练代码 混合精度训练：Accelerate支持混合精度训练，通过同时使用浮点16位和浮点32位精度来加快模型的训练速度。它自动处理数据类型转换和梯度缩放，用户只需简单地指定使用混合精度训练即可 梯度累积：Accelerate支持梯度累积，这在GPU显存有限的情况下特别有用。梯度累积允许在多个小批次上累积梯度，然后进行一次大批次的参数更新，从而减少显存占用并提高训练效率 自动调节批次大小：Accelerate可以自动调整批次大小以适应可用的GPU内存。它会动态调整批次大小，以达到最佳的GPU利用率和训练性能 总之，Hugging Face的Accelerate是一个功能强大的库，旨在简化和加速深度学习模型的训练和推理过程。它提供了高级API和一系列工具，使用户能够轻松地实现分布式训练、混合精度训练和梯度累积等高效训练策略 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/nlp关键词和摘要提取技术整理.html":{"url":"chapters/nlp关键词和摘要提取技术整理.html","title":"nlp关键词和摘要提取技术整理.md","summary":"nlp关键词和摘要提取技术整理","keywords":"","body":"切词关键词提取概述分类基于统计tf-idfYAKE概述基于图技术TextRankPageRank实现RAKE基于深度学习SIFRankSIF模型Bert-KPEKeyBert概述多样性示例摘要提取 切词 等待... 关键词提取 自动关键词抽取研究综述2017 特征驱动的关键词提取算法综述2018 关键词提取研究综述2021 概述 概念 关键词提取技术是一种自然语言处理技术，旨在从给定的文本中自动识别出最具代表性和重要性的关键词或短语 关键词通常是文本中具有特殊含义、能够概括文本主题或内容的词语或短语 使用场景 关键词提取技术的目标是对文本进行语义分析和内容抽取，从而提取出最能代表文本主题和内容的关键词 这些关键词可以用于文本分类、信息检索、文本摘要、主题建模、信息过滤等自然语言处理任务 经典方法 关键词提取技术通常结合了文本的语言统计特征、词频分布、词性、上下文关系、语义相似度等多种信息源，以识别并提取出最相关和具有区分度的关键词 常见的关键词提取方法包括基于词频、TF-IDF、文本图结构、语言模型、图模型、深度学习等多种技术手段 关键词提取技术在信息处理、文本挖掘、自动化文档处理等领域具有重要应用价值，能够帮助人们更快速、准确地理解和处理大量文本信息 分类 NLP中关键字提取方法总结和概述 基于统计 统计方法是最简单的。他们计算关键字的统计数据并使用这些统计数据对它们进行评分。一些最简单的统计方法是词频、词搭配和共现 也有一些更复杂的，例如TF-IDF和YAKE! tf-idf 概述 TF-IDF或term frequency–inverse document frequency，会计算文档中单词相对于整个语料库(更多文档集)的重要性 它计算文档中每个词的频率，并通过词在整个语料库中的频率的倒数对其进行加权。最后，选择得分最高的词作为关键词，TF-IDF有两层意思 词频: Term Frequency，缩写为TF，通常来说，一个分词出现的次数越多，代表越重要 逆文档频率: Inverse Document Frequency，缩写为IDF，在现实生活中，出现次数最多的词是一些无意义的词，比如停用词，对搜索结果毫无帮助，必须通过分词器提前过滤掉的词 公式 - TF-IDF from scratch in python on a real-world dataset. 术语 t d N corpus 释义 term(word) document(set of words)一篇文档中的词集合 count of corpus语料数量 the total document set总体文档集合 \\begin{array}{c} tf(t,d) = \\frac{count\\ of\\ t\\ in\\ d}{number\\ of\\ words\\ in\\ d} = \\frac {单词t在文档d中出现的频次}{文档d的词频总数} \\\\\\\\ df(t) = occurrence\\ of\\ t\\ in\\ N\\ documents = 单词t在N篇文档中多少篇中出现 \\\\\\\\ idf(t) = \\frac {N}{df} = log(\\frac {N}{df+1}) \\\\\\\\ tf\\_idf(t, d) = tf(t, d) * idf(t) \\end{array} 一个分词Term的相关性由tf*idf公式简化表达。tf-idf模型包含了二个简单事实： 某个term分词在一个文档中出现次数(tf)越多，这个词与文档越相关 某个索引中包含某个term分词的文档数量越少(idf)，这个term分词越重要 例子 考虑一个包含100个单词的文档，其中Leon这个分词出现了10次。这个时候TF=(10/100)=0.1 并且假设Lucene索引中有1000W份文档数量，其中有1000份文档中出现了Leon这个分词，此时逆文档频率(IDF)计算为IDF=log(10,000,000/1,000)=4 因此，TD-IDF计算为TF*IDF=0.1 * 4=0.4 import jieba import jieba.analyse sentence = '中华蜜蜂原产于中国，是中国的土著蜂，适应中国各地的气候和蜜源条件，适于定地饲养且稳产，尤其是在南方山区，有着其他蜂种不可替代的地位' seg_list = jieba.cut(sentence, cut_all=True) print(\", \".join(seg_list)) keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=('n', 'nr', 'ns')) print(keywords) >>> 中华, 蜜蜂, 原产, 产于, 中国, ，, 是, 中国, 的, 土著, 蜂, ，, 适应, 中国, 各地, 的, 气候, 和, 蜜源, 条件, ，, 适于, 定, 地, 饲养, 且, 稳产, ，, 尤其, 是, 在, 南方, 方山, 山区, ，, 有着, 其他, 蜂, 种, 不可, 替代, 的, 地位 [('定地', 0.7969), ('蜂种', 0.7969), ('稳产', 0.7340), ('蜜源', 0.66725), ('中国', 0.60546), ('蜜蜂', 0.5859), ('土著', 0.55968), ('原产', 0.544705), ('替代', 0.484315), ('山区', 0.44390), ('气候', 0.38804), ('地位', 0.34710), ('条件', 0.32636)] 优缺点 速度快: TF-IDF的优点是速度快 语言无关: TF-IDF与语言无关 需要语料: 需要至少几十个文档的语料库 不够全面: 缺点是单纯于词频来判断一个分词的重要性，不够全面 无法捕获语义: 缺点是不能捕捉分词Term在文档中的位置 变种 TF-IDF 的4大常见变种 变种1: 对数函数变换 TF，解决TF现行增长问题 变种2: 对 TF 进行标准化，解决长短文档问题 变种3: 对数函数变换 IDF，解决IDF 现行增长问题 变种4: 查询词及文档向量标准化，解决长短文档问题 YAKE 2018ECIR的最佳短论文奖 A Text Feature Based Automatic Keyword Extraction Method for Single Documents github Yet Another Keyword Extractor (Yake) 概述 YAKE(Yet Another Keyword Extractor)是一种关键字提取方法，它利用单个文档的统计特征来提取关键字 它通过五个步骤提取关键字，旨在从文本中自动提取最相关的关键词和关键短语 YAKE算法的工作原理如下： 文本预处理：将输入文本进行预处理，包括分词、去除停用词等 特征提取：使用词频、位置权重、长度等特征来衡量单词和短语的重要性 关键词候选生成：根据特征权重，生成候选关键词和关键短语 关键词权重计算：根据候选关键词的特征权重，计算它们的最终权重 关键词筛选：根据设定的阈值或排序方法，筛选出具有高权重的关键词和关键短语作为最终结果 YAKE算法与传统的基于统计和语言模型的关键词提取方法不同，它采用了基于特征权重的方法，使得算法更加灵活和可定制 此外，YAKE还支持多语言关键词提取，并能够处理领域特定的文本 总体而言，YAKE算法通过综合考虑单词和短语的特征权重，以及它们在文本中的频率和位置等信息，来提取与文本内容相关的关键词和关键短语 官方实现仅支持英文 基于图技术 TextRank TextRank算法介绍及实现 TextRank是一种基于图的算法，用于关键词提取和文本摘要。它基于PageRank算法的思想，将文本表示为一个图，其中节点表示文本中的单词或短语，边表示它们之间的关系 通过计算节点之间的权重和连接关系，TextRank可以确定文本中最重要的单词或短语。 TextRank的步骤如下： 分割文本：将文本分割成句子或单词 构建图：根据文本中的句子或单词构建一个图，其中每个句子或单词作为一个节点，边表示它们之间的关系。常见的关系可以是共现关系或语义关系 计算权重：为图中的每个节点计算权重。通常使用词频或TF-IDF作为初始权重 迭代计算：通过迭代计算节点之间的权重，更新每个节点的权重值。迭代过程中，节点的权重将考虑其相邻节点的权重值 排序节点：根据节点的权重值对节点进行排序，得到关键词或摘要 TextRank算法在关键词提取和文本摘要等任务中表现良好，它不需要依赖预训练模型，可以直接应用于各种领域的文本处理任务 PageRank The PageRank Citation Ranking: Bringing Order to the Web 1999 关键词提取和摘要算法TextRank详解与实战 PageRank算法通过计算网页链接的数量和质量来粗略估计网页的重要性，算法创立之初即应用在谷歌的搜索引擎中，对网页进行排名。 PageRank算法的核心思想如下： 链接数量：如果一个网页被越多的其他网页链接，说明这个网页越重要，即该网页的PR值(PageRank值)会相对较高 链接质量：如果一个网页被一个越高权值的网页链接，也能表明这个网页越重要，即一个PR值很高的网页链接到一个其他网页，那么被链接到的网页的PR值会相应地因此而提高 PageRank算法计算公式 S\\left(V_{i}\\right)=(1-\\mathrm{d})+\\mathrm{d} * \\sum_{j \\in \\operatorname{In}\\left(V_{i}\\right)} \\frac{1}{\\left|O u t\\left(V_{j}\\right)\\right|} S\\left(V_{j}\\right) 其中，S\\left(V_{i}\\right)是网页i的重要性(PR值)，\\mathrm{d}是阻尼系数，一般为0.85，\\operatorname{In}\\left(V_{i}\\right)是整个互联网中所存在的有指同网页i的链接的网页集合，Out\\left(V_{j}\\right)是网页j中存在的指向所有外部网页的链辖的集合，|Out \\left(V_{j}\\right) \\mid是该集合中元素的个数 例子 等待... PageRank算法与TextRank算法的区别 PageRank算法根据网页之间的链接关系构造网络，TextRank算法根据词之间的共现关系构造网络 PageRank算法构造的网络中的边是有向无权边，TextRank算法构造的网络中的边是无向有权边 实现 基于Textrank4zh的TextRank算法实现 from textrank4zh import TextRank4Keyword, TextRank4Sentence import jieba.analyse from snownlp import SnowNLP import pandas as pd import numpy as np #关键词抽取 def keywords_extraction(text): tr4w = TextRank4Keyword(allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz']) # allow_speech_tags --词性列表，用于过滤某些词性的词 tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words', pagerank_config={'alpha': 0.85, }) # text -- 文本内容，字符串 # window -- 窗口大小，int，用来构造单词之间的边。默认值为2 # lower -- 是否将英文文本转换为小写，默认值为False # vertex_source -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点 # -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters' # edge_source -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边 # -- 默认值为`'no_stop_words'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'`。边的构造要结合`window`参数 # pagerank_config -- pagerank算法参数配置，阻尼系数为0.85 keywords = tr4w.get_keywords(num=6, word_min_len=2) # num -- 返回关键词数量 # word_min_len -- 词的最小长度，默认值为1 return keywords #关键短语抽取 def keyphrases_extraction(text): tr4w = TextRank4Keyword() tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words', pagerank_config={'alpha': 0.85, }) keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1) # keywords_num -- 抽取的关键词数量 # min_occur_num -- 关键短语在文中的最少出现次数 return keyphrases #关键句抽取 def keysentences_extraction(text): tr4s = TextRank4Sentence() tr4s.analyze(text, lower=True, source='all_filters') # text -- 文本内容，字符串 # lower -- 是否将英文文本转换为小写，默认值为False # source -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。 # -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters' # sim_func -- 指定计算句子相似度的函数 # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要 keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6) return keysentences def keywords_textrank(text): keywords = jieba.analyse.textrank(text, topK=6) return keywords if __name__ == \"__main__\": text = \"来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，\" \\ \"我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、\" \\ \"副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”\" \\ \"据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，\" \\ \"获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，\" \\ \"国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，\" \\ \"正式将该小行星命名为“周又元星”。\" #关键词抽取 keywords=keywords_extraction(text) print(keywords) #关键短语抽取 keyphrases=keyphrases_extraction(text) print(keyphrases) #关键句抽取 keysentences=keysentences_extraction(text) print(keysentences) >>> [{'word': '小行星', 'weight': 0.05808}, {'word': '天文台', 'weight': 0.05721}, {'word': '命名', 'weight': 0.048517}, {'word': '中国', 'weight': 0.045716}, {'word': '中国科学院', 'weight': 0.037818}, {'word': '国家', 'weight': 0.03438}] ['小行星命名'] [{'index': 4, 'sentence': '2018年9月25日，经国家天文台申报，国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，正式将该小行星命名为“周又元星”', 'weight': 0.2281}, {'index': 3, 'sentence': '”据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，获得国际永久编号第120730号', 'weight': 0.2106}, {'index': 1, 'sentence': '4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂', 'weight': 0.20209}] 基于jieba的TextRank算法实现 if __name__ == \"__main__\": text = \"来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，\" \\ \"我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、\" \\ \"副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”\" \\ \"据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，\" \\ \"获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，\" \\ \"国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，\" \\ \"正式将该小行星命名为“周又元星”。\" # 基于jieba的textrank算法实现 keywords=keywords_textrank(text) print(keywords) >>> ['小行星', '命名', '国际', '中国', '国家', '天文学家'] RAKE RAKE和TextRank的主要区别在于RAKE考虑候选关键字内的共现而不是固定窗口。它使用更简单、更具统计性的评分程序。该算法对每个文档分别进行，因此不需要文档语料库来进行关键词提取 基于深度学习 深度学习的出现使基于嵌入的方法成为可能。研究人员开发了几种使用文档嵌入的关键字提取方法(例如Bennani等人) 这些方法主要查找候选关键字列表(例如，Bennani等人只考虑由名词和形容词组成的关键字) 他们将文档和候选关键字嵌入到相同的嵌入空间中，并测量文档和关键字嵌入之间的相似度(例如余弦相似度)。他们根据相似度度量选择与文档文本最相似的关键字 SIFRank SIFRank: A New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model 2019 论文阅读笔记： SIFRank and BERT-KPE github sunyilgdx/SIFRank_zh SIFRank比较适合短文本的关键词抽取，而SIFRank+大幅增加了长文本的关键词抽取效果 步骤 人工标注：分词+标词性 获取候选关键词列表：利用正则表达式确定名词短语(例如：形容词+名词)，将名词短语作为候选关键短语 通过预训练语言模型，得到关键词的embedding 同样地，得到句子或文档的embedding 计算3与4结果的余弦相似度，选取topN作为其最终提取的关键词 NP chunker 在SIFRank方法中，NP chunker是一种用于识别和提取名词短语(Noun Phrase)的工具或组件。NP chunker的目标是从给定的文本中定位和提取出包含一个或多个名词的短语。名词短语通常由一个名词作为核心词，并可能包含其他修饰词或限定词 NP chunker使用一系列语法规则或机器学习模型来识别名词短语的边界，并将它们标记为一个单独的短语单元 这个过程有助于更好地理解文本的结构和语义，特别是在文本中涉及到名词短语的关键短语提取任务中。在SIFRank方法中，NP chunker用于提取候选关键短语，并为后续的关键短语排序和评分提供基础 SIF模型 句子嵌入模型SIF(Smooth Inverse Frequency)是一种用于将句子转换为连续向量表示的方法 它旨在捕捉句子的语义信息，并将句子表示为稠密的低维向量。SIF模型的关键思想是结合词频信息来调整词向量的权重，以降低高频词的重要性，同时提高低频词的重要性。这样可以减轻一些常见词对句子表示的影响，使得句子表示更加注重那些在语义上更具区分度的词语。SIF模型通过简单的数学运算，如减法和加权平均，来计算句子的嵌入表示 它在自然语言处理任务中被广泛应用，如文本分类、情感分析和句子相似度计算等 sentence embedding的假设是：文章是由一个topic生成的，文章中的每个句子亦是如此，因此，句子的embedding应该与文章embedding的期望值(topic embedding)相近 Bert-KPE Capturing Global Informativeness in Open Domain Keyphrase Extraction 2021 BERT-KPE是最近由thunlp提出的方法，在OpenKP和KP20K上都达到了state-of-the-art和良好的鲁棒性 有监督的方式 KeyBert KeyBERT Keyword Extraction with BERT 「关键词」提取都有哪些方案？ 当我们想要从特定文档中了解关键信息时，通常会使用关键词提取。关键词提取是一种自动化的过程，用于提取与输入文本最相关的单词和短语 通过使用Rake和YAKE!等方法，我们已经可以使用易于使用的软件包来提取关键词和关键短语。然而，这些模型通常基于文本的统计特性而不是语义相似性进行工作 于是BERT登场。BERT是一个双向变换器模型，可以将短语和文档转化为能够捕捉其意义的向量 使用BERT提取文档向量(嵌入)以获取文档级表示。然后，针对N元语法词/短语提取词向量。最后，我们使用余弦相似度来查找与文档最相似的词/短语。然后，可以将最相似的词识定义为最能描述整个文档的词 概述 什么是Keybert Keybert是一种基于无监督学习的关键词抽取技术，不仅效果好，而且易于使用 Keybert主要通过Bert获取文档和候选词的embedding，然后使用余弦相似度计算得到文档中最相似的候选词作为关键词 多样性 在关键词提取中，多样性问题指的是关键词列表中存在大量相似或重复的关键词，缺乏多样性和代表性。这可能导致关键信息的丢失或重复，并降低关键词提取的效果 MSS 最大总距离(Max Sum Distance)：通过将文档中最相似的关键词/短语与候选关键词/短语进行组合，找到彼此之间相似性最低的组合，这样可以确保关键词之间的差异性 MMR 最大边际相关性（Maximal Marginal Relevance，MMR)：通过使用余弦相似度来创建关键词/短语，基于相似性的排序 然后，从排序后的结果中选择与文档最相关的关键词/短语，并选择与已选择关键词/短语最不相似的候选关键词/短语，这样可以确保结果具有高度的多样性 示例 KeyBert Quickstart 安装 # 默认hugging face pip install keybert # 其他后端 pip install keybert[flair] pip install keybert[gensim] pip install keybert[spacy] pip install keybert[use] 基础KeyBERT 基础用法 from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc) # 设置keyphrase_ngram_range来确定生成的关键词/关键短语的长度范围 >>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None) [('learning', 0.4604), ('algorithm', 0.4556), ('training', 0.4487), ('class', 0.4086), ('mapping', 0.3700)] # 要提取关键短语，只需将keyphrase_ngram_range设置为(1, 2)或更高，具体取决于您希望在生成的关键短语中包含的单词数量 >>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None) [('learning algorithm', 0.6978), ('machine learning', 0.6305), ('supervised learning', 0.5985), ('algorithm analyzes', 0.5860), ('learning function', 0.5850)] # 设置highlight来在文档中突出显示关键词 keywords = kw_model.extract_keywords(doc, highlight=True) 关键词多样化 默认情况下，KeyBERT仅基于余弦相似度比较文档和候选关键词/关键短语。然而，这可能导致非常相似的单词出现在最准确的关键词/关键短语列表中 为了确保它们更加多样化，我们可以采取两种方法来微调输出结果，即最大总距离(Max Sum Distance)和最大边际相关性(Maximal Marginal Relevance) 最大总距离: 为了使结果多样化，我们选取文档中与前top_n个最相似的单词/短语。然后，我们从这2 x top_n个单词中选取所有top_n个组合，并提取彼此之间最不相似的组合，通过余弦相似度进行比较 >>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5) [('set training examples', 0.7504), ('generalize training data', 0.7727), ('requires learning algorithm', 0.5050), ('supervised learning algorithm', 0.3779), ('learning machine learning', 0.2891)] 最大边际相关性: 为了使结果多样化，我们可以使用最大边际相关性（Maximal Marginal Relevance，MMR）来创建关键词/关键短语，它也基于余弦相似度 # 具有高多样性的结果 kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.7) [('algorithm generalize training', 0.7727), ('labels unseen instances', 0.1649), ('new examples optimal', 0.4185), ('determine class labels', 0.4774), ('supervised learning algorithm', 0.7502)] # 具有低多样性的结果 >>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.2) [('algorithm generalize training', 0.7727), ('supervised learning algorithm', 0.7502), ('learning machine learning', 0.7577), ('learning algorithm analyzes', 0.7587), ('learning algorithm generalize', 0.7514)] 其他关键词算法生成的候选关键词 在某些情况下，您可能希望使用其他关键词算法生成的候选关键词或从可能的关键词/关键短语列表中检索的候选关键词 在KeyBERT中，您可以轻松使用这些候选关键词进行关键词提取 import yake from keybert import KeyBERT # Create candidates kw_extractor = yake.KeywordExtractor(top=50) candidates = kw_extractor.extract_keywords(doc) candidates = [candidate[0] for candidate in candidates] # Pass candidates to KeyBERT kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc, candidates=candidates) Guided KeyBERT Guided KeyBERT(引导式KeyBERT)与引导式主题建模类似，它试图将训练引导到一组种子术语上。当应用KeyBERT时，它会自动提取与特定文档最相关的关键词。然而，有时利益相关者和用户正在寻找特定类型的关键词。例如，当通过contentful在您的网站上发布一篇文章时，您通常已经了解与该文章相关的全局关键词。但是，文章中可能存在您希望通过关键词提取出来的特定主题。为了实现这一点，我们只需给KeyBERT提供一组相关的种子关键词(可以是单个关键词)，并搜索与文档和种子关键词都相似的关键词 使用这个功能非常简单，只需定义一个种子关键词列表并将其传递给KeyBERT即可 from keybert import KeyBERT kw_model = KeyBERT() # Define our seeded term seed_keywords = [\"information\"] keywords = kw_model.extract_keywords(doc, seed_keywords=seed_keywords) 当你有一个大型数据集，并且想要微调诸如多样性之类的参数时，每次更改参数时重新计算文档和单词嵌入可能需要很长时间。相反，我们可以预先计算这些嵌入，并将它们传递给.extract_keywords，这样我们只需计算一次即可 from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs) 然后，你可以使用这些嵌入并将它们传递给.extract_keywords来加快模型的调整 keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) .extract_embeddings中有几个参数定义了如何生成候选关键词/关键短语的列表： candidates keyphrase_ngram_range stop_words min_df vectorizer 这些参数的值在.extract_embeddings和.extract_keywords中需要完全相同，换句话说，以下内容将起作用，因为它们使用相同的参数子集 from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=1, stop_words=\"english\") keywords = kw_model.extract_keywords(docs, min_df=1, stop_words=\"english\", doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) 然而，以下内容将抛出错误，因为我们没有为min_df和stop_words使用相同的值 from keybert import KeyBERT kw_model = KeyBERT() doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=3, stop_words=\"dutch\") keywords = kw_model.extract_keywords(docs, min_df=1, stop_words=\"english\", doc_embeddings=doc_embeddings, word_embeddings=word_embeddings) 摘要提取 等待... 生成式 + 抽取式 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/pytorch学习_基础知识.html":{"url":"chapters/pytorch学习_基础知识.html","title":"pytorch学习_基础知识.md","summary":"pytorch学习_基础知识","keywords":"","body":"基本操作创建操作张量创建函数数据类型转换函数初始化填充函数特定区间填充函数其他辅助函数量化函数复数和其他特殊类型函数索引|切片|连接|换位索引和切片合并和拼接变换和重塑元素添加与替换搜索和条件操作扩展与重复操作随机抽样随机种子随机采样函数序列化并行化梯度管理数学操作基础操作三角函数位操作其他操作归约操作比较操作其它操作BLAS和LAPACK操作 PyTorch 是一个开源的机器学习库，广泛应用于计算机视觉和自然语言处理等人工智能领域。由Facebook的人工智能研究团队开发，它基于之前的Torch库。PyTorch以其高度灵活和动态的计算图特性，在科研领域尤其受到青睐。下面是对PyTorch基础知识的一些介绍： 核心特性 动态计算图：PyTorch 使用动态计算图（也称为Define-by-Run方法），这意味着计算图的构建是即时的，并且可以根据运行中的数据进行改变。这为复杂的动态输入和不同长度的输出提供了便利 简洁的接口：PyTorch 提供了简洁直观的API，便于快速实现和调试模型，使得研究人员可以将更多时间投入到实验设计而非代码调试上 Python优先：PyTorch 设计为符合Python语言习惯，并且可以无缝集成到Python生态中，与NumPy等库协同工作 基础组件 张量（Tensors）：张量是PyTorch中的基础数据结构，它类似于NumPy的ndarrays，但它也可以在GPU上运行以加速计算 自动微分（Autograd）：PyTorch 的 autograd 模块提供了自动计算梯度的功能，对于实现神经网络中的反向传播算法至关重要 神经网络（torch.nn）：torch.nn 模块包含了构建神经网络所需的所有元素。这些可重用的层（例如卷积层、线性层等）和损失函数可以帮助用户轻松构建复杂的网络结构 优化（torch.optim）：PyTorch 提供了常用的优化算法，如SGD、Adam等，用于网络参数的迭代优化 数据加载（torch.utils.data）：PyTorch 提供了数据加载和处理工具，方便用户创建数据加载管道，加速数据预处理和模型训练过程 序列化工具（Serialization）：PyTorch 模型和张量可以通过 torch.save 轻松地序列化到磁盘，并通过 torch.load 进行反序列化 CUDA集成 PyTorch 提供了与NVIDIA CUDA的深度集成，允许张量和模型被无缝地在GPU上运行，大幅提升了计算速度 社区和生态 PyTorch 拥有活跃的社区，提供了大量预训练模型和开箱即用的工具。同时，它也是一些高级API（如FastAI）和框架（如Hugging Face的Transformers）的基础 PyTorch 不仅适合于研究原型的开发，还能用于生产环境的部署 它提供了一系列工具来支持模型的量化、蒸馏和优化，使其在不牺牲性能的情况下运行更快、占用资源更少。随着其持续发展和完善，PyTorch 已经成为了机器学习研究者和开发者的首选工具之一 基本操作 torch — PyTorch 2.2 documentation pytorch中文文档 以下是一些常用的方法 torch.is_tensor: 如果obj是一个pytorch张量，则返回True x=torch.tensor([1,2,3]) torch.is_tensor(x) Out[0]: True torch.is_storage: 如何obj是一个pytorch storage对象，则返回True x=torch.tensor([1,2,3]) torch.is_storage(x) Out[0]: False torch.numel: 返回input 张量中的元素个数 a = torch.randn(1,2,3,4,5) torch.numel(a) Out[0]: 120 a = torch.zeros(4,4) torch.numel(a) Out[1]: 16 torch.set_printoptions: 设置打印选项 参数: precision – 浮点数输出的精度位数 (默认为8 ) threshold – 阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000） edgeitems – 汇总显示中，每维（轴）两端显示的项数（默认值为3） linewidth – 用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter. profile – pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full) 创建操作 张量创建函数 torch.tensor(): 通过复制数据创建一个具有自动求导历史的张量(如果数据是一个张量) >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]) tensor([[ 0.1000, 1.2000], [ 2.2000, 3.1000], [ 4.9000, 5.2000]]) >>> torch.tensor([0, 1]) # Type inference on data tensor([ 0, 1]) >>> torch.tensor([[0.11111, 0.222222, 0.3333333]], ... dtype=torch.float64, ... device=torch.device('cuda:0')) # creates a double tensor on a CUDA device tensor([[ 0.1111, 0.2222, 0.3333]], dtype=torch.float64, device='cuda:0') >>> torch.tensor(3.14159) # Create a zero-dimensional (scalar) tensor tensor(3.1416) >>> torch.tensor([]) # Create an empty tensor (of size (0,)) tensor([]) torch.sparse_coo_tensor(): 通过坐标格式的索引和值构建稀疏张量 使用稀疏矩阵的一个主要优点是，在存储和计算上更加高效，特别是对于非常大的数据集。例如，在矩阵乘法或其他线性代数运算中，利用稀疏性可以显著减少不必要的乘法和加法计算，因为零元素与任何数相乘都是零，并且不会影响加法运算的结果 # 假设我们有2个非零元素分别在(0, 2)和(1, 0)的位置 indices = torch.tensor([[0, 1], [2, 0]]) # 表示非零元素的坐标 values = torch.tensor([3, 4]) # 这些非零元素的值 # 创建COO格式的稀疏张量 sparse_coo = torch.sparse_coo_tensor(indices, values, (2, 3)) print(sparse_coo) tensor(indices=tensor([[0, 1], [2, 0]]), values=tensor([3, 4]), size=(2, 3), nnz=2, layout=torch.sparse_coo) torch.sparse_csr_tensor(): 通过压缩稀疏行格式的索引和值构建稀疏张量 torch.sparse_csc_tensor(): 通过压缩稀疏列格式的索引和值构建稀疏张量 # 定义CSR格式的三个组件：行索引、列索引和值 crow_indices = torch.tensor([0, 1, 2]) col_indices = torch.tensor([0, 1]) values = torch.tensor([1, 2]) # 创建CSR格式的稀疏张量 sparse_csr = torch.sparse_csr_tensor(crow_indices, col_indices, values) print(sparse_csr) tensor(crow_indices=tensor([0, 1, 2]), col_indices=tensor([0, 1]), values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr) torch.sparse_bsr_tensor(): 通过块压缩稀疏行格式的索引和2维块构建稀疏张量 torch.sparse_bsc_tensor(): 通过块压缩稀疏列格式的索引和2维块构建稀疏张量 >>> crow_indices = [0, 1, 2] >>> col_indices = [0, 1] >>> values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] >>> torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64), ... torch.tensor(col_indices, dtype=torch.int64), ... torch.tensor(values), dtype=torch.double) tensor(crow_indices=tensor([0, 1, 2]), col_indices=tensor([0, 1]), values=tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_bsr) 数据类型转换函数 torch.asarray(): 将对象转换为张量 torch.as_tensor(): 将数据转换为张量，共享数据并尽可能保留自动求导历史 torch.as_strided(): 创建一个具有指定大小、步长和存储偏移的现有张量的视图(不好理解) # 将对象转换为张量 >>> a = torch.tensor([1, 2, 3]) >>> # Shares memory with tensor 'a' >>> b = torch.asarray(a) >>> a.data_ptr() == b.data_ptr() True >>> # Forces memory copy >>> c = torch.asarray(a, copy=True) >>> a.data_ptr() == c.data_ptr() False # 将数据转换为张量，共享数据并尽可能保留自动求导历史 >>> a = numpy.array([1, 2, 3]) >>> t = torch.as_tensor(a) >>> t tensor([ 1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) >>> a = numpy.array([1, 2, 3]) >>> t = torch.as_tensor(a, device=torch.device('cuda')) >>> t tensor([ 1, 2, 3]) >>> t[0] = -1 >>> a array([1, 2, 3]) # 创建一个具有指定大小、步长和存储偏移的现有张量的视图 >>> x = torch.randn(3, 3) >>> x tensor([[ 0.9039, 0.6291, 1.0795], [ 0.1586, 2.1939, -0.4900], [-0.1909, -0.7503, 1.9355]]) >>> t = torch.as_strided(x, (2, 2), (1, 2)) >>> t tensor([[0.9039, 1.0795], [0.6291, 0.1586]]) >>> t = torch.as_strided(input=x, size=(2, 2), stride=(1, 2), storage_offset=1) tensor([[0.6291, 0.1586], [1.0795, 2.1939]]) torch.from_file(): 从内存映射文件创建CPU张量 torch.from_numpy(): 将numpy数组转换为张量 torch.from_dlpack(): 将来自外部库的张量转换为PyTorch张量 # 从内存映射文件创建CPU张量 >>> t = torch.randn(2, 5, dtype=torch.float64) >>> t.numpy().tofile('storage.pt') >>> t_mapped = torch.from_file('storage.pt', shared=False, size=10, dtype=torch.float64) # 将numpy数组转换为张量 >>> a = numpy.array([1, 2, 3]) >>> t = torch.from_numpy(a) >>> t tensor([ 1, 2, 3]) >>> t[0] = -1 >>> a array([-1, 2, 3]) 初始化填充函数 torch.zeros(): 返回一个指定形状且用0填充的张量 torch.zeros_like(): 返回一个与给定张量形状相同且用0填充的张量 >>> torch.zeros(2, 3) tensor([[ 0., 0., 0.], [ 0., 0., 0.]]) >>> torch.zeros(5) tensor([ 0., 0., 0., 0., 0.]) >>> input = torch.empty(2, 3) >>> torch.zeros_like(input) tensor([[ 0., 0., 0.], [ 0., 0., 0.]]) torch.ones(): 返回一个指定形状且用1填充的张量 torch.ones_like(): 返回一个与给定张量形状相同且用1填充的张量 >>> torch.ones(2, 3) tensor([[ 1., 1., 1.], [ 1., 1., 1.]]) >>> torch.ones(5) tensor([ 1., 1., 1., 1., 1.]) >>> input = torch.empty(2, 3) >>> torch.ones_like(input) tensor([[ 1., 1., 1.], [ 1., 1., 1.]]) torch.arange(): 返回一个从start到end（不包含end）且步长为step的1维张量 torch.range(): (未来版本弃用)返回一个从start到end（包含end）且步长为step的1维张量 >>> torch.arange(5) tensor([ 0, 1, 2, 3, 4]) >>> torch.arange(1, 4) tensor([ 1, 2, 3]) >>> torch.arange(1, 2.5, 0.5) tensor([ 1.0000, 1.5000, 2.0000]) 特定区间填充函数 torch.linspace(): 返回一个从start到end（包括end）且在其中均匀分布的指定大小的1维张量 (start, start+\\frac{end-start}{steps-1}, \\dots, start+(steps-2)* \\frac{end-start}{steps-1}, end) torch.logspace(): 返回一个在对数刻度上从base^{start}到base^{end}（包括end）且均匀分布的指定大小的1维张量 (base^{start}, base^{(start+ \\frac{end-start}{steps-1})}, \\dots, base^{(start+(steps-2)*\\frac{end-start}{steps-1})}, base^{end}) 这两个函数生成的张量常常用于数据预处理、数学模拟、绘图等需要生成规则数列的场景 >>> torch.linspace(start=3, end=10, steps=5) tensor([ 3.0000, 4.7500, 6.5000, 8.2500, 10.0000]) >>> torch.linspace(-10, 10, steps=5) tensor([-10., -5., 0., 5., 10.]) >>> torch.linspace(start=-10, end=10, steps=5) tensor([-10., -5., 0., 5., 10.]) >>> torch.linspace(start=-10, end=10, steps=1) tensor([-10.]) >>> torch.logspace(start=-10, end=10, steps=5) tensor([ 1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) >>> torch.logspace(start=0.1, end=1.0, steps=5) tensor([ 1.2589, 2.1135, 3.5481, 5.9566, 10.0000]) >>> torch.logspace(start=0.1, end=1.0, steps=1) tensor([1.2589]) >>> torch.logspace(start=2, end=2, steps=1, base=2) tensor([4.0]) 其他辅助函数 torch.eye(): 返回一个二维张量，对角线上为1，其他地方为0 >>> torch.eye(3) tensor([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) >>> torch.eye(n=3, m=2) tensor([[1., 0.], [0., 1.], [0., 0.]]) torch.empty(): 返回一个指定形状且未初始化的张量 torch.empty_like(): 返回一个与给定张量形状相同且未初始化的张量 torch.empty_strided(): 创建一个具有指定大小和跨度且未初始化的张量 >>> torch.empty((2,3), dtype=torch.int64) tensor([[ 9.4064e+13, 2.8000e+01, 9.3493e+13], [ 7.5751e+18, 7.1428e+18, 7.5955e+18]]) >>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda') >>> torch.empty_like(a) tensor([[0, 0, 0], [0, 0, 0]], device='cuda:0', dtype=torch.int32) >>> a = torch.empty_strided((2, 3), (1, 2)) >>> a tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07], [0.0000e+00, 0.0000e+00, 3.0705e-41]]) >>> a.stride() (1, 2) >>> a.size() torch.Size([2, 3]) torch.full(): 返回一个指定形状且用给定值填充的张量 torch.full_like(): 返回一个与给定张量形状相同且用给定值填充的张量 >>> torch.full((2, 3), fill_value=3.141592) tensor([[ 3.1416, 3.1416, 3.1416], [ 3.1416, 3.1416, 3.1416]]) >>> torch.full_like(torch.empty((2,3)), fill_value=9) tensor([[9., 9., 9.], [9., 9., 9.]]) 量化函数 torch.quantize_per_tensor(): 将浮点张量转换为给定比例和零点的量化张量 torch.quantize_per_channel(): 将浮点张量转换为按通道给定比例和零点的量化张量 torch.dequantize(): 通过去量化量化张量来返回一个fp32张量 复数和其他特殊类型函数 torch.complex(): 构造一个其实部等于real、虚部等于imag的复数张量 >>> real = torch.tensor([1, 2], dtype=torch.float32) >>> imag = torch.tensor([3, 4], dtype=torch.float32) >>> z = torch.complex(real, imag) >>> z tensor([(1.+3.j), (2.+4.j)]) >>> z.dtype torch.complex64 torch.polar(): 根据极坐标的绝对值abs和角度angle构造复数张量的笛卡尔坐标 >>> import numpy as np >>> abs = torch.tensor([1, 2], dtype=torch.float64) >>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64) >>> z = torch.polar(abs, angle) >>> z tensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128) torch.heaviside(): 计算输入张量每个元素的Heaviside阶跃函数 heaviside(input, values) = \\begin{cases} 0, & \\text{if } input \\lt 0 \\\\ values, & \\text{if } input == 0 \\\\ \\text{1}, & \\text{if } input \\gt 0 \\\\ \\end{cases} >>> input = torch.tensor([-1.5, 0, 2.0]) >>> values = torch.tensor([0.5]) >>> torch.heaviside(input, values) tensor([0.0000, 0.5000, 1.0000]) >>> values = torch.tensor([1.2, -2.0, 3.5]) >>> torch.heaviside(input, values) tensor([0., -2., 1.]) 这些函数在数据预处理、模型初始化和其他计算任务中非常有用。通过这些函数，你可以创建大小、形状、种类各异的张量来满足不同的需求 索引|切片|连接|换位 这部分主要分为索引和切片、合并和拼接、变换和重塑、元素添加与替换、搜索和条件操作、扩展与重复操作 索引和切片 argwhere: 返回非零元素的索引 nonzero: 返回非零元素的索引 argwhere 和 nonzero 函数都用于查找非零元素的索引，但它们返回的格式略有不同。在某些编程库中，argwhere 通常返回一个二维数组，其中每一行都是输入中非零元素的索引坐标；而 nonzero 返回的是一个元组，每个元素是一个一维数组，表示非零元素在各个维度上的位置 # argwhere >>> t = torch.tensor([1, 0, 1]) >>> torch.argwhere(t) tensor([[0], [2]]) >>> t = torch.tensor([[1, 0, 1], [0, 1, 1]]) >>> torch.argwhere(t) tensor([[0, 0], [0, 2], [1, 1], [1, 2]]) # nonzero >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) tensor([[ 0], [ 1], [ 2], [ 4]]) >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]])) tensor([[ 0, 0], [ 1, 1], [ 2, 2], [ 3, 3]]) >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True) (tensor([0, 1, 2, 4]),) >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0], ... [0.0, 0.4, 0.0, 0.0], ... [0.0, 0.0, 1.2, 0.0], ... [0.0, 0.0, 0.0,-0.4]]), as_tuple=True) (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])) >>> torch.nonzero(torch.tensor(5), as_tuple=True) (tensor([0]),) select: 在特定维度进行索引 index_select: 根据索引选择数据 input (Tensor) – the input tensor. dim (int) – the dimension in which we index index (IntTensor or LongTensor) – the 1-D tensor containing the indices to index masked_select: 根据布尔掩码选择数据 # 在特定维度进行索引 tensor = [[1, 2], [3, 4], [5, 6]] selected_row = select(tensor, dim=0, index=1) print(selected_row) # 输出: [3, 4] # 根据索引选择数据 >>> x = torch.randn(3, 4) >>> x tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-0.4664, 0.2647, -0.1228, -1.1068], [-1.1734, -0.6571, 0.7230, -0.6004]]) >>> indices = torch.tensor([0, 2]) >>> torch.index_select(x, 0, indices) tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-1.1734, -0.6571, 0.7230, -0.6004]]) >>> torch.index_select(x, 1, indices) tensor([[ 0.1427, -0.5414], [-0.4664, -0.1228], [-1.1734, 0.7230]]) # 根据布尔掩码选择数据 >>> x = torch.randn(3, 4) >>> x tensor([[ 0.3552, -2.3825, -0.8297, 0.3477], [-1.2035, 1.2252, 0.5002, 0.6248], [ 0.1307, -2.0608, 0.1244, 2.0139]]) >>> mask = x.ge(0.5) >>> mask tensor([[False, False, False, False], [False, True, True, True], [False, False, False, True]]) >>> torch.masked_select(x, mask) tensor([ 1.2252, 0.5002, 0.6248, 2.0139]) narrow: 缩小张量的一个维度 input (Tensor) – the tensor to narrow dim (int) – the dimension along which to narrow start (int or Tensor) – index of the element to start the narrowed dimension from. Can be negative, which means indexing from the end of dim. If Tensor, it must be an 0-dim integral Tensor (bools not allowed) length (int) – length of the narrowed dimension, must be weakly positive >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> torch.narrow(x, 0, 0, 2) tensor([[ 1, 2, 3], [ 4, 5, 6]]) >>> torch.narrow(x, 1, 1, 2) tensor([[ 2, 3], [ 5, 6], [ 8, 9]]) >>> torch.narrow(x, -1, torch.tensor(-1), 1) tensor([[3], [6], [9]]) narrow_copy: narrow操作的复制版本 take: 根据索引从输入张量中取元素 take_along_dim: 沿指定维度根据索引取元素 # 根据索引从输入张量中取元素 >>> src = torch.tensor([[4, 3, 5], ... [6, 7, 8]]) >>> torch.take(src, torch.tensor([0, 2, 5])) tensor([ 4, 5, 8]) # 沿指定维度根据索引取元素 >>> t = torch.tensor([[10, 30, 20], [60, 40, 50]]) >>> max_idx = torch.argmax(t) >>> torch.take_along_dim(t, max_idx) tensor([60]) >>> sorted_idx = torch.argsort(t, dim=1) >>> torch.take_along_dim(t, sorted_idx, dim=1) tensor([[10, 20, 30], [40, 50, 60]]) unbind: 按维度解绑张量 >>> torch.unbind(torch.tensor([[1, 2, 3], >>> [4, 5, 6], >>> [7, 8, 9]])) (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9])) unravel_index: 将平面索引转换为坐标索引 >>> import torch >>> torch.unravel_index(torch.tensor(4), (3, 2)) (tensor(2), tensor(0)) >>> torch.unravel_index(torch.tensor([4, 1]), (3, 2)) (tensor([2, 0]), tensor([0, 1])) >>> torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2)) (tensor([0, 0, 1, 1, 2, 2]), tensor([0, 1, 0, 1, 0, 1])) >>> torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10)) (tensor([1, 5]), tensor([2, 6]), tensor([3, 7]), tensor([4, 8])) >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10)) (tensor([[1], [5]]), tensor([[2], [6]]), tensor([[3], [7]]), tensor([[4], [8]])) >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100)) (tensor([[12], [56]]), tensor([[34], [78]])) squeeze: 去除大小为1的维度 unsqueeze: 在指定位置添加大小为1的维度 # 压缩维度 >>> x = torch.zeros(2, 1, 2, 1, 2) >>> x.size() torch.Size([2, 1, 2, 1, 2]) >>> y = torch.squeeze(x) >>> y.size() torch.Size([2, 2, 2]) >>> y = torch.squeeze(x, 0) >>> y.size() torch.Size([2, 1, 2, 1, 2]) >>> y = torch.squeeze(x, 1) >>> y.size() torch.Size([2, 2, 1, 2]) >>> y = torch.squeeze(x, (1, 2, 3)) torch.Size([2, 2, 2]) # 增加维度 >>> x = torch.tensor([1, 2, 3, 4]) >>> torch.unsqueeze(x, 0) tensor([[ 1, 2, 3, 4]]) >>> torch.unsqueeze(x, 1) tensor([[ 1], [ 2], [ 3], [ 4]]) 合并和拼接 numpy中的hstack()、vstack()、stack()、concatenate()函数详解 cat, concat, concatenate: 将序列的张量在指定维度连接(concat和concatenate是cat的别名) >>> x = torch.randn(2, 3) >>> x tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]]) >>> torch.cat((x, x, x), 0) tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]]) >>> torch.cat((x, x, x), 1) tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]]) chunk: 把张量分成指定数量的块 >>> torch.arange(11).chunk(6) (tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8, 9]), tensor([10])) # 创建一个张量 x = torch.tensor([1, 2, 3, 4, 5]) # 将这个张量分割成3个块 chunks = torch.chunk(x, chunks=3, dim=0) # 输出分割后的块 for i, chunk in enumerate(chunks): print(f\"Chunk {i}: {chunk}\") Chunk 0: tensor([1, 2]) Chunk 1: tensor([3, 4]) Chunk 2: tensor([5]) column_stack: 按列堆叠张量创建新张量 >>> a = torch.tensor([1, 2, 3]) >>> b = torch.tensor([4, 5, 6]) >>> torch.column_stack((a, b)) tensor([[1, 4], [2, 5], [3, 6]]) >>> a = torch.arange(5) >>> b = torch.arange(10).reshape(5, 2) >>> torch.column_stack((a, b, b)) tensor([[0, 0, 1, 0, 1], [1, 2, 3, 2, 3], [2, 4, 5, 4, 5], [3, 6, 7, 6, 7], [4, 8, 9, 8, 9]]) hstack: 水平方向堆叠张量 vstack(别名row_stack): 垂直方向堆叠张量 dstack: 深度方向堆叠张量 # 水平方向堆叠张量 >>> a = torch.tensor([1, 2, 3]) >>> b = torch.tensor([4, 5, 6]) >>> torch.hstack((a,b)) tensor([1, 2, 3, 4, 5, 6]) >>> a = torch.tensor([[1],[2],[3]]) >>> b = torch.tensor([[4],[5],[6]]) >>> torch.hstack((a,b)) tensor([[1, 4], [2, 5], [3, 6]]) # 垂直方向堆叠张量 >>> a = torch.tensor([1, 2, 3]) >>> b = torch.tensor([4, 5, 6]) >>> torch.vstack((a,b)) tensor([[1, 2, 3], [4, 5, 6]]) >>> a = torch.tensor([[1],[2],[3]]) >>> b = torch.tensor([[4],[5],[6]]) >>> torch.vstack((a,b)) tensor([[1], [2], [3], [4], [5], [6]]) # 深度方向堆叠张量 >>> a = torch.tensor([1, 2, 3]) >>> b = torch.tensor([4, 5, 6]) >>> torch.dstack((a,b)) tensor([[[1, 4], [2, 5], [3, 6]]]) >>> a = torch.tensor([[1],[2],[3]]) >>> b = torch.tensor([[4],[5],[6]]) >>> torch.dstack((a,b)) tensor([[[1, 4]], [[2, 5]], [[3, 6]]]) torch.dstack 和 torch.column_stack 函数都是用于堆叠张量的函数，但它们在堆叠的细节上有所不同 A = torch.tensor([[1, 2, 3], [4, 5, 6]]) B = torch.tensor([[7, 8, 9], [10, 11, 12]]) A.shape Out[27]: torch.Size([2, 3]) # dstack是一整个对象堆叠 dstack_result = torch.dstack((A, B)) dstack_result Out[30]: tensor([[[ 1, 7], [ 2, 8], [ 3, 9]], [[ 4, 10], [ 5, 11], [ 6, 12]]]) dstack_result.shape Out[31]: torch.Size([2, 3, 2]) # column_stack专门用于二维张量（矩阵），它会将这些矩阵堆叠成一个更宽的矩阵（即增加列） column_stack_result = torch.column_stack((A, B)) column_stack_result Out[33]: tensor([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) column_stack_result.shape Out[34]: torch.Size([2, 6]) stack: 在新维度上连接张量序列 pytorch的hstack、vstack、dstack、column_stack以及stack函数之间的区别和联系 这些堆叠函数之间的联系在于它们的核心目的：将多个张量组合成一个新的、更大的张量 不同的函数根据堆叠的方向（尺寸或维度）和具体的操作细节来区分，下面是它们之间联系的一个概览： 维度方向的联系： hstack（水平堆叠）通常用于增加列数，适用于1D和2D张量，对于1D张量会先将其视作列向量 vstack（垂直堆叠）常用于增加行数，也适用于1D和2D张量，对于1D张量会先将其视作行向量 dstack（深度堆叠）是在第三个维度上进行堆叠，适用于创建或扩展为3D张量的情况 column_stack与hstack相似，但它是专门设计来处理1D张量，将它们作为列向量来堆叠成2D张量的；对于2D张量，它的行为与hstack相同 stack是一个更通用的函数，可以在指定的任何维度上进行堆叠，而不局限于特定的堆叠方向。它总是增加一个新的维度来堆叠张量 操作联系： 所有这些函数都是用来组合张量的，但是stack函数会创建一个新的维度，而其他函数（hstack, vstack, dstack, column_stack）则在现有的维度上进行操作 hstack, vstack, dstack, column_stack可以看作是stack的特例，它们在指定的一个特定的现有维度上进行操作（hstack在最后一个维度，vstack在第一个维度，dstack在第三个维度，column_stack针对1D张量在第二个新建维度，对2D张量在最后一个维度） 使用场景联系： 当你想要在特定的轴方向上组合数据，而不想增加新的维度时，你会选择使用hstack, vstack, dstack, 或 column_stack 当你需要在新的维度上堆叠张量时（例如，在时间序列数据或不同样本之间），你会选择使用stack 在实际使用中，选择哪一个函数取决于你的具体需求以及你要操作的张量的维度。这些函数提供了方便的方式来对数据进行重构和整合，这是在准备数据集、构建深度学习模型等场景中非常常见的需求 hsplit: 水平方向分割张量 vsplit: 垂直方向分割张量 dsplit: 深度方向分割张量 split: 分割张量成多个块，函数将张量分割成特定大小的块。你可以指定每个块的大小，或者传递一个包含每个块大小的列表。如果张量不能均匀分割，最后一块的大小将小于前面的块 tensor_split: 沿特定维度分割张量，基于索引来分割张量的。你可以指定一个分割点的索引列表，函数会在这些索引处分割张量。这些索引指的是分割后每个新张量的第一个元素的索引 >>> t = torch.arange(16.0).reshape(4,4) >>> t tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]) # 水平方向分割张量 >>> torch.hsplit(t, 2) (tensor([[ 0., 1.], [ 4., 5.], [ 8., 9.], [12., 13.]]), tensor([[ 2., 3.], [ 6., 7.], [10., 11.], [14., 15.]])) >>> torch.hsplit(t, [3, 6]) (tensor([[ 0., 1., 2.], [ 4., 5., 6.], [ 8., 9., 10.], [12., 13., 14.]]), tensor([[ 3.], [ 7.], [11.], [15.]]), tensor([], size=(4, 0))) # 垂直方向分割张量 >>> torch.vsplit(t, 2) (tensor([[0., 1., 2., 3.], [4., 5., 6., 7.]]), tensor([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])) >>> torch.vsplit(t, [3, 6]) (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([[12., 13., 14., 15.]]), tensor([], size=(0, 4))) # 深度方向分割张量 >>> t = torch.arange(16.0).reshape(2, 2, 4) >>> t tensor([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.]], [[ 8., 9., 10., 11.], [12., 13., 14., 15.]]]) >>> torch.dsplit(t, 2) (tensor([[[ 0., 1.], [ 4., 5.]], [[ 8., 9.], [12., 13.]]]), tensor([[[ 2., 3.], [ 6., 7.]], [[10., 11.], [14., 15.]]])) >>> torch.dsplit(t, [3, 6]) (tensor([[[ 0., 1., 2.], [ 4., 5., 6.]], [[ 8., 9., 10.], [12., 13., 14.]]]), tensor([[[ 3.], [ 7.]], [[11.], [15.]]]), tensor([], size=(2, 2, 0))) # 分割张量成多个块 >>> torch.split(t, 2, dim=0) (tensor([[0., 1., 2., 3.], [4., 5., 6., 7.]]), tensor([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])) >>> torch.split(t, [1, 3], dim=0) (tensor([[0., 1., 2., 3.]]), tensor([[ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]])) # 沿特定维度分割张量 >>> torch.tensor_split(t, [1, 2], dim=0) (tensor([[0., 1., 2., 3.]]), tensor([[4., 5., 6., 7.]]), tensor([[ 8., 9., 10., 11.], [12., 13., 14., 15.]])) 变换和重塑 adjoint: 返回共轭的张量，并交换最后两维 conj: 返回共轭位翻转的张量视图 gather: 沿指定维度聚集值 torch.gather(input, dim, index, *, sparse_grad=False) -> Tensor 其中参数的意义如下： input 是要从中提取数据的张量 dim 是要沿着哪个维度进行提取 index 是与 input 张量在除了 dim 指定的维度外具有相同大小的张量，包含了要提取的元素的索引 sparse_grad 是布尔值，用于指示是否进行稀疏梯度的计算；通常用于高级用途 # 创建一个 3x3 的矩阵 input_tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # 创建一个索引，用于选择每一行的第二个元素 index = torch.tensor([[1], [1], [1]]) # 使用 gather 来提取元素，dim=1 表示沿着列的方向进行操作 torch.gather(input_tensor, 1, index) # 输出：[[2], [5], [8]] >>> t = torch.tensor([[1, 2], [3, 4]]) >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]])) tensor([[ 1, 1], [ 4, 3]]) movedim(别名moveaxis): 移动张量维度位置的函数，这个操作可以让你指定某个维度（或多个维度）从它的原始位置移动到一个新的位置 ```python t = torch.randn(3,2,1) t tensor([[[-0.3362], [-0.8437]], [[-0.9627], [ 0.1727]], [[ 0.5173], [-0.1398]]]) torch.moveaxis(t, 1, 0).shape torch.Size([2, 3, 1]) torch.moveaxis(t, 1, 0) tensor([[[-0.3362], [-0.9627], [ 0.5173]], [[-0.8437], [ 0.1727], [-0.1398]]]) torch.moveaxis(t, (1, 2), (0, 1)).shape torch.Size([2, 1, 3]) torch.moveaxis(t, (1, 2), (0, 1)) tensor([[[-0.3362, -0.9627, 0.5173]], [[-0.8437, 0.1727, -0.1398]]]) tensor = torch.randn(10, 3, 5) torch.movedim(tensor, 1, 0).size() Out[78]: torch.Size([3, 10, 5]) - `permute`: 重新排列张量的维度，重组tensor维度，支持高维操作，tensor.permute(dim0, dim1, … dimn)，表示原本的dim0放在第0维度，dim1放在第1维度，…, dimn放在第n维度，必须将所有维度写上 `reshape`: 改变张量的形状，需要指定最终的形状 `transpose`(等价于`swapaxes`、`swapdims`): 转置张量的维度 > [捋清pytorch的transpose、permute、view、reshape、contiguous](https://blog.csdn.net/nuohuang3371/article/details/113403755) permute可以完全替代transpose，transpose不能替代permute ```python # permute重新排列张量的维度 >>> x = torch.randn(2, 3, 5) >>> x.size() torch.Size([2, 3, 5]) >>> torch.permute(x, (2, 0, 1)).size() torch.Size([5, 2, 3]) # reshape改变张量的形状 >>> a = torch.arange(4.) >>> torch.reshape(a, (2, 2)) tensor([[ 0., 1.], [ 2., 3.]]) >>> b = torch.tensor([[0, 1], [2, 3]]) >>> torch.reshape(b, (-1,)) tensor([ 0, 1, 2, 3]) # transpose转置张量的维度 >>> x = torch.randn(2, 3) >>> x tensor([[ 1.0028, -0.9893, 0.5809], [-0.1669, 0.7299, 0.4942]]) >>> torch.transpose(x, dim0=0, dim1=1) tensor([[ 1.0028, -0.1669], [-0.9893, 0.7299], [ 0.5809, 0.4942]]) t: 转置二维张量的维度 期望输入是一个二维或二维以下的张量，并交换维度0和1 当输入是一个零维或一维张量时，返回的张量保持不变。当输入是一个二维张量时，这相当于 transpose(input, 0, 1) >>> x = torch.randn(()) >>> x tensor(0.1995) >>> torch.t(x) tensor(0.1995) >>> x = torch.randn(3) >>> x tensor([ 2.4320, -0.4608, 0.7702]) >>> torch.t(x) tensor([ 2.4320, -0.4608, 0.7702]) >>> x = torch.randn(2, 3) >>> x tensor([[ 0.4875, 0.9158, -0.5872], [ 0.3938, -0.6929, 0.6932]]) >>> torch.t(x) tensor([[ 0.4875, 0.3938], [ 0.9158, -0.6929], [-0.5872, 0.6932]]) 元素添加与替换 index_add: 根据索引向张量添加元素 index_copy: 根据索引复制元素到张量 index_reduce: 在指定维度上，根据索引减少元素 >>> x = torch.ones(5, 3) >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) >>> index = torch.tensor([0, 4, 2]) >>> x.index_add(0, index=index, source=t) tensor([[ 2., 3., 4.], [ 1., 1., 1.], [ 8., 9., 10.], [ 1., 1., 1.], [ 5., 6., 7.]]) >>> x.index_add(0, index, t, alpha=-1) tensor([[ 0., -1., -2.], [ 1., 1., 1.], [-6., -7., -8.], [ 1., 1., 1.], [-3., -4., -5.]]) scatter, scatter_add, scatter_reduce: 根据索引分散和添加元素 scatter(output, dim, index, src) catter函数就是把src数组中的数据重新分配到output数组当中，index数组中表示了要把src数组中的数据分配到output数组中的位置，若未指定，则填充0 >>> input = torch.randn(2, 4) >>> print(input) tensor([[ 1.4782, -1.1345, -1.1457, -0.6050], [-0.4183, -0.0229, 1.2361, -1.7747]]) >>> output = torch.zeros(2, 5) >>> index = torch.tensor([[3, 1, 2, 0], [1, 2, 0, 3]]) >>> output = output.scatter(dim=1, index=index, src=input) >>> print(output) tensor([[-0.6050, -1.1345, -1.1457, 1.4782, 0.0000], [ 1.2361, -0.4183, -0.0229, -1.7747, 0.0000]]) 一般scatter用于生成onehot向量，如下所示 >>> index = torch.tensor([[1], [2], [0], [3]]) >>> onehot = torch.zeros(4, 4) >>> onehot.scatter_(1, index, 1) >>> print(onehot) tensor([[0., 1., 0., 0.], [0., 0., 1., 0.], [1., 0., 0., 0.], [0., 0., 0., 1.]]) diagonal_scatter: 沿对角线分散元素 select_scatter: 在给定索引处分散元素 slice_scatter: 在给定维度上分散元素 搜索和条件操作 where: 根据条件从两个张量中选择元素 out_i = \\begin{cases} input_i, & \\text{if } condition_i \\\\ other_i, & \\text{otherwise} \\end{cases} >>> x = torch.randn(3, 2) >>> y = torch.ones(3, 2) >>> x tensor([[-0.4620, 0.3139], [ 0.3898, -0.7197], [ 0.0478, -0.1657]]) >>> torch.where(condition=x > 0, input=1.0, other=0.0) tensor([[0., 1.], [1., 0.], [1., 0.]]) >>> torch.where(condition=x > 0, input=x, other=y) tensor([[ 1.0000, 0.3139], [ 0.3898, 1.0000], [ 0.0478, 1.0000]]) >>> x = torch.randn(2, 2, dtype=torch.double) >>> x tensor([[ 1.0779, 0.0383], [-0.8785, -1.1089]], dtype=torch.float64) >>> torch.where(condition=x > 0, input=x, other=0.) tensor([[1.0779, 0.0383], [0.0000, 0.0000]], dtype=torch.float64) 扩展与重复操作 tile: 通过重复张量的元素来构建新张量 >>> x = torch.tensor([1, 2, 3]) >>> x.tile((2,)) tensor([1, 2, 3, 1, 2, 3]) >>> y = torch.tensor([[1, 2], [3, 4]]) >>> torch.tile(y, (2, 2)) tensor([[1, 2, 1, 2], [3, 4, 3, 4], [1, 2, 1, 2], [3, 4, 3, 4]]) 随机抽样 随机种子 torch.seed: 设置torch cpu随机数种子 torch.manual_seed: 设置torch cpu随机数种子，torch.manual_seed(seed) torch.cuda.manual_seed: 设置torch cuda随机数种子 torch.initial_seed: 查看设置的种子值 torch.seed() Out[112]: 2362131181677400 torch.initial_seed() Out[113]: 2362131181677400 torch.manual_seed(101) Out[114]: torch.initial_seed() Out[115]: 101 torch.cuda.manual_seed(0) torch.initial_seed() Out[116]: 0 get_rng_state(): 返回当前随机数生成器的状态。这个状态是一个torch.ByteTensor，它包含了RNG内部的所有状态信息，使得RNG可以在这个状态下继续生成随机数序列。这允许你在某个特定点“保存”RNG的状态，然后在需要的时候恢复到这个状态 set_rng_state(state): 设置随机数生成器的状态。state应该是通过get_rng_state()函数获取的状态张量。这个函数用于恢复RNG到一个特定的状态，这样可以从那个状态开始重新生成相同的随机数序列 随机采样函数 常见的概率分布参考 -- 兼一书虫-机器学习概率论(1) torch.rand(): 创建一个具有给定形状的张量，并用区间[0, 1)内的均匀分布的随机数填充 torch.rand_like(): 返回一个与给定张量形状相同的张量，并用区间[0, 1)内的均匀分布的随机数填充 >>> torch.rand(4) tensor([ 0.5204, 0.2503, 0.3525, 0.5673]) >>> torch.rand(2, 3) tensor([[ 0.8237, 0.5781, 0.6879], [ 0.3816, 0.7249, 0.0998]]) >>> torch.rand_like(torch.rand(2, 3)) tensor([[0.3885, 0.9888, 0.4838], [0.8154, 0.6068, 0.6895]]) torch.randn(): 返回一个具有给定形状的张量，并用标准正态分布的随机数填充 torch.randn_like(): 返回一个与给定张量形状相同的张量，并用标准正态分布的随机数填充 >>> torch.randn(4) tensor([-2.1436, 0.9966, 2.3426, -0.6366]) >>> torch.randn(2, 3) tensor([[ 1.5954, 2.8929, -1.0923], [ 1.1719, -0.4709, -0.1996]]) >>> torch.randn_like(torch.randn(2, 3)) tensor([[ 0.9979, 0.0471, -1.1305], [ 0.7216, -0.0747, 0.0610]]) torch.randint(): 返回一个具有给定形状的张量，并用区间[low, high)内的随机整数填充 torch.randint_like(): 返回一个与给定张量形状相同的张量，并用区间[low, high)内的随机整数填充 >>> torch.randint(low=3, high=5, size=(3,)) tensor([4, 3, 4]) >>> torch.randint(10, (2, 2)) tensor([[0, 2], [5, 5]]) >>> torch.randint(3, 10, (2, 2)) tensor([[4, 5], [6, 7]]) >>> torch.randint_like(input=torch.randint(low=3, high=5, size=(3,)), low=6, high=10) tensor([8, 9, 9]) torch.randperm(): 返回一个从0到给定参数n - 1的整数的随机排列 >>> torch.randperm(4) tensor([2, 1, 0, 3]) torch.bernoulli: 从伯努利分布中提取二进制随机数（0或1），输入张量应为包含用于绘制二进制随机数的概率的张量 因此，输入中的所有值都必须在以下范围内(0,1) >>> a = torch.empty(3, 3).uniform_(0, 1) # generate a uniform random matrix with range [0, 1] >>> a tensor([[ 0.1737, 0.0950, 0.3609], [ 0.7148, 0.0289, 0.2676], [ 0.9456, 0.8937, 0.7202]]) >>> torch.bernoulli(a) tensor([[ 1., 0., 0.], [ 0., 0., 0.], [ 1., 1., 1.]]) >>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1 >>> torch.bernoulli(a) tensor([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) >>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0 >>> torch.bernoulli(a) tensor([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) torch.poisson: 泊松分布用于计算一个事件在平均价值率(时间)的一定时间内发生的可能性。泊松分布是一个离散的概率分布 >>> rates = torch.rand(4, 4) * 5 # rate parameter between 0 and 5 >>> torch.poisson(rates) tensor([[9., 1., 3., 5.], [8., 6., 6., 0.], [0., 4., 5., 3.], [2., 1., 4., 2.]]) torch.multinomial: 对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标 input (Tensor) – 包含概率值的张量 num_samples (int) – 抽取的样本数 replacement (bool, optional) – 布尔值，决定是否能重复抽取 out (Tensor, optional) – 结果张量 weights = torch.Tensor([0, 10, 3, 0]) torch.multinomial(weights, 4) Out[0]: tensor([2, 1, 0, 3]) # replacement=True时 概率为0的没机会被取到 torch.multinomial(weights, 4, replacement=True) Out[1]: tensor([2, 1, 1, 1]) torch.normal: 返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数 torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) Out[0]: tensor([ 0.9732, 2.0833, 2.5282, 4.3588, 5.4837, 5.1150, 7.0366, 7.9774, 9.1679, 10.0248]) torch.normal(mean=0.5, std=torch.arange(1., 6.)) Out[1]: tensor([ 0.7067, 2.4856, -2.1957, -4.3114, 16.2506]) torch.normal(mean=torch.arange(1., 6.)) Out[2]: tensor([0.7835, 4.6096, 2.7244, 5.2810, 4.8413]) 序列化 torch.save: 保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True) obj – 保存对象 f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串 pickle_module – 用于pickling元数据和对象的模块 pickle_protocol – 指定pickle protocal 可以覆盖默认参数 x = torch.tensor([0, 1, 2, 3, 4]) # Save to file torch.save(x, 'tensor.pt') # Save to io.BytesIO buffer buffer = io.BytesIO() torch.save(x, buffer) torch.load: 从磁盘文件中读取一个通过torch.save()保存的对象 torch.load(f, map_location=None, pickle_module=pickle, , weights_only=False, pickle_load_args*) torch.load('tensors.pt', encoding='ascii') torch.load('tensors.pt', map_location=torch.device('cpu')) torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'}) # Load from io.BytesIO buffer with open('tensor.pt', 'rb') as f: buffer = io.BytesIO(f.read()) torch.load(buffer) 并行化 在PyTorch中，CPU操作可以通过多线程来并行化，以此提高性能。这里涉及到两种形式的并行化：intra-op并行和inter-op并行。下面是关于这些函数的解释： get_num_threads(): 这个函数返回用于在CPU上并行执行操作（即intra-op并行）的线程数。Intra-op并行是指单个操作（如矩阵乘法）内部的并行执行。PyTorch会尝试使用所有可用的CPU核心来加速这些操作 set_num_threads(int): 这个函数用来设置在CPU上进行intra-op并行操作时使用的线程数。如果你想限制PyTorch使用的CPU核心数量，可以调用这个函数 get_num_interop_threads(): 这个函数返回用于CPU上的inter-op并行的线程数。Inter-op并行是指不同操作之间的并行执行。例如，如果你有多个不依赖于彼此的操作，PyTorch可以同时执行它们以提高效率 set_num_interop_threads(int): 这个函数用来设置用于inter-op并行的线程数。通过设定线程数，可以控制同时进行的不同操作的数量 在多核CPU上，适当地设置这些值可以帮助你更好地利用系统资源，提高程序的运行效率。然而，如果设置的线程数太多，可能会导致线程竞争和上下文切换的开销，反而降低性能 通常默认设置是已经针对性能进行了优化，但是在特定的系统和应用场景下，手动调整这些值可以获得更佳的性能表现 梯度管理 在PyTorch中，梯度计算对于训练神经网络是必要的，因为它们用于优化模型的参数。然而，在某些情况下，比如在模型评估或应用阶段，你可能不需要计算梯度。梯度计算会占用额外的内存和计算资源，禁用它们可以提高效率。为了方便地开启和关闭梯度计算，PyTorch提供了几个上下文管理器： torch.no_grad(): with torch.no_grad(): # 在这个代码块中，所有的操作都不会跟踪梯度 predictions = model(inputs) 在这个例子中，model(inputs) 的执行不会计算梯度，这对于模型推断（inference）阶段非常有用，因为它减少了内存消耗并提高了计算速度 torch.enable_grad(): with torch.enable_grad(): # 在这个代码块中，梯度计算是启用的 predictions = model(inputs) loss = loss_fn(predictions, targets) loss.backward() 这里，即使全局梯度计算被禁用，torch.enable_grad() 仍可以在其作用域内启用梯度计算，以便计算损失函数的梯度 torch.set_grad_enabled(): torch.set_grad_enabled(mode=True) # 启用梯度计算 # 后续操作将会跟踪梯度 predictions = model(inputs) loss = loss_fn(predictions, targets) loss.backward() torch.set_grad_enabled(mode=False) # 禁用梯度计算 在这里，使用torch.set_grad_enabled()函数来全局地控制是否计算梯度。传递True或False可以分别开启或关闭梯度计算 torch.is_grad_enabled(): print(torch.is_grad_enabled()) # 打印当前是否启用了梯度计算 这个函数用来检查当前是否启用了梯度计算 torch.inference_mode(): with torch.inference_mode(): # 在这个代码块中，所有的操作都不会跟踪梯度，且某些优化会被应用以加速推断 predictions = model(inputs) torch.inference_mode() 更适合用在推断阶段，相比torch.no_grad()，它会启用额外的优化，比如禁用自动求导引擎和解除对操作immutable的限制，从而实现更高效的模型推断 torch.is_inference_mode_enabled(): with torch.inference_mode(): print(torch.is_inference_mode_enabled()) # 在 inference mode 中，这将输出 True print(torch.is_inference_mode_enabled()) # 在 inference mode 外部，这将输出 False 这个函数用来检查当前是否启用了推断模式。在torch.inference_mode()上下文管理器的内部，它会返回True 每个上下文管理器和函数都有其用途，根据需要进行梯度计算的控制，可以优化您的PyTorch 程序的性能 数学操作 torch — PyTorch 2.2 documentation-数学操作 基础操作 torch.add: 对输入张量input逐元素加上标量值value，并返回结果到一个新的张量 torch.addcdiv: 用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor，张量的形状不需要匹配，但元素数量必须一致 tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加 value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘 tensor1 (Tensor) – 张量，作为被除数(分子) tensor2 (Tensor) –张量，作为除数(分母) out (Tensor, optional) – 输出张量 out _{i}=\\operatorname{input}_{i}+ value \\times \\frac{\\text { tensor } 1_{i}}{\\text { tensor 2}_{i}} torch.addcmul: 用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加 value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘 tensor1 (Tensor) – 张量，作为乘子1 tensor2 (Tensor) –张量，作为乘子2 out (Tensor, optional) – 输出张量 out _{i}= input _{i}+ value \\times tensor 1_{i} \\times tensor 2_{i} 以上两个可以用于正则化操作 # 对输入张量`input`逐元素加上标量值`value`，并返回结果到一个新的张量 a = torch.randn(4) Out[0]: tensor([ 0.3510, -0.2226, -0.7971, -0.2564]) torch.add(a, 20) Out[1]: tensor([20.3510, 19.7774, 19.2029, 19.7436]) # 用`tensor2`对`tensor1`逐元素相除，然后乘以标量值`value` 并加到`tensor` t = torch.randn(1, 3) t1 = torch.randn(3, 1) t2 = torch.randn(1, 3) t, t1, t2 Out[0]: (tensor([[-1.2863, 1.1267, -1.7120]]), tensor([[-0.4294], [-0.5328], [-0.5373]]), tensor([[-0.0876, 0.4398, 1.3583]])) torch.addcdiv(t, t1, t2, value=0.1) Out[1]: tensor([[-0.7958, 1.0291, -1.7436], [-0.6778, 1.0056, -1.7512], [-0.6727, 1.0046, -1.7515]]) # 用`tensor2`对`tensor1`逐元素相乘，并对结果乘以标量值`value`然后加到`tensor` t = torch.randn(1, 3) t1 = torch.randn(3, 1) t2 = torch.randn(1, 3) t, t1, t2 Out[0]: (tensor([[-1.2863, 1.1267, -1.7120]]), tensor([[-0.4294], [-0.5328], [-0.5373]]), tensor([[-0.0876, 0.4398, 1.3583]])) torch.addcmul(t, t1, t2, value=0.1) Out[1]: tensor([[-1.2825, 1.1078, -1.7703], [-1.2816, 1.1033, -1.7844], [-1.2816, 1.1031, -1.7850]]) torch.ceil: 对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数 torch.clamp(别名torch.clip): 将输入input张量每个元素的夹紧到区间[min, max]，并返回结果到一个新张量 y_i = \\begin{cases} \\text{min}, & \\text{if } x_i \\text{max} \\end{cases} torch.floor: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数 # torch.ceil a = torch.randn(4) Out[0]: tensor([-0.9105, -0.7277, 0.9516, -0.1081]) torch.ceil(a) Out[1]: tensor([-0., -0., 1., -0.]) # torch.floor a = torch.randn(4) Out[0]: tensor([-0.5661, -0.9135, 1.1018, -0.2633]) torch.floor(a) Out[1]: tensor([-1., -1., 1., -1.]) # torch.clamp a = torch.randn(4) Out[0]: tensor([-0.9105, -0.7277, 0.9516, -0.1081]) torch.clamp(a, min=-0.5, max=0.5) Out[1]: tensor([-0.5000, -0.5000, 0.5000, -0.1081]) torch.div(别名torch.divide): 将input逐元素除以标量值value，并返回结果到输出张量out，torch.div(input, value, out=None) 两张量input和other逐元素相除，并将结果返回到输出，torch.div(input, other, **, rounding_mode=None, out=None*) → Tensor torch.mul(别米torch.multiply): 用标量值value乘以输入input的每个元素，并返回一个新的结果张量，torch.mul(input, value, out=None) 两个张量input,other按元素进行相乘，并返回到输出张量，torch.mul(input, other, out=None) # 元素除 a = torch.randn(4) Out[0]: tensor([-0.9105, -0.7277, 0.9516, -0.1081]) torch.div(a, 0.5) Out[1]: tensor([-1.8210, -1.4554, 1.9032, -0.2162]) a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917], [ 0.1815, -1.0111, 0.9805, -1.5923]]) b = torch.tensor([ 0.8032, 0.2930, -0.8113, -0.2308]) torch.div(a, b, rounding_mode='trunc') Out[2]: tensor([[-0., -6., 0., 1.], [ 0., -3., -1., 6.]]) torch.div(a, b, rounding_mode='floor') Out[3]: tensor([[-1., -7., 0., 1.], [ 0., -4., -2., 6.]]) # 元素乘 a = torch.randn(3) Out[0]: tensor([ 0.0603, -0.5258, -0.3810]) b = torch.randn(3) Out[1]: tensor([ 1.2408, -1.3506, 0.9296]) torch.mul(a, 100) Out[2]: tensor([ 6.0299, -52.5785, -38.0989]) torch.mul(a, b) Out[3]: tensor([ 0.0748, 0.7101, -0.3542]) torch.exp: 返回一个新张量，包含输入input张量每个元素的指数 torch.frac: 返回每个元素的分数部分 torch.log: 计算input 的自然对数 torch.log1p: 计算input + 1的自然对数y_i = log(x_i+1)，对值比较小的输入，此函数比torch.log()更准确 torch.neg(别名torch.negative): 返回一个新张量，包含输入input 张量按元素取负 torch.pow: 对输入input的按元素求exponent次幂值，并返回结果张量，幂值exponent 可以为单一 float 数或者与input相同元素数的张量 torch.round: (四舍五入)返回一个新张量，将输入input张量每个元素舍入到最近的整数 torch.trunc: (四舍五入(去尾法))返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数)，简而言之，有符号数的小数部分被舍弃 torch.rsqrt: 返回一个新张量，包含输入input张量每个元素的平方根倒数 torch.fmod: 计算逐元素余数，保留正负号 torch.remainder: 计算逐元素余数， 相当于python中的%操作符，不保留正负号 torch.reciprocal: 返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/x torch.sqrt: 返回一个新张量，包含输入input张量每个元素的平方根 torch.abs(别名为torch.absolute): 计算输入张量的每个元素绝对值 # 返回一个新张量，包含输入`input`张量每个元素的指数 torch.exp(torch.Tensor([0, math.log(2)])) Out[0]: tensor([1., 2.]) # 返回每个元素的分数部分 torch.frac(torch.Tensor([1, 2.5, -3.2])) Out[0]: tensor([ 0.0000, 0.5000, -0.2000]) # 计算`input` 的自然对数 a = torch.randn(5) Out[0]: tensor([-0.3466, 2.3803, -0.0423, -0.9744, 0.4976]) torch.log(a) Out[1]: tensor([ nan, 0.8672, nan, nan, -0.6980]) # 计算input + 1的自然对数 torch.log1p(a) Out[2]: tensor([-0.4256, 1.2180, -0.0432, -3.6633, 0.4039]) # 返回一个新张量，包含输入`input` 张量按元素取负 a = torch.randn(3) Out[0]: tensor([ 0.0603, -0.5258, -0.3810]) torch.neg(a) Out[1]: tensor([-0.0603, 0.5258, 0.3810]) # 求指数 a = torch.arange(1, 5) Out[0]: tensor([1, 2, 3, 4]) exp = torch.arange(1, 5) Out[1]: tensor([1, 2, 3, 4]) torch.pow(a, 2) Out[2]: tensor([ 1, 4, 9, 16]) torch.pow(a, exp) Out[3]: tensor([ 1, 4, 27, 256]) torch.pow(2, exp) Out[4]: tensor([ 2, 4, 8, 16]) # 四舍五入 a = torch.randn(4) Out[0]: tensor([ 0.7995, -2.0975, 0.7273, 0.7539]) torch.round(a) Out[1]: tensor([ 1., -2., 1., 1.]) # 四舍五入(去尾法) a = torch.randn(4) Out[0]: tensor([-2.1647, -0.2294, 0.4943, 1.5146]) torch.trunc(a) Out[1]: tensor([-2., -0., 0., 1.]) # 求平方根倒数 a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.rsqrt(a) Out[1]: tensor([1.0000, 0.7071, 0.5774, 0.5000]) # 计算逐元素余数， 保留正负号 t = torch.tensor([10, -22, 31, -47]) torch.fmod(t, 5) Out[0]: tensor([ 0, -2, 1, -2]) # 计算逐元素余数， 相当于python中的%操作符 torch.remainder(t, 5) Out[1]: tensor([0, 3, 1, 3]) np.mod(np.array([10, -22, 31, -47]), 5) Out[2]: array([0, 3, 1, 3], dtype=int32) # 求1/x a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.reciprocal(a) Out[1]: tensor([1.0000, 0.5000, 0.3333, 0.2500]) # 求平方根 a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.sqrt(a) Out[1]: tensor([1.0000, 1.4142, 1.7321, 2.0000]) # 求绝对值 torch.abs(torch.FloatTensor([-1, -2, 3])) Out[0]: tensor([1., 2., 3.]) 三角函数 torch.asin(别名torch.arcsin): 返回一个新张量，包含输入input张量每个元素的反正弦函数 torch.atan: 返回一个新张量，包含输入input张量每个元素的反正切函数 torch.atan2: 返回一个新张量，包含两个输入张量input1和input2的反正切函数 torch.cos: 返回一个新张量，包含输入input张量每个元素的余弦 torch.acos(别名torch.arccos): 返回一个新张量，包含输入张量每个元素的反余弦 torch.cosh: 返回一个新张量，包含输入input张量每个元素的双曲余弦 torch.sin: 返回一个新张量，包含输入input张量每个元素的正弦 torch.sinh: 返回一个新张量，包含输入input张量每个元素的双曲正弦 torch.tan: 返回一个新张量，包含输入input张量每个元素的正切 torch.tanh: 返回一个新张量，包含输入input张量每个元素的双曲正切 # 反正弦函数 a = torch.randn(4) Out[0]: tensor([ 0.2583, -0.5285, 0.8979, 1.0104]) torch.asin(a) Out[1]: tensor([ 0.2613, -0.5569, 1.1149, nan]) # 反正切函数 a = torch.randn(4) Out[0]: tensor([ 0.2583, -0.5285, 0.8979, 1.0104]) b = torch.randn(4) Out[2]: tensor([0.1100, 1.4311, 1.9536, 0.7652]) torch.atan(a) Out[1]: tensor([ 0.2528, -0.4862, 0.7316, 0.7906]) torch.atan2(a, b) Out[3]: tensor([ 1.1681, -0.3538, 0.4308, 0.9226]) # 余弦 a = torch.randn(4) Out[0]: tensor([-0.9105, -0.7277, 0.9516, -0.1081]) torch.cos(a) Out[1]: tensor([0.6133, 0.7467, 0.5804, 0.9942]) # 反余弦 torch.acos(torch.FloatTensor([-1, 1, 0])) Out[1]: tensor([3.1416, 0.0000, 1.5708]) # 双曲余弦 torch.cosh(a) Out[2]: tensor([1.4439, 1.2766, 1.4880, 1.0058]) # 正弦 a = torch.randn(4) torch.sin(a) Out[0]: tensor([-0.9215, 0.2650, 0.8285, 0.5914]) # 双曲正弦 torch.sinh(a) Out[1]: tensor([-1.4591, 0.2714, 1.1392, 0.6759]) # 正切 a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.tan(a) Out[1]: tensor([ 1.5574, -2.1850, -0.1425, 1.1578]) # 双曲正切 torch.tanh(a) Out[2]: tensor([0.7616, 0.9640, 0.9951, 0.9993]) 位操作 bitwise_not – 按位非: 计算给定输入张量的按位非，这个操作会将输入张量中的每个位反转，即将所有的1变成0，将所有的0变成1。在整数数据类型中，这通常意味着进行二进制补码的运算 bitwise_and – 按位与: 计算两个输入张量的按位与，只有当两个张量在同一位置的位都是1时，结果张量在该位置的位才是1，否则是0 bitwise_or – 按位或: 计算两个输入张量的按位或，只要两个张量在同一位置的位中有一个是1，结果张量在该位置的位就是1。如果都是0，结果就是0 bitwise_xor – 按位异或: 计算两个输入张量的按位异或，这个操作在两个张量在同一位置的位不同的时候返回1，相同的时候返回0 # 按位非 >>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8)) tensor([ 0, 1, -4], dtype=torch.int8) # 按位与 >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) tensor([1, 0, 3], dtype=torch.int8) >>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False])) tensor([ False, True, False]) # 按位或 >>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) tensor([-1, -2, 3], dtype=torch.int8) >>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False])) tensor([ True, True, False]) # 按位异或 >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) tensor([-2, -2, 0], dtype=torch.int8) >>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False])) tensor([ True, False, False]) bitwise_left_shift – 按位左移: 计算给定输入张量与另一张量(表示位移数量)的按位左移。这个操作将输入张量的每个位向左移动other指定的位数，左边溢出的位被丢弃，而右边则填充0 bitwise_right_shift – 按位右移: 计算给定输入张量与另一张量(表示位移数量)的按位右移。这个操作将输入张量的每个位向右移动other指定的位数，右边溢出的位被丢弃，对于无符号数据类型，左边填充0；对于有符号数据类型，一般会进行算术右移，填充的是最高位的值，即符号位 # 按位左移 >>> torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) tensor([-2, -2, 24], dtype=torch.int8) # 按位右移 >>> torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) tensor([-1, -7, 3], dtype=torch.int8) 其他操作 torch.sigmoid: 返回一个新张量，包含输入input张量每个元素的sigmoid值 torch.sign: 符号函数，返回一个新张量，包含输入input张量每个元素的正负 # 求sigmoid值 a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.sigmoid(a) Out[1]: tensor([0.7311, 0.8808, 0.9526, 0.9820]) # 符号函数 a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.sign(a) Out[1]: tensor([1., 1., 1., 1.]) torch.lerp: 对两个张量以start，end做线性插值， 将结果返回到输出张量 start (Tensor) – 起始点张量 end (Tensor) – 终止点张量 weight (float) – 插值公式的weight out (Tensor, optional) – 结果张量 out_i=start_i+weight_i∗(end_i−start_i) start = torch.arange(1., 5.) end = torch.empty(4).fill_(10) start, end Out[0]: (tensor([1., 2., 3., 4.]), tensor([10., 10., 10., 10.])) torch.lerp(start, end, 0.5) Out[1]: tensor([5.5000, 6.0000, 6.5000, 7.0000]) torch.lerp(start, end, torch.full_like(start, 0.5)) Out[2]: tensor([5.5000, 6.0000, 6.5000, 7.0000]) 归约操作 torch.cumprod: torch.cumprod(input, dim, out=None) → Tensor 返回输入沿指定维度的累积，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，y_i= \\prod _{i}{x_i} a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.cumprod(a, dim=0) Out[1]: tensor([ 1., 2., 6., 24.]) torch.cumsum: torch.cumsum(input, dim, out=None) → Tensor 返回输入沿指定维度的累加，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，y_i= \\sum _{i}{x_i} a = torch.Tensor([1, 2, 3, 4]) Out[0]: tensor([1., 2., 3., 4.]) torch.cumsum(a, dim=0) Out[1]: tensor([ 1., 3., 6., 10.]) torch.dist: 返回 (input - other) 的 p范数 torch.dist(input, other, p=2, out=None) → Tensor 参数： input (Tensor) – 输入张量 other (Tensor) – 右侧输入张量 p (float, optional) – 所计算的范数 out (Tensor, optional) – 结果张量 > ||x||_p = (\\sum _{i=1}^{n}{|x_i|^p})^{\\frac {1}{p}} > x = torch.Tensor([1, 2, 3, 4]) y = torch.Tensor([1, 2, 3, 0]) torch.dist(x, y, 3.5) Out[0]: tensor(4.0000) torch.dist(x, y, 3) Out[1]: tensor(4.) torch.norm: 返回输入张量input 的 p 范数 torch.norm(input, p=2) → float 返回输入张量给定维dim 上每行的p 范数 torch.norm(input, p, dim, out=None) → Tensor a = torch.randn(1, 3) Out[0]: tensor([[ 0.7848, -0.3629, 0.4028]]) torch.norm(a, 3) Out[1]: tensor(0.8418) a = torch.randn(3, 2) Out[2]: tensor([[ 1.0718, 3.1510], [-0.3178, -0.9579], [ 0.4065, 0.4106]]) torch.norm(a, 2, 1) Out[3]: tensor([3.3283, 1.0092, 0.5778]) torch.mean: 返回输入张量给定维度dim上每行的均值 torch.median: 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor 参数： input (Tensor) – 输入张量 dim (int) – 缩减的维度 values (Tensor, optional) – 结果张量 indices (Tensor, optional) – 返回的索引结果张量 x = torch.Tensor([1, 2, 3, 4]) torch.mean(x) Out[0]: tensor(2.5000) torch.median(x) Out[1]: tensor(2.) torch.mode: 返回给定维dim上，每行的众数值， 同时返回一个LongTensor，包含众数职的索引 torch.mode(input, dim=-1, values=None, indices=None) -> (Tensor, LongTensor) 返回输入张量给定维度上每行的积 torch.prod(input, dim, out=None) → Tensor a = torch.randn(2, 3) Out[0]: tensor([[-0.1038, 0.8983, -0.7463], [-0.6661, -0.5061, 0.2043]]) torch.mode(a, 1) Out[1]: torch.return_types.mode( values=tensor([-0.7463, -0.6661]), indices=tensor([2, 0])) torch.prod: 返回输入张量input 所有元素的积 x = torch.Tensor([1, 2, 3, 4]) torch.prod(x) Out[0]: tensor(24.) y = torch.Tensor([[1, 2], [3, 4]]) torch.prod(y, dim=1) Out[1]: tensor([ 2., 12.]) torch.std: 返回输入张量input 所有元素的标准差 torch.std(input) → float 返回输入张量给定维度上每行的标准差 torch.std(input, dim, out=None) → Tensor x = torch.Tensor([1, 2, 3, 4]) y = torch.Tensor([[1, 2], [3, 4]]) torch.std(x) Out[0]: tensor(1.2910) torch.std(y, dim=1) Out[1]: tensor([0.7071, 0.7071]) torch.sum: 返回输入张量所有元素的和 torch.sum(input) → float 返回输入张量给定维度上每行的和 torch.sum(input, dim, out=None) → Tensor x = torch.Tensor([1, 2, 3, 4]) y = torch.Tensor([[1, 2], [3, 4]]) torch.var(x) Out[0]: tensor(10.0) torch.var(y, dim=1) Out[1]: tensor([3., 7.]) torch.var: 返回输入张量所有元素的方差 torch.var(input) → float 返回输入张量给定维度上每行的方差 torch.var(input, dim, out=None) → Tensor x = torch.Tensor([1, 2, 3, 4]) y = torch.Tensor([[1, 2], [3, 4]]) torch.var(x) Out[0]: tensor(1.6667) torch.var(y, dim=1) Out[1]: tensor([0.5000, 0.5000]) 比较操作 torch.eq: torch.eq(input, other, out=None) → Tensor 比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量 torch.ge: torch.ge(input, other, out=None) → Tensor 逐元素比较input和other，即是否 input>=other torch.gt: torch.gt(input, other, out=None) → Tensor 逐元素比较input和other ， 即是否input>otherinput>other torch.le: torch.le(input, other, out=None) → Tensor 逐元素比较input和other ， 即是否input torch.lt: torch.lt(input, other, out=None) → Tensor 逐元素比较input和other ， 即是否 input torch.ne: torch.ne(input, other, out=None) → Tensor 逐元素比较input和other，即是否 input!=other torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) Out[0]: tensor([[ True, False], [False, True]]) torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) Out[1]: tensor([[ True, True], [False, True]]) torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) Out[2]: tensor([[False, True], [False, False]]) torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) Out[3]: tensor([[ True, False], [ True, True]]) torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) Out[4]: tensor([[False, False], [ True, False]]) torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]])) Out[5]: tensor([[False, True], [ True, False]]) torch.equal: torch.equal(tensor1, tensor2) → bool 如果两个张量有相同的形状和元素值，则返回True ，否则 False torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2])) Out[0]: True torch.kthvalue: torch.kthvalue(input, k, dim=None, out=None) -> (Tensor, LongTensor) 取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维 torch.topk: 沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值 返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为True，将会确保返回的 k 个值被排序 torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor) 参数: input (Tensor) – 输入张量 k (int) – “top-k”中的k dim (int, optional) – 排序的维 largest (bool, optional) – 布尔值，控制返回最大或最小值 sorted (bool, optional) – 布尔值，控制返回值是否排序 out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers x = torch.arange(1, 6) # torch.kthvalue torch.kthvalue(x, 4) Out[0]: torch.return_types.kthvalue( values=tensor(4), indices=tensor(3)) # torch.topk x = torch.arange(1, 6) Out[1]: tensor([1, 2, 3, 4, 5]) torch.topk(x, 3) Out[2]: torch.return_types.topk( values=tensor([5, 4, 3]), indices=tensor([4, 3, 2])) torch.topk(x, 3, 0, largest=False) Out[3]: torch.return_types.topk( values=tensor([1, 2, 3]), indices=tensor([0, 1, 2])) torch.max: 返回输入张量所有元素的最大值 torch.max() 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引 torch.max(input, dim, max=None, max_indices=None) -> (Tensor, LongTensor) input中逐元素与other相应位置的元素对比，返回最大值到输出张量 torch.max(input, other, out=None) → Tensor torch.min: 返回输入张量所有元素的最小值 torch.min(input) → float 返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引 torch.min(input, dim, min=None, min_indices=None) -> (Tensor, LongTensor) input中逐元素与other相应位置的元素对比，返回最小值到输出张量 torch.min(input, other, out=None) → Tensor a = torch.randn(2, 2) Out[0]: tensor([[-0.1204, -0.5016], [ 1.2717, 0.7351]]) b = torch.randn(2, 2) Out[1]: tensor([[-1.4497, 0.7534], [ 0.5994, -0.1490]]) # 最大值 torch.max(torch.arange(1, 5)) Out[2]: tensor(4) torch.max(a, 1) Out[3]: torch.return_types.max( values=tensor([-0.1204, 1.2717]), indices=tensor([0, 0])) torch.max(a, b) Out[4]: tensor([[-0.1204, 0.7534], [ 1.2717, 0.7351]]) # 最小值 torch.min(torch.arange(1, 5)) Out[5]: tensor(1) torch.min(a, 1) Out[6]: torch.return_types.min( values=tensor([-0.5016, 0.7351]), indices=tensor([1, 1])) torch.min(a, b) Out[7]: tensor([[-1.4497, -0.5016], [ 0.5994, -0.1490]]) torch.sort: torch.sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor) 对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序 返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标 x = torch.randn(3, 4) Out[0]: tensor([[-2.3460, 1.3734, 1.1444, -0.4736], [-1.1785, 0.8436, -1.4403, -0.1073], [-0.1198, 0.7067, -0.0734, -1.6181]]) sorted, indices = torch.sort(x) sorted, indices Out[1]: (tensor([[-2.3460, -0.4736, 1.1444, 1.3734], [-1.4403, -1.1785, -0.1073, 0.8436], [-1.6181, -0.1198, -0.0734, 0.7067]]), tensor([[0, 3, 2, 1], [2, 0, 3, 1], [3, 0, 2, 1]])) 其它操作 torch.cross: 返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3 如果不指定dim，则默认为第一个尺度为3的维 torch.cross(input, other, dim=-1, out=None) → Tensor \\left[\\begin{array}{l}a_{1} \\\\ a_{2} \\\\ a_{3}\\end{array}\\right] \\times\\left[\\begin{array}{l}b_{1} \\\\ b_{2} \\\\ b_{3}\\end{array}\\right]=\\left[\\begin{array}{c}a_{2} b_{3}-a_{3} b_{2} \\\\ a_{3} b_{1}-a_{1} b_{3} \\\\ a_{1} b_{2}-a_{2} b_{1}\\end{array}\\right] a = torch.randint(1, 6, (2, 3)) Out[0]: tensor([[5, 4, 5], [4, 2, 3]]) b = torch.randint(1, 6, (2, 3)) Out[1]: tensor([[1, 1, 2], [3, 4, 2]]) torch.cross(a, a) Out[2]: tensor([[0, 0, 0], [0, 0, 0]]) torch.cross(a, b) Out[3]: tensor([[ 3, -5, 1], [-8, 1, 10]]) torch.diag: torch.diag(input, diagonal=0, out=None) → Tensor 如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵 如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量 参数diagonal指定对角线: diagonal = 0, 主对角线 diagonal > 0, 主对角线之上 diagonal # 如果输入是一个向量(1D 张量)，则返回一个以`input`为对角线元素的2D方阵 a = torch.randn(3) Out[0]: tensor([-0.3509, 0.6176, -1.4976]) torch.diag(a) Out[1]: tensor([[-0.3509, 0.0000, 0.0000], [ 0.0000, 0.6176, 0.0000], [ 0.0000, 0.0000, -1.4976]]) torch.diag(a, 1) Out[2]: tensor([[ 0.0000, -0.3509, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.6176, 0.0000], [ 0.0000, 0.0000, 0.0000, -1.4976], [ 0.0000, 0.0000, 0.0000, 0.0000]]) # 如果输入是一个矩阵(2D 张量)，则返回一个包含`input`对角线元素的1D张量 # 取得给定矩阵第k个对角线: a = torch.randn(3, 3) Out[3]: tensor([[ 0.8224, 0.7792, 0.2605], [-0.8646, 0.2568, -0.8189], [ 1.1693, 0.8108, -1.9662]]) torch.diag(a, 0) Out[4]: tensor([ 0.8224, 0.2568, -1.9662]) torch.diag(a, 1) Out[5]: tensor([ 0.7792, -0.8189]) torch.histc: torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor 计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中 参数： input (Tensor) – 输入张量 bins (int) – 直方图 bins(直条)的个数(默认100个) min (int) – range的下边界(包含) max (int) – range的上边界(包含) out (Tensor, optional) – 结果张量 torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3) Out[0]: tensor([0., 2., 1., 0.]) torch.histc(torch.FloatTensor([1, 1, 2, 1]), bins=4, min=0, max=3) Out[1]: tensor([0., 3., 1., 0.]) torch.renorm: torch.renorm(input, p, dim, maxnorm, out=None) → Tensor 返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm 如果p范数的值小于maxnorm，则当前子张量不需要修改 参数： input (Tensor) – 输入张量 p (float) – 范数的p dim (int) – 沿着此维切片，得到张量子集 maxnorm (float) – 每个子张量的范数的最大值 out (Tensor, optional) – 结果张量 x = torch.ones(3, 3) x[1].fill_(2) x[2].fill_(3) Out[0]: tensor([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.]]) torch.renorm(x, p=1, dim=0, maxnorm=5) Out[1]: tensor([[1.0000, 1.0000, 1.0000], [1.6667, 1.6667, 1.6667], [1.6667, 1.6667, 1.6667]]) torch.trace: 返回输入2维矩阵对角线元素的和(迹) x = torch.arange(1, 10).view(3, 3) Out[0]: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.trace(x) Out[1]: tensor(15) torch.tril: torch.tril(input, diagonal=0, out=None) → Tensor 返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0 x = torch.arange(1, 10).view(3, 3) Out[0]: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.tril(x) Out[1]: tensor([[1, 0, 0], [4, 5, 0], [7, 8, 9]]) torch.tril(x, diagonal=1) Out[2]: tensor([[1, 2, 0], [4, 5, 6], [7, 8, 9]]) torch.triu: torch.triu(input, k=0, out=None) → Tensor 返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。 参数k控制对角线: - k = 0, 主对角线 - k > 0, 主对角线之上 - k x = torch.arange(1, 10).view(3, 3) Out[0]: tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) torch.triu(x) Out[1]: tensor([[1, 2, 3], [0, 5, 6], [0, 0, 9]]) torch.triu(x, diagonal=1) Out[2]: tensor([[0, 2, 3], [0, 0, 6], [0, 0, 0]]) BLAS和LAPACK操作 BLAS（Basic Linear Algebra Subprograms）和LAPACK（Linear Algebra Package）是两个广泛使用的数学库，它们提供了一系列的数学运算，这些运算是高性能线性代数计算的基础 BLAS 提供了基本的线性代数运算，它主要关注向量与向量之间（Level 1 BLAS）、矩阵与向量之间（Level 2 BLAS）以及矩阵与矩阵之间（Level 3 BLAS）的运算。BLAS 的这些操作是高度优化的，旨在提供高效率的计算，这对于任何需要大量线性代数计算的程序都是非常重要的。例如，BLAS 提供了矩阵乘法、向量加法、标量与向量的乘法等基础操作 LAPACK 构建于 BLAS 之上，提供了更复杂的线性代数运算，如求解线性方程组、计算矩阵特征值和特征向量、奇异值分解、LU分解、QR分解等。LAPACK 是为了解决更大规模的线性代数问题而设计的，它能够利用 BLAS 提供的基础操作来实现更高级的数学运算 在很多现代的数值计算环境或科学计算库中，例如 NumPy、SciPy、MATLAB 和 R，底层都直接或间接地使用了 BLAS 和 LAPACK 的实现。这些库通常会链接到特定硬件优化版本的 BLAS 和 LAPACK，如 MKL（Intel Math Kernel Library）或 OpenBLAS，以获得更好的性能 BLAS 和 LAPACK 是高性能数值计算领域的标准构件，它们为复杂的线性代数运算提供了强大的支持 torch.addbmm torch.addmm torch.addmv torch.addr torch.baddbmm torch.bmm torch.btrifact torch.btrisolve torch.dot: 计算两个张量的点乘(内乘),两个张量都为1-D 向量 torch.dot(tensor1, tensor2) → float torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1])) Out[0]: tensor(7.) torch.linalg.eig: 计算实方阵a 的特征值和特征向量 torch.linalg.eig(A, * , out=None) A = torch.randn(2, 2, dtype=torch.complex128) Out[0]: tensor([[-0.2029-0.0673j, -0.5188-0.6723j], [-1.1984+0.0585j, 0.5786-0.1849j]], dtype=torch.complex128) L, V = torch.linalg.eig(A) Out[1]: (tensor([-0.7870-0.5003j, 1.1626+0.2481j], dtype=torch.complex128), tensor([[ 0.7596+0.0000j, -0.4008-0.3285j], [ 0.6258-0.1770j, 0.8552+0.0000j]], dtype=torch.complex128)) Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/pytorch学习_进阶知识.html":{"url":"chapters/pytorch学习_进阶知识.html","title":"pytorch学习_进阶知识.md","summary":"pytorch学习_进阶知识","keywords":"","body":"Tensorstorage实例Pytorch加载数据TensorboardTransformstorchvision数据集损失函数优化器网络模型使用及修改网络模型保存与读取固定模型参数训练流程 pytorch中文文档 Tensor torch.Tensor是一种包含单一数据类型元素的多维矩阵 Torch定义了10种CPU tensor类型和GPU tensor类型： Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point [1] torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point [2] torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 or torch.chalf 64-bit complex torch.complex64 or torch.cfloat 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qint32 torch.IntTensor / quantized 4-bit integer (unsigned) [3] torch.quint4x2 torch.ByteTensor / 创建 一个张量tensor可以从Python的list或序列构建 torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) Out[0]: tensor([[1., 2., 3.], [4., 5., 6.]]) 根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了numpy.ndarray,torch.Tensor或torch.Storage，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor # 接口 一个空张量tensor可以通过规定其大小来构建 class torch.Tensor class torch.Tensor(*sizes) class torch.Tensor(size) class torch.Tensor(sequence) class torch.Tensor(ndarray) class torch.Tensor(tensor) class torch.Tensor(storage) # 实例化 torch.IntTensor(2, 4).zero_() 可以用python的索引和切片来获取和修改一个张量tensor中的内容 x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]]) x[1][2] Out[0]: tensor(6.) x[0][1] = 8 x Out[1]: tensor([[1., 8., 3.], [4., 5., 6.]]) 每一个张量tensor都有一个相应的torch.Storage用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算 会改变tensor的函数操作会用一个下划线后缀来标示。比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而tensor.FloatTensor.abs()将会在一个新的tensor中计算结果 关键属性和方法 Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor for a complex-valued input tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.adjoint Alias for adjoint() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.aminmax See torch.aminmax() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.argwhere See torch.argwhere() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.arctan2 See torch.arctan2() Tensor.arctan2_ atan2_(other) -> Tensor Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each \\texttt{result[i]}result[i] is independently sampled from \\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location of self with an independent sample from \\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16 self.bfloat16() is equivalent to self.to(torch.bfloat16). Tensor.bincount See torch.bincount() Tensor.bitwise_not See torch.bitwise_not() Tensor.bitwise_not_ In-place version of bitwise_not() Tensor.bitwise_and See torch.bitwise_and() Tensor.bitwise_and_ In-place version of bitwise_and() Tensor.bitwise_or See torch.bitwise_or() Tensor.bitwise_or_ In-place version of bitwise_or() Tensor.bitwise_xor See torch.bitwise_xor() Tensor.bitwise_xor_ In-place version of bitwise_xor() Tensor.bitwise_left_shift See torch.bitwise_left_shift() Tensor.bitwise_left_shift_ In-place version of bitwise_left_shift() Tensor.bitwise_right_shift See torch.bitwise_right_shift() Tensor.bitwise_right_shift_ In-place version of bitwise_right_shift() Tensor.bmm See torch.bmm() Tensor.bool self.bool() is equivalent to self.to(torch.bool). Tensor.byte self.byte() is equivalent to self.to(torch.uint8). Tensor.broadcast_to See torch.broadcast_to(). Tensor.cauchy_ Fills the tensor with numbers drawn from the Cauchy distribution: Tensor.ceil See torch.ceil() Tensor.ceil_ In-place version of ceil() Tensor.char self.char() is equivalent to self.to(torch.int8). Tensor.cholesky See torch.cholesky() Tensor.cholesky_inverse See torch.cholesky_inverse() Tensor.cholesky_solve See torch.cholesky_solve() Tensor.chunk See torch.chunk() Tensor.clamp See torch.clamp() Tensor.clamp_ In-place version of clamp() Tensor.clip Alias for clamp(). Tensor.clip_ Alias for clamp_(). Tensor.clone See torch.clone() Tensor.contiguous Returns a contiguous in memory tensor containing the same data as self tensor. Tensor.copy_ Copies the elements from src into self tensor and returns self. Tensor.conj See torch.conj() Tensor.conj_physical See torch.conj_physical() Tensor.conj_physical_ In-place version of conj_physical() Tensor.resolve_conj See torch.resolve_conj() Tensor.resolve_neg See torch.resolve_neg() Tensor.copysign See torch.copysign() Tensor.copysign_ In-place version of copysign() Tensor.cos See torch.cos() Tensor.cos_ In-place version of cos() Tensor.cosh See torch.cosh() Tensor.cosh_ In-place version of cosh() Tensor.corrcoef See torch.corrcoef() Tensor.count_nonzero See torch.count_nonzero() Tensor.cov See torch.cov() Tensor.acosh See torch.acosh() Tensor.acosh_ In-place version of acosh() Tensor.arccosh acosh() -> Tensor Tensor.arccosh_ acosh_() -> Tensor Tensor.cpu Returns a copy of this object in CPU memory. Tensor.cross See torch.cross() Tensor.cuda Returns a copy of this object in CUDA memory. Tensor.logcumsumexp See torch.logcumsumexp() Tensor.cummax See torch.cummax() Tensor.cummin See torch.cummin() Tensor.cumprod See torch.cumprod() Tensor.cumprod_ In-place version of cumprod() Tensor.cumsum See torch.cumsum() Tensor.cumsum_ In-place version of cumsum() Tensor.chalf self.chalf() is equivalent to self.to(torch.complex32). Tensor.cfloat self.cfloat() is equivalent to self.to(torch.complex64). Tensor.cdouble self.cdouble() is equivalent to self.to(torch.complex128). Tensor.data_ptr Returns the address of the first element of self tensor. Tensor.deg2rad See torch.deg2rad() Tensor.dequantize Given a quantized Tensor, dequantize it and return the dequantized float Tensor. Tensor.det See torch.det() Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.detach Returns a new Tensor, detached from the current graph. Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. Tensor.diag See torch.diag() Tensor.diag_embed See torch.diag_embed() Tensor.diagflat See torch.diagflat() Tensor.diagonal See torch.diagonal() Tensor.diagonal_scatter See torch.diagonal_scatter() Tensor.fill_diagonal_ Fill the main diagonal of a tensor that has at least 2-dimensions. Tensor.fmax See torch.fmax() Tensor.fmin See torch.fmin() Tensor.diff See torch.diff() Tensor.digamma See torch.digamma() Tensor.digamma_ In-place version of digamma() Tensor.dim Returns the number of dimensions of self tensor. Tensor.dist See torch.dist() Tensor.div See torch.div() Tensor.div_ In-place version of div() Tensor.divide See torch.divide() Tensor.divide_ In-place version of divide() Tensor.dot See torch.dot() Tensor.double self.double() is equivalent to self.to(torch.float64). Tensor.dsplit See torch.dsplit() Tensor.element_size Returns the size in bytes of an individual element. Tensor.eq See torch.eq() Tensor.eq_ In-place version of eq() Tensor.equal See torch.equal() Tensor.erf See torch.erf() Tensor.erf_ In-place version of erf() Tensor.erfc See torch.erfc() Tensor.erfc_ In-place version of erfc() Tensor.erfinv See torch.erfinv() Tensor.erfinv_ In-place version of erfinv() Tensor.exp See torch.exp() Tensor.exp_ In-place version of exp() Tensor.expm1 See torch.expm1() Tensor.expm1_ In-place version of expm1() Tensor.expand Returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor.expand_as Expand this tensor to the same size as other. Tensor.exponential_ Fills self tensor with elements drawn from the exponential distribution: Tensor.fix See torch.fix(). Tensor.fix_ In-place version of fix() Tensor.fill_ Fills self tensor with the specified value. Tensor.flatten See torch.flatten() Tensor.flip See torch.flip() Tensor.fliplr See torch.fliplr() Tensor.flipud See torch.flipud() Tensor.float self.float() is equivalent to self.to(torch.float32). Tensor.float_power See torch.float_power() Tensor.float_power_ In-place version of float_power() Tensor.floor See torch.floor() Tensor.floor_ In-place version of floor() Tensor.floor_divide See torch.floor_divide() Tensor.floor_divide_ In-place version of floor_divide() Tensor.fmod See torch.fmod() Tensor.fmod_ In-place version of fmod() Tensor.frac See torch.frac() Tensor.frac_ In-place version of frac() Tensor.frexp See torch.frexp() Tensor.gather See torch.gather() Tensor.gcd See torch.gcd() Tensor.gcd_ In-place version of gcd() Tensor.ge See torch.ge(). Tensor.ge_ In-place version of ge(). Tensor.greater_equal See torch.greater_equal(). Tensor.greater_equal_ In-place version of greater_equal(). Tensor.geometric_ Fills self tensor with elements drawn from the geometric distribution: Tensor.geqrf See torch.geqrf() Tensor.ger See torch.ger() Tensor.get_device For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. Tensor.gt See torch.gt(). Tensor.gt_ In-place version of gt(). Tensor.greater See torch.greater(). Tensor.greater_ In-place version of greater(). Tensor.half self.half() is equivalent to self.to(torch.float16). Tensor.hardshrink See torch.nn.functional.hardshrink() Tensor.heaviside See torch.heaviside() Tensor.histc See torch.histc() Tensor.histogram See torch.histogram() Tensor.hsplit See torch.hsplit() Tensor.hypot See torch.hypot() Tensor.hypot_ In-place version of hypot() Tensor.i0 See torch.i0() Tensor.i0_ In-place version of i0() Tensor.igamma See torch.igamma() Tensor.igamma_ In-place version of igamma() Tensor.igammac See torch.igammac() Tensor.igammac_ In-place version of igammac() Tensor.index_add_ Accumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index. Tensor.index_add Out-of-place version of torch.Tensor.index_add_(). Tensor.index_copy_ Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. Tensor.index_copy Out-of-place version of torch.Tensor.index_copy_(). Tensor.index_fill_ Fills the elements of the self tensor with value value by selecting the indices in the order given in index. Tensor.index_fill Out-of-place version of torch.Tensor.index_fill_(). Tensor.index_put_ Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). Tensor.index_put Out-place version of index_put_(). Tensor.index_reduce_ Accumulate the elements of source into the self tensor by accumulating to the indices in the order given in index using the reduction given by the reduce argument. Tensor.index_reduce Tensor.index_select See torch.index_select() Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.inner See torch.inner(). Tensor.int self.int() is equivalent to self.to(torch.int32). Tensor.int_repr Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor. Tensor.inverse See torch.inverse() Tensor.isclose See torch.isclose() Tensor.isfinite See torch.isfinite() Tensor.isinf See torch.isinf() Tensor.isposinf See torch.isposinf() Tensor.isneginf See torch.isneginf() Tensor.isnan See torch.isnan() Tensor.is_contiguous Returns True if self tensor is contiguous in memory in the order specified by memory format. Tensor.is_complex Returns True if the data type of self is a complex data type. Tensor.is_conj Returns True if the conjugate bit of self is set to true. Tensor.is_floating_point Returns True if the data type of self is a floating point data type. Tensor.is_inference See torch.is_inference() Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. Tensor.is_pinned Returns true if this tensor resides in pinned memory. Tensor.is_set_to Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride). Tensor.is_shared Checks if tensor is in shared memory. Tensor.is_signed Returns True if the data type of self is a signed data type. Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.istft See torch.istft() Tensor.isreal See torch.isreal() Tensor.item Returns the value of this tensor as a standard Python number. Tensor.kthvalue See torch.kthvalue() Tensor.lcm See torch.lcm() Tensor.lcm_ In-place version of lcm() Tensor.ldexp See torch.ldexp() Tensor.ldexp_ In-place version of ldexp() Tensor.le See torch.le(). Tensor.le_ In-place version of le(). Tensor.less_equal See torch.less_equal(). Tensor.less_equal_ In-place version of less_equal(). Tensor.lerp See torch.lerp() Tensor.lerp_ In-place version of lerp() Tensor.lgamma See torch.lgamma() Tensor.lgamma_ In-place version of lgamma() Tensor.log See torch.log() Tensor.log_ In-place version of log() Tensor.logdet See torch.logdet() Tensor.log10 See torch.log10() Tensor.log10_ In-place version of log10() Tensor.log1p See torch.log1p() Tensor.log1p_ In-place version of log1p() Tensor.log2 See torch.log2() Tensor.log2_ In-place version of log2() Tensor.log_normal_ Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \\muμ and standard deviation \\sigmaσ. Tensor.logaddexp See torch.logaddexp() Tensor.logaddexp2 See torch.logaddexp2() Tensor.logsumexp See torch.logsumexp() Tensor.logical_and See torch.logical_and() Tensor.logical_and_ In-place version of logical_and() Tensor.logical_not See torch.logical_not() Tensor.logical_not_ In-place version of logical_not() Tensor.logical_or See torch.logical_or() Tensor.logical_or_ In-place version of logical_or() Tensor.logical_xor See torch.logical_xor() Tensor.logical_xor_ In-place version of logical_xor() Tensor.logit See torch.logit() Tensor.logit_ In-place version of logit() Tensor.long self.long() is equivalent to self.to(torch.int64). Tensor.lt See torch.lt(). Tensor.lt_ In-place version of lt(). Tensor.less lt(other) -> Tensor Tensor.less_ In-place version of less(). Tensor.lu See torch.lu() Tensor.lu_solve See torch.lu_solve() Tensor.as_subclass Makes a cls instance with the same data pointer as self. Tensor.map_ Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. Tensor.masked_scatter_ Copies elements from source into self tensor at positions where the mask is True. Tensor.masked_scatter Out-of-place version of torch.Tensor.masked_scatter_() Tensor.masked_fill_ Fills elements of self tensor with value where mask is True. Tensor.masked_fill Out-of-place version of torch.Tensor.masked_fill_() Tensor.masked_select See torch.masked_select() Tensor.matmul See torch.matmul() Tensor.matrix_power NOTEmatrix_power() is deprecated, use torch.linalg.matrix_power() instead. Tensor.matrix_exp See torch.matrix_exp() Tensor.max See torch.max() Tensor.maximum See torch.maximum() Tensor.mean See torch.mean() Tensor.nanmean See torch.nanmean() Tensor.median See torch.median() Tensor.nanmedian See torch.nanmedian() Tensor.min See torch.min() Tensor.minimum See torch.minimum() Tensor.mm See torch.mm() Tensor.smm See torch.smm() Tensor.mode See torch.mode() Tensor.movedim See torch.movedim() Tensor.moveaxis See torch.moveaxis() Tensor.msort See torch.msort() Tensor.mul See torch.mul(). Tensor.mul_ In-place version of mul(). Tensor.multiply See torch.multiply(). Tensor.multiply_ In-place version of multiply(). Tensor.multinomial See torch.multinomial() Tensor.mv See torch.mv() Tensor.mvlgamma See torch.mvlgamma() Tensor.mvlgamma_ In-place version of mvlgamma() Tensor.nansum See torch.nansum() Tensor.narrow See torch.narrow() Tensor.narrow_copy See torch.narrow_copy(). Tensor.ndimension Alias for dim() Tensor.nan_to_num See torch.nan_to_num(). Tensor.nan_to_num_ In-place version of nan_to_num(). Tensor.ne See torch.ne(). Tensor.ne_ In-place version of ne(). Tensor.not_equal See torch.not_equal(). Tensor.not_equal_ In-place version of not_equal(). Tensor.neg See torch.neg() Tensor.neg_ In-place version of neg() Tensor.negative See torch.negative() Tensor.negative_ In-place version of negative() Tensor.nelement Alias for numel() Tensor.nextafter See torch.nextafter() Tensor.nextafter_ In-place version of nextafter() Tensor.nonzero See torch.nonzero() Tensor.norm See torch.norm() Tensor.normal_ Fills self tensor with elements samples from the normal distribution parameterized by mean and std. Tensor.numel See torch.numel() Tensor.numpy Returns the tensor as a NumPy ndarray. Tensor.orgqr See torch.orgqr() Tensor.ormqr See torch.ormqr() Tensor.outer See torch.outer(). Tensor.permute See torch.permute() Tensor.pin_memory Copies the tensor to pinned memory, if it's not already pinned. Tensor.pinverse See torch.pinverse() Tensor.polygamma See torch.polygamma() Tensor.polygamma_ In-place version of polygamma() Tensor.positive See torch.positive() Tensor.pow See torch.pow() Tensor.pow_ In-place version of pow() Tensor.prod See torch.prod() Tensor.put_ Copies the elements from source into the positions specified by index. Tensor.qr See torch.qr() Tensor.qscheme Returns the quantization scheme of a given QTensor. Tensor.quantile See torch.quantile() Tensor.nanquantile See torch.nanquantile() Tensor.q_scale Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer(). Tensor.q_zero_point Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer(). Tensor.q_per_channel_scales Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. Tensor.q_per_channel_zero_points Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. Tensor.q_per_channel_axis Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied. Tensor.rad2deg See torch.rad2deg() Tensor.random_ Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. Tensor.ravel see torch.ravel() Tensor.reciprocal See torch.reciprocal() Tensor.reciprocal_ In-place version of reciprocal() Tensor.record_stream Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete. Tensor.register_hook Registers a backward hook. Tensor.remainder See torch.remainder() Tensor.remainder_ In-place version of remainder() Tensor.renorm See torch.renorm() Tensor.renorm_ In-place version of renorm() Tensor.repeat Repeats this tensor along the specified dimensions. Tensor.repeat_interleave See torch.repeat_interleave(). Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. Tensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor's requires_grad attribute in-place. Tensor.reshape Returns a tensor with the same data and number of elements as self but with the specified shape. Tensor.reshape_as Returns this tensor as the same shape as other. Tensor.resize_ Resizes self tensor to the specified size. Tensor.resize_as_ Resizes the self tensor to be the same size as the specified tensor. Tensor.retain_grad Enables this Tensor to have their grad populated during backward(). Tensor.retains_grad Is True if this Tensor is non-leaf and its grad is enabled to be populated during backward(), False otherwise. Tensor.roll See torch.roll() Tensor.rot90 See torch.rot90() Tensor.round See torch.round() Tensor.round_ In-place version of round() Tensor.rsqrt See torch.rsqrt() Tensor.rsqrt_ In-place version of rsqrt() Tensor.scatter Out-of-place version of torch.Tensor.scatter_() Tensor.scatter_ Writes all values from the tensor src into self at the indices specified in the index tensor. Tensor.scatter_add_ Adds all values from the tensor src into self at the indices specified in the index tensor in a similar fashion as scatter_(). Tensor.scatter_add Out-of-place version of torch.Tensor.scatter_add_() Tensor.scatter_reduce_ Reduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (\"sum\", \"prod\", \"mean\", \"amax\", \"amin\"). Tensor.scatter_reduce Out-of-place version of torch.Tensor.scatter_reduce_() Tensor.select See torch.select() Tensor.select_scatter See torch.select_scatter() Tensor.set_ Sets the underlying storage, size, and strides. Tensor.share_memory_ Moves the underlying storage to shared memory. Tensor.short self.short() is equivalent to self.to(torch.int16). Tensor.sigmoid See torch.sigmoid() Tensor.sigmoid_ In-place version of sigmoid() Tensor.sign See torch.sign() Tensor.sign_ In-place version of sign() Tensor.signbit See torch.signbit() Tensor.sgn See torch.sgn() Tensor.sgn_ In-place version of sgn() Tensor.sin See torch.sin() Tensor.sin_ In-place version of sin() Tensor.sinc See torch.sinc() Tensor.sinc_ In-place version of sinc() Tensor.sinh See torch.sinh() Tensor.sinh_ In-place version of sinh() Tensor.asinh See torch.asinh() Tensor.asinh_ In-place version of asinh() Tensor.arcsinh See torch.arcsinh() Tensor.arcsinh_ In-place version of arcsinh() Tensor.size Returns the size of the self tensor. Tensor.slogdet See torch.slogdet() Tensor.slice_scatter See torch.slice_scatter() Tensor.sort See torch.sort() Tensor.split See torch.split() Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sqrt See torch.sqrt() Tensor.sqrt_ In-place version of sqrt() Tensor.square See torch.square() Tensor.square_ In-place version of square() Tensor.squeeze See torch.squeeze() Tensor.squeeze_ In-place version of squeeze() Tensor.std See torch.std() Tensor.stft See torch.stft() Tensor.storage Returns the underlying storage. Tensor.storage_offset Returns self tensor's offset in the underlying storage in terms of number of storage elements (not bytes). Tensor.storage_type Returns the type of the underlying storage. Tensor.stride Returns the stride of self tensor. Tensor.sub See torch.sub(). Tensor.sub_ In-place version of sub() Tensor.subtract See torch.subtract(). Tensor.subtract_ In-place version of subtract(). Tensor.sum See torch.sum() Tensor.sum_to_size Sum this tensor to size. Tensor.svd See torch.svd() Tensor.swapaxes See torch.swapaxes() Tensor.swapdims See torch.swapdims() Tensor.symeig See torch.symeig() Tensor.t See torch.t() Tensor.t_ In-place version of t() Tensor.tensor_split See torch.tensor_split() Tensor.tile See torch.tile() Tensor.to Performs Tensor dtype and/or device conversion. Tensor.to_mkldnn Returns a copy of the tensor in torch.mkldnn layout. Tensor.take See torch.take() Tensor.take_along_dim See torch.take_along_dim() Tensor.tan See torch.tan() Tensor.tan_ In-place version of tan() Tensor.tanh See torch.tanh() Tensor.tanh_ In-place version of tanh() Tensor.atanh See torch.atanh() Tensor.atanh_ In-place version of atanh() Tensor.arctanh See torch.arctanh() Tensor.arctanh_ In-place version of arctanh() Tensor.tolist Returns the tensor as a (nested) list. Tensor.topk See torch.topk() Tensor.to_dense Creates a strided copy of self if self is not a strided tensor, otherwise returns self. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor.to_sparse_csr Convert a tensor to compressed row storage format (CSR). Tensor.to_sparse_csc Convert a tensor to compressed column storage (CSC) format. Tensor.to_sparse_bsr Convert a CSR tensor to a block sparse row (BSR) storage format of given blocksize. Tensor.to_sparse_bsc Convert a CSR tensor to a block sparse column (BSC) storage format of given blocksize. Tensor.trace See torch.trace() Tensor.transpose See torch.transpose() Tensor.transpose_ In-place version of transpose() Tensor.triangular_solve See torch.triangular_solve() Tensor.tril See torch.tril() Tensor.tril_ In-place version of tril() Tensor.triu See torch.triu() Tensor.triu_ In-place version of triu() Tensor.true_divide See torch.true_divide() Tensor.true_divide_ In-place version of true_divide_() Tensor.trunc See torch.trunc() Tensor.trunc_ In-place version of trunc() Tensor.type Returns the type if dtype is not provided, else casts this object to the specified type. Tensor.type_as Returns this tensor cast to the type of the given tensor. Tensor.unbind See torch.unbind() Tensor.unflatten See torch.unflatten(). Tensor.unfold Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension. Tensor.uniform_ Fills self tensor with numbers sampled from the continuous uniform distribution: Tensor.unique Returns the unique elements of the input tensor. Tensor.unique_consecutive Eliminates all but the first element from every consecutive group of equivalent elements. Tensor.unsqueeze See torch.unsqueeze() Tensor.unsqueeze_ In-place version of unsqueeze() Tensor.values Return the values tensor of a sparse COO tensor. Tensor.var See torch.var() Tensor.vdot See torch.vdot() Tensor.view Returns a new tensor with the same data as the self tensor but of a different shape. Tensor.view_as View this tensor as the same size as other. Tensor.vsplit See torch.vsplit() Tensor.where self.where(condition, y) is equivalent to torch.where(condition, self, y). Tensor.xlogy See torch.xlogy() Tensor.xlogy_ In-place version of xlogy() Tensor.zero_ Fills self tensor with zeros. storage tensor的数据结构、storage()、stride()、storage_offset() pytorch中一个tensor对象分为头信息区（Tensor）和存储区（Storage）两部分 头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以连续一维数组的形式放在存储区，由torch.Storage实例管理着 注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中 获取tensor的storage a = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]]) a.storage() Out[0]: 1.0 4.0 2.0 1.0 3.0 5.0 [torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6] a.storage()[2] = 9 id(a.storage()) Out[1]: 1343354913168 实例 小土堆+李沐课程笔记 PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】 Pytorch加载数据 Pytorch中加载数据需要Dataset、Dataloader。 Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。 Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。 Tensorboard import torchvision from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter # 准备的测试数据集 test_data = torchvision.datasets.CIFAR10(\"./dataset\",train=False,transform=torchvision.transforms.ToTensor()) # batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回 test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=True,num_workers=0,drop_last=False) # 用for循环取出DataLoader打包好的四个数据 writer = SummaryWriter(\"logs\") step = 0 for data in test_loader: imgs, targets = data # 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成 writer.add_images(\"test_data\",imgs,step) step = step + 1 writer.close() Transforms ① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。 ② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。 from torchvision import transforms from PIL import Image img_path = \"Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg\" img = Image.open(img_path) tensor_trans = transforms.ToTensor() # 创建 transforms.ToTensor类 的实例化对象 tensor_img = tensor_trans(img) # 调用 transforms.ToTensor类 的__call__的魔术方法 print(tensor_img) torchvision数据集 ① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。 ② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。 import torchvision train_set = torchvision.datasets.CIFAR10(root=\"./dataset\",train=True,download=True) # root为存放数据集的相对路线 test_set = torchvision.datasets.CIFAR10(root=\"./dataset\",train=False,download=True) # train=True是训练集，train=False是测试集 print(test_set[0]) # 输出的3是target print(test_set.classes) # 测试数据集中有多少种 img, target = test_set[0] # 分别获得图片、target print(img) print(target) print(test_set.classes[target]) # 3号target对应的种类 img.show() 损失函数 ① Loss损失函数一方面计算实际输出和目标之间的差距。 ② Loss损失函数另一方面为我们更新输出提供一定的依据 L1loss损失函数 import torch from torch.nn import L1Loss inputs = torch.tensor([1,2,3],dtype=torch.float32) targets = torch.tensor([1,2,5],dtype=torch.float32) inputs = torch.reshape(inputs,(1,1,1,3)) targets = torch.reshape(targets,(1,1,1,3)) loss = L1Loss() # 默认为 maen result = loss(inputs,targets) print(result) MSE损失函数 import torch from torch.nn import L1Loss from torch import nn inputs = torch.tensor([1,2,3],dtype=torch.float32) targets = torch.tensor([1,2,5],dtype=torch.float32) inputs = torch.reshape(inputs,(1,1,1,3)) targets = torch.reshape(targets,(1,1,1,3)) loss_mse = nn.MSELoss() result_mse = loss_mse(inputs,targets) print(result_mse) 交叉熵损失函数 import torch from torch.nn import L1Loss from torch import nn x = torch.tensor([0.1,0.2,0.3]) y = torch.tensor([1]) x = torch.reshape(x,(1,3)) # 1的 batch_size，有三类 loss_cross = nn.CrossEntropyLoss() result_cross = loss_cross(x,y) print(result_cross) 优化器 ① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。 ② 梯度要清零，如果梯度不清零会导致梯度累加 loss = nn.CrossEntropyLoss() # 交叉熵 tudui = Tudui() optim = torch.optim.SGD(tudui.parameters(),lr=0.01) # 随机梯度下降优化器 for data in dataloader: imgs, targets = data outputs = tudui(imgs) result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距 optim.zero_grad() # 梯度清零 result_loss.backward() # 反向传播，计算损失函数的梯度 optim.step() # 根据梯度，对网络的参数进行调优 print(result_loss) # 对数据只看了一遍，只看了一轮，所以loss下降不大 神经网络学习率优化 import torch import torchvision from torch import nn from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter dataset = torchvision.datasets.CIFAR10(\"./dataset\",train=False,transform=torchvision.transforms.ToTensor(),download=True) dataloader = DataLoader(dataset, batch_size=64,drop_last=True) class Tudui(nn.Module): def __init__(self): super(Tudui, self).__init__() self.model1 = Sequential( Conv2d(3,32,5,padding=2), MaxPool2d(2), Conv2d(32,32,5,padding=2), MaxPool2d(2), Conv2d(32,64,5,padding=2), MaxPool2d(2), Flatten(), Linear(1024,64), Linear(64,10) ) def forward(self, x): x = self.model1(x) return x loss = nn.CrossEntropyLoss() # 交叉熵 tudui = Tudui() optim = torch.optim.SGD(tudui.parameters(),lr=0.01) # 随机梯度下降优化器 scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1) # 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍 for epoch in range(20): running_loss = 0.0 for data in dataloader: imgs, targets = data outputs = tudui(imgs) result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距 optim.zero_grad() # 梯度清零 result_loss.backward() # 反向传播，计算损失函数的梯度 optim.step() # 根据梯度，对网络的参数进行调优 scheduler.step() # 学习率太小了，所以20个轮次后，相当于没走多少 running_loss = running_loss + result_loss print(running_loss) # 对这一轮所有误差的总和 网络模型使用及修改 网络模型添加 import torchvision from torch import nn dataset = torchvision.datasets.CIFAR10(\"./dataset\",train=True,transform=torchvision.transforms.ToTensor(),download=True) vgg16_true = torchvision.models.vgg16(pretrained=True) # 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的 vgg16_true.add_module('add_linear',nn.Linear(1000,10)) # 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类 print(vgg16_true) 网络模型修改 import torchvision from torch import nn vgg16_false = torchvision.models.vgg16(pretrained=False) # 没有预训练的参数 print(vgg16_false) vgg16_false.classifier[6] = nn.Linear(4096,10) print(vgg16_false) 网络模型保存与读取 模型结构 + 模型参数 import torchvision import torch vgg16 = torchvision.models.vgg16(pretrained=False) torch.save(vgg16,\"./model/vgg16_method1.pth\") # 保存方式一：模型结构 + 模型参数 print(vgg16) model = torch.load(\"./model/vgg16_method1.pth\") # 保存方式一对应的加载模型 print(model) 模型参数（官方推荐），不保存网络模型结构 import torchvision import torch vgg16 = torchvision.models.vgg16(pretrained=False) torch.save(vgg16.state_dict(),\"./model/vgg16_method2.pth\") # 保存方式二：模型参数（官方推荐）,不再保存网络模型结构 print(vgg16) model = torch.load(\"./model/vgg16_method2.pth\") # 导入模型参数 print(model) 固定模型参数 在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标 一个是设置不要更新参数的网络层为false 另一个就是在定义优化器时只传入要更新的参数 当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高 import torch import torch.nn as nn import torch.optim as optim # 定义一个简单的网络 class net(nn.Module): def __init__(self, num_class=3): super(net, self).__init__() self.fc1 = nn.Linear(8, 4) self.fc2 = nn.Linear(4, num_class) def forward(self, x): return self.fc2(self.fc1(x)) model = net() # 冻结fc1层的参数 for name, param in model.named_parameters(): if \"fc1\" in name: param.requires_grad = False loss_fn = nn.CrossEntropyLoss() # 只传入requires_grad = True的参数 optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters(), lr=1e-2) print(\"model.fc1.weight\", model.fc1.weight) print(\"model.fc2.weight\", model.fc2.weight) model.train() for epoch in range(10): x = torch.randn((3, 8)) label = torch.randint(0, 3, [3]).long() output = model(x) loss = loss_fn(output, label) optimizer.zero_grad() loss.backward() optimizer.step() print(\"model.fc1.weight\", model.fc1.weight) print(\"model.fc2.weight\", model.fc2.weight) 训练流程 DataLoader加载数据集 import torchvision from torch import nn from torch.utils.data import DataLoader # 准备数据集 train_data = torchvision.datasets.CIFAR10(\"./dataset\",train=True,transform=torchvision.transforms.ToTensor(),download=True) test_data = torchvision.datasets.CIFAR10(\"./dataset\",train=False,transform=torchvision.transforms.ToTensor(),download=True) # length 长度 train_data_size = len(train_data) test_data_size = len(test_data) # 如果train_data_size=10，则打印：训练数据集的长度为：10 print(\"训练数据集的长度：{}\".format(train_data_size)) print(\"测试数据集的长度：{}\".format(test_data_size)) # 利用 Dataloader 来加载数据集 train_dataloader = DataLoader(train_data_size, batch_size=64) test_dataloader = DataLoader(test_data_size, batch_size=64) 测试网络正确 import torch from torch import nn # 搭建神经网络 class Tudui(nn.Module): def __init__(self): super(Tudui, self).__init__() self.model1 = nn.Sequential( nn.Conv2d(3,32,5,1,2), # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2 nn.MaxPool2d(2), nn.Conv2d(32,32,5,1,2), nn.MaxPool2d(2), nn.Conv2d(32,64,5,1,2), nn.MaxPool2d(2), nn.Flatten(), # 展平后变成 64*4*4 了 nn.Linear(64*4*4,64), nn.Linear(64,10) ) def forward(self, x): x = self.model1(x) return x if __name__ == '__main__': tudui = Tudui() input = torch.ones((64,3,32,32)) output = tudui(input) print(output.shape) # 测试输出的尺寸是不是我们想要的 网络训练数据 ① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。 ② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。 ③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。 ④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。 ⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点 import torchvision import torch from torch import nn from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter # from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里 class Tudui(nn.Module): def __init__(self): super(Tudui, self).__init__() self.model1 = nn.Sequential( nn.Conv2d(3,32,5,1,2), # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2 nn.MaxPool2d(2), nn.Conv2d(32,32,5,1,2), nn.MaxPool2d(2), nn.Conv2d(32,64,5,1,2), nn.MaxPool2d(2), nn.Flatten(), # 展平后变成 64*4*4 了 nn.Linear(64*4*4,64), nn.Linear(64,10) ) def forward(self, x): x = self.model1(x) return x # 准备数据集 train_data = torchvision.datasets.CIFAR10(\"./dataset\",train=True,transform=torchvision.transforms.ToTensor(),download=True) test_data = torchvision.datasets.CIFAR10(\"./dataset\",train=False,transform=torchvision.transforms.ToTensor(),download=True) # length 长度 train_data_size = len(train_data) test_data_size = len(test_data) # 如果train_data_size=10，则打印：训练数据集的长度为：10 print(\"训练数据集的长度：{}\".format(train_data_size)) print(\"测试数据集的长度：{}\".format(test_data_size)) # 利用 Dataloader 来加载数据集 train_dataloader = DataLoader(train_data, batch_size=64) test_dataloader = DataLoader(test_data, batch_size=64) # 创建网络模型 tudui = Tudui() # 损失函数 loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写 # 优化器 learning = 0.01 # 1e-2 就是 0.01 的意思 optimizer = torch.optim.SGD(tudui.parameters(),learning) # 随机梯度下降优化器 # 设置网络的一些参数 # 记录训练的次数 total_train_step = 0 # 记录测试的次数 total_test_step = 0 # 训练的轮次 epoch = 10 # 添加 tensorboard writer = SummaryWriter(\"logs\") for i in range(epoch): print(\"-----第 {} 轮训练开始-----\".format(i+1)) # 训练步骤开始 tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用 for data in train_dataloader: imgs, targets = data outputs = tudui(imgs) loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距 # 优化器对模型调优 optimizer.zero_grad() # 梯度清零 loss.backward() # 反向传播，计算损失函数的梯度 optimizer.step() # 根据梯度，对网络的参数进行调优 total_train_step = total_train_step + 1 if total_train_step % 100 == 0: print(\"训练次数：{}，Loss：{}\".format(total_train_step,loss.item())) # 方式二：获得loss值 writer.add_scalar(\"train_loss\",loss.item(),total_train_step) # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况） tudui.eval() # 当网络中有dropout层、batchnorm层时，这些层不能起作用 total_test_loss = 0 total_accuracy = 0 with torch.no_grad(): # 没有梯度了 for data in test_dataloader: # 测试数据集提取数据 imgs, targets = data outputs = tudui(imgs) loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失 total_test_loss = total_test_loss + loss.item() # 所有loss accuracy = (outputs.argmax(1) == targets).sum() total_accuracy = total_accuracy + accuracy print(\"整体测试集上的Loss：{}\".format(total_test_loss)) print(\"整体测试集上的正确率：{}\".format(total_accuracy/test_data_size)) writer.add_scalar(\"test_loss\",total_test_loss,total_test_step) writer.add_scalar(\"test_accuracy\",total_accuracy/test_data_size,total_test_step) total_test_step = total_test_step + 1 torch.save(tudui, \"./model/tudui_{}.pth\".format(i)) # 保存每一轮训练后的结果 #torch.save(tudui.state_dict(),\"tudui_{}.path\".format(i)) # 保存方式二 print(\"模型已保存\") writer.close() Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/transformer.html":{"url":"chapters/transformer.html","title":"transformer.md","summary":null,"keywords":"","body":"Transformer词向量模型介绍ArchitectureEmbeddingAttentionEncoderDecoderBertbert概况Architecture Transformer Attention Is All You Need 论文解读: Attention is All you need Hugging Face的GitHub代码库 通常而言，绝大部分NLP问题可以归入上图所示的四类任务中 序列标注: 这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题 它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别 分类任务: 比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可 句子关系判断: 比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，给定两个句子，模型判断出两个句子是否具备某种语义关系 生成式任务: 比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类，输入文本内容后，需要自主生成另外一段文字 预训练语言模型 目前有两种预训练语言模型用于下游任务的方法：feature-based(以ELMo为例)和fine-tuning(以BERT、GPT为例) 有专门的论文(To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks)讨论了这个话题 Feature-based Pre-Training: 在Feature-based Pre-Training中，首先使用大规模的未标记数据集对模型进行预训练 预训练任务通常是通过自监督学习或其他无监督学习方法来完成，例如预测下一个词语、图像的旋转角度等 预训练的目标是学习到具有良好表示能力的特征，能够捕捉数据中的一般性信息 预训练模型通常是一个通用的模型，不针对特定的任务。它学习到的特征表示可以应用于各种不同的任务 预训练模型可以作为迁移学习的基础，通过将其特征提取部分应用于具体的任务 Fine-tuning: Fine-tuning是在预训练模型的基础上，在特定任务的有标签数据集上进行进一步训练和优化 Fine-tuning阶段会调整预训练模型的权重和参数，以使其适应目标任务的特定要求 Fine-tuning过程中，通常会保持预训练模型的一部分权重固定，只更新部分权重，以保留预训练阶段学习到的通用特征表示 Fine-tuning旨在在特定任务的有限标记数据集上优化模型，使其更好地适应该任务的数据和特征 总结来说，Feature-based Pre-Training是通过在未标记数据上预训练模型来学习通用的特征表示，而Fine-tuning是在预训练模型的基础上，在特定任务的有标签数据上进行进一步优化和微调 Feature-based Pre-Training提供了一种学习通用特征表示的方式，而Fine-tuning则将这些通用特征应用于特定任务，以提升任务性能，一句话概括： Feature-based Pre-Training把输入转特征，特征丢给后面的模型(新模型)，其他就和它无关了 Fine-tuning是同一个网络结构，换了数据，可以固定或不固定前几层，继续训练 词向量 预训练语言模型的前世今生 - 从Word Embedding到BERT 年份 2013 年 2014 年 2015 年 2016 年 2017 年 技术 word2vec GloVe LSTM/Attention Self-Attention Transformer 年份 2018 年 2019 年 2020 年 技术 GPT/ELMo/BERT/GNN XLNet/BoBERTa/GPT-2/ERNIE/T5 GPT-3/ELECTRA/ALBERT One-hot编码：早期的自然语言处理中，词语通常被表示为离散的one-hot向量。每个词语都被表示为一个维度等于词汇表大小的向量，其中只有一个维度为1，其余维度都为0 这种表示方法无法捕捉词语之间的语义关系和相似度 Word Embedding：为了克服one-hot编码的局限性，提出了基于分布假设的词向量表示方法，即Word Embedding。Word Embedding使用低维实数向量表示词语，通过训练模型将词语映射到一个连续的向量空间中。其中，每个维度代表一个语义特征。Word Embedding能够捕捉到词语之间的语义关系和相似度，提供更丰富的表示 Word2Vec(静态)：Word2Vec是一种经典的词向量模型，由Tomas Mikolov等人于2013年提出。它基于神经网络模型，通过训练预测词语周围的上下文或预测目标词语。Word2Vec模型包括两种算法：CBOW(Continuous Bag-of-Words)和Skip-gram`。这两种算法使用浅层的神经网络来学习词向量，具有高效、快速训练的优势。Word2Vec模型能够生成静态的词向量，但无法捕捉词语的上下文相关特征 ELMo(Embeddings from Language Models)(动态)：ELMo是在Word2Vec之后提出的一种上下文相关的词向量表示方法，由Peters等人于2018年提出。ELMo利用双向语言模型，通过训练正向和逆向的LSTM模型来学习词语的上下文表示。ELMo能够根据上下文动态地生成词向量，捕捉到词语在不同上下文中的语义特征。与静态词向量不同，ELMo提供了更丰富、更具语义的词语表示，适用于各种自然语言处理任务 ELMo(Embeddings from Language Models)模型在训练过程中使用了双向长短期记忆网络（Bi-LSTM） 总的来说，历史发展中，从one-hot编码到Word Embedding，再到Word2Vec和ELMo，词向量表示方法逐渐从离散、静态的表示发展到了连续、上下文相关的表示。这些方法的提出和发展使得自然语言处理模型能够更好地理解和处理文本数据，提高了各种文本相关任务的性能 Word Embedding、Word2Vec和ELMo关系如下： Word2Vec是Word Embedding的一种具体实现方式。Word Embedding指的是将词语映射到低维实数向量空间的表示方法，而Word2Vec则是一种用于训练Word Embedding的算法 ELMo和Word2Vec是两种不同的词向量表示方法。ELMo是一种上下文相关的词向量表示方法，通过训练双向语言模型来学习词语在不同上下文中的动态表示。而Word2Vec是一种上下文无关的词向量表示方法，通过训练预测词语的上下文或目标词语来学习静态的词向量 模型介绍 介绍Transformer比较好的文章 一个是Jay Alammar可视化地介绍Transformer的博客文章The Illustrated Transformer，非常容易理解整个机制 哈佛大学NLP研究组写的The Annotated Transformer，代码原理双管齐下 Attention机制 Attention机制最早在视觉领域提出，2014年Google Mind发表了《Recurrent Models of Visual Attention》，使Attention机制流行起来，这篇论文采用了RNN模型，并加入了Attention机制来进行图像的分类 2005年，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，将attention机制首次应用在nlp领域，其采用Seq2Seq+Attention模型来进行机器翻译，并且得到了效果的提升，Seq2Seq With Attention中进行了介绍 2017 年，Google 机器翻译团队发表的《Attention is All You Need》中，完全抛弃了RNN和CNN等网络结构，而仅仅采用自注意力(self-attention)机制来学习文本表示来进行机器翻译任务，并且取得了很好的效果，注意力机制也成为了大家近期的研究热点 本文首先介绍常见的Attention机制，然后对论文《Attention is All You Need》进行介绍，该论文发表在NIPS 2017上 Architecture 模型结构如下 输入层 词嵌入编码层 位置编码层 Encoder 多头自注意力 残差连接 全连接网络 Dncoder 多头自注意力 多头注意力(不是自注意, 因为QK来自Encoder) 残差连接 全连接网络 模型整体结构如下所示 Transformer是一种基于自注意力机制的序列到序列模型，广泛应用于自然语言处理任务，如机器翻译、文本摘要和语言生成等 Transformer整体结构由以下几个主要组件组成： 编码器（Encoder）：编码器负责将输入序列（例如源语言句子）转换为一系列高级特征表示。它由多个相同的层堆叠而成，每个层都包含两个子层：多头自注意力机制和全连接前馈神经网络。自注意力机制允许模型对输入序列中的不同位置进行自适应地关注，从而捕捉序列中的上下文信息 解码器（Decoder）：解码器负责从编码器生成的特征表示中生成目标序列（例如目标语言句子）。解码器也由多个相同的层堆叠而成，每个层包含三个子层：多头自注意力机制、编码器-解码器注意力机制和全连接前馈神经网络。编码器-解码器注意力机制用于在生成目标序列时，引入对源语言句子的关注 自注意力机制（Self-Attention）：自注意力机制是Transformer的关键组件之一。它允许模型在进行编码或解码时，根据输入序列中不同位置之间的关系，动态地计算注意力权重。通过自适应地关注不同位置的信息，自注意力机制能够捕捉输入序列中的上下文信息，提供更全面的表示 注意力机制（Attention）：除了自注意力机制，Transformer还使用编码器-解码器注意力机制。这种注意力机制允许解码器在生成目标序列时，对编码器输出的特征表示进行关注。它能够帮助解码器对源语言句子中与当前生成位置相关的信息进行处理 前馈神经网络（Feed-Forward Network）：Transformer中的每个子层都包含一个前馈神经网络。该网络由两个全连接层组成，通过应用非线性激活函数（如ReLU）来对特征表示进行映射和变换。前馈神经网络有助于捕捉特征之间的非线性关系 通过编码器和解码器的组合，Transformer模型能够将输入序列转换为输出序列，实现不同的序列到序列任务 它的并行计算性质和自注意力机制的能力使得它在处理长序列和捕捉全局依赖关系方面具有优势，成为自然语言处理领域的重要模型，更详细的模型结构如下所示 Embedding Embedding层是Transformer模型中的一个重要组成部分，用于将离散的输入序列（如单词、字符等）映射到连续的低维向量表示 它负责将输入的符号型数据转换为密集的实数向量，从而能够在模型中进行有效的学习和处理 在Transformer中，Embedding层主要有两个作用： 词嵌入（Word Embedding）：对于自然语言处理任务，Embedding层将每个词汇或字符映射到一个低维的连续向量表示，称为词嵌入或字符嵌入。这些嵌入向量捕捉了词汇或字符之间的语义和语法关系，能够编码单词的上下文信息，使得模型能够更好地理解和表示输入数据 位置编码（Positional Encoding）：Transformer模型中没有使用循环神经网络或卷积神经网络，因此无法直接捕捉输入序列中顺序信息。为了引入位置信息，Embedding层会添加位置编码到词嵌入中。位置编码是一种用于表示输入序列位置的向量，它提供了关于词汇在序列中相对位置的信息，帮助模型理解序列中的顺序关系 在实现上，Embedding层可以使用一个矩阵作为参数来进行词嵌入的查找。每个词汇对应矩阵中的一行，通过查找输入序列中的词汇对应的行向量，得到词嵌入表示 位置编码通常使用正弦和余弦函数的组合来计算，以获取不同位置的编码向量 需要注意的是，Embedding层的参数通常是在模型训练的过程中学习得到的，根据任务和数据来调整嵌入向量的表示能力。通过Embedding层，模型能够在低维连续向量空间中对输入序列进行表征和建模，从而更好地处理自然语言处理等任务 导入库 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: embedding.py @Description: @time: 2023/2/19 15:00 \"\"\" import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable import math import matplotlib.pyplot as plt import numpy as np import copy Embedding层定义 class Embeddings(nn.Module): def __init__(self, d_model: int, vocab: int): \"\"\" 构建Embedding类来实现文本嵌入层 :param d_model: 词嵌入的维度 :param vocab: 词表的大小 \"\"\" super(Embeddings, self).__init__() # 定义Embedding层 self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) PositionalEncoding层 # 定义位置编码器类, 我们同样把它看做一个层,因此会继承nn.Module class PositionalEncoding(nn.Module): def __init__(self, d_model: int, dropout: float, max_len: int = 5000): \"\"\" 位置编码器类的初始化函数, 共有三个参数 :param d_model: 词嵌入维度 :param dropout: 置0比率 :param max_len: 每个句子的最大长度 :return: \"\"\" super(PositionalEncoding, self).__init__() # 实例化nn中预定义的Dropout层, 并将dropout传入其中,获得对象self.dropout self.dropout = nn.Dropout(p=dropout) # 初始化一个位置编码矩阵,它是一个0阵, 矩阵的大小是max_len * d_model pe = torch.zeros(max_len, d_model) # 初始化一个绝对位置矩阵, 在我们这里, 词汇的绝对位置就是用它的索引去表示 # 所以我们首先使用arange方法获得一个连续自然数向量, 然后再使用unsqueeze方法拓展向量维度 # #又因为参数传的是1, 代表矩阵拓展的位置, 会使向量变成一个max_len * 1的矩阵 position = torch.arange(0, max_len).unsqueeze(1) # 绝对位置矩阵初始化之后, 接下来就是考虑如何将这些位置信息加入到位置编码矩阵中 # 最简单思路就是先将max_len * 1的绝对位置矩阵, 变换成max_len * d_model形状, 然后覆盖原来的初始位置编码矩阵即可 # 要做这种矩阵变换, 就需要一个1 * d_model形状的变换矩阵div_term, 我们对这个变换矩阵的要求除了形状外 # 还希望它能够将自然数的绝对位置编码缩放成足够小的数字, 有助于在之后的梯度下降过程中更快的收敛 # 首先使用arange获得一个自然数矩阵, 但是细心的同学们会发现, 我们这里并没有按照预计的一样初始化一个1 * d_model的矩阵 # 而是有了一个跳跃，只初始化了一半即1*d_mode1/2的矩阵. 为什么是一半呢, 其实这里并不是真正意义上的初始化 # 我们可以把它看作是初始化了两次, 而每次初始化的变换矩阵会做不同的处理, 第一次初始化的变换矩阵分布在正弦波上, 第二次初始化的变换矩阵分布在余弦波上 # 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上, 组成最终的位置编码矩阵 div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) # 这样我们就得到了位置编码矩阵pe, pe现在还是个二维矩阵，要想和embedding的输出(一个三维张量)相加, 需要扩展维度 pe = pe.unsqueeze(0) # 我们把它认为是对模型效果有帮助的, 但是却不是模型结构中超参数或者参数,不需要随着优化步骤优化 # 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载 self.register_buffer('pe', pe) def forward(self, x): \"\"\" :param x: 文本序列的词嵌入表示 \"\"\" # 根据句子最大长度切割, pe不需要做梯度求解 x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 实际测试 + 位置编码可视化 if __name__ == '__main__': p_d_model = 512 p_vocab = 1000 p_dropout: float = 0.1 p_max_len = 60 # 词嵌入测试 x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]])) emb = Embeddings(d_model=p_d_model, vocab=p_vocab) embr = emb(x) print(\"embr\", embr) print(\"embr size\", embr.shape) # 位置编码测试 pe = PositionalEncoding(d_model=p_d_model, dropout=p_dropout, max_len=p_max_len) pe_result = pe(embr) print(\"pe_result\", pe_result) print(\"pe_result size\", pe_result.shape) # 创建一张15x5大小的画布 plt.figure(figsize=(15, 5)) # 实例化PositionalEncoding类得到pe对象, 输入参数是20和0 pe = PositionalEncoding(20, 0) # 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, # 且这个tensor里的数值都是0, 被处理后相当于位置编码张量 y = pe(Variable(torch.zeros(1, 100, 20))) # 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值 # 因为总共有20维之多, 我们这里只查看4,5,6,7维的值 plt.plot(np.arange(100), y[0, :, 4:8].data.numpy()) # 在画布上填写维度提示信息 plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]]) plt.show() embr tensor([[[-17.5113, -6.0699, 11.6839, ..., -8.1281, -7.7986, 35.1275], [ -6.3789, -7.7614, 13.2975, ..., 16.8397, -31.3230, -68.4385], [ -4.1841, 8.4322, 34.6418, ..., 38.4747, -4.9060, 25.4163], [-23.4562, -28.9742, 18.1234, ..., 38.6039, 15.0049, -2.8916]], [[-21.7485, 0.3263, 54.4449, ..., -18.3120, -15.5987, -11.4275], [ -0.6414, 2.9492, -32.3063, ..., -21.9781, -16.3307, -15.4014], [-16.1775, 20.8547, -21.0333, ..., -11.7583, -7.2429, 5.8607], [ -4.7708, -51.9955, 14.8529, ..., 21.0973, 13.4664, -10.8492]]], grad_fn=) embr size torch.Size([2, 4, 512]) pe_result tensor([[[-19.4569, -5.6332, 12.9821, ..., -7.9201, -0.0000, 0.0000], [ -6.1526, -8.0235, 15.6882, ..., 19.8219, -34.8032, -74.9317], [ -3.6387, 8.9068, 39.5314, ..., 43.8608, -5.4509, 29.3514], [-25.9057, -33.2935, 20.4094, ..., 44.0043, 16.6725, -2.1017]], [[-24.1650, 1.4736, 0.0000, ..., -19.2356, -17.3319, -11.5861], [ 0.2223, 3.8772, -34.9827, ..., -23.3090, -18.1452, -16.0015], [-16.9647, 22.7095, -22.3299, ..., -11.9537, -8.0475, 7.6230], [ -0.0000, -58.8727, 0.0000, ..., 24.5526, 14.9630, -0.0000]]], grad_fn=) pe_result size torch.Size([2, 4, 512]) Attention 超详细图解Self-Attention的那些事儿 除了Scaled Dot-Product Attention，Transformer模型中还有几种常见的注意力机制 点积注意力(Dot-Product Attention)：它是Scaled Dot-Product Attention的简化版本，直接计算查询（Q）和键（K）之间的点积，然后通过softmax函数将结果转化为注意力权重。点积注意力相比于Scaled Dot-Product Attention没有进行缩放操作 加性注意力(Additive Attention)：加性注意力使用了一个额外的前馈神经网络来计算注意力权重。它通过将查询（Q）和键（K）映射到相同的低维空间，然后计算它们的相似度得分，最后将相似度得分通过softmax函数进行归一化。加性注意力在一些场景中能够更好地捕捉输入序列之间的非线性关系 缩放点积注意力(Scaled Dot-Product Attention)：它是Transformer中最常用的注意力机制。在计算注意力权重时，对点积注意力进行了缩放操作，通过除以特征维度的平方根，以减小注意力权重的大小变化。这有助于防止梯度消失或梯度爆炸，并使得模型更稳定 按位置加权注意力(Relative Positional Attention)：这种注意力机制考虑了位置信息对注意力计算的影响。它引入了位置编码，通过计算相对位置的差异，对注意力权重进行调整。这种注意力机制在处理序列任务时能够更好地建模长距离依赖关系 在Transformer中使用的Attention是Scaled Dot-Product Attention，是归一化的点乘Attention，假设输入的query q 、key维度、value维度为d_{k}，那么就计算query和每个key 的点乘操作，并除以\\sqrt{d_{k}}，然后应用Softmax函数计算权重 \\operatorname{Attention}\\left(Q_{i}, K_{i}, V_{i}\\right)=\\operatorname{softmax}\\left(\\frac{Q_{i} K_{i}^{T}}{\\sqrt{d_{k}}}\\right) V_{i} 在实践中，将query和keys、values分别处理为矩阵Q, K, V，那么计算输出矩阵为: \\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V 其中Q \\in R^{m \\times d_{k}}, K \\in R^{m \\times d_{k}}, V \\in R^{m \\times d_{k}}，输出矩阵维度为R^{m \\times d_{k}}，其中m为句子长度，d_k为Embedding后的特征长度 其中QK的维度为m \\times m，表示句子中每个字之间的关注度(self-attention)， 而(QK)V的维度为m \\times d_k，表示attention下的句子特征向量，QKV如下所示 Q=K=V=\\left[ \\begin{matrix} d_{11} & d_{12} & \\cdots & d_{1 d_k} \\\\ d_{21} & d_{22} & \\cdots & d_{2 d_k} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ d_{m1} & d_{m2} & \\cdots & d_{m d_k} \\\\ \\end{matrix} \\right] _{m \\times d_k} attention代码如下: 其中qkv是x经过线性变换之后的结果 def attention(query, key, value, mask=None, dropout=None): \"Compute 'Scaled Dot Product Attention'\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim = -1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn 矩阵与其转置的乘积 向量数量积的几何意义：一个向量在另一个向量上的投影 向量的相似性是用两者的角度余弦来度量，余弦值越大则两者越相似 cos \\theta = \\frac {x^Ty}{||x|| \\cdot ||y||} 而余弦值等于两者内积与两者模长积的比，当两个向量模长固定的情形下，内积大小则反映了两者相似性的大小 import numpy as np mat_a = np.array([1, 2, 2], [4, 5, 8]) np.matmul(mat_a,mat_a.T) Out[5]: array([[ 9, 30], [ 30, 105]]) 那么Scaled Dot-Product Attention的示意图如下所示，Mask是可选的，如果是能够获取到所有时刻的输入(K, V)，那么就不使用Mask；如果是不能获取到，那么就需要使用Mask 使用了Mask的Transformer模型也被称为Transformer Decoder，不使用Mask的Transformer模型也被称为Transformer Encoder 如果只对Q、K、V做一次这样的权重操作是不够的，这里提出了Multi-Head Attention操作，包括： 首先对Q, \\mathrm{~K}, \\mathrm{~V}做一次线性映射，将输入维度均为d_{\\text {model }}的Q, K, V矩阵映射到Q \\in R^{m \\times d_{k}}, K \\in R^{m \\times d_{k}}, V \\in R^{m \\times d_{v}} 然后再采用Scaled Dot-Product Attention算出结果 多次进行上述两步操作，然后将得到的结果进行合并 将合并的结果进行线性变换 多头注意力的引入有以下几个目的： 平行计算：通过使用多个注意力头，可以并行地计算注意力权重和加权求和，从而加快模型的计算速度。每个注意力头都专注于不同的表示子空间，因此可以独立地计算和处理信息，提高模型的效率。 多样性表达：每个注意力头学习到的表示子空间不同，通过多个注意力头的组合，可以获得更丰富、多样性的表示。这有助于模型更好地捕捉输入序列中的不同特征和关系，提高模型的表达能力。 组合注意力：多头注意力允许模型在不同的表示子空间上进行多次注意力计算，并将这些计算的结果进行组合。这种组合能够从不同的关注角度和视角来处理输入序列，帮助模型更全面地理解序列中的信息。 通过这些方式，多头注意力可以提供更灵活、更强大的建模能力，增强模型对序列中的长距离依赖关系、全局上下文和特征之间复杂关系的建模能力 它是Transformer模型在处理自然语言处理任务时取得成功的重要组成部分，总结来说公式如下所示 \\begin{array}{l} \\operatorname{Attention}(Q, K, V)=\\text { Concat }\\left(\\text {head}_{1}, \\text {head}_{2}, \\cdots, \\text {head}_{h}\\right) W^{O} \\\\ where \\quad {head}_{i}=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right) \\\\ \\end{array} 其中第1步的线性变换参数为W_{i}^{Q} \\in R^{d_{\\text {model }} \\times d_{k}}, W_{i}^{K} \\in R^{d_{\\text {model }} \\times d_{k}}, W_{i}^{V} \\in R^{d_{\\text {model }} \\times d_{v}}，第4 步的线性变化参数为W^{O} \\in R^{h d_{v} \\times d_{\\text {model }}}，而第三步计算的次数是h 在论文中取 d_{\\text {model }}=512，表示每个时刻的输入维度和输出维度，h=8 表示8次Attention操作，d_{k}=d_{v}=\\frac{d_{\\text {model }}}{h}=64 表示经过线性变换之后、进行Attention操作之前的维度 进行一次Attention之后输出的矩阵维度是R^{m \\times d_{v}}=R^{m \\times 64}，然后进行\\mathrm{h}=8次操作合并之后输出的结果是R^{m \\times\\left(h \\times d_{v}\\right)}=R^{m \\times 512}，因此输入和输出的矩阵维度相同 这样输出的矩阵R^{m \\times 512}，每行的向量都是对V向量中每一行v_{i}的加权，示意图如上所示 多头注意力代码实现 class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \"Take in model size and number of heads.\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h # 多头在这里体现 self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = \\ [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # 3) \"Concat\" using a view and apply a final linear. x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) # 4) 线性变换投影回原始表示维度 return self.linears[-1](x) 如果不好理解可以看下这部分代码 def multihead_attention(Q, K, V, num_heads): # 线性变换得到查询、键和值的表示 Q_transformed = linear_transform(Q) K_transformed = linear_transform(K) V_transformed = linear_transform(V) # 分割头 Q_heads = split_heads(Q_transformed, num_heads) K_heads = split_heads(K_transformed, num_heads) V_heads = split_heads(V_transformed, num_heads) # 每个头的注意力计算 attention_heads = [] for i in range(num_heads): attention_head = scaled_dot_product_attention(Q_heads[i], K_heads[i], V_heads[i]) attention_heads.append(attention_head) # 拼接注意力头 concatenated_attention = concatenate_heads(attention_heads) # 线性变换投影回原始表示维度 output = linear_transform(concatenated_attention) return output Encoder 编码器和解码器如下所示 Decoder 在encoder部分中的self-attention是不需要mask的，而decoder部分的self-attention是需要mask的，因为正是有了mask遮挡后面的信息，才能将transformer用来做推理 编码器和解码器如下所示 编码器把最后一层的KV喂给了编码器，此时Q来源解码器，K=V来源于编码器，是为了让解码器能够在生成输出时使用编码器的信息 通过给解码器提供编码器的键和值矩阵，可以实现以下两个目的： 上下文信息传递：编码器中的自注意力机制能够捕捉到输入序列中的局部和全局关系，生成对应的键和值。将这些键和值传递给解码器，可以将编码器的上下文信息传递给解码器，以帮助解码器在生成输出时了解输入序列的相关内容。 对齐和信息提取：解码器可以通过注意力机制对编码器的键和值进行加权汇总，以获取与当前解码位置相关的信息。通过计算解码器当前位置与编码器中每个位置之间的注意力分数，可以实现对齐和信息提取，使解码器能够专注于与当前位置相关的输入信息。 总结来说，通过将编码器的键和值矩阵提供给解码器，可以实现上下文信息传递和对齐机制，帮助解码器在生成输出时利用编码器的信息，从而改善模型的性能和输出质量 mask Attention Mask Padding Mask Bert The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) ELMo、GPT 和 BERT 三者的区别 GPT：GPT 使用Transformer Decoder作为特征提取器，实现了单向编码、具有良好的文本生成能力，然而当前词的语义只能由其前序词决定，并且在语义理解上不足 BERT：使用了Transformer Encoder作为特征提取器，为双向编码器，并使用了与其配套的掩码训练方法。虽然使用双向编码让BERT不再具有文本生成能力，但是BERT的语义信息提取能力更强 ELMo: 使用自左向右编码和自右向左编码的两个LSTM网络，分别以P\\left(w_{i} \\mid w_{1}, \\cdots, w_{i-1}\\right)和P\\left(w_{i} \\mid w_{i+1}, \\cdots, w_{n}\\right)为目标函数独立训练，将训练得到的特征向量以拼接的形式实现双向编码，本质上还是单向编码，只不过是两个方向上的单向编码的拼接而成的双向编码 bert是自编码模型，而gpt是自回归模型 bert概况 BERT(Bidirectional Encoder Representations from Transformers)模型的编码器由多个Transformer编码器层组成，通常使用了多个重复的编码器来形成深层的表示 每个BERT编码器层包含了以下组件： 多头自注意力（Multi-Head Self-Attention）：该层使用多头自注意力机制来对输入序列进行建模。自注意力允许模型在处理序列时关注不同位置之间的相关性，有助于捕捉上下文信息 前馈神经网络（Feed-Forward Neural Network）：在自注意力层后面是一个前馈神经网络。该网络通常由两个线性层和激活函数（如ReLU）组成，用于对自注意力输出进行非线性变换和特征提取 残差连接（Residual Connections）和层归一化（Layer Normalization）：在每个子层（自注意力和前馈神经网络）之后都应用了残差连接和层归一化操作。这些操作有助于缓解梯度消失问题，并提供更稳定和高效的训练 BERT模型中通常会堆叠多个编码器层来形成深层表示。每个编码器层的输出会作为下一层的输入，通过多次重复这个过程，可以逐渐丰富输入序列的表示能力 值得注意的是，BERT模型还在编码器输入的开头添加了特殊的标记，如[CLS]（用于分类任务）和[SEP]（用于分隔输入）。这些特殊标记有助于模型在处理不同任务时进行序列级别的操作和分类 总结起来，BERT的编码器由多个Transformer编码器层组成，每个编码器层由多头自注意力、前馈神经网络和残差连接/层归一化组成。通过堆叠多个编码器层，BERT模型可以获得深层、高质量的语言表示 Architecture 输入 训练方式 由于无法使用标准语言模型的训练模式，BERT借鉴完形填空任务和CBOW的思想，使用语言掩码模型(MLM)方法训练模型 训练中的mask MLM方法也就是随机去掉句子中的部分token(单词)，然后模型来预测被去掉的token是什么。这样实际上已经不是传统的神经网络语言模型(类似于生成模型)了，而是单纯作为分类问题，根据这个时刻的hidden state来预测这个时刻的token应该是什么，而不是预测下一个时刻的词的概率分布了 随机去掉的token被称作掩码词，在训练中，掩码词将以15%的概率被替换成[MASK]，也就是说随机mask语料中15%的token，这个操作则称为掩码操作 在选择15%的词作为掩码词后这些掩码词有三类替换选项： 80% 练样本中：将选中的词用 [MASK] 来代替 10% 的训练样本中：选中的词不发生变化，该做法是为了缓解训练文本和预测文本的偏差带来的性能损失 10% 的训练样本中：将选中的词用任意的词来进行代替，该做法是为了让 BERT 学会根据上下文信息自动纠错 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/图像分割算法.html":{"url":"chapters/图像分割算法.html","title":"图像分割算法.md","summary":"图像分割算法","keywords":"","body":"图像分割UNet语义分割实例分割 图像分割 UNet U-Net: Convolutional Networks for Biomedical Image Segmentation 2015 图像分割必备知识点 | Unet详解 理论+ 代码 U-Net是一种常用的卷积神经网络结构，特别适用于图像分割任务。它由Olaf Ronneberger等人在2015年提出，并因其在生物医学图像分割中的出色表现而受到广泛关注 U-Net的整体结构呈U字形，因此得名。它包含两个主要部分：编码器(Encoder)和解码器(Decoder) 编码器由一系列卷积层和池化层组成，用于提取图像中的特征并逐渐缩小感受野 解码器由一系列反卷积层和跳跃连接(Skip Connection)组成，用于将编码器提取的特征映射重新放大，并与解码器中的特征进行融合 跳跃连接的作用是将底层的细节信息传递给解码器，有助于更好地恢复分割结果的细节 U-Net的设计思想是在特征提取的同时保留更多的上下文信息，以及保留高分辨率的细节。通过编码器和解码器之间的特征传递和融合，U-Net能够同时获得局部和全局的上下文信息，并生成细致准确的分割结果 由于其简单而有效的结构，U-Net在医学图像分割、语义分割、遥感图像分析等领域得到了广泛应用，并成为图像分割任务中的经典模型之一 语义分割 实例分割 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/图像分类算法.html":{"url":"chapters/图像分类算法.html","title":"图像分类算法.md","summary":"基于深度学习的图像分类算法","keywords":"","body":"图像分类经典模型综述分类数据集ImageNettorchvisionLenetAlexNetVgg概述模型缺点GoogleNetInception-v1Inception-v2Inception-v3Inception-v4与ResNetResNet模型残差DenseNet概述模型ResnextVitDeiTModel知识蒸馏Clip概述zero-shot分类TOnICS 图像分类 计算机视觉中图像分类任务脉络梳理 An Analysis of Deep Neural Network Models for Practical Applications 经典模型综述 模型综述 LeNet-5: 早期卷积神经网络中最有代表性的架构，是Yann LeCun在1998年设计的，用于手写数字识别的卷积神经网络 AlexNet: 2012年ILSVRC冠军，6千万参数。由于准确率远超传统方法的第二名（top5错误率为15.3%，第二名为26.2%），引起了很大的轰动。自此之后，CNN成为在图像识别分类的核心算法模型，带来了深度学习的大爆发 ZF-Net: 2013年ILSVRC冠军，结构和AlexNet区别不大，分类效果也差不多。这篇文章的贡献在于，提出了一种CNN特征可视化方法：反池化、反激活、反卷积，从而成为CNN特征可视化的开山之作 GoogLeNet: 2014年ILSVRC冠军网络。同样也是5+3的模式（以池化层为界），参数量约为5百万，核心模块是Inception Module。Inception历经了V1、V2、V3、V4等多个版本的发展，不断趋于完善 Inception V1：主要提出了多分支(多分辨率的filter组合)的网络 Inception V2： 主要提出了BN层，提高网络性能(减少梯度消失和爆炸、防止过拟合、代替dropout层、使初始化学习参数更大) Inception V3：主要提出了分解卷积，把大卷积因式分解成小卷积和非对称卷积 VGG: 2014年ILSVRC亚军网络，1.38亿参数。由于网络结构十分简单，很适合迁移学习 ResNet: 2015年ILSVRC冠军网络。核心是带短连接的残差模块，其中主路径有两层卷积核（Res34），短连接把模块的输入信息直接和经过两次卷积之后的信息融合，相当于加了一个恒等变换。短连接是深度学习又一重要思想，除计算机视觉外，短连接思想也被用到了机器翻译、语音识别/合成领域 ResNeXt: ResNet的另一改进。主要是采用了VGG堆叠思想和Inception的split-transform-merge思想，在不增加参数复杂度的前提下提高准确率。ResNeXt发现，增加分支数是比加深或加宽更有效地提升网络性能的方式 DenseNet: CVPR2017的oral。主要思想是将每一层都与后面的层连接起来，如果一个网络中有L层，那么会有L(L+1)/2个连接。通过这样的密集连接，每一层在正向时候都能直接接受原始输入信号，在反向时候也都能直接接受损失函数的梯度，即这种连接方式使得特征和梯度的传递更加有效，网络也就更加容易训练 当然，如果全部采用这种密集连接的方式，特征图的厚度就会很大。于是采用两种方式降低参数量：一是将密集连接的层做成一个模块，整个网络采用模块堆叠的方式，而不是所有层全部密集连接；二是在dense block中引入bottleneck layer，即卷积3x3前增加1x1卷积，以此来减少feature map数量 缺点是太吃显存。通常占用显存的主要是推断过程中产生的feature map和参数量。有些框架会有优化，自动把比较靠前的层的feature map释放掉，所以显存就会减少，或者inplace操作通过重新计算的方法减少一部分显存，但是densenet因为需要重复利用比较靠前的feature map，所以无法释放，导致显存占用过大 SENet: 2017年ILSVRC冠军网络。是一个模块，可以和其他的网络架构结合，比如GoogLeNet、ResNet等 历史脉络 1998 2012 2013 2014 2014 LeNet-5 AlexNet ZF-Net GoogLeNetV1、V2、V3、V4 VGG 2015 2016 2017 2017 ResNet ResNeXt DenseNet SENet 2020 2021 2022 2023 2024 Vit DeiT、Clip TOnICS 分类数据集 ImageNet ImageNet是计算机视觉领域常用的数据集之一。在 图像分类、目标分割和 目标检测中有着无法撼动的地位 ImageNet最初是由李飞飞等人在CVPR 2009年发表的论文——「ImageNet: A Large-Scale Hierarchical Image Database」中发布的 多年来，ImageNet 的相关论文对业内有极大的影响 ImageNet本身则是一个海量的带标注图像数据集。通过众包等方式进行标注，从2007年开始直到2009年完成。ImageNet有超过1500万张图片，仅汽车图像的数量达到了70万张，类别数量为2567个。如此巨量、 标注错误极低且免费的数据集，已经成为图像处理领域研究者首先接触的数据集之一 毫不夸张的说，ImageNet是图像处理算法的试金石。从2010年起，每年ImageNet官方会举办挑战赛。2017年后的比赛由Kaggle社区主持。自2012年Hinton等的团队提出AlexNet开始，每年都有层出不穷的模型希望在ImageNet排行榜上取得一席之地 torchvision Models and pre-trained weights — Torchvision main documentation (pytorch.org) Datasets torchvision是PyTorch库中的一个子模块，专门用于处理计算机视觉任务。它提供了许多有用的函数、工具和预训练模型，使得处理图像和视频数据变得更加简单和高效 torchvision的功能主要分为以下几个方面： 数据集和数据加载：torchvision提供了常见的计算机视觉数据集，如MNIST、CIFAR10、ImageNet等。它还提供了方便的数据加载函数和数据转换工具，使得加载和预处理数据变得简单。可以使用这些功能来准备训练数据集、验证数据集和测试数据集 数据转换：torchvision包含了各种常用的数据转换操作，例如图像缩放、裁剪、旋转、翻转、标准化等。这些转换操作可以方便地应用于数据集，以增强数据的多样性和适应模型的需求 模型和预训练模型：torchvision提供了一些经典的计算机视觉模型，如AlexNet、VGG、ResNet、Inception等。这些模型都在大规模图像数据集上进行了预训练，可以用于图像分类、目标检测、语义分割等任务。此外，torchvision还提供了加载和使用这些预训练模型的便捷接口 图像工具：torchvision还包含了一些图像处理工具，如绘制边界框、绘制图像网格、绘制类别标签等。这些工具可以用于可视化和调试模型的输出结果 总之，torchvision是一个功能强大的PyTorch模块，提供了许多处理计算机视觉任务所需的工具和功能。它简化了数据加载、数据转换、模型加载和预测等操作，为计算机视觉研究人员和开发者提供了便利 Lenet Gradient-Based Learning Applied to Document Recognition 1998 LeNet-5, convolutional neural networks 手写字体识别模型LeNet5诞生于1994年，是最早的卷积神经网络之一。LeNet5通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点 LeNet是由Yann Lecun(2018年图灵奖得主，CNN的缔造者)创造的CNN经典网络，是卷积神经网络史上的开篇之作 代码 import torch import torch.nn as nn import torch.nn.functional as F class LeNet5(nn.Module): def __init__(self, num_classes, grayscale=False): \"\"\" num_classes: 分类的数量 grayscale：是否为灰度图 \"\"\" super(LeNet5, self).__init__() self.grayscale = grayscale self.num_classes = num_classes if self.grayscale: # 可以适用单通道和三通道的图像 in_channels = 1 else: in_channels = 3 # 卷积神经网络 self.features = nn.Sequential( nn.Conv2d(in_channels, 6, kernel_size=5), nn.ReLU(), nn.MaxPool2d(kernel_size=2), nn.Conv2d(6, 16, kernel_size=5), nn.ReLU(), nn.MaxPool2d(kernel_size=2) # 原始的模型使用的是 平均池化 ) # 分类器 self.classifier = nn.Sequential( nn.Linear(16*5*5, 120), # 这里把第三个卷积当作是全连接层了 nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, num_classes) ) def forward(self, x): x = self.features(x) # 输出 16*5*5 特征图 x = torch.flatten(x, 1) # 展平 （1， 16*5*5） logits = self.classifier(x) # 输出 10 probas = F.softmax(logits, dim=1) return logits, probas AlexNet AlexNet与LeNet区别： 层数更多: 相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层 激活函数: AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数 dropout: AlexNet通过dropout来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法 数据增强: AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合 最大池化: 用MaxPooling而不是AvgPooling 模型结构比较 Vgg Very Deep Convolutional Networks For Large-Scale Image Recognition VGG — Torchvision main documentation (pytorch.org) 概述 快速理解VGG网络 VGG(Visual Geometry Group)是一种经典的卷积神经网络架构，是牛津大学计算机视觉组(Visual Geometry Group)和谷歌DeepMind一起研究出来的深度卷积神经网络。其在在2014年的ImageNet大规模视觉识别挑战(ILSVRC-2014)中获得了亚军，其主要贡献是通过增加网络的深度来提高准确率，当年获得冠军的是GoogLeNet 虽然其屈居亚军，但是由于其规律的设计、简洁可堆叠的卷积块，且在其他数据集上都有着很好的表现，从而被人们广泛使用，从这点上还是超过了GoogLenet VGG16相比AlexNet的一个改进是采用连续的几个3 \\times 3的卷积核代替AlexNet中的较大卷积核(11 \\times 11，7 \\times 7，5 \\times 5) VGG网络的核心思想 使用多个连续的 3 \\times 3卷积层来替代较大感受野的卷积层，这种设计的优势 可以增加网络的深度，使网络能够更好地捕捉图像的细节和复杂特征 对于给定的感受野(与输出有关的输入图片的局部大小)，采用堆积的小卷积核是优于采用大的卷积核 参数更少: 比如，3个步长为1的 3 \\times 3卷积核的一层层叠加作用可看成一个大小为7的感受野(其实就表示3个 3 \\times 3连续卷积相当于一个 7 \\times 7卷积) 其参数总量为 3 \\times (9 \\times C^2) ，如果直接使用 7 \\times 7卷积核，其参数总量为 49 \\times C^2 ，这里C指的是输入和输出的通道数 很明显减少了参数，而且3x3卷积核有利于更好地保持图像性质 上图就是用两个 3 \\times 3卷积级联(叠加)起来代替一个 5 \\times 5卷积，同理可以用三个 3 \\times 3卷积级联(叠加)起来代替一个 7 \\times 7 卷积 简洁一致 VGG网络的一个重要特点是其简洁而一致的结构。它使用了小尺寸的卷积核( 3 \\times 3)，并且在每个卷积层块中都使用了相同数量的卷积层和池化层，这种设计使得网络的结构非常规整，方便理解和实现 VGG网络的架构可以根据深度的不同进行分类，最常见的是VGG16和VGG19。VGG16包含16个卷积层和3个全连接层，而VGG19则包含19个卷积层和3个全连接层。这些网络在ImageNet图像分类任务上取得了很好的性能，成为了后续卷积神经网络设计的重要参考 尽管VGG网络已经被更先进的网络架构所取代，但其简洁而一致的结构以及良好的性能使其仍然被广泛应用于图像分类、特征提取和迁移学习等任务。同时，VGG网络也为后续深度学习研究提供了重要的启示，尤其是关于网络深度和卷积核尺寸对性能的影响 模型 整体结构 VGGNet以下6种不同结构，我们以通常所说的VGG-16(即下图D列)为例，展示其结构示意图 官方给出的VGG系列神经网络的参数量如下： Network A, A-LRN B C D E 参数量(in millions) 133 133 134 138 144 对于VGG16来讲，它的网络结结构图就如下所示 vgg-block块由n个相同结构的卷积层+1个的池化层构成，意味着输入和输出的尺寸一样，且卷积层可以堆叠复用 对于Vgg-16，整个网络有5个vgg-block块和5个maxpool层逐个相连，然后进入FC层，直到最后1000路softmax输出 来计算一下VGG16的参数量 layer shape filter 参数数量(带bias) 2-block 224 \\times 224 \\times 64 3 \\times 3 \\times 3 \\times 64 1792+36864(3 \\times 3 \\times 64 \\times 64 \\times 1) 4-block 112 \\times 112 \\times 128 3 \\times 3 \\times 64 \\times 128 73856+147456(3 \\times 3 \\times 128 \\times 128 \\times 1) 6-block 56 \\times 56 \\times 256 3 \\times 3 \\times 128 \\times 256 295168+1179648(3 \\times 3 \\times 256 \\times 256 \\times 2) 8-block 55 \\times 55 \\times 96 3 \\times 3 \\times 256 \\times 512 1180160+4718592(3 \\times 3 \\times 512 \\times 512 \\times 2) 10-block 28 \\times 28 \\times 512 3 \\times 3 \\times 512 \\times 512 2359808+4718592(3 \\times 3 \\times 512 \\times 512 \\times 2) 12-Dense 1 \\times 1 \\times 4096 4096 \\times 25088+4096=102764544 13-Dense 1 \\times 1 \\times 4096 4096 \\times 4096+4096=16781312 14-Dense 1 \\times 1 \\times 1000 1000 \\times 4096+1000=4097000 总数 138354792(1.38亿) VGG的特点 vgg-block内的卷积层都是同结构的 池化层都得上一层的卷积层特征缩减一半 深度较深，参数量够大 较小的filter size/kernel size 数据增强方面 VGG网络中，数据增强使用的是Multi-Scale 这里的Multi-Scale主要是将图像放大到随机的大小，然后再裁剪到224*224的图像 核心代码-经典卷积神经网络——VGG import torch from torch import nn from torchvision import transforms import torchvision from torch.utils import data from d2l import torch as d2l import numpy as np import matplotlib.pyplot as plt # 设计VGG块，多个卷积过后一个最大池化层 # 卷积过后的输入输出图片大小不变，通道有变化 # 经过最大池化后，宽高缩减一半 def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) # VGG架构，VGG块(卷积层，outtput),经过五层VGG块过后，宽高为（7，7） # 这个架构可以称为VGG-11,1+1+2*3+1+1+1 = 11 conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) def vgg(conv_arch): conv_blocks = [] in_channels = 1 for (num_convs, out_channels) in conv_arch: conv_blocks.append(vgg_block(num_convs,in_channels,out_channels)) in_channels = out_channels return nn.Sequential( *conv_blocks, nn.Flatten(), # 全连接层部分 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10) ) net = vgg(conv_arch) # 观察每个层的输出情况 x = torch.randn(1,1,224,224) for layer in net: x = layer(x) print(layer.__class__.__name__,\"output shape:\",x.shape) small_conv_arch = ((1, 16), (1, 32), (2, 32), (2, 64), (2, 64)) net = vgg(small_conv_arch) # 现在使用mnist数据集测试一下结果 def load_data_fashion_mnist(batch_size, resize=None): \"\"\"下载或者加载Fashion-MNIST数据集\"\"\" trans = [transforms.ToTensor()] if resize: # 需要把图片拉长,正常时不会这么做的 trans.insert(0, transforms.Resize(resize)) trans = transforms.Compose(trans) # 这是一步可以去掉的操作，这个就是把多个图像处理的步骤整合到一起 mnist_train = torchvision.datasets.FashionMNIST( root=\"../data/\", train=True, transform=trans, download=False # 要是没下载过就选择true ) mnist_test = torchvision.datasets.FashionMNIST( root=\"../data/\", train=False, transform=trans, download=False # 要是没下载过就选择true ) return (data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=0), data.DataLoader(mnist_test,batch_size=batch_size,shuffle=True,num_workers=0)) batch_size = 64 learning_rate = 0.01 epochs = 10 train_iter,test_iter = load_data_fashion_mnist(batch_size,resize=(224)) d2l.train_ch6(net,train_iter,test_iter,epochs,lr=learning_rate,device=d2l.try_gpu()) 缺点 尽管VGG在深度学习中具有重要的地位和影响力，但它也存在一些缺点，包括： 大量参数：VGG网络具有很深的结构，其中包含多个卷积层和全连接层。这导致了网络中的参数数量很大，需要更多的计算资源和存储空间。在训练和推理过程中，这会增加计算的复杂性和时间成本 计算资源要求高：由于VGG网络的深度和参数量较大，需要较高的计算资源来进行训练和推理。这对于一些资源受限的环境来说可能是一个挑战，特别是在移动设备或嵌入式系统上应用VGG网络时 过度拟合：由于VGG网络的深度和参数量较大，它对于较小的数据集容易发生过拟合的情况。在应用VGG网络时，如果训练数据不够丰富，模型可能会过度依赖于训练集的特点，导致在新数据上的泛化能力下降 缺乏空间信息利用：VGG网络仅使用了池化层来减小特征图的尺寸，但在减小尺寸的同时丢失了一部分空间信息。相比于一些具有跳跃连接或注意力机制的网络，VGG在利用图像中的空间关系方面相对较弱 较高的内存需求：由于VGG网络中的卷积层和全连接层较多，其生成的特征图较大，需要较大的内存来存储中间结果。这可能会限制VGG网络在一些内存受限的设备或平台上的应用 GoogleNet Rethinking the Inception Architecture for Computer Vision 2015 Inception-V3论文翻译——中英文对照 GoogLeNet网络系列解读 GoogLeNet是由Google团队在2014年提出的一种深度卷积神经网络架构，也被称为Inception网络。相比于传统的卷积神经网络，GoogLeNet采用了一种特殊的模块化设计，旨在提高网络的计算效率和表达能力 GooLeNet深度只有22层，但大小却比AlexNet和VGG小很多，GooLeNet的参数为500万个，AlexNet参数个数是GooLeNet的12倍，VGGNet参数又是AlexNet的3倍 InceptionV1 如何提升网络性能 一般提升网络性能最直接的方法是增加网络深度和宽度，深度指网络层数，宽度指神经元数量，但是会存在一些问题： 参数太多，如果训练数据集有限，很容易产生过拟合 网络越大，参数越多，则计算复杂度越大，难以应用 网络越深，容易出现梯度弥散问题(梯度越往后越容易消失)，难以优化模型 有一种解决方式是增加网络的深度和宽度的同时减少参数，为了减少参数一种方式是将全连接变成稀疏连接(Dropout) 但实际上稀疏连接的计算性能并不会有质的提升。这是因为大部分硬件是针对密集矩阵计算优化的 GooLeNet提出了一种Inception网络结构，构造一种“基础神经元”结构，来搭建一个稀疏性，高计算性能的网络结构。既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能 Inception模块 GoogLeNet的核心是Inception模块，这是一种多尺度特征提取模块。它通过并行地使用不同大小的卷积核和池化操作来捕捉图像中不同尺度的特征。这样的设计可以在保持计算效率的同时，增加网络对不同尺度信息的感知能力 另一个值得注意的特点是GoogLeNet中采用了 1 \\times 1卷积核的卷积层，称为瓶颈层。这些 1 \\times 1卷积层主要用于降低输入通道的维度，减少网络的参数量和计算复杂度。同时，它们还能够引入非线性变换，提高网络的表达能力 GoogLeNet还采用了全局平均池化层，将最后一个卷积层的特征图进行平均池化，得到全局的特征表示。这样可以显著减少全连接层的参数量，提高模型的泛化能力，并且降低过拟合的风险 Inception网络和VGG网络 VGG网络注重增加网络的深度来提取更复杂的特征，而Inception网络则通过并行的卷积分支来捕捉多尺度的特征信息。因此，Inception网络相对于VGG网络来说具有更高的计算效率和参数效率 Inception-v1 Going Deeper with Convolutions 2014 Inception-v1 Inception Module是GoogLeNet的核心组成单元，结构如下图 Inception Module基本组成结构有四个成分。 1 \\times 1卷积， 3 \\times 3卷积， 5 \\times 5卷积， 3 \\times 3最大池化，最后对四个成分运算结果进行通道上组合 这就是Inception Module的核心思想，通过多个卷积核提取图像不同尺度的信息，最后进行融合，可以得到图像更好的表征 辅助分类器(期望缓解梯度消失问题) 完整的结构可以看原论文，或者是这个链接 为了避免梯度消失，网络额外增加2个辅助的softmax用于向前传导梯度(辅助分类器)，辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重(0.3)加到最终分类结果中，这样就相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益，实际测试时会去掉这两个额外的softmax Inception-v2 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 2015 Inception-v2 Inception v2在原始的Inception v1的基础上引入了Batch Normalization(批量归一化)技术，这是它的主要贡献。Batch Normalization是一种用于加速神经网络训练和提高网络性能的技术 通过使用Batch Normalization，Inception v2实现了以下几个重要的优势： 加速训练：Batch Normalization可以使网络更快地收敛，因为它减少了训练过程中的梯度消失和梯度爆炸问题，从而加速了梯度传播和参数更新 提高网络的稳定性：Batch Normalization 使得网络对输入数据的变化更加鲁棒，减少了对输入数据分布和大小的敏感性，提高了网络的稳定性 减少对超参数的敏感性：Batch Normalization 减少了网络对学习率和权重初始化等超参数的敏感性，使得网络更容易调优和训练 正则化效果：Batch Normalization 具有一定的正则化效果，可以减少过拟合问题，提高网络的泛化能力 因此，Inception v2的主要贡献在于引入了Batch Normalization技术，使得网络的训练更加稳定和高效，进一步推动了深度学习模型的发展和应用 相比较于v1 5 \\times 5卷积层被替换为两个连续的3 \\times 3卷积层. 网络的最大深度增加9个权重层. 参数量增加了大约25%，计算量增加了大约30% 使用BN层，将每一层的输出都规范化到一个N(0,1)的正态分布，提高网络收敛速度 Inception-v3 Rethinking the Inception Architecture for Computer Vision 2015 Inception-v3 Inception V3一个最重要的改进是分解(Factorization)，将7 \\times 7分解成两个一维的卷积(1 \\times 7，7 \\times 1)，3 \\times 3也是一样(1 \\times 3，3 \\times 1)，这样的好处，既可以加速计算，又可以将1个卷积拆成2个卷积，使得网络深度进一步增加，增加了网络的非线性(每增加一层都要进行ReLU)，另外，网络输入从 224 \\times 224变为了 229 \\times 229 在Inception v2的基础上引入了一些重要的改进，其主要贡献如下： 辅助分类器：Inception v3在网络的中间层添加了辅助分类器，这些分类器有助于在训练过程中引导梯度流动和提供正则化。辅助分类器位于网络的不同层级，并与主分类器共同进行训练。这些辅助分类器有助于减轻梯度消失问题，提高网络的稳定性和收敛速度 更深的网络结构：Inception v3相对于之前的版本增加了更多的网络层，使得网络更深。更深的网络结构有助于提高特征表示的能力，使得模型能够更好地学习复杂的图像特征 更多的1x1卷积核：Inception v3进一步增加了网络中的1x1卷积核的数量。1x1卷积核具有降低通道数和维度的作用，它能够减少网络的计算量，并引入了更多的非线性变换，提高了网络的表达能力和特征提取能力 分支结构：Inception v3中的Inception模块引入了分支结构，即在不同尺度上使用不同大小的卷积核进行特征提取。这种分支结构有助于捕捉不同尺度的图像特征，并提高了网络对图像的感知能力 其他优化措施：Inception v3还引入了其他一些优化措施，如使用更小的卷积核、引入批量归一化等，以进一步提升网络的性能和训练效果 Inception-v4与ResNet Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning 2016 Inception-v4 Inception-v4与Inception-ResNet结构详解(原创) 微软亚洲研究院的何恺明在2015年提出了震惊业界的ResNet结构，这种结构和以往的Inception结构走了两条不同的道路： 前者主要关注加大网络深度后的收敛问题 而Inception更关注特征维度上的利用 如果把这两种方法结合起来会有什么效果呢？Szegedy在2016年就试验了一把，把这两种 最顶尖的结构混合到一起提出了Inception-ResNet，它的收敛速度更快但在错误率上和同层次的Inception相同；Szegedy还对自己以前提出的Inception-v3进行了一番改良，提出了Inception-v4 Inception-v4网络结构 Inception-v4与Inception-ResNet集成的结构在ImageNet竞赛上达到了3.08%的top5错误率，也算当时的state-of-art performance了 Inception-v4网络，对于Inception块的每个网格大小进行了统一 Inception V4主要利用残差连接（Residual Connection）来改进V3结构，得到Inception-ResNet-v1，Inception-ResNet-v2，Inception-v4网络 Inception-ResNet-v1结构 Inception-ResNet-v2结构 ResNet Deep residual learning for image recognition 2015 深度学习——ResNet超详细讲解，详解层数计算、各层维度计算 ResNet(Residual Network)是一种深度残差网络，它是由Kaiming He等人于2015年提出的。ResNet的核心思想是引入了残差连接（Residual Connection），通过跨层直接连接来解决深层网络训练中的梯度消失和模型退化问题 传统的深度神经网络在层数增加时会面临梯度消失和梯度爆炸的问题，导致模型难以训练。ResNet通过在网络中添加残差块（Residual Block），允许信息在跳过层的路径上直接传递，使得网络可以更容易地学习恒等映射。具体来说，残差块将输入和输出进行相加，然后通过激活函数进行非线性变换。这样的设计允许网络在需要时将残差信号传递到后续层，解决了梯度消失和模型退化的问题 ResNet的一个重要变种是ResNet-50，它由50个卷积层组成，其中包括残差块、池化层和全连接层。ResNet-50在ImageNet图像分类任务上取得了很好的性能，成为当时最先进的模型之一 事实上，ResNet并不是第一个利用近路连接、Highway Networks引入门控近路连接的。这些参数化门控制允许多少信息流过近路(shortcut)。类似的想法可以在长短期记忆网络(LSTM)单元中找到，其中存在参数化的忘记门，其控制多少信息将流向下一个时间步。因此，ResNet可以被认为是Highway Networks的一种特殊情况 层数越多越好吗 在ResNet之前的网络层数都不是很高，14年的VGG网络才只有19层，但是ResNet的网络层数达到了惊人的152层。许多人会有一个直观的印象，也就是网络层数越多，训练效果越好，但是这样的话VGG网络为什么不采取152层而是采用19层呢？其实是因为训练模型的准确度不一定和模型层数呈真相关的关系。因为随着网络层数的加深，网络准确需出现饱和，会出现下降的现象 56层的网络比20层网络的训练效果要差，许多人第一反应就是过拟合，但事实并不如此，因为过拟合现象的训练集准确度会很高，但是从图中可以看出56层网络的训练集准确度同样很低。很显然可知的是，随着层度加深，会出现梯度消失或梯度爆炸的问题，使得深度模型很难训练，但是已经存在BatchNorm等手段缓解这一问题，因此如何解决深度网络的退化问题是神经网络发展的下一个方向 模型 官方给了两个ResNet块的结构图，图一为BasicBlock也就是最常规的块，图二被成为BottleBlock BasicBlock(常规)(两层结构) 在ResNet34的时候是用的这个 BottleBlock(三层结构) 在ResNet50/101/152的时候用的是这个 是参考GoogleNet的方式对网络内容进行的一定优化 在计算前先接用 1 \\times 1的卷阶层降维，既保持精度又减少计算量，再对64维进行计算后经过 1 \\times 1的卷积恢复 残差 对残差网络的理解 为什么残差学习相对更容易，从直观上看残差学习需要学习的内容少，因为残差一般会比较小，学习难度小点。不过我们可以从数学的角度来分析这个问题，首先残差单元可以表示为： \\begin{array}{l} y_{l}=h\\left(x_{l}\\right)+F\\left(x_{l}, W_{l}\\right) \\\\ x_{l+1}=f\\left(y_{l}\\right) \\end{array} 其中x_{l}和x_{l+1}分别表示的是第l个残差单元的输入和输出，注意每个残差单元一般包含多层结构。F是残差函数，表示学习到的残差，而h\\left(x_{l}\\right)=x_{l}表示恒等映射，f是ReLU激活函数。基于上式，我们求得从浅层l到深层l的学习特征为: x_{L}=x_{l}+\\sum_{i=l}^{L-1} F\\left(x_{i}, W_{i}\\right) 利用链式规则，可以求得反向过程的梯度： \\frac{\\partial \\text { loss }}{\\partial x_{l}}=\\frac{\\partial \\text { loss }}{\\partial x_{L}} \\cdot \\frac{\\partial x_{L}}{\\partial x_{l}}=\\frac{\\partial \\text { loss }}{\\partial x_{L}} \\cdot\\left(1+\\frac{\\partial}{\\partial x_{l}} \\sum_{i=l}^{L-1} F\\left(x_{i}, W_{i}\\right)\\right) 式子的第一个因子\\frac{\\partial l o s s}{\\partial x_{L}}表示的损失函数到达L的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weight的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。要注意上面的推导并不是严格的证明 如果从ResNet的论文来看，确实ResNet出发点不是梯度消失而是网络退化；但是Kaiming隔年的论文确实有提到，残差结构可以使得反向的梯度总不消失，即便中间权重矩阵很小 残差映射更容易学习有个原因是反向传播的时候H(x)=x+F(x)，x分走了一部分梯度，所以同样的误差F(x)得到的梯度更小 到一定深度的时候，梯度会变成0，但是我们还有上一层的梯度，所以说不会比之前的差 DenseNet Densely Connected Convolutional Networks 2018 概述 作为CVPR2017年的Best Paper，DenseNet脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维，从特征的角度考虑，通过特征重用和旁路(Bypass)设置，既大幅度减少了网络的参数量，又在一定程度上缓解了gradient vanishing问题的产生。结合信息流和特征复用的假设，DenseNet当之无愧成为2017年计算机视觉顶会的年度最佳论文 DenseNet作为另一种拥有较深层数的卷积神经网络，具有如下优点: 相比ResNet拥有更少的参数数量 旁路加强了特征的重用 网络更易于训练，并具有一定的正则效果 缓解了gradient vanishing和model degradation的问题 何恺明在提出ResNet时做出了这样的假设：若某一较深的网络多出另一较浅网络的若干层有能力学习到恒等映射，那么这一较深网络训练得到的模型性能一定不会弱于该浅层网络 通俗的说就是如果对某一网络中增添一些可以学到恒等映射的层组成新的网路，那么最差的结果也是新网络中的这些层在训练后成为恒等映射而不会影响原网络的性能 同样DenseNet在提出时也做过假设：与其多次学习冗余的特征，特征复用是一种更好的特征提取方式 模型 DenseNet是一种深度神经网络架构，其核心思想是密集连接(Dense Connectivity)。相比于传统的神经网络结构，如VGG和ResNet，DenseNet通过引入密集连接的方式，在网络中每一层都与前面所有层直接相连，从而增强了信息流动和特征重用的能力 DenseNet的主要特点如下： 密集连接：在DenseNet中，每个层都与前面所有层直接相连。具体而言，某一层的输入包括它之前所有层的输出。这种密集连接的方式使得信息可以在网络中自由地流动，促进了特征的传递和共享 混合特征重用：由于密集连接的存在，每个层可以直接访问之前所有层的特征图。这样，低层特征可以直接传递给后续层，实现了混合特征重用。这种特征重用机制有效地利用了网络中的信息，增强了特征的多样性和丰富性 基本组件：DenseNet的基本组件是\"Dense Block\"，它由多个具有相同输出通道数的卷积层组成。在每个Dense Block内部，层与层之间通过密集连接相连。为了控制参数数量和计算量，每个卷积层通常采用较小的3x3卷积 过渡层：为了控制网络的宽度，DenseNet在相邻的Dense Block之间引入了过渡层(Transition Layer)。过渡层由一个1x1卷积层和一个2x2的平均池化层组成，它可以减小特征图的尺寸并降低通道数，从而减少计算量 DenseNet的优点包括模型参数相对较少、特征重用性强、梯度传播更加顺畅等 Dense Block DenseNet中的核心组件是\"Dense Block\"，它由多个密集连接的卷积层组成。Dense Block的设计旨在促进特征的传递和重用，增强网络的表示能力 具体来说，Dense Block由一系列堆叠在一起的卷积层组成，每个卷积层都直接连接到前面所有层的输出。这意味着某一层的输入是其之前所有层的输出的串联。这种密集连接的方式使得信息可以在网络中自由地流动，从而有效地提高了特征传递和共享的能力 为了控制参数数量和计算量，每个卷积层通常采用具有相同输出通道数的3 \\times 3卷积。这样，每个卷积层都可以利用之前层的丰富特征来生成更加复杂和抽象的特征表示。这种密集连接的方式不仅增加了特征的多样性，还减轻了梯度消失的问题，使得网络更容易训练 在每个Dense Block之间，为了控制网络的宽度和深度，通常会引入过渡层(Transition Layer)。过渡层由一个1 \\times 1卷积层和一个2 \\times 2的平均池化层组成。1 \\times 1卷积层用于降低通道数，减少计算量。平均池化层则用于减小特征图的尺寸，进一步减少参数和计算复杂度 模型 DenseNet是一种基于密集连接的卷积神经网络(CNN)，其主要特点是在网络中引入了密集连接层，从而改善了信息的流动和梯度的传递。下面是DenseNet的网络结构： 1.输入层：输入层接收输入数据，并将其送入第一个卷积层中 2.卷积层：DenseNet中的卷积层通常采用3\\times 3的卷积核，并采用padding来保持特征图的大小不变。在每个卷积层后面，都会接上BN层和ReLU激活函数 3.密集块(Dense Block)：密集块是DenseNet的核心，它由多个密集连接层组成。每个密集块中，所有前面层的输出都会与当前层的输入进行连接，并通过一个非线性变换进行处理 4.过渡层(Transition Block)：为了避免网络过深导致梯度消失和计算资源过度消耗，DenseNet中采用了过渡层来控制网络的大小。在每个密集块之间，都会接上一个过渡层，它包含一个1\\times 1的卷积层、BN层和平均池化层，其中平均池化的步幅为2，用于减少特征图的大小 5.全局池化层和全连接层：最后，DenseNet使用全局平均池化层将特征图降维为一个向量，然后通过一个全连接层进行分类 最后一个池化用的是全局池化层 具体来说，DenseNet的密集连接机制使得前面的层可以直接连接到后面的层，从而保留了更多的特征信息。然而，这种密集连接也导致了特征图的尺寸逐渐增大。为了控制模型的复杂性和计算量，并且能够更好地适应不同尺度的输入，DenseNet引入了全局池化层 全局池化层可以将整个特征图转化为固定长度的特征向量，这样可以有效地降低特征的维度，并且保留了全局感受野的特征信息。通过将特征图的每个通道进行平均池化或最大池化操作，全局池化层可以捕捉到整个特征图的统计特征，从而对全局信息进行汇聚 使用全局池化层的好处是减少了模型的参数数量和计算量，同时仍然能够保留重要的全局特征。这有助于提高模型的效率和泛化能力，并且在训练和推断阶段都能够更好地适应不同尺度的输入图像 Resnext Resnet性能最好的变体是Resnext Vit Vit An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale 2020 Vision Transformer详解 Vision Transformer (base-sized model) 概述 ViT(Vision Transformer)是一种基于Transformer的视觉模型，它将自然语言处理中的Transformer模型成功应用于计算机视觉任务 传统的计算机视觉模型主要基于卷积神经网络(CNN)，而ViT尝试使用Transformer的自注意力机制来处理图像数据 ViT的关键思想 将输入图像切分为固定大小的图像块(patches)，并将这些图像块展平为序列形式的输入。每个图像块通过一个线性映射层进行特征嵌入，然后通过添加位置嵌入来引入位置信息 之前学习的Transformer结构中，输入需要是一个二维的矩阵，矩阵的形状可以表示为(N, D)，其中N是sequence的长度，而D是sequence中每个向量的维度 因此，在ViT算法中，首先需要设法将H \\times W \\times C的三维图像转化为(N, D)的二维输入 ViT中的具体实现方式为: 将H \\times W \\times C的图像，变为一个N \\times\\left(P^{2} * C\\right)的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了N=H W / P^{2}个图像块，每个图像块的维度则是\\left(P^{2} * C\\right)。其中P是图像块的大小，C是通道数量。经过如上变换，就可以将N视为sequence的长度了 但是，此时每个图像块的维度是\\left(P^{2} * C\\right)，而我们实际需要的向量维度是D，因此我们还需要对图像块进行Embedding。这里Embedding的方式非常简单，只需要对每个\\left(P^{2} * C\\right)的图像块做一个线性变换，将维度压缩为D即可 模型结构 ViT模型通常包含多个Transformer编码器层，每个层由多头自注意力机制和前馈神经网络组成 在ViT中，序列中的每个位置都可以进行自注意力计算，使模型能够在全局上对图像进行编码和理解 最后，ViT模型将序列的表示通过一个池化操作得到整个图像的表示，然后可以通过一个线性分类器进行分类或进行其他任务 优缺点 ViT的优点之一是它能够捕捉全局信息，并且在一些计算机视觉任务上取得了很好的表现，例如图像分类、目标检测和图像分割 然而，ViT对于大尺寸高分辨率图像的处理相对较慢，且对于空间信息的建模相对较弱，因此在处理具有细粒度结构的图像时可能存在一定的限制 ViT通过将图像划分为序列，并利用Transformer的自注意力机制，将自然语言处理中的Transformer模型引入了计算机视觉领域，为图像理解任务提供了一种新的思路和方法 Transformer模型是如何在CV领域里用起来的 在计算机视觉领域，Transformer模型通常用于处理序列数据和实现一些特定任务，而不是直接应用于图像输入。以下是一些使用Transformer模型的常见方式： 图像分类：可以将图像划分为网格单元，并将每个单元的特征表示为序列。然后，将序列输入Transformer模型进行分类任务。这样做的一个例子是Vision Transformer(ViT)模型，它将图像划分为图像块，然后通过Transformer编码器对这些块进行处理 目标检测：一种使用Transformer的目标检测方法是将图像划分为一组固定大小的区域，然后对每个区域提取特征，并将这些特征序列输入Transformer模型中进行对象分类和边界框回归。这种方法的一个例子是DETR(Detection Transformer)模型 图像生成：Transformer模型也可以用于生成视觉内容，如图像生成、图像描述生成等任务。通过将Transformer模型作为生成器，可以学习生成高质量的图像或图像描述 需要注意的是，由于图像数据的高维性和空间结构，直接将Transformer模型应用于整个图像通常不是常见的做法。相比之下，卷积神经网络(CNN)在计算机视觉领域中更为常见，因为它们更适合处理图像数据的局部特征和空间结构。但是，通过将Transformer模型与CNN结合使用，可以利用Transformer模型的序列建模能力和注意力机制来处理图像中的序列或局部特征，从而提高计算机视觉任务的性能 Tranformer和CNN比较 Tranformer相较于CNN结构，缺少一定的平移不变性和局部感知性，因此在数据量不充分时，很难达到同等的效果，表现为使用中等规模的ImageNet训练的Tranformer会比ResNet在精度上低几个百分点 当有大量的训练样本时，结果则会发生改变。使用大规模数据集进行预训练后，再使用迁移学习的方式应用到其他数据集上，可以达到或超越当前的SOTA水平 DeiTModel Training data-efficient image transformers & distillation through attention 2021 视觉Transformer经典论文——ViT、DeiT的与原理解读与实现 huggingface DeiTModel DeiT：注意力Attention也能蒸馏 DeiTModel(Data-efficient Image Transformers Model)和ViT(Vision Transformer)之间存在关系，DeiTModel可以看作是对ViT模型的改进和优化 ViT在大数据集 mageNet-21k(14million)或者JFT-300M(300million)上进行训练，Batch Size 128下NVIDIA A100 32G GPU的计算资源加持下预训练ViT-Base/32需要3天时间 ViT是一种基于Transformer的图像分类模型，通过将图像拆分成固定大小的图像块，并使用线性嵌入将每个图像块转换为向量序列，然后将序列输入到Transformer编码器中进行处理。ViT模型在图像分类任务中取得了出色的性能，但需要大量的训练数据和计算资源 Facebook与索邦大学Matthieu Cord教授合作DeiTModel，DeiT模型(8600万参数)仅用一台GPU服务器在53 hours train，20 hours finetune，仅使用ImageNet就达到了 84.2 top-1准确性，而无需使用任何外部数据进行训练。性能与最先进的卷积神经网络(CNN)可以抗衡 较于Vit的改进点 DeiTModel则是在ViT的基础上进行了改进，旨在提高数据效率 训练策略: 高低精度+数据增强 更少的数据和计算资源: DeiT模型使用更少的数据和计算资源，仅使用ImageNet数据集进行训练，并在较短的时间内完成训练 蒸馏机制: DeiT模型还引入了一种教师-学生策略，通过蒸馏机制使学生模型从教师模型中学习，这种策略有助于提高模型的泛化能力和性能 知识蒸馏 DeiT：注意力Attention也能蒸馏 知识蒸馏使用的是Teacher—Student模型，其中Teacher是知识的输出者，Student是知识的接受者。知识蒸馏的过程分为2个阶段: 原始模型训练: 训练Teacher模型, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对Teacher模型不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值 精简模型训练: 训练Student模型, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值 将问题限定在分类问题下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。知识蒸馏时，由于已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力 上图是教师学生模型的一般形式，就是有两部分的loss，一个关注真实标签，另一个关注Net-T的输出，训练的模型可以兼顾自身和教师模型的约束，即 L = \\alpha L_{soft} + \\beta L_{hard} 下面左图比较了改变训练策略和添加蒸馏学习的结果比较，右图是DeiT模型结构 左图 图中的指标均为在ImageNet数据集上进行训练，且在ImageNet数据集上评估的结果 其中Ours(Deit)为使用与ViT完全一致的网络结构，但是改进了训练策略 而Ours⚗(DeiT⚗)则是在DeiT的基础上继续使用了蒸馏学习的方式进行改进 可以看到，ViT算法在这种中等规模的数据集上，指标远不如CNN网络EfficientNet 而通过改变训练策略，使用蒸馏学习，网络结构与ViT基本一致的DeiT性能有了很大的提升，超过了EfficientNet 右图 DeiT与ViT的主要差异在于引入了一个distillation token，其主要用于网络训练中的蒸馏学习 这个distillation token与class token很像，其在self-attention layers中会跟class token以及图像patch不断交互 而distillation token与class token唯一区别在于，class token的目标是跟真实的label一致，而distillation token是要跟蒸馏学习中教师网络预测的label一致 Loss = Loss\\{class\\_token, label\\} + Loss\\{distillation\\_token , Net_T's\\_label\\} 在最终预测时，网络既会输出class token的结果，也会输出distillation token的结果，论文答案是将两者的softmax结果进行相加，即可简单地得到算法的最终预测结果 这里在计算和Net-T的loss时，还可以细分为两种，分别是软蒸馏(soft distillation)和硬蒸馏(hard distillation) 软蒸馏: 将学生网络的输出结果与教师网络的softmax输出结果取KL Loss L_{\\text {global }}^{\\text {SoftDistill }}=(1-\\lambda) L_{C E}\\left(\\psi\\left(Z_{s}\\right), y\\right)+\\lambda \\tau^{2} K L\\left(\\psi\\left(Z_{s} / \\tau\\right), \\psi\\left(Z_{t} / \\tau\\right)\\right) 硬蒸馏: 将学生网络的输出结果与教师网络的标签取交叉熵损失 L_{\\text {global }}^{\\text {Hardistill }}=\\frac{1}{2} L_{C E}\\left(\\psi\\left(Z_{s}\\right), y\\right)+\\frac{1}{2} L_{C E}\\left(\\psi\\left(Z_{s}\\right), y_{t}\\right) Hard Label也可以通过标签平滑技术(Label smoothing)转换成Soft Labe，其中真值对应的标签被认为具有1-esilon的概率，剩余的esilon由剩余的类别共享 ViT、Deit这类视觉transformer是如何处理变长序列输入的 当增加输入图像的分辨率时，例如DeiT从224到384，一般来说会保持patch size(例如9)，因此patch的数量N会发生了变化 由于Transformer结构的原因，内置了position embedding位置编码的差值，一般将位置编码双线性插值到图片分辨率，当N发生变化时，模型的权重不需要做出任何变化也可以以同样的方式计算出Q、K、V的值，所以Visual transformer的模型结构适用于任何长度的sequence。最终输出预测的时候，看样子序列长了好多，但其实还是只取cls token输出作为输出预测 Clip Learning Transferable Visual Models From Natural Language Supervision Clip 2021 CLIP openai官方源码 Openai连接文本和图像CLIP模型(Huggingface版)zero-shot分类代码案例 2021年见证了vision transformer的大爆发，随着谷歌提出ViT之后，一大批的vision transformer的工作席卷计算机视觉任务。除了vision transformer，另外一个对计算机视觉影响比较大的工作就是Open AI在2021年1月份发布的DALL-E和CLIP，这两个都属于结合图像和文本的多模态模型 DALL-E是基于文本来生成模型的模型 CLIP是用文本作为监督信号来训练可迁移的视觉模型 这两个工作也像ViT一样带动了一波新的研究高潮 概述 Openai连接文本和图像CLIP模型(Huggingface版)zero-shot分类代码案例 CLIP的英文全称是Contrastive Language-Image Pre-training，即一种基于对比文本-图像对的预训练方法或者模型 CLIP是一种基于对比学习的多模态模型，与CV中的一些对比学习方法如moco和simclr不同的是，CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系 如上图(1)所示，CLIP包括两个模型：Text Encoder和Image Encoder，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer 对提取的文本特征和图像特征进行对比学习 对于一个包含N个文本-图像对的训练batch，将N个文本特征和N个图像特征两两组合，CLIP模型会预测出N方个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性(cosine similarity)，即上图所示的矩阵 这里共有N个正样本，即真正属于一对的文本和图像(矩阵中的对角线元素)，而剩余的N*(N-1)个文本-图像对为负样本 那么CLIP的训练目标就是最大N个正样本的相似度，同时最小化N方-N个负样本的相似度 为了训练CLIP，OpenAI从互联网收集了共4个亿的文本-图像对 zero-shot分类 与CV中常用的先预训练然后微调不同，CLIP可以直接实现zero-shot的图像分类，即不需要任何训练数据，就能在某个具体下游任务上实现分类，这也是CLIP亮点和强大之处 根据任务的分类标签构建每个类别的描述文本：A photo of {label}，然后将这些文本送入Text Encoder得到对应的文本特征 将要预测的图像送入Image Encoder得到图像特征，然后与N个文本特征计算缩放的余弦相似度(和训练过程一致) 然后选择相似度最大的文本对应的类别作为图像分类预测结果，进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率 from PIL import Image from transformers import CLIPProcessor,CLIPModel model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\") processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") #这里加入自己图片的地址就行 image = Image.open('xxx.jpg') #这里加入类别的标签类别 text = ['plane','car','dog','bird'] inputs = processor(text=text,images = image,return_tensors=\"pt\",padding=True) outputs = model(**inputs) logits_per_image = outputs.logits_per_image probs = logits_per_image.softmax(dim=1) for i in range(len(text)): print(text[i],\":\",probs[0][i]) 使用CLIP进行zero-shot分类，另外一个比较重要的地方是文本描述的生成，上面的例子我们采用分类标签，但其实也有其它选择 比如我们直接用类别标签，这其实属于最近NLP领域比较火的一个研究：prompt learning或者prompt engineering 扩展 CLIP是基于文本-图像对来做的，但是它可以扩展到文本-视频，比如VideoCLIP就是将CLIP应用在视频领域来实现一些zero-shot视频理解任务 VQGAN+CLIP实现各种图像生成模型 TOnICS Curriculum Learning for Data-Efficient Vision-Language Alignment TOnICS 2022 超越CLIP的多模态模型，只需不到1%的训练数据！南加大最新研究来了 CLIP(Contrastive Language–Image Pre-training)，是一种基于对比的图片-文本学习的跨模态预训练模型，由OpenAI于2021年1月发布 它存在一个缺点就是数据需求太大：4亿个图像文本对、256个GPU，这对许多公司和个人都很不友好 对此，南加州大学的最新研究发现了一种基于本体的课程学习(Curriculum Learning)算法，只需不到1%的训练数据就能达到CLIP同款效果，甚至在图像检索方面表现更好 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/图神经网络.html":{"url":"chapters/图神经网络.html","title":"图神经网络.md","summary":"图神经网络","keywords":"","body":"概述概述 PyG Documentation PyG(PyTorch Geometric)是一个建立在 PyTorch 基础上的库，用于轻松编写和训练图神经网络(GNN)，用于与结构化数据相关的广泛应用 任务形态 GNN中常见任务：Graph级别任务、Node级别任务、Edge级别任务 包括：节点分类，节点连接预测，图相似度检测，异常检测，图分类 多层GNN不会改变图的拓扑结构，但是会改变点的特征，多层GNN处理可以扩大感受野 GCN和CNN有什么不同和相同点 不同点：图卷积中每个点的邻居数量是不确定的，图卷积中的数据输入格式不确定 相同点：GCN和CNN本质都需要对输入数据做特征提取，且都是根据某个点周围点的情况提取特征 GCN 可以处理 semi-supervised learning，即只有部分节点有标签的情况，计算损失时只用有标签的计算 GCN 的层数一般3~5层比较好，层数叠加可能导致反效果 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/数据标注工具.html":{"url":"chapters/数据标注工具.html","title":"数据标注工具.md","summary":null,"keywords":"","body":"label-studio前端安装数据库配置导入现有标注Cloudreve数据导入新建标注项目后端介绍模型创建使用步骤例子doccanoRoboflow label-studio label-studio社区版和企业版比较 基于UIE的命名实体识别下的label-studio-ml教程 Specific-examples-for-pre-annotations Label Studio是一个开源的，可配置的数据注释工具，其目的是使您能够使用标准化输出格式的最方便的界面标记不同类型的数据 特色：支持多人协作，支持主动学习、支持多种的标注的任务 缺点：在目标检测标注时，好像没有十字辅助线 支持的标注任务，包括了如下所示，具体的见官方git介绍 label studio 结合 MMDetection 实现数据集自动标记、模型迭代训练的闭环 Label Studio使用技巧 关键的两个git仓库，其中label-studio的官方文档在这里 label-studio， 进行普通的图片标记工作，如果要使用其提供的辅助预标记功能，则需要进行后续配置 label-studio-ml-backend，主要提供深度学习服务，包括预标记和模型训练，结合前端形成模型迭代训练的主动学习效果 版本 -- 前端 label-studio==1.10.0.post0 -- 后端 label-studio-ml==1.0.9 gunicorn==20.1.0 rq==1.10.1 -- 下面这两个自己会安装 label-studio-converter==0.0.57 label-studio-tools==0.0.3 前后端结合可以达到模型自动标记数据集、数据集更新迭代训练模型的闭环 前端 安装 pip install label-studio # 如果安装psycopg2==2.9.5 失败, 可以先装下下面的库 sudo yum install python-devel postgresql-devel postgresql-libs gcc 环境安装完成后在任意位置打开命令行，使用以下命令启动 label studio # 不指定路径时, 默认放在了 /root/.local/share/label-studio label-studio --data-dir Label-Studio -p 80 其中 --data-dir 用于指定工作目录， -p 用来指定运行端口，运行成功后会当前目录会生成 Label-Studio 目录 浏览器打开label studio工作界面，创建用户后即可登录使用(这里的创建很简单，符合规范就可以) 文件夹中主要包括了export和media文件夹，以及一个label_studio.sqlite3数据库文件 数据库配置 数据库默认使用的是sqlite3，还可以配置其他的关系库，如PostgreSQL等，通常如果你想标注数百万个任务，或者预计会有很多并发用户，或者你计划在现实生活中的项目中工作，那么使用PostgreSQL数据库 官方提供的设置pg的环境变量好像不起效果，不知道把数据库建哪里去了，这里直接在命令行带上pg库的参数 DJANGO_DB=default POSTGRE_NAME=label_studio_db POSTGRE_USER=postgres POSTGRE_PASSWORD=xxx POSTGRE_PORT=5432 POSTGRE_HOST=192.168.123.xx LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/home/huangyc/label_ws label-studio start --data-dir /home/huangyc/label_ws -p 8080 LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT用于指定本地文件的位置，项目设置里面配置本地路径为/home/huangyc/label_ws/media(这只是个例子好像配成/home/huangyc/label_ws/media/upload也是一样的)，此时可以访问 # 注意?d后面的路径是LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT配置的路径后面继续开始 http://192.168.123.xx:8080/data/local-files?d=media/upload/3/demo.jpg 当前目录结构如下 (label38) [root@uslave02 label_ws]# tree -d . ├── export └── media ├── avatars ├── export └── upload ├── 1 ├── 3 ├── demo.jpg └── 4 8 directories 注意这里需要提前创建好数据库label_studio_db，启动完成控制台输出如下的日志 label-studio start --data-dir /home/huangyc/label_ws -p 8080 => Database and media directory: /root/.local/share/label-studio => Static URL is set to: /static/ => Database and media directory: /home/huangyc/label_ws => Static URL is set to: /static/ Starting new HTTPS connection (1): pypi.org:443 https://pypi.org:443 \"GET /pypi/label-studio/json HTTP/1.1\" 200 56156 Initializing database.. Performing system checks... [2023-03-29 05:01:46,560] [django::register_actions_from_dir::97] [INFO] No module named 'data_manager.actions.__pycache_' [2023-03-29 05:01:46,560] [django::register_actions_from_dir::97] [INFO] No module named 'data_manager.actions.__pycache_' System check identified no issues (1 silenced). March 29, 2023 - 05:01:46 Django version 3.2.16, using settings 'label_studio.core.settings.label_studio' Starting development server at http://0.0.0.0:8080/ Quit the server with CONTROL-C. 此时数据库label_studio_db里面会建好所有的表，比如htx_user存的就是注册的用户信息 如果要后台执行，可以新建一个start.sh脚本 DJANGO_DB=default POSTGRE_NAME=label_studio_db POSTGRE_USER=postgres POSTGRE_PASSWORD=xxx POSTGRE_PORT=5432 POSTGRE_HOST=192.168.123.xx label-studio start --data-dir /home/huangyc/label_ws -p 8080 然后执行nohup sh start.sh > log_start.log & 导入现有标注 将现有标注转为json格式可以用label-studio-converter工具，安装命令为pip install label-studio-converter 转换的例子，目录结构为: Q:. ├─images ├─ 1.jpg ├─ 2.jpg └─labels ├─ 1.txt ├─ 2.txt ├─ classes.txt # 注意这里的classes.txt里面的类别顺序一定要正确，如果是从label-studio前端导出的，好像顺序会有问题，可以用notes.json去获取正确的顺序 # 代码为 annotation_labels = json_load_op('notes.json')['categories'] annotation_labels = sorted(annotation_labels, key=lambda k: int(k['id'])) annotation_labels = [anno['name'] for anno in annotation_labels] 命令行为 label-studio-converter import yolo -i project-1 -o ls-tasks.json --image-root-url /data/local-files?d=tmp/images 输出ls-tasks.json和ls-tasks.label_config.xml两个文件，第一个是数据的索引和标签信息，第二个是项目的标签配置文件 此时要把文件上传到LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT/tmp/images下才可以看到 如果这里转换出来的路径不对，可以修改label_studio_converter\\imports\\yolo.py.py下的 task = { \"data\": { # eg. '../../foo+you.py' -> '../../foo%2Byou.py' \"image\": f\"{image_root_url}{sp}{image_file_base}\" # 改完后的 }, # 'annotations' or 'predictions' out_type: [ { \"result\": [], \"ground_truth\": False, } ] } 不然会访问不到数据 如果数据没有标注信息，可以将数据传到LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT/tmp/images下，并生成如下json文件，并在项目界面导入该文件即可 [ {\"image\":\"/data/local-files?d=tmp/images/175.jpg\"}, {\"image\":\"/data/local-files?d=tmp/images/696.jpg\"} ] Cloudreve数据导入 cloudreve私有云盘配置后台运行 安装配置Cloudreve Cloudreve可助你即刻构建出兼备自用或公用的网盘服务，通过多种存储策略的支持、虚拟文件系统等特性实现灵活的文件管理体验 Linux 下，直接解压并执行主程序即可： #解压获取到的主程序 tar -zxvf cloudreve_VERSION_OS_ARCH.tar.gz # 赋予执行权限 chmod +x ./cloudreve # 启动 Cloudreve nohup ./cloudreve > start.log & Windows下，直接解压获取到的 zip 压缩包，启动 cloudreve.exe 即可 Cloudreve在首次启动时，会创建初始管理员账号，请注意保管管理员密码，此密码只会在首次启动时出现。如果您忘记初始管理员密码，需要删除同级目录下的cloudreve.db，重新启动主程序以初始化新的管理员账户。或者使用./cloudreve --database-script ResetAdminPassword重置密码 Cloudreve默认会监听5212端口。你可以在浏览器中访问http://服务器IP:5212进入 Cloudreve。 以上步骤操作完后，最简单的部署就完成了。你可能需要一些更为具体的配置，才能让 Cloudreve 更好的工作 登录管理员，编辑存储策略下的Default storage policy，将存储文件名改为{originname}，不然会默认会带上一些前缀 Cloudreve的文件实际上是存储在/home/huangyc/cloudreve/uploads/1下，在此目录下新建软链接label_ws到label-studio的工作目录下，至此 [root@uslave02 1]# ln -s /home/huangyc/label_ws/uploads label_ws [root@uslave02 1]# pwd /home/huangyc/cloudreve/uploads/1 此时，界面网页上可以看到label_ws文件夹，后续的项目文件可以传到这里，至此Cloudreve配置完成 label-studio项目设置 这里可以将label-studio项目的云存储位置设置为本地/home/huangyc/label_ws/uploads，然后导入一下json文件到项目 [ {\"image\":\"/data/local-files?d=uploads/tricycle_samples/灭鬼之刃.png\"} ] 最后通过Cloudreve，把文件灭鬼之刃.png上传到label_ws/tricycle_samples下就可以看到图片了，注意：这里的label_ws相当于/home/huangyc/label_ws/uploads，这就是软链接的魅力 新建标注项目 在 label studio 前端主页中选择创建项目，主要流程 填写项目基本信息 导入数据 选择标记模板: label studio内置了很多常见的深度学习标记模板 以下是目标检测的模板 category很重要，能保证标签的顺序，这里要从0开始 此时我们已经可以通过 label studio 进行普通的图片标记工作，如果要使用其提供的辅助预标记功能，则需要进行后续配置 后端 介绍 label studio ml是label studio的后端配置，其主要提供了一种能够快速将AI模型封装为label studio可使用的预标记服务(提供模型预测服务) 安装: 依赖了rq库和redis库，注意有些库的版本不能过高 pip install label-studio-ml 模型创建 创建后端模型 导入相关库 import os from PIL import Image from label_studio_ml import model from label_studio_ml.model import LabelStudioMLBase from label_studio_ml.utils import get_env from label_studio_tools.core.utils.io import get_local_path model.LABEL_STUDIO_ML_BACKEND_V2 = True os.environ['HOSTNAME'] = 'http://192.168.123.xx:8080' os.environ['API_KEY'] = 'TOKEN,在项目的settings里面复制' os.environ['LABEL_STUDIO_ML_BACKEND_V2'] = 'True' 模型类 class MyModel(LabelStudioMLBase): def __init__(self, **kwargs): super(MyModel, self).__init__(**kwargs) # 按 mmdetection 的方式加载模型及权重 print(kwargs) print(\"初始化完成\") def predict(self, tasks, **kwargs): # 获取待标记图片 print(\"开始预测\") images = [get_local_path(task['data']['image'], hostname=HOSTNAME, access_token=API_KEY) for task in tasks] results = [] all_scores = [] # 这里只是示例, 返回结果都一样 for img_path in images: for _ in range(1): w, h = Image.open(img_path).size pixel_x, pixel_y, pixel_width, pixel_height = convert_to_ls(0, 0, 200, 250, w, h) result = { 'id': '0', # 必须为 str，否则前端不显示 'from_name': 'label', 'to_name': 'image', 'type': 'rectanglelabels', 'value': { 'rectanglelabels': ['tricycle'], 'x': pixel_x, # xy 为左上角坐标点 'y': pixel_y, 'width': pixel_width, # width,height 为宽高 'height': pixel_height }, 'score': 0.95 } results.append(result) all_scores.append(0.95) print(tasks) print(kwargs) avg_score = sum(all_scores) / max(len(all_scores), 1) results = [{ 'result': results, 'score': avg_score }] return results def fit(self, completions, num_epochs=5, **kwargs): \"\"\" 模型训练 \"\"\" print(\"开始训练\") if self.gen_train_data(project_id): # 训练模型 return {'model_path': r'\\runs\\detect\\TriCycle\\weights\\best.pt'} else: raise \"gen_train_data error\" def gen_train_data(self, project_id): \"\"\" 获取数据训练数据 \"\"\" print(\"获取数据 project_id\", project_id) import zipfile import glob download_url = f'{HOSTNAME.rstrip(\"/\")}/api/projects/{project_id}/export?export_type=COCO&download_all_tasks=false&download_resources=true' response = requests.get(download_url, headers={'Authorization': f'Token {API_KEY}'}) zip_path = os.path.join(conf['workdir'], \"train.zip\") train_path = os.path.join(conf['workdir'], \"train\") with open(zip_path, 'wb') as file: file.write(response.content) # 通过二进制写文件的方式保存获取的内容 file.flush() f = zipfile.ZipFile(zip_path) # 创建压缩包对象 f.extractall(train_path) # 压缩包解压缩 f.close() os.remove(zip_path) if not os.path.exists(os.path.join(train_path, \"images\", str(project_id))): os.makedirs(os.path.join(train_path, \"images\", str(project_id))) for img in glob.glob(os.path.join(train_path, \"images\", \"*.jpg\")): basename = os.path.basename(img) shutil.move(img, os.path.join(train_path, \"images\", str(project_id), basename)) return True 使用步骤 启动后端服务: 分为三步，生成服务代码+启动服务+连接服务+训练模型 第一步：生成服务代码 label-studio-ml init backend/model --script label_studio_backend/yolo_detection.py --force label-studio-ml init 命令提供了一种根据后端模型自动生成后端服务代码的功能， model 为输出目录， --script 指定后端模型路径， --force 表示覆盖生成。该命令执行成功后会在 backend 目录下生成 model 目录 主要包括了_wsgi.py、docker-compose.yml、Dockerfile、yolo_detection.py等文件 第二步：启动服务 如果有依赖的文件，需要自己复制到 model 目录下，接着启动后端服务 label-studio-ml start backend/model --host 0.0.0.0 -p 8888 # or python backend/model/_wsgi.py --host 0.0.0.0 -p 8888 # 方便debug 启动成功的话，控制台会输出如下的日志 => ROOT = Q:\\pyCharmWS\\object_detection\\smart_city_management\\label_studio_backend\\backend\\yolo-detector => LABEL STUDIO HOSTNAME = http://192.168.123.xx:8080 * Serving Flask app \"label_studio_ml.api\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off [2023-03-29 14:21:59,286] [WARNING] [werkzeug::_log::225] * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. [2023-03-29 14:21:59,286] [INFO] [werkzeug::_log::225] * Running on http://10.10.0.xx:8888/ (Press CTRL+C to quit) PS: 测试的时候发现，直接执行_wsgi.py文件一样可以启动后端，跟踪label_studio_ml的server.py文件可以看到执行核心代码 def main(): args, subargs = get_args() if args.command == 'init': create_dir(args) elif args.command == 'start': start_server(args, subargs) elif args.command == 'deploy': if args.provider == 'gcp': deploy_to_gcp(args) 第三步：连接服务 在我们创建的前端项目中依次选择 Settings -> Machine Learning -> Add model ，然后输入后端地址 http://10.100.143.xxx:8888/(这里是后端的地址和端口)，点击保存 此时我们从前端项目中打开待标记图片，前端会自动请求后端对其进行标记(调用后端的 predict 方法)，等待片刻后即可看见预标记结果，我们只需要大致核对无误后点击 submit 即可 如果觉得每次打开图片都需要等待片刻才会收到后端预测结果比较费时，可以在 Settings -> Machine Learning 设置中选择打开 Retrieve predictions when loading a task automatically ，此后前端会在我们每次打开项目时自动对所有任务进行自动预测，基本能够做到无等待 第四步：训练模型 在 Settings -> Machine Learning 中点击后端服务的 Start Training 按钮，即可调用后端模型使用已标记信息进行训练 也可以 Settings -> Machine Learning 中允许模型自动训练，但训练频率过高会影响程序效率 目标检测任务注意事项 predict返回的坐标为convert_to_ls转换后的坐标值 # convert from LS percent units to pixels def convert_from_ls(result): if 'original_width' not in result or 'original_height' not in result: return None value = result['value'] w, h = result['original_width'], result['original_height'] if all([key in value for key in ['x', 'y', 'width', 'height']]): return w * value['x'] / 100.0, \\ h * value['y'] / 100.0, \\ w * value['width'] / 100.0, \\ h * value['height'] / 100.0 # convert from pixels to LS percent units def convert_to_ls(x, y, width, height, original_width, original_height): return x / original_width * 100.0, y / original_height * 100.0, \\ width / original_width * 100.0, height / original_height * 100 # convert from LS output = convert_from_ls(task['annotations'][0]['result'][0]) if output is None: raise Exception('Wrong convert') pixel_x, pixel_y, pixel_width, pixel_height = output print(pixel_x, pixel_y, pixel_width, pixel_height) # convert back to LS x, y, width, height = convert_to_ls(pixel_x, pixel_y, pixel_width, pixel_height, 600, 403) print(x, y, width, height) 例子 Labelstudio的UIE半监督智能标注方案本地版，赶快用起来啦 基于ner的模型实现 #!/usr/bin/env Python # -- coding: utf-8 -- \"\"\" @version: v1.0 @author: huangyc @file: uie_model.py @Description: label-studio-ml init backend/model --script uie_model.py --force @time: 2023/12/14 17:04 \"\"\" import json import os from typing import List, Dict import requests from basic_support.logger.logger_config import logger from label_studio_ml import model from label_studio_ml.model import LabelStudioMLBase from label_studio_ml.utils import get_env model.LABEL_STUDIO_ML_BACKEND_V2 = True os.environ['HOSTNAME'] = 'http://192.168.xx.xx:8080' os.environ['API_KEY'] = 'd818922361ade7e2d570346d2bd6fe1668fd4e81' os.environ['LABEL_STUDIO_ML_BACKEND_V2'] = 'True' HOSTNAME = get_env('HOSTNAME') API_KEY = get_env('API_KEY') ROOT = os.path.join(os.path.dirname(__file__)) model_url = 'http://region-46.seetacloud.com:12851/xxx' def anno(sentences: List[str]) -> List[Dict]: \"\"\" content = [\"2018年10月12日，当事人办理个体工商户营业执照，经营场所位于金湖县园林南路5号自北向南第10间，经营范围为食品经营，新鲜水果零售等。\"] resps = [{'日期时间': [{'text': '2018年10月12日', 'start': 0, 'end': 11, 'probability': 0.9213111485894672}], '地名': [{'text': '金湖县', 'start': 33, 'end': 36, 'probability': 0.9889604263625813}, {'text': '园林南路', 'start': 36, 'end': 40, 'probability': 0.8983399204114662}]}] :param sentences: :return: \"\"\" x = requests.post(model_url, json=sentences) if x.status_code == 200: resps = x.json()['response'] else: raise Exception(\"访问异常\") predicts = [] for resp in resps: result = [] scores = [] for label, v in resp.items(): for iv in v: score = iv['probability'] scores.append(score) res = { 'from_name': 'label', 'to_name': 'text', 'type': 'labels', 'value': { 'start': iv['start'], 'end': iv['end'], 'score': score, 'text': iv['text'], 'labels': [label] } } result.append(res) avg_score = round(sum(scores) / len(scores), 4) if scores else 0.0 predicts.append({\"result\": result, 'score': avg_score, 'model_version': 'uie-ner-large'}) return predicts class UieModel(LabelStudioMLBase): def __init__(self, **kwargs): super(UieModel, self).__init__(**kwargs) # 按 mmdetection 的方式加载模型及权重 print(kwargs) logger.info(f\"初始化完成\") def predict(self, tasks, **kwargs): # 获取待标记图片 sentences = [] for data in tasks: sentence = data['data']['text'] sentences.append(sentence) return anno(sentences=sentences) def fit(self, completions: List[Dict], workdir=None, **kwargs): \"\"\" 模型训练 :param completions: 标注的样本 :param workdir: :param kwargs: :return: \"\"\" logger.info(f\"开始构建训练样本\") sample_path = self.gen_train_data_path(completions=completions) return {'path': workdir, 'model_path': None, 'sample_path': sample_path} def gen_train_data_path(self, completions: List[Dict]): \"\"\" 获取数据训练数据 :param completions: :return: \"\"\" samples = [] for completion in completions: sentence = completion['data']['text'] m_id = completion['id'] created_at = completion[\"created_at\"] updated_at = completion[\"updated_at\"] labels = [] for annotation in completion['annotations']: for label_info in annotation['result']: labels.append(label_info['value']) sample = {\"text\": sentence, \"id\": m_id, \"label\": labels, \"created_at\": created_at, \"updated_at\": updated_at} samples.append(sample) logger.info(f\"构建训练样本完毕，开始输出到文件\") sample_path = \"./doccano_ext.jsonl\" with open(sample_path, \"w\", encoding=\"utf-8\") as outfile: outfile.write(json.dumps(samples, ensure_ascii=False)) logger.info(f\"样本路径为：{sample_path}\") return sample_path doccano doccano官网 Roboflow 如何使用 Roboflow 标注关键点 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/深度学习核心之优化器.html":{"url":"chapters/深度学习核心之优化器.html","title":"深度学习核心之优化器.md","summary":"深度学习核心之优化器","keywords":"","body":"优化算法概述基本的梯度下降法BGDSGDMBGD动量优化法SGDMNAG自适应学习率优化器AdaGradRMSPropAdadeltaAdamAdaMaxNadam其他优化器学习率衰减 优化算法 最优化是指非线性最优化，解非线性最优化的方法有很多 比如梯度下降法、共轭梯度法、变尺度法和步长加速法等 参考本站链接机器学习_最优化方法 概述 pytorch优化器 飞浆官方文档，总结到位 深度学习优化器方法及学习率衰减方式综述 An overview of gradient descent optimization algorithms 1847 1951 1983 2011 2012 GD(BGD) SGD SGDM(Momentum)、NAG AdaGrad Adadelta、RMSprop 2015 2016 2018 Adam、AdaMax Nadam AMSGrad 相对应的论文 A Stochastic Approximation Method SGD 1951 Learning representations by back-propagating errors Momentum 1983 A method for unconstrained convex minimization problem with the rate of convergence o(1/k2) NAG 1983 Adaptive subgradient methods for online learning and stochastic optimization AdaGrad 2011 ADADELTA: an adaptive learning rate method. Adadelta 2012 Neural Networks for Machine Learning Lecture 6a Overview of mini-batch gradient descent RMSprop 2012 Adam: A method for stochastic optimization Adam & AdaMax 2014 Incorporating Nesterov Momentum into Adam NAdam 2016 以下是优化器的发展脉络，按照时间顺序列出了一些重要的优化器及其年份： Gradient Descent (GD)：最早的优化器之一，用于求解无约束优化问题。没有特定的年份，但早在20世纪50年代就开始被广泛应用 Stochastic Gradient Descent (SGD)：引入随机性来估计梯度的优化器，用于大规模数据集和深度学习模型。没有特定的年份，但在深度学习的早期就被广泛使用 Momentum（1983）：提出了动量概念，通过累积梯度的指数加权平均来加速收敛 AdaGrad（2011）：自适应梯度算法，通过对梯度进行归一化和调整学习率，适应不同参数的更新需求 Adadelta（2012）：改进了AdaGrad的缺点，通过考虑历史梯度的平均值来调整学习率 RMSprop（2012）：引入了指数加权移动平均的概念，用于调整学习率以平衡历史梯度信息 Adam（2014）：结合了动量和自适应学习率的优点，通过自适应调整学习率和梯度的一阶矩估计和二阶矩估计来进行参数更新 AdaMax（2014）：基于Adam算法，通过替换二阶矩估计的范数为无穷范数，提供了更稳定的更新规则 Nadam（2016）：结合了Nesterov动量和Adam算法，利用动量来加速收敛 AMSGrad（2018）：对Adam算法进行了改进，解决了Adam算法学习率下降不稳定的问题 这些是一些比较重要的优化器，并且按照时间顺序列出。然而，需要注意的是，并非所有的优化器都是线性发展的，而是相互借鉴、改进和结合的结果。优化器的发展是一个活跃的研究领域，仍然有许多新的优化算法被提出和改进 什么是优化器 深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，所以如何去更新参数是深度学习研究的重点 通常将更新参数的算法称为优化器，字面理解就是通过什么算法去优化网络模型的参数 梯度下降核心点 方向: 确定优化的方向，一般通过求导便可以求得 步长: 步子就是决定当前走多大，如果学习率设的过大，梯度会在最优点来回跳动，设的过小需要很久的训练才能达到最优点 优化器的主要作用 参数更新：优化器根据损失函数的梯度信息，计算出每个参数的更新量，并将更新量应用于参数，从而更新模型的参数。这样，模型的参数就可以朝着能够更好地拟合训练数据的方向进行调整 学习率调整：优化器通常会自动调整学习率，以控制参数更新的步幅。学习率决定了每次参数更新的幅度，过大的学习率可能导致参数更新过快而错过最优解，而过小的学习率可能导致收敛速度缓慢。优化器根据当前训练的进度和参数的变化情况，动态地调整学习率，以获得更好的训练效果 优化算法选择：优化器提供了多种不同的优化算法，如梯度下降、动量优化、自适应学习率等。这些算法在参数更新的方式、学习率调整策略等方面有所不同，可以根据具体任务的需求选择合适的优化算法 通过合适的优化器选择和参数调整，可以提高神经网络的训练效率和性能，加速收敛过程，使得模型能够更好地拟合训练数据，并在测试数据上取得较好的泛化能力 优化器分类 梯度下降优化器(Gradient Descent Optimizers)：基于梯度信息来更新参数的优化器，包括批量梯度下降(BGD)、随机梯度下降(SGD)和小批量梯度下降(Mini-Batch Gradient Descent，MBGD)等 基于动量的优化器(Momentum-based Optimizers)：在梯度下降的基础上引入动量的概念，旨在加速收敛过程并减少震荡，常见的包括动量优化器(Momentum Optimizer)、牛顿加速度动量优化法Nesterov Accelerated Gradient(NAG)等 自适应学习率优化器(Adaptive Learning Rate Optimizers)：根据参数更新的情况动态地调整学习率，以提高收敛速度和效果，常见的包括AdaGrad、RMSprop、Adam、AdaDelta、Adamax等 学习率衰减优化器(Learning Rate Decay Optimizers)：在训练过程中逐渐减小学习率的优化器，常见的包括Step Decay、Exponential Decay、Piecewise Decay等 正则化优化器(Regularization Optimizers)：结合正则化技术，通过对损失函数添加正则化项来控制模型的复杂度，常见的包括L1正则化、L2正则化等 二阶优化器(Second-Order Optimizers)：考虑参数二阶信息的优化器，如牛顿法(Newton's Method)、共轭梯度法(Conjugate Gradient)等 不同类型的优化器在更新参数的方式、学习率调整策略、收敛速度、对噪声和局部最优的鲁棒性等方面有所区别，选择合适的优化器取决于具体的问题和数据集特征 基本的梯度下降法 优化器综述 深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam） 优化器的存在就是确定优化的方向和面对当前的情况动态的调整步子 BGD、SGD和MBGD的区别 优化器 BGD SGD MBGD 样本数 N(所有) 1 batch_size BGD BGD(Batch Gradient Descent)采用整个训练集的数据来计算cost function对参数的梯度 w_{t+1}=w_{t}-\\alpha \\Delta L\\left(w_{t}\\right) 其中\\alpha 为学习率，而\\Delta L\\left(w_{t}\\right)为损失函数的一阶导数 BGD在计算梯度时会出现冗余 因为BGD在每一次迭代中都使用了整个训练集，而且在梯度计算过程中并没有考虑样本之间的相关性 因此，对于样本中的某些部分，其梯度计算可能会与其他样本的梯度计算重复，这种冗余计算可能会导致计算效率的降低，特别是在训练集很大的情况下 优点 收敛稳定：由于每次迭代使用整个训练集的所有样本进行参数更新，收敛过程相对稳定 参数更新准确：使用全局梯度来更新参数，对于凸优化问题，可以达到全局最优解 缺点 训练速度慢：BGD 是一种批量梯度下降算法，每次更新模型参数时使用整个训练数据集 计算开销大：需要计算整个训练集的梯度，对于大规模数据集或复杂模型，计算开销较高 内存占用高：需要存储整个训练集的数据和梯度信息 SGD SGD(Stochastic Gradient Descent)是一种随机梯度下降算法，每次更新模型参数时使用单个样本或一小批样本(通常称为mini-batch，也称MBGD) w_{t+1}=w_{t}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\Delta L\\left(w_{i}\\right) 其中\\alpha 为学习率，而\\Delta L\\left(w_{t}\\right)为损失函数的一阶导数，m为batch_size，当m=1就是SGD，否则就是MBGD 优点 计算开销小：每次迭代只使用一个样本进行参数更新，计算开销较小 适用于大规模数据集：由于样本的随机选择，可以处理大规模数据集，且易于并行处理 缺点 参数更新不稳定：由于单个样本的梯度计算可能存在噪声，参数更新不稳定，可能引起参数在最优点附近震荡 收敛速度较慢：由于参数更新的不稳定性，收敛速度相对较慢 MBGD MBGD(Mini-Batch Gradient Descent)每一次利用一小批样本，即batch_size个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算 w_{t+1}=w_{t}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\Delta L\\left(w_{i}\\right) 其中\\alpha 为学习率，而\\Delta L\\left(w_{t}\\right)为损失函数的一阶导数，m为batch_size，当m=1就是SGD，否则就是MBGD 优点 平衡了开销和参数稳定性：使用一小批样本进行参数更新，综合了全局梯度和随机梯度的信息，计算开销和参数更新的稳定性得到一定的平衡 收敛速度较快：相对于BGD，使用较小的批量样本更新参数，收敛速度更快 缺点 批量大小需调优：批量大小的选择可能会影响模型的性能，需要进行调优 可能导致局部最优：较小的批量样本可能会引入一定的随机性，可能导致陷入局部最优而无法达到全局最优 动量优化法 SGDM 随机梯度下降法虽然有效，但容易陷入局部最小值点，甚至在驻点附近以及梯度值非常小的点附近时参数更新极为缓慢 为了抑制SGD的震荡，SGDM(Stochastic Gradient Descent Momentum)认为梯度下降过程可以加入惯性 主要思想是下降过程中，如果发现是陡坡，那就利用惯性跑的快一些。因此，其在SGD基础上引入了一阶动量 在坡度比较陡的地方，会有较大的惯性，这是下降的多。坡度平缓的地方，惯性较小，下降的会比较慢 \\begin{array}{c} m_{t}=\\lambda m_{t-1}+ \\alpha \\Delta L(w_t) \\\\ w_{t+1}=w_{t}-m_{t} \\end{array} 其中\\alpha 为学习率，\\Delta L(w_t)表示当前t时刻梯度，m_t表示当前时刻的加权后的梯度，\\lambda是动量系数 而\\lambda的经验值为0.9(表示最大速度10倍于SGD)，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向 一阶动量是各个时刻梯度方向的指数移动平均值，也就是说，t时刻的下降方向，不仅由当前点的梯度方向\\Delta L(w_t)决定，而且由此前累积的下降方向m_{t-1}决定 梯度是如何累积的 这里将m_t展开，可以看到当前时刻的梯度是对历史梯度进行加权得到的，其中，\\lambda是一个介于0和1之间的参数，控制了历史梯度对当前动量的贡献程度 较大的\\lambda值会使历史梯度的贡献更大，从而使动量更加平滑 较小的\\lambda值会使当前梯度的贡献更大，从而对变化更为敏感 这种权重衰减的方式使得历史梯度的贡献逐渐减小，更加关注近期的梯度变化，有助于适应变化的数据和模型参数 \\begin{aligned} m_{t}= & -\\alpha \\Delta L(w_t) -\\alpha \\lambda \\Delta L(w_{t-1})-\\alpha \\lambda^{2} \\Delta L(w_{t-2})-\\alpha \\lambda^{3} \\Delta L(w_{t-3}) \\cdots \\end{aligned} 优点 加速收敛：SGDM引入了动量的概念，通过累积之前的动量信息，有助于加速模型的收敛速度，特别是在存在平坦区域的情况下更为明显 减少震荡：动量的累积作用可以减少参数更新时的震荡现象，有助于更稳定地更新模型参数 尽可能跳出局部最优：动量的引入可以帮助模型跳出局部最优点，以便更好地搜索全局最优点 当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡 缺点 需要调整超参数：SGDM中的动量系数需要手动设置，选择合适的动量系数对于模型的性能影响较大，需要进行调试和调优 可能导致过拟合：当动量系数较大时，SGDM可能在优化过程中过度依赖之前的动量信息，导致模型过拟合 难以处理非平稳数据：对于非平稳数据，SGDM的动量累积可能会导致模型在变化快速的方向上过度追踪，而无法及时适应变化 改进方法 自适应调整动量系数：可以采用自适应的方式来调整动量系数，例如使用自适应的动量方法(如Adam)来根据梯度的变化自动调整动量系数 学习率调度策略：结合学习率调度策略，如学习率衰减或自适应学习率方法，可以更好地控制模型的学习速度和方向 正则化技术：使用正则化技术，如L1正则化或L2正则化，可以缓解过拟合问题，使模型更具泛化能力 因为加入了动量因素，SGDM缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题，但是并没有完全解决，当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡 NAG 深度学习优化函数详解-- Nesterov accelerated gradient (NAG) 比Momentum更快：揭开Nesterov Accelerated Gradient的真面目 动量法每下降一步都是由前面下降方向的一个累积和当前点的梯度方向组合而成。于是一位大神(Nesterov)就开始思考，既然每一步都要将两个梯度方向(历史梯度、当前梯度)做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的超前梯度来做梯度合并呢 如此一来，小球就可以先不管三七二十一先往前走一步，在靠前一点的位置看到梯度，然后按照那个位置再来修正这一步的梯度方向，同SGDM比较的差一点如下公式所示 \\Delta L(w_{t-1}) \\longrightarrow \\Delta L(w_{t-1}-\\lambda m_{t-1}) 既然知道会走\\lambda m_{t-1}，就不需要还用当前位置的梯度，可以直接走到\\lambda m_{t-1}的位置计算梯度，这样子就有了超前眼光 有了超前的眼光，小球就会更加聪明, 这种方法被命名为牛顿加速梯度(Nesterov accelerated gradient)简称NAG，下图是SGDM下降法与NAG下降法的可视化比较 NAG算法公式表达如下： \\begin{array}{c} m_{t}=\\lambda m_{t-1}+ \\alpha \\Delta L(w_{t-1}-\\lambda m_{t-1}) \\\\ w_{t+1}=w_{t}-m_{t} \\end{array} 为什么NAG比SGDM快 对NAG原来的更新公式进行变换，得到这样的等效形式(具体推导过程) \\begin{array}{c} m_{t}=\\lambda m_{t-1}+ \\alpha \\Delta L(w_{t-1}) + \\lambda ( \\Delta L(w_{t-1}) - \\Delta L(w_{t-2}) ) \\\\ w_{t+1}=w_{t}-m_{t} \\end{array} 与Momentum的区别在于，本次更新方向多加了一个\\lambda ( \\Delta L(w_{t-1}) - \\Delta L(w_{t-2}) ) 直观含义就很明显了：如果这次的梯度比上次的梯度变大了，那么有可能会继续变大，可以把预计增大的部分提前加进来；变小的情况类似 这个多加上去的项就是在近似目标函数的二阶导嘛，因此，NAG本质上是多考虑了目标函数的二阶导信息，其实所谓往前看的说法，在牛顿法这样的二阶方法中也是经常提到的，从数学角度上看，则是利用了目标函数的二阶导信息 自适应学习率优化器 AdaGrad 李宏毅深度学习笔记-Adagrad算法 AdaGrad 算法 优化的变量对于目标函数的依赖是各不相同 在基本的梯度下降法优化中，有一个常见的问题是，要优化的变量对于目标函数的依赖是各不相同的 对于某些变量，已经优化到了极小值附近，但是有的变量仍然在梯度很大的地方，这时候一个统一的全局学习率是可能出现问题的 如果学习率太小，则梯度很大的变量会收敛很慢，如果梯度太大，已经优化差不多的变量就可能会不稳定 现实世界的数据集中，一些特征是稀疏的(大部分特征为零，所以它是稀疏的)，而另一些则是密集的(dense，大部分特征是非零的)，因此为所有权值保持相同的学习率不利于优化 针对这个问题，当时在伯克利加州大学读博士的Jhon Duchi，2011年提出了AdaGrad(Adaptive Gradient)，也就是自适应学习率 基本思想 AdaGrad的基本思想是对每个变量用不同的学习率，设置了全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同 这个学习率在一开始会比较大，用于快速梯度下降。随着优化过程的进行，对于已经下降很多的变量，则减缓学习率，对于还没怎么下降的变量，则保持一个较大的学习率 公式 w_{t+1} =w_{t}-\\frac{\\alpha}{\\sqrt{G_{t}+\\epsilon }} \\odot \\eta \\Delta L(w_t) 其中\\eta为学习率，而\\Delta L\\left(w_{t}\\right)为损失函数的一阶导数，\\epsilon是一个平滑项，避免了除以零(通常取值在1e-8左右)，\\odot表示元素逐元素相乘操作 G_{t+1}可以写成下式，每个参数的所有偏微分的平方和，g_i是对梯度的缩写 G_{t+1} =G_{t}+g_{t+1} \\odot g_{t+1} = \\sum _{i=0}^{t} { {g_i} ^2 } 优缺点 优点： 自适应的学习率，无需人工调节，AdaGrad在迭代过程中不断调整学习率，并让目标函数中的每个参数都分别拥有自己的学习率，学习率默认值为0.01 有效地处理稀疏特征，因为它能够自动调整每个特征的学习率，使得稀疏特征的更新更少 缺点： 全局学习率: 仍需要手工设置一个全局学习率\\eta, 如果\\eta设置过大的话，会使regularizer过于敏感，对梯度的调节太大 训练停止: 由于梯度平方和的累积，学习率会不断衰减，可能导致在训练后期学习率过小，造成收敛速度过慢或者提前停止训练的问题(Adadelta算法解决) 附上别人写的代码 def sgd_adagrad(parameters, sqrs, lr): eps = 1e-10 for param, sqr in zip(parameters, sqrs): sqr[:] = sqr + param.grad.data ** 2 div = lr / torch.sqrt(sqr + eps) * param.grad.data param.data = param.data - div RMSProp An overview of gradient descent optimization algorithms 2017 RMSProp(Root Mean Square Propagation)是Hinton大神于2012年在一门叫Neural Networks for Machine Learning的在线课程中提出(并未正式发表)，是梯度下降优化算法的扩展 RMSProp实际上是Adagrad引入了Momentum，公式表达如下所示 \\begin{array}{c} G_{t+1} = \\beta G_t + (1-\\beta) \\Delta L (w_{t})^2 \\\\ w_{t+1} = w_t - \\frac {\\alpha}{\\sqrt{ G_{t} + \\epsilon }} \\odot \\Delta L (w_t) \\end{array} \\alpha是学习率，\\beta则类似于动量梯度下降法中的衰减因子，代表过去梯度对当前梯度的影响，一般取值0.9，\\epsilon是一个平滑项，避免了除以零(通常取值在1e-8左右)，\\odot表示元素逐元素相乘操作(也可以省略不写) 公式里的累积梯度平方和G_{t+1}可以展开写成下面的形式，g_i是对梯度的缩写(同AdaGrad) G_{t+1} = \\beta G_t + (1-\\beta) \\sum _{i=0}^{t} { {g_i} ^2 } 优点 缺点 代码(参考文档) drad_squared = 0 for _ in num_iterations: dw = compute_gradients(x, y) grad_squared = 0.9 * grads_squared + 0.1 * dx * dx w = w - (lr / np.sqrt(grad_squared)) * dw Adadelta 优化器(AdaGrad,AdaDelta,RmsProp,Adam,Nadam,Nesterovs,Sgd,momentum) AdaDelta算法两种解决方案 Adadelta 优化器 由于AdaGrad调整学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度 即Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值(指数移动平均值)，这就避免了二阶动量持续累积、导致训练过程提前结束的问题了 论文中提到了两种实现策略 方法一: Accumulate Over Window 从全部历史梯度变为当前时间向前的一个窗口期内的累积，计算定义为 \\mathrm{E}\\left[\\mathrm{g}^{2}\\right]_{\\mathrm{t}}=\\rho * \\mathrm{E}\\left[\\mathrm{g}^{2}\\right]_{\\mathrm{t}-1}+(1-\\rho) * \\mathrm{~g}_{\\mathrm{t}}^{2} 相当于历史梯度信息的累计乘上一个衰减系数\\rho，然后用\\rho作为当前梯度的平方加权系数相加 梯度更新公式为 \\mathrm{w}_{\\mathrm{t+1}}=\\mathrm{w}_{\\mathrm{t}}-\\frac{\\eta}{\\sqrt{\\mathrm{E}\\left[\\mathrm{g}^{2}\\right]_{\\mathrm{t}}+\\epsilon}} * \\mathrm{~g}_{\\mathrm{t}} 解决了对历史梯度一直累加而导致学习率一直下降的问题 方法二: Correct Units with Hessian Approximation 在1988年LeCun等人曾经提出一种用矩阵对角线元素来近似逆矩阵 \\Delta x_{t}=-\\frac{1}{\\left|\\operatorname{diag}\\left(H_{t}\\right)\\right|+\\mu} g_{t} diag函数指的是构造Hessian矩阵的对角矩阵，\\mu是常数项，防止分母为0 如果学过数值分析的同学应该知道，牛顿法用Hessian矩阵替代人工设置的学习率，在梯度下降的时候，可以完美的找出下降方向，不会陷入局部最小值当中，是理想的方法，但是Hessian矩阵的逆在数据很大的情况下根本没办法求 2012年，[Schaul&S. Zhang&LeCun]借鉴了AdaGrad的做法，提出了更精确的近似 \\Delta x_{t}=-\\frac{1}{\\left|\\operatorname{diag}\\left(H_{t}\\right)\\right|} \\frac{E\\left[g_{t-w: t}\\right]^{2}}{E\\left[g_{t-w: t}^{2}\\right]} g_{t} E\\left[g_{t-w: t}\\right]指的是从当前t开始的前w个梯度状态的期望值 E\\left[g_{t-w: t}^{2}\\right]指的是从当前t开始的前w个梯度状态的平方的期望值 同样是基于Gradient的Regularizer，不过只取最近的w个状态，这样不会让梯度被惩罚至0 这里如果求期望的话，非常的麻烦，所以采取了移动平均法来计算。这里作者在论文中也给出了近似的证明 \\Delta \\mathrm{x} \\propto \\mathrm{g} \\propto \\frac{\\mathrm{df}}{\\mathrm{dx}} \\propto \\frac{1}{x} 这里是当为指数型函数, 最后一个近似成立。 对于牛顿法： \\Delta \\mathrm{x} \\propto H^{-1} \\mathrm{~g} \\propto \\frac{\\frac{\\mathrm{df}}{\\mathrm{dx}}}{\\frac{\\partial^{2} f}{\\partial x^{2}}} 由上式可得： \\frac{\\frac{d f}{d x}}{\\frac{\\partial^{2} f}{\\partial x^{2}}}=\\frac{1}{\\frac{\\partial^{2} f}{\\partial x^{2}}} g_{t} 基中: \\frac{\\frac{d f}{d x}}{\\frac{\\partial^{2} f}{\\partial x^{2}}}=\\frac{1}{\\frac{\\partial^{2} f}{\\partial x^{2}}} g_{t} 这里可以用局部的加权指数平滑来替代，即： \\frac{\\Delta x}{\\frac{\\partial f}{\\partial x}} \\approx-\\frac{R M S[\\Delta x]_{t-1}}{R M S[\\Delta g]_{t}} 这里的RMS表示均方: \\operatorname{RMS}[g]_{t}=\\sqrt{E\\left[g^{2}\\right]_{t}+\\epsilon} 可以得到: \\Delta x_{t}=-\\frac{\\operatorname{RMS}[\\Delta x]_{t-1}}{\\operatorname{RMS}[g]_{t}} g_{t} 最后的更新公式为 \\mathrm{x}_{\\mathrm{t+1}}=\\mathrm{x}_{\\mathrm{t}} + \\Delta x_{t} 优点 无需手动设置学习率：Adadelta能够自适应地调整学习率，无需手动设置 解决了学习率衰减问题：由于采用了衰减平均的方式，Adadelta能够解决学习率随时间衰减过快的问题，使得模型能够更好地收敛 不依赖全局学习率：Adadelta不需要设置全局学习率，因此可以适应不同参数的学习速度需求 对初始学习率不敏感：Adadelta相对于其他优化器对初始学习率的选择并不敏感，使得模型更具鲁棒性 缺点 存储额外的状态信息：Adadelta需要保存额外的状态信息(如梯度平方的累积)，增加了存储的开销 算法参数要调整：Adadelta中的衰减系数\\rho需要进行适当的调整，不同任务可能需要不同的设置 Adam Adam(Adaptive Moment Estimation)自适应矩估计，是另一种自适应学习率的算法，本质上是带有动量项的Adadelta或RMSprop 是Diederik P. Kingma等人在2014年提出的优化算法，引入了两个参数\\beta 1和\\beta 2 思路 Adam不仅如RMSProp算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值(即有偏方差)，适合解决含大规模的数据和参数的优化目标，也适合解决包含高噪声或稀疏梯度的问题，让参数更新时保持稳定 \\begin{aligned} m_{t} & =\\beta_{1} m_{t-1}+\\left(1-\\beta_{1}\\right) g_{t} \\\\ \\\\ v_{t} & =\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) g_{t}^{2} \\\\ \\\\ \\hat{m}_{t} & =\\frac{m_{t}}{1-\\beta_{1}^{t}} \\\\ \\\\ \\hat{v}_{t} & =\\frac{v_{t}}{1-\\beta_{2}^{t}} \\\\ \\\\ \\end{aligned} 其中\\beta _1控制一阶动量，\\beta _2控制二阶动量 最终的参数更新公式为 w_{t+1} = w_{t}- \\eta \\frac{\\hat{m}_{t}}{\\sqrt{\\hat{v}_{t}}+\\epsilon} 默认值设置\\alpha=0.001 ， \\beta 1=0.9 ， \\beta 2=0.999 ， \\varepsilon=10-8 优点 自适应学习率：Adam通过自适应地调整每个参数的学习率，可以有效地应对不同参数的梯度变化情况。这使得它在训练过程中更容易收敛，并且对于大多数任务具有较好的性能，但是需要注意的是它的效果有时候不如SGDM 速度快：Adam结合了动量方法，能够在训练过程中积累梯度的动量，从而加速参数更新的速度，尤其在具有平坦或稀疏梯度的情况下更加明显 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 适用性广泛：也适用于大多非凸优化，适用于大数据集和高维空间 缺点 内存消耗较大：Adam需要存储每个参数的动量和平方梯度估计，这会占用较大的内存空间，特别是在具有大量参数的深度神经网络中 AdaMax AdaMax是一种自适应学习率优化算法，是Adam优化器的一种变体 AdaMax使用了梯度的无穷范数来估计梯度的大小，而Adam使用了梯度的二范数(核心区别)，变化如下所示 v_{t} = \\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) g_{t}^{2} \\longrightarrow v_{t} = \\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) |g_{t}|^{p} 论文中AdaMax的p= \\infty 为什么是选择了无穷范数 AdaMax选择了无穷范数(\\infty范数)是因为在大多数情况下，\\infty范数具有稳定的行为 对于一些问题，特别是在深度学习中，\\infty范数可以提供更好的数值稳定性和收敛性 相比于其他范数，\\infty范数能够更好地控制梯度的最大值，从而减少参数更新的不稳定性 因此，AdaMax选择了\\infty范数作为其更新规则的一部分，以提高优化算法的稳定性和效果 Nadam 深度学习优化策略Nadam Nadam(Nesterov-accelerated Adaptive Moment Estimation)是将Adam与Nesterov加速梯度结合在一起，它对学习率的约束将更强，具备二者的优势，使得此算法在某些问题上的效果更好 Nadam的更新规则与Adam类似，但在计算梯度更新时引入了Nesterov动量项。具体而言，Nadam在计算梯度的移动平均和梯度更新时，使用了Nesterov动量的修正梯度来更新模型参数。这使得Nadam在处理凸优化问题时能够更好地逼近最优解，并且在处理非凸问题时能够更快地收敛 其他优化器 AdamW（Adam with Weight Decay）： AdamW是对Adam优化器的改进，通过添加权重衰减（Weight Decay）的正则化项来解决权重衰减对Adam优化器的影响。传统的Adam优化器在计算梯度更新时，会将权重衰减项也纳入梯度计算中，导致权重衰减效果不准确。而AdamW在计算梯度更新时将权重衰减项单独处理，使得权重衰减的效果更加准确和稳定 ASGD（Average Stochastic Gradient Descent）： ASGD是一种随机梯度下降法的变体，它通过计算一定数量的随机梯度的平均值来更新模型参数。ASGD使用一个平均模型参数的历史记录，以减小训练过程中参数更新的方差。这样可以使模型的收敛速度更稳定，并且能够在训练过程中逐渐减小学习率，使得模型在训练后期更加趋于收敛 LBFGS（Limited-memory Broyden-Fletcher-Goldfarb-Shanno）： LBFGS是一种基于拟牛顿法的优化算法，用于解决无约束非线性优化问题。它利用函数的一阶导数和二阶导数信息来逼近目标函数的局部二次模型，并通过迭代更新参数来寻找最优解。LBFGS使用有限的内存来存储历史信息，以减少内存消耗。由于它不需要显式计算二阶导数矩阵，LBFGS适用于参数较多的问题，并且通常具有较好的收敛性能 总结： AdamW是对Adam优化器的改进，解决了权重衰减对Adam优化器的影响 ASGD是一种随机梯度下降法的变体，通过平均随机梯度来减小参数更新的方差，提高收敛速度和稳定性 LBFGS是一种基于拟牛顿法的优化算法，通过逼近目标函数的局部二次模型来寻找最优解，具有较好的收敛性能和适用性 学习率衰减 在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减 深度学习优化器方法及学习率衰减方式综述 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/深度学习核心之损失函数.html":{"url":"chapters/深度学习核心之损失函数.html","title":"深度学习核心之损失函数.md","summary":"深度学习核心之损失函数","keywords":"","body":"概述回归损失函数L1 LossL2 LossSmooth L1 LossHuber LossQuantile LossIoU Loss分类损失函数Binary Cross EntropyCross Entropy Loss最大似然角度信息论角度Hinge LossFocal Loss基于概率的损失KL散度正则化技术L1正则化L2正则化弹性网正则化其他正则 概述 一文看尽深度学习中的各种损失函数 常用的损失函数合集 深度学习常用损失函数的基本形式、原理及特点 Loss Functions 损失函数(Loss Function)是用来衡量模型预测值与真实值之间差异的函数，它是深度学习中的一个重要组成部分，用于评估模型的性能并指导模型的优化过程 损失函数、代价函数、目标函数的关系 损失函数(Loss Function)：损失函数是用来衡量模型在单个样本上的预测结果与真实标签之间的差异。它是一个标量值，表示模型预测的误差或损失程度。损失函数通常是针对单个样本计算的，例如均方误差(MSE)、交叉熵损失等。在训练过程中，通过最小化损失函数来优化模型参数，使模型的预测结果与真实标签更接近 代价函数(Cost Function)：代价函数是指整个训练集上的平均损失或误差函数。代价函数是损失函数的求和或平均，用于衡量模型在整个训练集上的预测结果与真实标签之间的总体差异。代价函数通常是在训练过程中使用的，用于计算梯度并更新模型参数 目标函数(Objective Function)：目标函数是在训练过程中要最小化或最大化的函数，可以是损失函数或代价函数。目标函数是模型训练的目标，通过优化目标函数来调整模型参数，使得模型在训练集上的性能达到最优 定义 损失函数(Loss Function) 代价函数(Cost Function) 目标函数(Objective Function) 数据集 单个样本 整个训练集 训练要优化函数 在实际应用中，损失函数、代价函数和目标函数这些术语有时会被混用，但它们都涉及到衡量模型的预测结果与真实标签之间的差异，并在训练过程中用于优化模型 损失函数大致可分为两种：回归损失(针对连续型变量)和分类损失(针对离散型变量) 具体使用哪个术语取决于上下文和个人偏好，但它们都指向类似的概念 回归损失函数 深度学习常用损失函数总览：基本形式、原理、特点 L1 Loss L1 Loss也称为Mean Absolute Error，即平均绝对误差(MAE)，公式定义为 J_{M A E}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| 优点: 对离群点(Outliers)或者异常值更具有鲁棒性 缺点: 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习 模型预测与真实值之间的误差服从拉普拉斯分布Laplace distribution 可以在一定的假设下通过最大化似然可以得到MAE损失的形式，假设模型预测与真实值之间的误差服从拉普拉斯分布Laplace distribution (\\mu=0, b=1)，则给定一个输入x_{i}，模型输出真实值y_{i}的概率为 p\\left(y_{i} \\mid x_{i}\\right)=\\frac{1}{2} \\exp \\left(-\\left|y_{i}-\\hat{y}_{i}\\right|\\right) 对其求对数可以得到的负对数似然实际上就是MAE损失的形式 \\begin{array}{l} L(x, y)=\\prod_{i=1}^{N} \\frac{1}{2} \\exp \\left(-\\left|y_{i}-\\hat{y}_{i}\\right|\\right) \\\\ \\\\ L L(x, y)=-\\frac{N}{2}-\\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| \\\\ \\\\ N L L(x, y)=\\sum_{i=1}^{N}\\left|y_{i}-\\hat{y}_{i}\\right| \\end{array} L2 Loss L2 Loss也称为Mean Squred Error，即均方差(MSE)，它衡量的是预测值与真实值之间距离的平方和 J_{M S E}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} 优点: 收敛速度快，能够对梯度给予合适的惩罚权重，而不是一视同仁，使梯度更新的方向可以更加精确 缺点: 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性，假如我们训练数据中存在较大的异常值，此时我们将会有一个巨大的权重更新，这有可能会使模型失去平衡 在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的 MSE假设了误差服从高斯分布，在高斯分布假设下，可以使用最大化似然得到均方差损失的形式，假设模型预测与真实值之间的误差服从标准高斯分布(\\mu=0, \\sigma=1)，则给定一个输入x_{i}，模型输出真实值y_{i}的概率为 p\\left(y_{i} \\mid x_{i}\\right) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left[-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right] =\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{2}\\right) 进一步我们假设数据集中\\mathrm{N}个样本点之间相互独立，则给定所有x输出所有真实值y的概率，即似然Likelihood为所有p\\left(y_{i} \\mid x_{i}\\right)的累乘 L(x, y)=\\prod_{i=1}^{N} \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{2}\\right) 通常为了计算方便，我们通常最大化对数似然Log-Likelihood L L(x, y)=\\log (L(x, y))=-\\frac{N}{2} \\log 2 \\pi-\\frac{1}{2} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} 去掉与\\hat{y}_{i}无关的第一项，然后转化为最小化负对数似然Negative Log-Likelihood N L L(x, y)=\\frac{1}{2} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} 可以看到这个实际上就是均方差损失的形式，也就是说在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的 因此在这个假设能被满足的场景中(比如回归)，均方差损失是一个很好的损失函数选择 当这个假设没能被满足的场景中(比如分类)，均方差损失不是一个好的选择 MAE和MSE作为损失函数的主要区别是 MSE损失相比MAE通常可以更快地收敛 当使用梯度下降算法时，MSE的梯度为-\\hat{y_{i}}，而MAE损失的梯度为\\pm 1 ，即MSE的梯度的scale会随误差大小变化，而MAE的梯度的scale则一直保持为1，即便在绝对误差\\left|y_{i}-\\hat{y_{i}}\\right|很小的时候，MAE的梯度scale也同样为1，这实际上是非常不利于模型的训练的 当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了MSE在大部分时候比MAE收敛地更快，这也是MSE更为流行的原因 MAE损失对于outlier更加健壮，即更加不易受到outlier影响，当误差非常大的时候，MSE损失会远远大于MAE损失 MSE假设了误差服从高斯分布，MAE假设了误差服从拉普拉斯分布，拉普拉斯分布本身对于outlier更加robust 适用场景 Smooth L1 Loss pytorch SMOOTHL LOSS Smooth L1 Loss即平滑的L1损失(SLL)，出自Fast RCNN，也称为SLL，Smooth L1 loss也具备了L1 loss和L2 loss各自的优点，本质就是L1和L2的组合 J_{SLL} =\\left\\{ \\begin{array}{ll}0.5\\left(y_{i}-\\hat{y}_{i}\\right)^{2} / \\beta , & \\text { if }\\left|y_{i}-\\hat{y}_{i}\\right| Huber Loss pytorch Huber LOSS Huber Loss是一种类似于Smooth L1 Loss的损失函数，它也能够平衡L2范数和L1范数之间的权衡 Huber loss和Smooth L1 loss具有相同的曲线走势，当Huber loss中的δ等于1时，Huber loss等价于Smooth L1 loss J_{HL} =\\left\\{ \\begin{array}{ll}0.5\\left(y_{i}-\\hat{y}_{i}\\right)^{2} , & \\text { if }\\left|y_{i}-\\hat{y}_{i}\\right| 对于Huber损失来说，\\delta的选择十分重要，它决定了模型处理局外点的行为。当残差大于\\delta时使用L1损失，很小时则使用更为合适的L2损失来进行优化 优点 零点导数连续: Huber损失函数克服了MAE和MSE的缺点，不仅可以保持损失函数具有连续的导数 解决离群点梯度爆炸的问题: 利用MSE梯度随误差减小的特性来得到更精确的最小值，也对局外点具有更好的鲁棒性 但Huber损失函数的良好表现得益于精心训练的超参数\\delta，当\\delta趋向于0时它就退化成了MAE，而当\\delta趋向于无穷时则退化为了MSE Quantile Loss 分位数回归Quantile Regression是一类在实际应用中非常有用的回归算法，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数 IoU Loss UnitBox: An Advanced Object Detection Network 2016 分类损失函数 Binary Cross Entropy 简单的交叉熵损失函数，你真的懂了吗 对于分类问题，最常用的损失函数是交叉熵损失函数Cross Entropy Loss 考虑二分类，在二分类中我们通常使用Sigmoid函数将模型的输出压缩到(0,1)区间内\\hat{y_{i}} \\in(0,1)，用来代表给定输入x_{i}，模型判断为正类的概率 由于只有正负两类， 因此同时也得到了负类的概率 \\begin{array}{l} p\\left(y_{i}=1 \\mid x_{i}\\right)=\\hat{y_{i}} \\\\ p\\left(y_{i}=0 \\mid x_{i}\\right)=1-\\hat{y_{i}} \\end{array} 将两条式子合并成一条 p\\left(y_{i} \\mid x_{i}\\right)=\\left(\\hat{y}_{i}\\right)^{y_{i}}\\left(1-\\hat{y}_{i}\\right)^{1-y_{i}} 假设数据点之间独立同分布，则似然可以表示为 L(x, y)=\\prod_{i=1}^{N}\\left(\\hat{y}_{i}\\right)^{y_{i}}\\left(1-\\hat{y}_{i}\\right)^{1-y_{i}} 对似然取对数，然后加负号变成最小化负对数似然，即为交叉熵损失函数的形式 N L L(x, y)=J_{C E}=-\\sum_{i=1}^{N}\\left(y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y_{i}}\\right)\\right) 可视化 下图是对二分类的交叉熵损失函数的可视化，蓝线是目标值为0时输出不同输出的损失，黄线是目标值为1时的损失 可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长 图中蓝线是y_i=0的图线，此时损失函数变为 J_{CE} = -log(1-\\hat{y_{i}}) 图中黄线是y_i=1的图线，此时损失函数变为 J_{CE} = -log (\\hat{y_{i}}) 从图形中我们可以发现：预测输出与y差得越多，J_{CE}的值越大，也就是说对当前模型的惩罚越大，而且是非线性增大，是一种类似指数增长的级别 这是由log函数本身的特性所决定的，这样的好处是模型会倾向于让预测输出更接近真实样本标签y Cross Entropy Loss 交叉熵损失函数（CrossEntropy Loss） 在多分类的任务中，交叉樀损失函数的推导思路和二分类是一样的，变化的地方主要有两个 维度变化: 真实值y_{i}现在是一个one-hot向量 激活函数: 模型输出的最后的激活函数由原来的Sigmoid函数换成Softmax函数 为什么分类用交叉熵损失，而不是均方差损失 均方差损失实际上均方差损失假设了误差服从高斯分布，在分类任务下这个假设没办法被满足，因此效果会很差 为什么是交叉熵损失呢? 有两个角度可以解释这个事情，一个角度从最大似然的角度，另一个角度是可以用信息论来解释交叉熵损失 最大似然角度 Softmax函数将每个维度的输出范围都限定在(0,1)之间，同时所有维度的输出和为1，用于表示一个概率分布 p\\left(y_{i} \\mid x_{i}\\right)=\\prod_{k=1}^{K}\\left(y_{i}^{k}\\right)^{y_{i}^{k}} 其中k \\in K表示\\mathrm{K}个类别中的一类，同样的假设数据点之间独立同分布，可得到负对数似然为 N L L(x, y)=J_{C E}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(y_{i}^{k}\\right) 由于y_{i}是一个one-hot向量，除了目标类为1之外其他类别上的输出都为0，因此上式也可以写为 J_{C E}=-\\sum_{i=1}^{N} y_{i}^{c_{i}} \\log \\left(y_{i}^{\\hat{c}_{i}}\\right) 其中c_{i}是样本x_{i}的目标类。通常这个应用于多分类的交叉樀损失函数也被称为Softmax Loss或者Categorical Cross Entropy Loss 信息论角度 假设对于样本x_{i}存在一个最优分布y_{i}^{\\star}真实地表明了这个样本属于各个类别的概率，那么我们希望模型的输出\\hat{y}_{i}尽可能地逼近这个最优分布 在信息论中，我们可以使用KL散度(Kullback-Leibler Divergence)来衡量两个分布的相似性 给定分布p和分布q，两者的KL散度公式如下 K L(p, q)=\\sum_{k=1}^{K} p^{k} \\log \\left(p^{k}\\right)-\\sum_{k=1}^{K} p^{k} \\log \\left(q^{k}\\right) 其中第一项为分布p的信息熵，第二项为分布p和q的交叉熵。将最优分布y_{i}^{\\star}和输出分布\\hat{y}_{i}带入p和q得到 K L\\left(y_{i}^{\\star}, \\hat{y_{i}}\\right)=\\sum_{k=1}^{K} y_{i}^{\\star k} \\log \\left(y_{i}^{\\star k}\\right)-\\sum_{k=1}^{K} y_{i}^{\\star k} \\log \\left(y_{i}^{\\hat{k}}\\right) 由于我们希望两个分布尽量相近，因此我们最小化KL散度。同时由于上式第一项信息熵仅与最优分布本身相关，因此我们在最小化的过程中可以忽略掉，变成最小化 -\\sum_{k=1}^{K} y_{i}^{\\star k} \\log \\left(y_{i}^{\\hat{k}}\\right) 我们并不知道最优分布y_{i}^{\\star}，但训练数据里面的目标值y_{i}可以看做是y_{i}^{\\star}的一个近似分布 -\\sum_{k=1}^{K} y_{i}^{k} \\log \\left(y_{i}^{\\hat{k}}\\right) 这个是针对单个训练样本的损失函数，如果考虑整个数据集，则 J_{K L}=-\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i}^{k} \\log \\left(y_{i}^{\\hat{k}}\\right)=-\\sum_{i=1}^{N} y_{i}^{c_{i}} \\log \\left(y_{i}^{\\hat{c}_{i}}\\right) 可以看到通过最小化交叉嫡的角度推导出来的结果和使用最大化似然得到的结果是一致的 Hinge Loss 合页损失Hinge Loss是另外一种二分类损失函数，适用于maximum-margin的分类，支持向量机Support Vector Machine (SVM)模型的损失函数本质上就是Hinge Loss + L2正则化 J_{\\text {hinge }}=\\sum_{i=1}^{N} \\max \\left(0,1-\\operatorname{sgn}\\left(y_{i}\\right) \\hat{y_{i}}\\right) 下图是y为正类，即sgn(y)=1时，不同输出的合页损失示意图 可以看到当y为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在区间时还会有一个较小的惩罚 即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失 使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类 Focal Loss Focal Loss for Dense Object Detection 2018 ppt: Focal Loss for Dense Object Detection Focal Loss损失函数(超级详细的解读) Focal loss损失函数是为了解决one-stage目标检测中正负样本极度不平衡的问题，由何恺明(Kaiming He)团队提出 Focal loss是基于BCE(二分类交叉熵)的。它是一个动态缩放的交叉熵损失，通过一个动态缩放因子，可以动态降低训练过程中易区分样本的权重，从而将重心快速聚焦在那些难区分的样本(有可能是正样本，也有可能是负样本，但都是对训练网络有帮助的样本) 正负样本不平衡(Class Imbalance) 在一张图像中能够匹配到目标的候选框(正样本)个数一般只有十几个或几十个，而没有匹配到的候选框(负样本)则有10000~100000个 这么多的负样本不仅对训练网络起不到什么作用，反而会淹没掉少量但有助于训练的样本 Focal loss是为了解决一阶段目标检测模型，那为什么二阶段不用解决 在two-stage中分了两步，第一步时同样也会生成许多的负样本以及很少的正样本，但到第二步时，它会在第一步的基础上选取特定数量的正负样本去检测，所以正负样本并不会特别不平衡，二阶段模型还可以采用更复杂的采样策略和hard negative mining (难例挖掘)等方法来处理样本不平衡和难易样本的问题，因此对于二阶段目标检测模型来说，Focal Loss的优势可能相对较小 引出Focal loss 为了方便接下来的描述，这里先定义p_t为 p_{\\mathrm{t}}=\\left\\{\\begin{array}{ll}p & \\text { if } y=1 \\\\ 1-p & \\text { otherwise }\\end{array}\\right. 此时cross entropy可以定义为 J_{FL}(p, y)=\\left\\{\\begin{array}{ll}-\\log (p) & \\text { if } y=1 \\\\ -\\log (1-p) & \\text { otherwise }\\end{array}\\right. \\longrightarrow J_{FL}(p, y)=J_{FL}\\left(p_{\\mathrm{t}}\\right)=-\\log \\left(p_{\\mathrm{t}}\\right) 解决类别不平衡的常见方法是为类别1引入一个权重因子 \\alpha \\in [0, 1]，而对于类别非1引入权重因子1-\\alpha，这里引出Balanced Cross Entropy(平衡交叉熵) J_{FL}\\left(p_{\\mathrm{t}}\\right)=-\\alpha_{\\mathrm{t}} \\log \\left(p_{\\mathrm{t}}\\right) 在论文实验中显示，密集检测器训练过程中遇到的类别不平衡问题使得交叉熵损失失去了效果，易于分类的负样本占据了大部分损失并主导了梯度 虽然\\alpha平衡了正样本和负样本的重要性，但它无法区分易于和困难的样本 因此，论文提出了新的损失函数以减小易于样本的权重，从而将训练的重点放在困难的负样本上，更具体地说，论文提出在交叉熵损失中添加一个调制因子\\left(1-p_{\\mathrm{t}}\\right)^{\\gamma}，其中\\gamma是可调的Focal参数。我们将这个损失函数称为Focal loss，定义Focal Loss公式如下 J_{FL}\\left(p_{\\mathrm{t}}\\right) = - \\alpha_{t} \\left(1-p_{\\mathrm{t}}\\right)^{\\gamma} \\log \\left(p_{\\mathrm{t}}\\right) 其中\\gamma作用是调节难易，较小的\\gamma值会使得易样本的损失权重下降更慢，而较大的\\gamma值则会加速易样本的损失权重下降 而\\alpha作用是平衡正负样(正负样本数量不均衡)，当\\alpha接近0时，负样本的损失贡献被放大，从而平衡了正负样本之间的重要性 通过调整\\alpha和\\gamma的值，可以根据具体情况调节模型对不同样本的关注程度，提高模型对难样本的学习和训练效果 可视化 下图可视化了\\gamma \\in[0,5]的值，可以观察到 增加了分类不准确样本在损失函数中的权重 增加了难分样本在损失函数的权重，使得损失函数倾向于难分的样本，有助于提高难分样本的准确度 \\gamma作用是调节难易样本对于总loss的权重(正负样本中都有难易，都进行了调节) \\gamma调节简单样本权重降低的速率，当\\gamma=0时即为交叉熵损失函数，当\\gamma增加时，调整因子的影响也在增加。实验发现\\gamma=2是最优 基于概率的损失 KL散度 KL-散度损失函数的定义如下 J_{KL} = -\\sum_{i=0}^{C} y_{i} \\log \\left(\\hat{y}_{i}\\right)-y_{i} \\log \\left(y_{i}\\right)=\\sum_{i=0}^{C} y_{i}\\left(\\frac{y_{i}}{\\hat{y}_{i}}\\right) 优点： 适用于近似复杂的目标分布，如图像 如上所示，KL散度损失是从我们网络预测的交叉熵分布与目标分布的熵之间的差异。它告诉我们模型离期望的分布有多远 那么我们什么情况下使用它 如果我们的任务是生成图像，那么目标分布要复杂得多，在这种情况下，使用KL散度损失的效果最好 正则化技术 L1正则化 在损失函数中添加模型参数的 L1 范数作为正则项。它促使模型参数稀疏化，即将一些参数压缩为零，从而实现特征选择和模型简化 L2正则化 在损失函数中添加模型参数的 L2 范数作为正则项。它对模型参数进行平滑约束，使模型参数值趋向于较小的值，有助于防止过拟合 弹性网正则化 弹性网正则化是 L1 正则化和 L2 正则化的一种结合。它同时对模型参数使用 L1 和 L2 正则化，从而综合考虑了稀疏性和平滑性的影响 其他正则 Dropout：Dropout 是一种在训练过程中随机丢弃部分神经元的技术。它可以防止神经网络过拟合，并提高模型的泛化能力。通过在训练过程中以一定概率将部分神经元置零，Dropout 可以强制模型在没有完整神经网络的情况下进行学习 数据增强(Data Augmentation)：数据增强是一种通过对训练数据进行随机变换来扩充数据集的技术。常见的数据增强操作包括随机裁剪、旋转、翻转、平移、缩放等。数据增强可以增加训练数据的多样性，提高模型的泛化能力和鲁棒性 图像增强技术：了解常见的图像增强方法，如对比度调整、亮度调整、色彩平衡、直方图均衡化等，以及它们在图像处理和计算机视觉中的应用 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/深度学习核心之激活函数.html":{"url":"chapters/深度学习核心之激活函数.html","title":"深度学习核心之激活函数.md","summary":"深度学习核心之激活函数","keywords":"","body":"概述常见激活函数SigmoidTanhReluLeaky ReLUPReLUELUGELUSoftplusMaxoutSwishMishSoftSign激活函数的选择 概述 激活函数（Activation Function） 激活函数可视化 激活函数(Activation Function)是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式 引入激活函数的目的 不使用激活函数的话，神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。因为线性模型的表达能力通常不够 所以这时候就体现了激活函数的作用了，激活函数可以引入非线性因素，设有三个线性变换 \\begin{array}{l}\\mathrm{z}_{1}=\\mathrm{w}_{1} \\mathrm{x}+\\mathrm{b}_{1} \\\\ \\mathrm{z}_{2}=\\mathrm{w}_{2} \\mathrm{x}+\\mathrm{b}_{2} \\\\ \\mathrm{z}_{3}=\\mathrm{w}_{3} \\mathrm{x}+\\mathrm{b}_{3}\\end{array} 将这三个线性变换加到一起，可以得到 \\begin{aligned} \\mathrm{y} & =\\mathrm{w}_{4} \\mathrm{z}_{1}+\\mathrm{w}_{5} \\mathrm{z}_{2}+\\mathrm{w}_{6} \\mathrm{z}_{3}+\\mathrm{b}_{4} \\\\ & =\\left(\\mathrm{w}_{1} \\mathrm{w}_{4}+\\mathrm{w}_{2} \\mathrm{w}_{5}+\\mathrm{w}_{3} \\mathrm{w}_{6}\\right) \\mathrm{x}+\\left(\\mathrm{b}_{1} \\mathrm{w}_{4}+\\mathrm{b}_{2} \\mathrm{w}_{5}+\\mathrm{b}_{3} \\mathrm{w}_{6}+\\mathrm{b}_{4}\\right) \\\\ & =\\mathrm{ax}+\\mathrm{b}\\end{aligned} 所以，如果神经网络只有线性，那么不论有多少隐层，有多少神经元，最终还是线性的 此时就需要通过添加激活函数来对每一层的输出做处理，引入非线性因素，使得神经网络可以逼近任意的非线性函数 激活函数在神经网络中的应用，除了引入非线性表达能力，其在提高模型鲁棒性、缓解梯度消失问题、将特征输入映射到新的特征空间以及加速模型收敛等方面都有不同程度的改善作用 小结: 引入非线性表达能力 提高模型鲁棒性：非线性的激活函数能够引入非线性变换，从而使神经网络具有更强的表达能力，能够更好地拟合复杂的数据分布和模式 缓解梯度消失问题：在深层神经网络中，由于链式求导的关系，梯度在反向传播过程中会逐渐减小，导致梯度消失的问题 使用非线性激活函数可以帮助缓解梯度消失，因为这些函数在输入的不同范围内具有不同的斜率，从而允许梯度能够在反向传播中更好地传递 常见激活函数 Activation Functions with Derivative and Python code: Sigmoid vs Tanh Vs Relu 【激活函数合集】盘点当前最流行的激活函数及选择经验 激活函数发展历程 早期 1970 1980 2010 2013 阶跃函数(Step Function) Sigmoid 双曲正切(Tanh) ReLU(Rectified Linear Unit) Leaky ReLUMaxout 2015 2016 2017 2020 PReLU(Parametric ReLU) ELU(Exponential Linear Unit)GELUS(Gaussian Error Linear Units) Swish Mish 相对应的论文链接如下 Maxout Networks 2013 Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 2015 PReLU Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) 2016 Gaussian Error Linear Units (GELUS) 2016年提出 Searching for Activation Functions (Swish) 2017 Swish: a Self-Gated Activation Function 2017 Mish: A Self Regularized Non-Monotonic Neural Activation Function 2019 Sigmoid Sigmoid函数(Logistic函数)函数的图像看起来像一个S形曲线，函数表达式如下 f(x) = \\frac{1}{1 + e^{-x}} 对应的导数为 \\begin{array}{l}f^{\\prime}(x)=-\\frac{1}{\\left(1+e^{-x}\\right)^{2}} \\times\\left(1+e^{-x}\\right)^{\\prime}=-\\frac{1}{\\left(1+e^{-x}\\right)^{2}} \\times\\left(-e^{-x}\\right)=\\frac{1}{1+e^{-x}} \\times \\frac{e^{-x}}{1+e^{-x}} \\\\ =\\frac{1}{1+e^{-x}} \\times \\frac{1+e^{-x}-1}{1+e^{-x}}=f(x)(1-f(x))\\end{array} 函数图像和对应的导数图像如下图所示 作用 将输入映射到0到1之间的连续值，常用于二分类问题或输出层的概率预测 Sigmoid激活函数的优缺点 优点 平滑 易于求导 可以作为概率，辅助模型解 缺点 梯度消失: Sigmoid函数的导数形式为 f'(x) = f(x)(1-f(x))，其中f(x)是Sigmoid函数 当输入值非常大时，Sigmoid函数接近于1，导数接近于0，而当输入值非常小时，Sigmoid函数接近于0，导数同样接近于0 这意味着在反向传播时，梯度逐渐变小并趋近于零，传递到较早层时，梯度几乎消失了 梯度更新慢: 如果神经元的输出大部分位于饱和区域(接近0或1)，那么梯度将非常小，因为sigmoid函数在饱和区域的导数接近于0(如图所示) 这样，权重的更新将变得非常缓慢，可能导致训练过程变得困难，并且网络的收敛速度减慢 计算效率低: Sigmoid函数执行指数运算，计算机运行得较慢 Tanh Tanh函数(双曲正切函数)函数的图像看起来也像一个S形曲线，函数表达式如下 \\tanh (x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} 对应的导数为 \\begin{aligned} \\tanh ^{\\prime}(x) & =\\left(\\left(e^{x}-e^{-x}\\right)\\left(e^{x}+e^{-x}\\right)^{-1}\\right)^{\\prime} \\\\ & =\\left(e^{x}+e^{-x}\\right)\\left(e^{x}+e^{-x}\\right)^{-1}-\\left(e^{x}-e^{-x}\\right)\\left(e^{x}+e^{-x}\\right)^{-2}\\left(e^{x}-e^{-x}\\right) \\\\ & =1-\\frac{\\left(e^{x}-e^{-x}\\right)^{2}}{\\left(e^{x}+e^{-x}\\right)^{2}} \\\\ & =1-\\tanh ^{2}(x)\\end{aligned} Tanh图像和sigmoid函数比较如下图所示 与Sigmoid函数对比 Tanh是一个双曲正切函数，Tanh函数和sigmoid函数的曲线相对相似，但是它比sigmoid函数更有一些优势，这两种激活函数均为饱和激活函数 优点： 范围更广：tanh函数的输出范围是(-1, 1)，而sigmoid函数的输出范围是(0, 1)，tanh函数在均值为0附近更集中，对于某些任务可能更适用 收敛速度快：tanh函数在输入的绝对值较大时，输出的梯度也较大，这可以帮助网络更快地收敛 具有零中心性：tanh函数的均值为0，相对于sigmoid函数来说更容易处理正负值的输入，训练相对容易 缺点： 梯度消失问题: 与Sigmoid函数类似，梯度消失问题仍然存在 计算代价较高：相对于sigmoid函数来说，tanh函数的计算代价较高，因为它需要进行指数运算 Relu ReLU函数的主要优点是计算简单、非线性表达能力强、使网络更快速地收敛，并且在深度神经网络中表现出良好的性能，函数表达式如下 \\operatorname{ReLU}(x)=\\left\\{\\begin{array}{ll}x & \\text { if } x>0 \\\\ 0 & \\text { if } x \\leq 0\\end{array}\\right. 对应的导数为 \\operatorname{ReLU} ^{\\prime} (x)=\\left\\{\\begin{array}{ll}1 & \\text { if } x>0 \\\\ 0 & \\text { if } x \\leq 0\\end{array}\\right. 函数图像和对应的导数图像如下图所示 主要优缺点 优点： 计算速度快：ReLU函数的计算非常简单，只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快 缓解梯度消失问题：相比于sigmoid和tanh等函数，在正区间上ReLU函数的导数为常数1，这有助于缓解梯度消失问题，使得深层网络的梯度能够更好地传播，更容易进行反向传播算法的优化，至少x在正区间内，神经元不会饱和 激活稀疏性：ReLU函数对于负输入值直接输出为0，这导致神经元的激活变得稀疏，即只有部分神经元会被激活。这种稀疏性有助于减少参数之间的冗余，并且在一定程度上具有正则化的效果 缺点： 神经元死亡问题：ReLU函数在负区间上输出恒为0，当神经元在训练过程中出现负输入时，会导致该神经元永远不会被激活，称为神经元死亡问题。这会导致一部分神经元失去了学习能力 输出不是零中心：ReLU函数的输出范围是[0, + \\infty)，并不以0为中心，这可能会对某些优化算法的收敛速度产生影响 训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入的是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活 在实际训练中，如果学习率设置的太高，可能会发现网络中40%的神经元都会死掉，且在整个训练集中这些神经元都不会被激活 所以，设置一个合适的较小的学习率会降低这种情况的发生 为了解决神经元节点死亡的情况，有人提出了Leaky ReLU、P-ReLU、R-ReLU、ELU等激活函数 Leaky ReLU Leaky ReLU函数在负数范围内引入了一个小的斜率，解决了ReLU函数中负数部分的死亡问题，函数表达式如下 f(x)=\\left\\{\\begin{array}{cc}x & x>0 \\\\ \\alpha x & x \\leq 0\\end{array}\\right. 对应的导数为 f^{\\prime}(x)=\\left\\{\\begin{array}{ll}1 & x>0 \\\\ \\alpha & x 函数图像和对应的导数图像如下图所示 优缺点 优点: Leaky ReLU中引入了超参数，一般设置为0.01。在反向传播过程中，对于Leaky ReLU的输入小于零的情况，也可以计算得到一个梯度(而不是像ReLU一样值为0)，这样就避免了神经元死亡的问题 缺点: 稀疏性差: 相较于ReLU，神经网络的稀疏性要差一些 额外的超参数: 引入了额外的超参数\\alpha 神经网络不学习\\alpha值 为什么Leaky ReLU比ReLU更好 调整负值的零梯度: Leaky ReLU通过把x的非常小的线性分量给予负输入(0.01x)来调整负值的零梯度(zero gradients)问题 扩大函数的范围: Leaky ReLU有助于扩大ReLU函数的范围，通常\\alpha的值为0.01左右 取值范围: Leaky ReLU的函数范围是(负无穷到正无穷) 从理论上讲，Leaky ReLU具有ReLU的所有优点，而且Dead ReLU不会有任何问题，但在实际操作中，尚未完全证明Leaky ReLU总是比ReLU更好 PReLU P-Relu激活函数的解析式 f\\left(x\\right)=\\left\\{\\begin{array}{ll}x, & \\text { if } x>0 \\\\ \\alpha_{i} x, & \\text { if } x \\leq 0\\end{array}\\right. P-Relu函数及其导数的图像如下图所示 其中\\alpha是超参数。这里引入了一个随机的超参数\\alpha，它可以被学习，因为你可以对它进行反向传播 这使神经元能够选择负区域最好的梯度，有了这种能力，它们可以变成ReLU或Leaky ReLU 与ReLU和Leaky ReLU的关系 看一下PReLU的公式：参数α通常为0到1之间的数字，并且通常相对较小 如果\\alpha_{i}=0，则f变为ReLU 如果\\alpha_{i} \\gt 0，则f变为Leaky ReLU 如果\\alpha_{i}是可学习的参数，则f变为PReLU ELU ELU as an Activation Function in Neural Networks CUDA编程入门之激活函数ELU ELU激活函数解决了ReLU的一些问题，同时也保留了一些好的方面。这种激活函数要选取一个\\alpha值；常见的取值是在0.1到0.3之间 f(x)=\\left\\{\\begin{aligned} a\\left(e^{x}-1\\right) \\quad \\quad & x 对应的导数为 \\operatorname{ELU}^{\\prime}(x)=\\left\\{\\begin{array}{ll} \\operatorname{ELU}(x)+\\alpha & \\text { if } x \\lt 0 \\\\ 1 & \\text { if } x \\geq 0 \\end{array}\\right. 函数及其导数的图像如下图所示 如果我们输入的x值大于0，则结果与ReLU一样，即y值等于x值；但如果输入的x值小于0，则我们会得到一个稍微小于0的值，所得到的y值取决于输入的x值，但还要兼顾参数\\alpha你可以根据需要来调整这个参数。更进一步，我们引入了指数运算e^x，因此ELU的计算成本比ReLU高 优缺点 优点 与ReLU不同，它没有神经元死亡的问题。 这是因为 ELU 的梯度对于所有负值都是非零的 与其他线性非饱和激活函数（如 ReLU 及其变体）相比，它有着更快的训练时间 作为非饱和激活函数，它不会遇到梯度爆炸或消失的问题 所有点上都是连续的和可微 缺点 包含指数运算，计算时间长 无法避免梯度爆炸问题 神经网络无法学习\\alpha 值 与Leaky-ReLU和PReLU类似，与ReLU不同的是，ELU没有神经元死亡的问题(ReLU Dying 问题是指当出现异常输入时，在反向传播中会产生大的梯度，这种大的梯度会导致神经元死亡和梯度消失)。 它已被证明优于ReLU及其变体，如Leaky-ReLU(LReLU)和Parameterized-ReLU(PReLU)。与ReLU及其变体相比，使用ELU可在神经网络中缩短训练时间并提高准确度 GELU Gaussian Error Linear Units (GELUS) 2016年提出 论文地址 GELU可以看做是RELU的一个平滑版本；GELU激活函数的最大特点是：将非线性与依赖输入数据分布的随机正则化器相结合在一个激活函数的表达中 \\begin{array}{c} GELU(x) = 0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left(x+0.044715 x^{3}\\right)\\right]\\right) \\approx x \\sigma(1.702 x) \\end{array} 在预训练语言模型中，GELU可以说是主流的激活函数；Bert，RoBERTa，ALBERT，GPT-2等顶尖的NLP预训练模型都是使用的GELU bert中使用的激活函数，作者经过实验证明比relu等要好。原点可导，不会有Dead ReLU问题 与Relu和ELU比较如下 Softplus Softplus函数类似于ReLU函数，但是相对较平滑，像ReLU一样是单侧抑制，它的接受范围很广 f(x)=\\ln \\left(1+e^{x}\\right) 其对应的导数如下 \\left.f^{\\prime}(x)=\\ln \\left(e^{x}+1\\right)\\right)^{\\prime} = \\frac{1}{1 + e^{-x}} 优点： 作为relu的一个不错的替代选择，softplus能够返回任何大于0的值 与relu不同，softplus的导数是连续的、非零的，无处不在，从而防止出现死神经元 缺点： 导数常常小于1，也可能出现梯度消失的问题 softplus另一个不同于 relu的地方在于其不对称性，不以零为中心，可能会妨碍学习 Maxout 等待... Swish 将Sigmoid函数与线性函数的乘积作为激活函数，平衡了非线性和线性表达能力 Mish Mish: A Self Regularized Non-Monotonic Neural Activation Function 2020 f(x)=x \\tanh (\\operatorname{softplus}(x))=x \\tanh \\left(\\ln \\left(1+e^{x}\\right)\\right) SoftSign 优点： softsign是 tanh激活函数的另一个替代选择 softsign是反对称、去中心、可微分，并返回−1和1之间的值 softsign更平坦的曲线与更慢的下降导数表明它可以更高效地学习 缺点： 导数的计算比tanh更麻烦 激活函数的选择 选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后看看在验证集或者测试集上的效果 然后看哪一种表现的更好，就去使用它 以下是常见的选择情况： 如果输出是0、1值(二分类问题)，则输出层选择Sigmoid函数，然后其它的所有单元都选择ReLU函数 如果在隐藏层上不确定使用哪个激活函数，那么通常会使用ReLU激活函数。有时，也会使用Tanh激活函数 Sigmoid激活函数：除了输出层是一个二分类问题基本不会用它 Tanh激活函数：Tanh是非常优秀的，几乎适合所有场合 ReLU激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用ReLU或者Leaky ReLU，再去尝试其他的激活函数 如果遇到了一些死的神经元，我们可以使 Leaky ReLU函数 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/深度学习核心基础知识点.html":{"url":"chapters/深度学习核心基础知识点.html","title":"深度学习核心基础知识点.md","summary":"深度学习核心基础知识点","keywords":"","body":"基础知识反向传播算法梯度消失和爆炸解决方案泛化问题过拟合欠拟合核心网络层全连接层Conv卷积操作普通卷积3D卷积扩张卷积(膨胀、空洞)分组卷积反卷积(转置)可分离卷积可变形卷积池化操作最大最小池化平均池化随机池化Lp池化组合池化SPP池化ROI池化反池化其他池化归一化批归一化层归一化实例归一化组归一化Embedding层Dropout层指标 基础知识 简单说明下机器学习和深度学习的主要区别： 模型结构：一般深度学习基于深度神经网络架构较为复杂化 自动提取特征：传统机器学习需要人工构造特征但深度学习基于神经网络自动提取特征 数据量：深度学习需要大量的数据集来训练多层神经网络 反向传播算法 反向传播(Backpropagation)算法是一种用于计算神经网络中各个参数梯度的方法，它基于链式法则和梯度下降算法，下面是反向传播算法的基本步骤： 前向传播(Forward Propagation)： 输入一个样本，通过神经网络的前向传播计算得到预测输出 逐层计算每个神经元的输出值，直至输出层 在前向传播过程中，将每一层的输入、权重和激活函数的结果保存下来，用于后续的反向传播计算 计算损失函数(Loss Calculation)： 根据预测输出和真实标签计算损失函数的值 常见的损失函数包括均方误差(Mean Squared Error)、交叉熵损失(Cross-Entropy Loss)等 反向传播计算梯度(Backward Propagation)： 从输出层开始，根据链式法则计算每个参数的梯度 逐层向后传播，通过链式法则将上一层的梯度乘以当前层的局部梯度，得到当前层的梯度 根据梯度下降算法更新网络参数 参数更新(Parameter Update)： 根据计算得到的梯度值，使用梯度下降算法或其变种方法来更新网络的参数 常见的梯度下降算法包括随机梯度下降(Stochastic Gradient Descent，SGD)、动量法(Momentum)、Adam等 重复以上步骤： 对于每个训练样本，重复执行前向传播、损失计算、反向传播和参数更新的步骤 迭代训练过程直到达到预设的停止条件，如达到最大迭代次数或损失函数收敛 注意，反向传播算法会重复利用前向传播中存储的中间值，以避免重复计算，因此，需要保留前向传播的中间结果，这也会导致模型训练比单纯的预测需要更多的内存(显存) 同时这些中间结果占用内存(显存)大小与网络层的数量和批量(batch_size)大小成正比 因此使用大batch_size训练更深层次的网络更容易导致内存不足(out of memory)的错误 反向传播算法通过有效地计算每个参数的梯度，使得神经网络可以通过梯度下降等优化方法来不断更新参数以最小化损失函数 这样，神经网络可以逐步优化自身的权重和偏置，从而提高模型的性能和准确度 深度学习优化存在许多挑战，其中一些最令人烦恼的是局部最小值、鞍点和梯度消失 局部最小值(local minimum): 对于任何目标函数f(x)，如果在x处对应的f(x)值小于在x附近任何其他点的f(x)值，那么f(x)可能是局部最小值 如果f(x)在x处的值是整个域上目标函数的最小值，那么f(x)是全局最小值 鞍点(saddle point): 指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置 梯度消失(vanishing gradient): 因为某些原因导致目标函数f的梯度接近零(即梯度消失问题)，是在引入ReLU激活函数和ResNet之前训练深度学习模型相当棘手的原因之一 在深度学习中，大多数目标函数都很复杂，没有解析解，因此，我们需使用数值优化算法 梯度消失和爆炸 梯度不稳定的原因：核心在于链式法则，前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸 梯度爆炸 在反向传播过程中需要对激活函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸 梯度消失 同样如果导数小于1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失 根本原因 梯度消失和梯度爆炸，其根本原因在于反向传播训练法则，属于先天不足 计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适 比如使用sigmoid，梯度消失就会很明显了，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25，这样经过链式求导之后，很容易发生梯度消失 解决方案 梯度消失和梯度爆炸是在神经网络训练过程中常见的问题，可能导致模型收敛困难或无法有效更新权重。下面是梯度消失和梯度爆炸的产生原因和解决方法： 梯度消失(Gradient Vanishing) 原因：在深层神经网络中，梯度通过多个层传递时可能会逐渐衰减。特别是使用具有小梯度的激活函数(如Sigmoid、Tanh)时，梯度消失问题更为严重 解决方法： 大梯度的激活函数：选择具有较大梯度的激活函数，如ReLU、Leaky ReLU等 参数初始化：合理初始化权重，避免初始梯度过小或过大 批归一化(Batch Normalization)：通过对每一层的输入进行归一化，可以使得激活函数输入值的分布更稳定，有助于缓解梯度消失问题 残差连接(Residual Connections)：引入跨层的直接连接，使得梯度可以更容易地通过网络传播，例如ResNet中的残差连接结构 梯度爆炸(Gradient Explosion) 原因：在某些情况下，梯度可能会指数级增长，导致数值溢出 解决方法： 梯度裁剪(Gradient Clipping)：限制梯度的范围，通过设置阈值或缩放梯度来防止梯度爆炸 参数初始化：合理初始化权重，避免初始梯度过大 使用稀疏连接：减少网络中的连接数量，可以降低梯度爆炸的风险 更小的学习率：降低学习率，使得梯度更新更加稳定 正则化方法可以在一定程度上缓解梯度爆炸和梯度消失的问题 总的来说，解决梯度消失和梯度爆炸问题的方法包括选择合适的激活函数、参数初始化策略、批归一化、残差连接、梯度裁剪等 需要根据具体情况综合考虑并适当调整这些方法以提高训练的稳定性和效果 泛化问题 过拟合(Over fitting)和欠拟合(Under fitting)是指机器学习模型在训练过程中对训练数据的拟合程度不合适的问题 过拟合 现象 过拟合指的是模型过于复杂，过度适应了训练数据的特征，导致在新的未见过的数据上表现不佳 过拟合的表现是在训练数据上表现出较好的拟合效果，但在测试数据上的表现较差 过拟合的原因可能是模型过于复杂，参数过多，导致模型具有很强的记忆能力而忽略了数据的真实规律 产生原因 模型太复杂: 训练集的数量级和模型的复杂度不匹配，训练集的数量级要小于模型的复杂度 特征分布不一致: 训练集和测试集特征分布不一致 学习到噪音: 样本里的噪音数据干扰过大，大到模型过分的记住了噪音特征，而忽略了真实的输入输出间的关系 迭代过多: 权值学习迭代次数足够多(Over training)，拟合了训练数据中的噪声和训练样例中没有代表性的特征 解决方案 简化模型: 使其适合自己训练集的数量级(缩小宽度和减小深度)，使模型更好地学习数据的真实分布 数据增强: 训练集越多，过拟合的概率越小。在计算机视觉领域中，增广的方式是对图像旋转，缩放，剪切，添加噪声等 正则化: 指通过引入额外新信息来解决过拟合问题的一种，这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化可以保持模型简单 dropout: 在训练的时候让神经元以一定概率不工作 Early stopping: 迭代次数截断的方法来防止过拟合的，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合 欠拟合 现象 欠拟合指的是模型过于简单，不能很好地拟合训练数据的特征，导致在训练数据和测试数据上的表现都不佳 欠拟合的表现是模型无法捕捉到数据中的复杂关系或规律，无法很好地拟合训练数据 欠拟合的原因可能是模型过于简单，参数过少，无法充分学习数据的特征 产生原因 模型复杂度不足：模型的复杂度不足以捕捉数据中的复杂关系。例如，线性模型无法很好地拟合非线性数据，导致欠拟合 特征提取不足：特征提取不足意味着模型无法从原始数据中提取出有用的特征。如果特征不具备较强的区分性，模型就无法很好地进行拟合 数据量不足：当训练数据较少时，模型难以学习数据的真实分布和规律。有限的数据样本可能无法提供足够的信息来训练一个复杂的模型 过度正则化：过度正则化可以导致模型过于简单，无法很好地拟合训练数据。例如，强制施加较强的正则化项或使用较高的正则化系数会限制模型的灵活性 数据噪声：当训练数据中存在噪声或错误标签时，模型可能会过于关注这些异常样本，从而导致欠拟合 解决方案 特征工程: 欠拟合是由于学习不足，可以考虑添加特征，从数据中挖掘更多的特征，有时候还需要对特征进行变换，使用组合特征和高次特征 更强的模型: 模型简单也会导致欠拟合，如线性模型只能拟合一次函数的数据，尝试使用更高级的模型有助于解决欠拟合 调整超参数: 调整模型的超参数，如学习率、正则化系数等，以获得更好的模型拟合效果 核心网络层 全连接层 等待... Conv卷积操作 一文详解各种卷积操作 pytorch中卷积网络的几种卷积和池化 pytorch官网关于cnn的说明 卷积操作是神经网络中常用的操作之一，特别是在图像处理和计算机视觉任务中 卷积操作通过将输入数据与卷积核(也称为滤波器)进行逐元素相乘，并将结果进行求和，从而产生输出特征图 在卷积神经网络(Convolutional Neural Network，CNN)中，卷积操作用于提取输入数据中的局部特征，它利用了特征的局部相关性和平移不变性 局部相关性: 局部相关性指的是每个神经元只与输入数据的一小块区域连接，可以捕捉到输入数据的局部特征，减少了参数数量，降低了计算复杂度 平移不变性: 平移不变性指的是它对输入数据的平移具有不变性，卷积层可以通过共享权重和局部连接的方式来提取特征，从而使得对于不同位置的物体能够产生相似的特征表示 卷积输出大小计算公式 L_{out} = \\lfloor \\frac {L_{in} + 2 \\times padding - dilation \\times (kernel\\_size-1) -1}{stride}+1 \\rfloor 其中L_{in}表示输入的特征大小，padding是填充，kernel\\_size是卷积核大小，stride是步长 一维卷积在线可视化 二维卷积在线可视化 普通卷积 深度学习中卷积的概念与信号处理领域的卷积相似(只是深度学习的卷积在运算过程中的滤波器不经过翻转)，卷积核以一定的步长在输入图片上滑动，每一步都将对应元素相乘后求和的过程，如下图所示 卷积核大小(kernel_size) 卷积核大小定义了卷积的视图，根据卷积核的大小，常见的卷积尺寸有1×1卷积、3×3 卷积、5×5卷积、7×7卷积等 1 \\times 1卷积：通常用于输出维度的升维或降维。若特征图是尺寸是 H \\times W \\times D，卷积核尺寸是1 \\times 1 \\times D，输出通道尺寸是H \\times W \\times 1。当我们将N次1 \\times 1卷积结果连接在一起时，可以得到 H \\times W \\times N的输出，从而实现升维或降维的功能 3 \\times 3卷积：由于大尺寸的卷积核的参数量较大，研究人员发现两个 3 \\times 3卷积的堆叠，感受野等同于一个 5 \\times 5 卷积，但是参数量却减少了，所以从VGG的时代开始，基本后面的网络都离不开它的身影 5 \\times 5卷积：卷积核越大，感受野越大，看到的图片信息越多，所获得的全局特征越好。但是这样参数很多，会导致计算量暴增，层次少不利于模型深度的增加，表达能力弱。所以我们会看到在早期的网络会出现大卷积核的堆叠，或者当下研究人员一般将大尺寸卷积放在对输入图片的初始操作处(7 \\times 7同理) 步长(stride) 核的步长定义了卷积核在图像中移动的每一步的大小，代表提取的精度，通常为1，也可以用大于等于2的步长，对图像进行下采样，替代池化操作 填充(padding) 卷积核与输入图像的尺寸不匹配，这时就需要填充图像，例如输入图片尺寸为5 \\times 5，卷积核的大小为3 \\times 3 如果不进行填充，步长为1的话，当卷积核沿着图片滑动后只能滑动出一个3 \\times 3的图片出来，卷积后的图片越变越小，且输入图片边缘像素只被计算过一次 而中间像素会被卷积计算多次，意味着丢失图像角落信息，所以为了避免这种情况，需要先对原始图片做边界填充处理 3D卷积 3D卷积的卷积核可以在输入图像的3个方向，即图像的高度，宽度，深度上移动。并与二维卷积类似，在每个位置各元素先相乘再相加，最后输出一个3D数据 卷积特性：相比于普通的二维卷积，多了一个维度(深度)，可以把这个深度当作视频中的连续帧，或者是立体图像中的不同的切片。但是其参数量较大，会延缓网络的推理速度 应用场景：3D卷积常应用在视频分类、医学影像、点云处理等领域。比如经典的VoxelNet网络就采用3D CNN提取点云的体素特征做目标检测任务 扩张卷积(膨胀、空洞) 扩张卷积(Dilated Convolution)，也称为空洞卷积(Atrous Convolution)，最早是由DeepLab团队提出的 DeepLab是由Google Brain开发的一种用于图像语义分割的深度学习架构，随后，扩张卷积被广泛应用于其他计算机视觉任务，如图像超分辨率、目标检测、图像生成和图像修复等 扩张卷积的主要目的是增加卷积操作的感受野，从而捕捉更大范围的上下文信息 传统的卷积操作具有固定的卷积核尺寸和步幅，限制了感受野的大小。而扩张卷积通过在卷积核的元素之间引入固定的间隔(扩张率或空洞率)，使得卷积核在输入特征图上的采样间隔扩大，从而实现感受野的扩大 扩张卷积最初用于图像语义分割任务，旨在提高分割结果的精度。语义分割需要将图像中的每个像素分类到不同的语义类别中，因此需要充分考虑像素周围的上下文信息 通过使用扩张卷积，DeepLab团队可以更好地捕捉像素之间的长距离依赖关系，提高分割模型对细粒度边界和小尺寸物体的感知能力 它在这些任务中的应用主要是为了增加感受野、提高分辨率和实现多尺度特征融合，以增强模型的感知能力和表达能力 扩张的好处：使得在相同的计算成本下，避免因池化损失信息而增大了感受野 膨胀卷积使用的方法 连续使用多个膨胀卷积时应该如何设计它的膨胀系数 将膨胀系数设置为锯齿形状，例如[1,2,3,1,2,3] 公约数不能大于1，比如可以是[1,2,3]，而不是[2,4,8] 分组卷积 分组卷积（Group Convolution） Depthwise卷积与Pointwise卷积 分组卷积最开始被使用在经典入门卷积神经网络AlexNet上，用于解决显存不足的问题。在现在被广泛用于各种轻量化模型中，用于减少运算量和参数量，其中应用最广的就是深度可分离卷积(Depthwise Separable Convolution) Depthwise卷积(深度卷积)和Pointwise卷积(逐点卷积) Depthwise卷积(深度卷积)和Pointwise卷积(逐点卷积)是MobileNet等轻量级神经网络中常用的卷积操作卷积，合起来被称作Depthwise Separable Convolution(参见Google的Xception)，该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些轻量级网络中会碰到这种结构如MobileNet 可以将深度卷积和逐点卷积结合使用，作为分组卷积的一种实现方式，以提高模型的计算效率和参数效率 深度卷积用于捕捉空间上的相关信息，逐点卷积用于整合和转换通道间的特征关系。这样的组合可以在一定程度上减少计算量，并保持模型的表达能力 传统和分组卷积的比较 在传统的卷积操作中，输入特征图的每个通道都与卷积核的每个通道进行卷积运算，输出的特征图是所有通道的叠加 在分组卷积中，将输入特征图和卷积核分成多个组，每个组中的通道进行独立的卷积运算，最后将各组的输出特征图进行连接，得到最终的输出结果 左图是普通的卷积，右图是分组卷积 对于尺寸为H_{1} \\times W_{1} \\times C_{1}的输入矩阵，当标准卷积核的尺寸为h_{1} \\times w_{1} \\times C_{1}，共有C_{2}个标准卷积核时，标准卷积会对完整的输入数据进行运算，最终得到的输出矩阵尺寸为 H_{2} \\times W_{2} \\times C_{2}。这里我们假设卷积运算前后的特征图尺寸保持不变，则上述过程可以展示为左图 分组卷积中，通过指定组数g来确定分组数量，将输入数据分成g组。需要注意的是，这里的分组指的是在深度上进行分组，输入的宽和高保持不变，即将每\\frac{C_{1}}{g}个通道的数据分为一组。因为输入数据发生了改变，相应的卷积核也需要进行对应的变化，即每个卷积核的输入通道数也就变为了\\frac{C_{1}}{g}，而卷积核的大小是不需要改变的 同时，每组的卷积核个数也由原来的C_{2}变为\\frac{C_{2}}{g}。对于每个组内的卷积运算，同样采用标准卷积运算的计算方式，这样就可以得到g组尺寸为H_{2} \\times W_{2} \\times \\frac{C_{2}}{g}的输出矩阵，最终将这g组输出矩阵进行拼接就可以得到最终的结果。这样拼接完成后，最终的输出尺寸就可以保持不变，仍然是H_{2} \\times W_{2} \\times C_{2}。分组卷积的运算过程如右图所示 分组卷积的主要优点是减少了计算量和参数数量，因为每个组内的卷积操作是独立进行的，相当于将整个卷积操作拆分成了多个较小的卷积操作 这在一些计算资源有限的情况下，如移动设备或嵌入式系统中，可以显著减少计算成本，提高模型的速度和效率 需要注意的是，分组卷积的使用需要在考虑计算效率的同时权衡模型性能。分组卷积可能会损失一定的表示能力，特别是对于那些具有跨通道相关性的特征 因此，在设计网络结构时，需要根据任务的需求和资源的限制来选择合适的分组数和组内通道数，以在效率和准确性之间找到平衡点 反卷积(转置) UNet Architecture Breakdown Pytorch 转置卷积 转置卷积(Transposed Convolution)，也称为反卷积(Deconvolution)，是卷积神经网络中常用的一种操作 它与标准卷积操作相反，用于将低维特征映射扩展为高维特征映射 在传统的卷积操作中，输入特征图通过卷积核进行卷积运算，得到输出特征图。而转置卷积则是通过对输出特征图应用反向卷积操作，以重建高维特征映射 转置卷积常用于以下几个任务和应用中： 图像生成：转置卷积被广泛应用于图像生成任务，如图像生成、图像修复等。通过将低维特征映射转换为高维特征映射，可以生成具有更高分辨率和更多细节的图像 图像分割：在图像分割任务中，转置卷积常用于将低分辨率的语义特征映射恢复到与输入图像相同的尺寸，以获得像素级别的分割结果 目标检测：转置卷积在目标检测任务中被用于生成高分辨率的特征映射，以便更准确地定位和识别目标 需要注意的是，转置卷积的名称可能会引起一些误解，因为它实际上并不是真正的卷积操作的逆运算。转置卷积的名称起源于其与卷积操作的相似性，但其计算过程与卷积并不完全相同 在神经网络中，我们经常需要上采样来提高低分辨率图片的分辨率。而转置卷积就可以作为一种通过卷积学习参数，从而获得最优上采样的方法 可视化解释 下图是一个4 \\times 4的输入矩阵，用3 \\times 3的卷积核进行没有填充，步长为1的卷积操作，结果是一个2 \\times 2的矩阵 我们将3 \\times 3的卷积核重排为4 \\times 16的形式(下图第一项)，同时将4 \\times 4的输入矩阵展开为16 \\times 1的列向量的形式(下图第二项)。通过矩阵乘法得到一个4 \\times 1的列向量，可以看出这个列向量正是由上图的2 \\times 2卷积输出矩阵展开得到的，也就是说，我们可以将卷积操作写成矩阵乘法运算 通过这样的操作，可以把16(4 \\times 4的矩阵)个值映射为4(2 \\times 2的矩阵)个值，那么将这个操作反过来，我们就可以把4(2 \\times 2的矩阵)个值映射为16(4 \\times 4的矩阵) 最后将输出矩阵可以reshape为4 \\times 4的形式 卷积特征：虽然转置卷积能够通过学习参数进行最优的上采样，但是实际应用中，研究人员往往更加倾向于使用线性插值的方式做上采样，效率更高 可分离卷积 空间可分离卷积 空间可分离卷积主要处理图像和卷积核的空间维度：宽度和高度 将一个卷积核划分为两个较小的卷积核，如将3 \\times 3的卷积核分为3 \\times 1和1 \\times 3的卷积核，再依次进行卷积 如此，则可以将一次卷积的9次乘法减少为两次卷积共6次乘法，减少计算量，从而加快网络运行速度 \\left[\\begin{array}{lll}-1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 2 \\\\ 1\\end{array}\\right] \\times\\left[\\begin{array}{lll}-1 & 0 & 1\\end{array}\\right] 深度可分离卷积 深度可分离卷积不仅涉及空间维度，还对深度维度进行处理。它主要分为两个过程，分别为逐通道卷积和逐点卷积 逐通道卷积: 逐通道卷积的卷积核与通道是一一对应的，所以输出的特征图片的深度和输入的深度完全一样 逐点卷积: 逐点卷积的运算与常规卷积运算相似，它的卷积核的尺寸为1 \\times 1 \\times M，M为上一层的深度。它是将上一步的特征图在深度方向上进行加权相加，生成新的特征图,有几个卷积核就有几层新的特征图输出 卷积特性：深度可分离卷积的优势在于需要提取的属性越多，就能够节省越多的参数，减少计算量。最早出现在mobilenet中，也是用于轻量化网络的特征提取部分，使嵌入式部署的神经网络推理更快速 相较于传统的标准卷积，它只需要对每个通道进行卷积操作，而不是对每个通道和每个位置都进行卷积操作。因此，它能够在保持模型性能的同时，显著减少计算开销，特别适用于轻量级模型和计算资源受限的场景 可分离卷积常用于移动设备、嵌入式设备和计算资源有限的环境中，如移动端的图像分类、目标检测、语义分割等任务 通过采用可分离卷积，可以实现轻量级的模型结构，提高模型的计算效率和速度，同时降低模型的存储空间和内存消耗 可变形卷积 Deformable Convolutional Networks 2017 可变形卷积(Deformable Convolution)是一种具有自适应感受野的卷积操作，能够更好地捕捉图像中非刚性形变的特征。与传统的卷积操作相比，可变形卷积引入了可学习的偏移参数，用于调整卷积核在输入特征图上的采样位置 在传统的卷积操作中，卷积核的采样位置是固定的，对输入特征图的不同区域应用相同的采样模式。而可变形卷积通过在每个位置引入偏移参数，使得卷积核可以自适应地调整采样位置，从而适应不同区域的非刚性形变 思路 可变形卷积是一种卷积核内部点按不规则的方式组合，如下图所示 (a)普通卷积采样的9个点(绿点)；(b)(c)(d)都为可变形卷积，在普通卷积采样坐标上加上一个位移量(蓝色箭头)，得到变形的采样位置(深蓝点) 对于输入的一张特征图，把普通卷积的过程分成两路，先通过上面一路学习得到offset；下面的可变形卷积是基于上面生成的offset，我们的卷积窗口将由规整的绿色窗口变成蓝色部分，然后再执行普通的卷积，这种实现方式相当于于比正常的卷积操作多学习了卷积核的偏移offset 如下图所示，左边普通卷积方法没有提取到完整绵羊的特征，而右边的可变形卷积方法提取到了完整的绵羊的特征 实现DCN中的两个问题QA QA1: 如何将可变形卷积变成单独的一个层，而不影响别的层 在实际操作时，并不是真正地把卷积核进行扩展，而是对卷积前图片的像素重新整合，变相地实现卷积核的扩张。也就是说，实际上变的是每次进行卷积后得到的带偏移值的坐标值，根据这些坐标取像素点，然后双线性差值，得到新feature map，然后作为输出并成为下一层的新输入 QA2: 在前向传播实现可变性卷积中，如何能有效地进行反向传播 在图片像素整合时，需要对像素进行偏移操作，偏移量的生成会产生浮点数类型，而偏移量又必须转换为整形，直接对偏移量取整的话无法进行反向传播，这时采用双线性差值的方式来得到对应的像素 可变形卷积的使用程度取决于具体的应用场景和任务要求 目标检测：可变形卷积在目标检测任务中被广泛使用。由于目标在图像中可能存在尺度变化、形变等非刚性变化，传统的固定感受野的卷积核难以准确捕捉目标的细节和形状信息。可变形卷积通过引入可学习的偏移参数，能够自适应地调整感受野，从而提高目标检测的精度和鲁棒性 语义分割：对于语义分割任务，可变形卷积也被广泛使用。语义分割需要对图像中的每个像素进行分类，因此准确地捕捉像素周围的上下文信息至关重要。可变形卷积能够通过自适应调整采样位置来更好地捕捉非刚性形变的目标边界和细节，提高语义分割的精度和细节保留能力 人体姿态估计：在人体姿态估计任务中，可变形卷积也被广泛应用。人体姿态具有复杂的非刚性形变，传统的卷积操作往往无法准确地捕捉到人体关节的位置和姿态信息。通过引入可变形卷积，可以更好地建模人体的非刚性形变，提高姿态估计的准确性和鲁棒性 需要注意的是，可变形卷积的计算量较大，可能会增加模型的复杂性和训练的难度 因此，它通常在需要对非刚性形变建模的任务中使用，并且在设计模型时需要根据具体情况进行权衡和选择 池化操作 卷积神经网络池化方法综述 那些鬼斧神工的池化操作，看完我炸裂！ 池化的作用 抑制噪声，降低信息冗余 提升模型的尺度不变性、旋转不变性 降低模型计算量 防止过拟合 池化回传梯度的原则是保证传递的loss(或者说梯度)总和不变 最大池化: 取每个块的最大值作为下一层的一个元素值，因此下一个元素的Loss只来源于这个最大值，因此梯度更新也只更新这个最大值，其他值梯度为0 平均池化: 将输入区域内的梯度均匀地分配给该区域中的每个位置。反向传播过程中，将梯度均匀分配给输入区域内的所有位置 池化输出大小计算公式 L_{out} = \\left\\lfloor \\frac{L_{in}-kernel\\_size}{stride}\\right\\rfloor+1 最大最小池化 定义 最大池化(Max Pooling): 选择输入区域中的最大值作为输出，忽略其他值 最小池化(Min Pooling): 选择输入区域中的最小值作为输出，忽略其他值 下图是最大池化的示例图 重叠池化和非重叠池化 池化又分为重叠池化和非重叠池化 非重叠池化: stride=kernel size的情况 重叠池化: stride 重叠池化相比于非重叠池化不仅可以提升预测精度，同时在一定程度上可以缓解过拟合 平均池化 平均池化(Average Pooling): 计算输入区域内的平均值作为输出，将输入区域的值平均分配 最大池化和均值池化的弊端如下所示 随机池化 Stochastic Pooling for Regularization of Deep Convolutional Neural Networks 2013 Stochastic pooling是论文《Stochastic Pooling for Regularization of Deep Convolutional Neural Networks》中提到的一种池化策略，大意是只需对特征区域元素按照其概率值大小随机选择，元素值大的被选中的概率也大 下表是随机池化在CIFAR-10上的表现，可以看出，使用随机池化效果和采用dropout的结果接近，证明了其有一定防止过拟合的作用 Lp池化 Lp池化(Lp Pooling)是通过调整参数p的值来实现不同的池化方式，当p=1时，为平均池化，当p趋近于无穷大时，为最大池化 Lp池化的好处是可以在一定程度上保留更多的信息。平均池化会平均化局部区域的特征，可能丢失一些细节信息；而最大池化只保留局部区域的最大值，可能丢失其他重要的信息 Lp池化通过调整参数p的值，可以在特征汇聚时平衡信息的丰富性和抗噪性能 池化具体操作如下公式所描述 f(X)=\\sqrt[p]{\\sum_{x \\in X} x^{p}} 当p=1时，使用局部求和，而p为无穷大时，对应max-pooling 在Lp池化中，p的选择需要根据具体任务和数据集进行调整 较小的p值可以更加注重细节信息，适用于需要保留细粒度特征的任务 较大的p值可以更加注重全局信息，适用于更加整体化的任务 通常情况下，p取2时能够获得较好的性能 组合池化 组合池化则是同时利用最大值池化与均值池化两种的优势而引申的一种池化策略，常见组合策略有Cat和Add这两种 常常被当做分类任务的一个trick，其作用就是丰富特征层，maxpool更关注重要的局部特征，而average pooling更关注全局特征 def add_avgmax_pool2d(x, output_size=1): x_avg = F.adaptive_avg_pool2d(x, output_size) x_max = F.adaptive_max_pool2d(x, output_size) return 0.5 * (x_avg + x_max) def cat_avgmax_pool2d(x, output_size=1): x_avg = F.adaptive_avg_pool2d(x, output_size) x_max = F.adaptive_max_pool2d(x, output_size) return torch.cat([x_avg, x_max], 1) SPP池化 Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 2015 SPP(空间金字塔池化)是在SPPNet中提出的，SPPNet提出比较早，在RCNN之后提出的，用于解决重复卷积计算和固定输出的两个问题，具体方法如下图所示 空间金字塔池化的基本思想是将输入特征图分割成多个不同大小的子区域，并对每个子区域进行池化操作。这些子区域的大小和数量构成了一个金字塔形状的层次结构 然后，对每个子区域进行池化操作（通常是最大池化），生成固定大小的特征向量。最后，将所有尺度上得到的特征向量拼接在一起，形成最终的特征表示 通过空间金字塔池化，模型能够在不同尺度上捕捉到更加丰富的局部特征，从而增强了模型对尺度变化和物体大小变化的鲁棒性 这对于处理不同尺度的物体检测、图像分类和语义分割等任务非常有用 ROI池化 ROI池化(Region of Interest Pooling)是在目标检测任务中广泛使用的操作。它对于来自输入列表的每个感兴趣区域，它采用与其对应的输入特征图的一部分并将其缩放到某个预定义的大小。这可以显着加快训练和测试时间，它允许重新使用卷积网络中的特征映射，同时也允许以端到端的方式训练物体检测系统 ROI Pooling与SPP池化的区别 通过上面的介绍，可以看到两者起到的作用是相同的，把不同尺寸的特征输入转化为相同尺寸的特征输出 SPP针对同一个输入使用了多个不同尺寸的池化操作，把不同尺度的结果拼接作为输出；而ROI Pooling可看作单尺度的SPP，对于一个输入只进行一次池化操作 反池化 [CNN] 卷积、反卷积、池化、反池化 反池化(Unpooling)与池化操作相反，用于恢复特征图的尺寸，常见的方法包括最大反池化和平均反池化 在传统的池化操作中，通过将特征图的大小减小，以实现特征的降维和减少计算量。然而，这也导致了信息的损失和空间分辨率的降低 为了补偿这种信息损失，反池化和Unpooling操作用于还原特征图的空间分辨率，使其与输入特征图具有相同的尺寸 反池化通常是通过转置卷积(Transpose Convolution)来实现的。转置卷积通过在特征图之间插入空白像素或零填充，并使用适当的卷积核对这些空白像素进行卷积运算，以恢复特征图的尺寸 Unpooling则是在反池化操作中与转置卷积结合使用的一种方式。在池化操作中，通常会记录池化操作时的最大值或平均值的位置。在Unpooling中，根据这些位置信息，将反池化操作的输出值放置回原始特征图的对应位置。这样可以在恢复空间分辨率的同时保留一定的位置信息 反池化和Unpooling在卷积神经网络中起着重要的作用，特别是在语义分割和图像重建等任务中。它们帮助网络恢复输入图像的详细结构和空间信息，从而提高模型的精度和质量 其他池化 LIP池化: LIP: Local Importance-based Pooling 自适应池化(Adaptive Pooling): 根据输入的尺寸自动调整池化窗口的大小，适应不同大小的输入 归一化 BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm简介 神经网络中有各种归一化算法，从公式看它们都差不多：无非是减去均值，除以标准差，再施以线性映射 y=\\gamma\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\beta 归一化算法的主要区别在于操作的feature map维度不同 下图来自何凯明的Group Normalization 2018 可以看出 BN: 针对整个batch不同通道，具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道...加上第N个样本第1个通道，求平均，得到通道1的均值 \\begin{align}{c} \\mu_{c}(x)=\\frac{1}{N H W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n c h w} \\\\ \\sigma_{c}(x)=\\sqrt{\\frac{1}{N H W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W}\\left(x_{n c h w}-\\mu_{c}(x)\\right)^{2}+\\epsilon} \\end{align} LN：针对每个样本，对每个样本的C、H、W维度上的数据求均值和标准差，保留N维度 \\begin{align}{c} \\mu_{n}(x)=\\frac{1}{C H W} \\sum_{c=1}^{C} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n c h w} \\\\ \\sigma_{n}(x)=\\sqrt{\\frac{1}{C H W} \\sum_{c=1}^{C} \\sum_{h=1}^{H} \\sum_{w=1}^{W}\\left(x_{n c h w}-\\mu_{n}(x)\\right)^{2}+\\epsilon} \\end{align} IN：针对每个样本下的每个通道，对每个样本的H、W维度的数据求均值和标准差，保留N 、C维度，也就是说，它只在channel内部求均值和标准差 \\begin{align}{c} \\mu_{n c}(x)=\\frac{1}{H W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n c h w} \\\\ \\sigma_{n c}(x)=\\sqrt{\\frac{1}{H W} \\sum_{h=1}^{H} \\sum_{w=1}^{W}\\left(x_{n c h w}-\\mu_{n c}(x)\\right)^{2}+\\epsilon} \\end{align} GN：针对每个样本下的多个通道，计算均值和标准差时，把每一个样本feature map的channel分成G组，每组将有C/G个channel，然后将这些channel中的元素求均值和标准差。各组channel用其对应的归一化参数独立地归一化 \\begin{align}{c} \\mu_{n g}(x)=\\frac{1}{(C / G) H W} \\sum_{c=g C / G}^{(g+1) C / G} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{n c h w} \\\\ \\sigma_{n g}(x)=\\sqrt{\\frac{1}{(C / G) H W} \\sum_{c=g C / G}^{(g+1) C / G} \\sum_{h=1}^{H} \\sum_{w=1}^{W}\\left(x_{n c h w}-\\mu_{n g}(x)\\right)^{2}+\\epsilon} \\end{align} 批归一化 背景 机器学习领域有个很重要的假设：IID 独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障 在把数据喂给机器学习模型之前，白化(whitening)是一个重要的数据预处理步骤 独立: 去除特征之间的相关性 同分布: 使得所有特征具有相同的均值和方差 每批训练数据的分布各不相同，那么网络需要在每次迭代中去学习适应不同的分布，这样将会大大降低网络的训练速度。对于深度网络的训练是一个非常复杂的过程，只要网络的前面几层发生微小的改变，那么这些微小的改变在后面的层就会被累积放大下去 一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度 一文搞懂BN的原理及其实现过程（Batch Normalization） 深入理解NLP中LayerNorm的原理以及LN的代码详解 引入批归一化Batch Normalization的原因：在图像预处理过程中通常会对图像进行标准化处理，也就是image normalization，使得每张输入图片的数据分布能够统均值为\\mu，方差为 \\sigma ^2的分布。这样能够加速网络的收敛。但是当一张图片输入到神经网络经过卷积计算之后，这个分布就不会满足刚才经过image normalization操作之后的分布了，可能适应了新的数据分布规律，这个时候将数据接入激活函数中，很可能一些新的数据会落入激活函数的饱和区，导致神经网络训练的梯度消失 这个时候我们引入Batch Normalization的目的就是使我们卷积以后的feature map满足均值为0，方差为1的分布规律，再接入激活函数就不会发生这样的情况 数据归一化都是在数据输入时做的，但是实际上在任何位置都是可以进行数据归一化，在神经网络里上一层网络的输出正好就是下一层网络的输入 优点 加速收敛：归一化操作可以减少网络中不稳定的因素，有助于梯度的传播和收敛。它使得网络在训练过程中更容易优化，加速了训练的速度 减少梯度弥散问题：神经网络在深层时容易出现梯度消失或梯度爆炸的问题。Batch Normalization通过将数据归一化到一个合适的范围，使得激活函数的输入更加稳定，减少了梯度消失的风险 提高泛化能力：Batch Normalization作为一种正则化方法，有助于减少过拟合的风险。通过在训练过程中引入一些噪声，它可以提高模型的泛化能力，并使得网络对输入数据的变化更加鲁棒 Batch Normalization在每个小批量训练样本中对输入进行标准化，使其具有零均值和单位方差 图像下的BN示例 下图展示了一个batch size为2(两张图片，每张图片有3个通道，其中颜色红，绿，蓝分别代表r,g,b通道)的Batch Normalization的原理，首先会统计每个通道数目所有点的像素值，求得均值和方差，然后在每个通道上分别用该点的像素值减均值除方差得到该点的像素值，此过程就是BN。最后将其接入到激活函数中 如下图所示，其中红，绿，蓝分别代表图像不同的通道，假设假设feature map1、feature map2分别是由image1、image2经过一系列卷积池化后得到的特征矩阵 其中每个网格的值代表该点的像素值，分别统计feature map1和feature map2每个通道的像素值，得到一个矩阵，在使用BN的计算公式计算经过BN以后每个通道每个像素点的像素值 定义x为以下矩阵 \\begin{array}{l}x^{(1)}=\\{1,1,1,2,0,-1,2,2\\} \\\\ x^{(2)}=\\{-1,1,0,1,0,-1,3,1\\} \\\\ x^{(3)}=\\{2,1,1,2,1,2,-1,0\\}\\end{array} 可以计算得到\\mu和\\sigma ^2 \\begin{array}{l}\\mu_{1}=\\frac{1}{m} \\sum_{i=1}^{m} x_{i}^{(1)}=1 \\\\ \\mu_{2}=\\frac{1}{m} \\sum_{i=1}^{m} x_{i}^{(2)}=0.5 \\\\ \\mu_{3}=\\frac{1}{m} \\sum_{i=1}^{m} x_{i}^{(3)}=1 \\\\ \\sigma_{1}^{2}=\\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}^{(1)}-u_{1}\\right)^{2}=1 \\\\ \\sigma_{2}^{2}=\\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}^{(2)}-u_{2}\\right)^{2}=1.5 \\\\ \\sigma_{3}^{2}=\\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}^{(3)}-u_{3}\\right)^{2}=1 \\end{array} 即得到均值和方差为\\mu=\\left[\\begin{array}{c}1 \\\\ 0.5 \\\\ 1\\end{array}\\right]和\\sigma^{2}=\\left[\\begin{array}{c}1 \\\\ 1.5 \\\\ 1\\end{array}\\right] 这有助于解决训练过程中的内部协变量偏移(Internal Covariate Shift)问题，即网络在训练过程中的每一层的输入分布发生变化，导致网络的训练变得困难 代码示例 import torch.nn as nn import torch data = [[[1, 2, 5], [2, 5, 8.5], [3, 3, 3]], [[2, 8, 4], [1, 3, 9], [2, 6, 4]], [[1, 1, 1], [1, 3, 5], [0.5, 6, 0.2]]] data = torch.tensor(data) data_bn = nn.BatchNorm1d(3)(data) data_ln = nn.LayerNorm(3)(data) mean = torch.sum(data_bn) mu = torch.sum(th.pow(data_bn - mean, 2) / 27) print(data_bn) print(mean) print(mu) >>> tensor([[[-0.7734, -0.3384, 0.9667], [-0.7714, 0.2967, 1.5428], [-0.0400, -0.0400, -0.0400]], [[-0.3384, 2.2718, 0.5317], [-1.1274, -0.4154, 1.7208], [-0.5542, 1.5027, 0.4742]], [[-0.7734, -0.7734, -0.7734], [-1.1274, -0.4154, 0.2967], [-1.3256, 1.5027, -1.4798]]], grad_fn=) tensor(-5.9605e-07, grad_fn=) # 约等于0 tensor(1.0000, grad_fn=) 层归一化 深度学习基础之BatchNorm和LayerNorm 为什么Transformer使用LayerNorm ，而不使用BatchNorm BN的特点是强行拉平数据之间的分布，使得模型收敛速度更快，并且起到了正则化的作用，使模型效果更佳。但是，BatchNorm对Batch Size大小很敏感，并且在LSTM网络上效果极差。 LayerNorm是横向归一化，不受Batch Size大小的影响，并且可以很好地应用在时序数据中，而且不需要额外的储存空间。 《Rethinking Batch Normalization in Transformers》一文对比了LayerNorm和BatchNorm对于Transformer的作用，并且提出了一种新的归一化方式 批归一化和层归一化比较 适用场景 batch norm: 适用于CV，因为计算机视觉喂入的数据都是像素点，可以说数据点与点之间是可以比较的，所以使用batch norm可以有比较好的效果， layer norm: NLP里，每个词的词向量是一组向量表示一个词，一个词向量割裂开来看是没有意义的，因此不同词向量里的数据点是不能混为一谈的 所以batch norm之后可能会使得词损失语义，效果就可能不好了，但是使用layer norm只是让各个词向量进行标准化，就能够有比较理想的效果了 归一化维度 batch norm: 以CV为例，归一化的是相同通道，均值和方差计算是每个通道的值 layer norm: 以NLP为例，归一化的每个词向量，一个词向量自己做归一化 实例归一化 等待... 组归一化 def GroupNorm(x, gamma, beta, G, eps=1e-5): # x: input features with shape [N,C,H,W] # gamma, beta: scale and offset, with shape [1,C,1,1] # G: number of groups for GN N, C, H, W = x.shape x = tf.reshape(x, [N, G, C // G, H, W]) mean, var = tf.nn.moments(x, [2, 3, 4], keepdims=True) x = (x - mean) / tf.sqrt(var + eps) x = tf.reshape(x, [N, C, H, W]) return x * gamma + beta Embedding层 等待... Dropout层 等待... 指标 等待... Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/深度学习模型压缩技术.html":{"url":"chapters/深度学习模型压缩技术.html","title":"深度学习模型压缩技术.md","summary":"深度学习模型压缩技术","keywords":"","body":"模型压缩概述参数剪枝权重共享低秩近似知识蒸馏概述蒸馏的目的蒸馏机制离线蒸馏在线蒸馏自蒸馏温度参数网络量化网络剪枝网络蒸馏 模型压缩 概述 深度学习模型压缩与加速七大方法总结！ 随着深度学习的发展与高性能GPU处理能力的增强，神经网络结构变得越来越复杂，模型参数量也越来越庞大，这使得深度学习在移动嵌入式设备上的部署遇到巨大的困难和挑战 因此，如何在不影响深度学习模型性能的情况下进行模型压缩与加速成为了研究热点 在深度学习中，模型压缩是一种通过减小模型的大小、参数数量或计算量来降低模型复杂度的技术。以下是一些常见的模型压缩技术： 参数剪枝(Pruning)：通过剪除模型中不重要的连接或参数，减少模型的参数数量和计算量。剪枝可以基于权重的重要性进行，剪除较小的权重或通过稀疏化技术将权重设置为零 权重共享(Weight Sharing)：将模型中的一些权重共享，减少模型中需要存储的参数数量。这可以通过对权重进行聚类或使用哈希函数将相似的权重映射到同一个值来实现 低秩近似(Low-Rank Approximation)：通过将模型的权重矩阵分解为较低秩的矩阵乘积形式，减少模型的参数数量。常见的方法包括奇异值分解(SVD)和矩阵分解技术 知识蒸馏(Knowledge Distillation)：将一个复杂的模型的知识传递给一个较简单的模型，通过让学生模型学习教师模型的输出概率分布或中间表示来提高学生模型的性能 网络量化(Network Quantization)：减少模型中参数的位数表示，例如将浮点数参数转换为较低位数的整数表示，从而减小模型的存储需求和计算复杂度 网络剪枝(Network Pruning)：除了剪枝权重和连接之外，还剪枝模型的网络结构。这包括剪枝层、剪枝通道或剪枝模块等 网络蒸馏(Network Distillation)：类似于知识蒸馏，但是不仅传递教师模型的输出概率分布或中间表示，还传递教师模型的激活值或其他辅助信息 这些技术可以单独应用或结合使用，以减小深度学习模型的大小、复杂度和计算需求，从而实现模型压缩和加速的效果 参数剪枝 对模型的网络进行修剪，比如减掉多余的头(因为Transformer使用多头注意力机制)，或者直接粗暴的使用更少的Transformer层数 权重共享 等待... 低秩近似 等待... 知识蒸馏 手写数字识别中多元分类原理_广告行业中那些趣事系列21：从理论到实战BERT知识蒸馏... 深度学习中的知识蒸馏技术` 一分钟带你认识深度学习中的知识蒸馏 概述 知识蒸馏的概念最早是2015年Geoffrey Hinton在《Distilling the Knowledge in a Neural Network》这篇论文中提出来的 在深度学习中，知识蒸馏(Knowledge Distillation)指的是将一个复杂的模型(称为教师模型)的知识传递给一个较简单的模型(称为学生模型)的过程 这种方法旨在通过训练学生模型使其能够学习教师模型的推理能力和泛化能力 通常情况下，教师模型是一个较大、复杂、准确度较高的模型，而学生模型则是一个较小、简化的模型 知识蒸馏通过让学生模型学习教师模型的输出概率分布或中间表示，从而提高学生模型的性能。这种知识传递可以帮助学生模型更好地捕捉数据集的特征，提高泛化能力，并且在具有较少训练数据的情况下取得较好的性能 例子: TextCNN知识蒸馏 BERT这类大模型精度高但是线上推理速度慢，传统的文本分类模型比如TextCNN等线上推理速度快(因为模型比较小)但是精度有待提升。针对上面的问题，我们的需求是获得媲美BERT等大模型的精度，还能满足线上推理速度的时延要求 业务中可以把BERT作为老师模型去教作为学生模型的TextCNN来学习知识，从而使TextCNN不仅达到了媲美BERT的分类效果，而且还能很好的满足线上推理速度的要求 暗知识 对于老师或者没有使用知识蒸馏的小模型来说，主要是通过训练数据来学习知识。我们的训练数据集是一张一张手写数字的图片，还有对应0到9十个数字的标签。在这种学习中我们可以用的只有十个类别值，比如一张手写数字1的图片样本的标签是1，告诉模型的知识就是这个样本标签是1，不是其他类别 而使用知识蒸馏的时候模型可以学到更多的知识，比如手写数字1的图片样本有0.7的可能是数字1，0.2的可能是数字7，还有0.1的可能是数字9 这非常有意思，模型不仅学到了标签本身的知识，还学习到了标签之间的关联知识，就是1和7、9可能存在某些关联，这些知识称为暗知识 其他的蒸馏 对抗蒸馏、多教师蒸馏、跨模态蒸馏、无数据蒸馏 蒸馏的目的 模型压缩(Model Compression)：通过知识蒸馏，可以将一个较大、复杂的模型压缩成一个更小、轻量级的模型，减少模型的存储空间和计算资源的消耗。这使得模型可以在资源受限的设备上运行，同时仍保持较高的性能 模型加速(Model Acceleration)：知识蒸馏可以加速模型的推理过程，使得模型在实时性要求较高的场景中能够更快地进行预测。通过将较复杂的模型的知识转移给简化的模型，可以加快推理速度 提升泛化能力(Improving Generalization)：知识蒸馏可以通过将一个复杂模型的知识传递给一个简化模型，帮助简化模型学习到更好的特征表示和泛化能力。较复杂的模型通常具有更强大的表示能力和表达能力，通过知识蒸馏，这些能力可以传递给简化模型，提升其泛化性能 迁移学习(Transfer Learning)：知识蒸馏可以通过将一个在大规模数据集上训练过的复杂模型的知识迁移到一个相似任务的简化模型上，从而加快简化模型在新任务上的学习速度和性能 总之，知识蒸馏的主要目的是通过将一个复杂模型的知识转移到一个简化模型上，从而实现模型压缩、加速、提升泛化能力和迁移学习等目标 蒸馏机制 【深度学习】（一）知识蒸馏 根据教师网络是否和学生网络一起更新，可以分为离线蒸馏，在线蒸馏和自蒸馏 感性上理解三种蒸馏方式： 离线蒸馏(Offline Distillation): 可以理解为知识渊博的老师给学生传授知识 在线蒸馏(Online Distillation): 可以理解为教师和学生一起学习 自蒸馏(Self-Distillation): 意味着学生自己学习知识 离线蒸馏 早期的知识蒸馏方法都属于离线蒸馏，将一个预训练好的教师模型的知识迁移到学生网络，所以通常包括两个阶段： 在蒸馏前，教师网络在训练集上进行训练 教师网络通过logits层信息或者中间层信息提取知识，引导学生网络的训练 第一个阶段通常不被认为属于知识蒸馏的一部分，因为默认教师网络本身就是已经预训练好的 一般离线蒸馏算法关注与提升知识迁移的不同部分，包括：知识的形式，损失函数的设计，分布的匹配 优点 实现起来比较简单，形式上通常是单向的知识迁移(即从教师网络到学生网络)，同时需要两个阶段的训练(训练教师网络和知识蒸馏) 缺点 教师网络通常容量大，模型复杂，需要大量训练时间，还需要注意教师网络和学生网络之间的容量差异，当容量差异过大的时候，学生网络可能很难学习好这些知识 在线蒸馏 教师模型和学生模型都是to be trained的状态，即教师模型并没有预训练 在大容量教师网络没有现成模型的时候，可以考虑使用online distillation。使用在线蒸馏的时候，教师网络和学生网络的参数会同时更新，整个知识蒸馏框架是端到端训练的 可以参考本站图像分类算法章节下的DeiTModel，DeiTModel模型是对Vit模型的改进和优化，使用了在线蒸馏策略(还是离线?) 自蒸馏 自蒸馏中，教师和学生模型使用相同的网络。自蒸馏可以看作是在线蒸馏的一种特殊情况，因为教师网络和学生网络使用的是相同的模型 温度参数 等待... 网络量化 等待... 网络剪枝 等待... 网络蒸馏 等待... Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "},"chapters/目标检测与跟踪算法.html":{"url":"chapters/目标检测与跟踪算法.html","title":"目标检测与跟踪算法.md","summary":"目标检测与跟踪算法","keywords":"","body":"目标检测概述指标和数据RCNN系列rcnnspp-netfast-rcnnfaster-rcnnYolo系列Yolov1网络结构lossYolov2(Yolo9000)yolov3yolov5Yolov8其他检测系列SSDFPNRetinaNetDETR目标跟踪SortDeepSortStrongSortBotSortByteTrack依赖的算法卡尔曼滤波匈牙利匹配算法追踪指标 目标检测 经典目标检测Object Detection模型整理 深入浅出Yolo系列之Yolov5核心基础知识完整讲解 目标检测 YOLO系列算法 Object Detection in 20 Years: A Survey 论文详解 R-CNN史上最全讲解 YOLOv1论文翻译解读 YOLO v4：物体检测的最佳速度和精度 Yolo系列代码 概述 目标检测——RCNN与YOLO系列 核心问题 分类问题：即图片(或某个区域)中的图像属于哪个类别 定位问题：目标可能出现在图像的任何位置 大小问题：目标有各种不同的大小 形状问题：目标可能有各种不同的形状 算法分类 Anchor Based模型 Anchor Free模型 One-stage模型 YoloV2-5系列、SSD、RetinaNet YoloV1、FCOS、CornerNet Two-stage模型 Faster RCNN、Cascade RCNN、MaskRCNN two stage： 先进行区域生成，该区域称为region proposal（RP，一个有可能包含物体的预选框）；再通过卷积神经网络进行样本分类 任务流程：特征提取—生成RP—分类/定位回归 常见two stage：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN、R-FCN one stage： 不用RP，直接在网络中提取特征来预测物体的分类和位置 任务流程：特征提取—分类/定位回归 常见one stage：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD、RetinaNet 目标检测分为两大系列——RCNN系列和YOLO系列： RCNN系列是基于区域检测的代表性算法 YOLO是基于区域提取的代表性算法 还有著名的SSD是基于前两个系列的改进 发展脉络 2014 2015 2016 2017 2018 R-CNN Fast R-CNNFaster R-CNNYOLO YOLOSSD YOLOv2RetinaNetMask R-CNNFPN YOLOv3Cascade R-CNN 2020 2022 2023 YOLOv4YOLOv5EfficientDet YOLOv6(美团)YOLOv7 YOLOv8 指标和数据 map指标 mAP—目标检测模型的评估指标 YOLO 模型的评估指标——IOU、Precision、Recall、F1-score、mAP mAP@0.5 在YOLO模型中，你会见到mAP@0.5这样的表现形式，这种形式表示在IOU阈值为0.5的情况下，mAP的值为多少。当预测框与标注框的IOU大于0.5时，就认为这个对象预测正确，在这个前提下再去计算mAP。一般来说，mAP@0.5即为评价YOLO模型的指标之一 mAP@[0.5:0.95] YOLO模型中还存在mAP@[0.5:0.95]这样一种表现形式，这形式是多个IOU阈值下的mAP，会在q区间[0.5,0.95]内，以0.05为步长，取10个IOU阈值，分别计算这10个IOU阈值下的mAP，再取平均值。mAP@[0.5:0.95]越大，表示预测框越精准，因为它去取到了更多IOU阈值大的情况 数据集准备 How to Train YOLOv8 Object Detection on a Custom Dataset 深度学习系列之Anchor based 和 Anchor free 目标检测方法 RCNN系列 候选区域的产生 选择性搜索算法 （Selective Search) 选择性搜索算法(Selective Search)超详解（通俗易懂版） 很多目标检测技术都会涉及候选框(bounding boxes)的生成，物体候选框获取当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等) 滑动窗口法 滑动：首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动 检测：每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体 不同尺度：对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分， NMS：采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选，最终，经过NMS筛选后获得检测到的物体 选择性搜索：selective search(简称SS)方法是当下最为熟知的图像bounding boxes提取算法，由Koen E.A于2011年提出 只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率，图像中物体可能存在的区域应该是有某些相似性或者连续性区域的 分割：对输入图像进行分割算法产生许多小的子区域 合并：根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并 候选框：每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框 滑窗法简单易于理解，但是不同窗口大小进行图像全局搜索导致效率低下，而且设计窗口大小时候还需要考虑物体的长宽比。所以，对于实时性要求较高的分类器，不推荐使用滑窗法 选择搜索计算效率优于滑窗法，由于采用子区域合并策略，所以可以包含各种大小的疑似物体框，合并区域相似的指标多样性，提高了检测物体的概率 数据表示 预测输出可以表示为： \\mathrm{y}=\\left[\\begin{array}{l} \\mathrm{p}_{\\mathrm{c}} \\\\ \\mathrm{b}_{\\mathrm{x}} \\\\ \\mathrm{b}_{\\mathrm{y}} \\\\ \\mathrm{b}_{\\mathrm{w}} \\\\ \\mathrm{b}_{\\mathrm{h}} \\\\ \\mathrm{C}_{1} \\\\ \\mathrm{C}_{2} \\\\ \\mathrm{C}_{3} \\end{array}\\right] \\quad \\quad \\quad \\mathrm{y}_{\\text {true }}=\\left[\\begin{array}{c} 1 \\\\ 40 \\\\ 45 \\\\ 80 \\\\ 60 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right] \\quad \\quad \\quad \\mathrm{y}_{\\mathrm{pred}}=\\left[\\begin{array}{c} 0.88 \\\\ 41 \\\\ 46 \\\\ 82 \\\\ 59 \\\\ 0.01 \\\\ 0.95 \\\\ 0.04 \\end{array}\\right] 其中， \\mathrm{p}_{\\mathrm{c}}为预测结果的置信概率， \\mathrm{b}_{\\mathrm{x}}, \\mathrm{b}_{\\mathrm{y}}, \\mathrm{b}_{\\mathrm{w}}, \\mathrm{b}_{\\mathrm{h}}为边框坐标， \\mathrm{C}_{1}, \\mathrm{C}_{2}, \\mathrm{C}_{3}为属于某个类别的概率，通过预测结果，实际结果，构建损失函数，损失函数包含了分类、回归两部分组成 效果评估 使用IoU(Intersection over Union，交并比)来判断模型的好坏。所谓交并比，是指预测边框、实际边框交集和并集的比率，一般约定0.5为一个可以接受的值 非极大值抑制 预测结果中，可能多个预测结果间存在重叠部分，需要保留交并比最大的、去掉非最大的预测结果，这就是非极大值抑制(Non-Maximum Suppression，简写作NMS)，非极大值抑制的流程如下： 根据置信度得分进行排序 选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除 计算所有边界框的面积 计算置信度最高的边界框与其它候选框的IoU。 删除IoU大于阈值的边界框 重复上述过程，直至边界框列表为空 bbox回归训练：其实就是训练 d 矩阵向 t 矩阵靠齐的过程 【目标检测】基础知识：IoU、NMS、Bounding box regression 即: 给定 \\left(P_{x}, P_{y}, P_{w}, P_{h}\\right)，寻找一种映射f，使得 f\\left(P_{x}, P_{y}, P_{w}, P_{h}\\right)=\\left(G_{x}^{\\prime}, G_{y}^{\\prime}, G_{w}^{\\prime}, G_{h}^{\\prime}\\right) \\text { , 且 }\\left(G_{x}^{\\prime}, G_{y}^{\\prime}, G_{w}^{\\prime}, G_{h}^{\\prime}\\right) \\approx\\left(G_{x}, G_{y}, G_{w}, G_{h}\\right) 主要操作就是平移+缩放 rcnn RCNN 分为三个module： 独立类别的候选区域（category-independent region proposals），生成一组对检测器可用的检测坐标 常见的候选区生成的方法有很多（objectness、selective search、category-independent object proposals、constrained parametric min-cuts (CPMC) 、multi-scale combinatorial grouping），本文用的是选择搜索。产生了2000个候选区域（region proposal） 使用卷积神经网络从每个区域从提取固定的特征向量 本文每个区域提取到的固定长度的特征向量是4096，使用的网络是AlexNet 需要注意的是 Alextnet 的输入图像大小是 227\\times227，而通过 Selective Search 产生的候选区域大小不一，为了与 Alexnet 兼容，R-CNN 采用了非常暴力的手段，那就是无视候选区域的大小和形状，统一变换到 227\\times227的尺寸(就是只有候选框里保留，剩余部分填充其它像素，或者先在候选框周围加上16的padding，再进行各向异性缩放，这种形变使得mAp提高了3到5个百分点)。有一个细节，在对 Region 进行变换的时候，首先对这些区域进行膨胀处理，在其 box 周围附加了 p 个像素，也就是人为添加了边框，在这里 p=16 在 ImageNet 上先进行预训练，然后利用成熟的权重参数在 PASCAL VOC 数据集上进行 fine-tune，如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。 训练过程：首先对 PASCAL VOC数据集 进行Selective Search，搜索到2000个Region Proposal对Pre-trained模型进行fine-tuning。将原来预训练模型最后的1000-way的全连接层（分类层）换成21-way的分类层（20类物体+背景），然后计算每个region proposal和ground truth 的IoU，对于IoU>0.5的region proposal被视为正样本，否则为负样本（即背景）。另外，由于对于一张图片的多有候选区域来说，负样本是远远大于正样本数，所以需要将正样本进行上采样来保证样本分布均衡。在每次迭代的过程中，选择层次采样，每个mini-batch中采样两张图像，从中随机选取32个正样本和96个负样本组成一个mini-batch（128，正负比：1：3）。我们使用0.001的学习率和SGD来进行训练，提取特征的CNN网络经过了预训练和微调后不再训练，就固定不变了，只单纯的作为一个提特征的工具了 SVM线性分类器，对特征进行分类：在训练CNN提取特征时，设置的IOU是0.5以上为正样本，小于0.5的是负样本。但在SVM分类中，只有bbox完全包围了物体（也可以理解为IOU＞0.7时）才是正样本，IOU小于0.3的是负样本。前者是大样本训练，后者是小样本训练，svm适用于少样本训练，如果用CNN反而不合适 用SVM对每个特征向量进行评分，然后用非极大值抑制 简单说就是： 给定一张输入图片，从图片中提取 2000 个类别独立的候选区域 对于每个区域利用 CNN 抽取一个固定长度的特征向量 再对每个特征向量利用 SVM 进行目标分类 测试步骤： Region proposal的确定：VOC测试图像输入后，利用SS搜索方法，根据相似度从大到小排序，筛选出2000个region proposals RP的Features提取：将RP通过resize成 227 \\times 227，然后分别输入进CNN特征提取网络，得到了2000个4096维features SVM分类：将(2000,4096)维矩阵输入进SVM分类器中，最终得到(2000，21)矩阵。每一行的21个列值，分别代表了这个RP属于每一个类的可能性。通过提前设置好的background阈值\\alpha和所属于类的阈值\\beta，筛选出满足条件的m个RP区域 BoundingBox-Regression：将(m,4096)维矩阵输入进 (4096,4)的回归矩阵 d dd 中，最后输出(m,4)偏移矩阵。代表RP中心点的位置偏移 和 bbox的尺寸变换 将SVM筛选出的m个RP区域对应的特征向量，组成(m,4096)矩阵 代入 (4096,4)的回归矩阵d中，最后输出(m,4)偏移矩阵 Non-maximum suppression处理：只画出SVM筛选出的m个RP区域的修正后的检测框，进行非极大值抑制(NMS)，得到最终检测结果 缺点： 重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间 selective search方法生成region proposal，对一帧图像，需要花费2秒 三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大 spp-net SPP-Net: 出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》 所有的神经网络都是需要输入固定尺寸的图片，比如 224 \\times 224(ImageNet)、 32 \\times 32(LenNet)、 96\\times 96等。这样对于我们希望检测各种大小的图片的时候，需要经过crop，或者warp等一系列操作，这都在一定程度上导致图片信息的丢失和变形，限制了识别精确度 为什么要固定输入图片的大小？：卷积层的参数和输入大小无关，它仅仅是一个卷积核在图像上滑动，不管输入图像多大都没关系，只是对不同大小的图片卷积出不同大小的特征图，但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来,需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的feature的大小，因此，固定长度的约束仅限于全连接层 SPP-Net在最后一个卷积层后，接入了金字塔池化层，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出 金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义(多尺度特征提取出固定大小的特征向量) SPP-Net，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。 特征提取阶段。这一步就是和R-CNN最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度会大大提升 最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别 难点 候选区域（原图与特征图）的映射 假设(x’,y’)表示特征图上的坐标点，坐标点(x,y)表示原输入图片上的点，那么它们之间有如下转换关系，这种映射关心与网络结构有关： (x,y) = (S \\times x' , S \\times y' ) 其中S就是CNN中所有的strides的乘积，包含了池化、卷积的stride fast-rcnn Fast-Rcnn：提出了ROI pooling R-CNN存在一些问题： 训练分多步：R-CNN的训练先要fine tuning一个预训练的网络，然后针对每个类别都训练一个SVM分类器，最后还要用regressors对bounding-box进行回归，另外region proposal也要单独用selective search的方式获得，步骤比较繁琐 时间和内存消耗大：在训练SVM和回归的时候需要用网络训练的特征作为输入，特征保存在磁盘上再读入的时间消耗还是比较大的 测试慢：每张图片的每个region proposal都要做卷积，重复操作太多 虽然在Fast RCNN之前有提出过SPPnet算法来解决RCNN中重复卷积的问题，但是SPPnet依然存在和RCNN一样的一些缺点比如：训练步骤过多，需要训练SVM分类器，需要额外的回归器，特征也是保存在磁盘上 因此Fast RCNN相当于全面改进了原有的这两个算法，不仅训练步骤减少了，也不需要额外将特征保存在磁盘上 基于VGG16的Fast RCNN算法的速度： 在训练速度上比RCNN快了将近9倍，比SPPnet快大概3倍 测试速度比RCNN快了213倍，比SPPnet快了10倍 在VOC2012上的mAP在66%左右 网络有两个输入：图像和对应的region proposal。其中region proposal由selective search方法得到，没有表示在流程图中 对每个类别都训练一个回归器，且只有非背景的region proposal才需要进行回归 ROI pooling：ROI Pooling的作用是对不同大小的region proposal，从最后卷积层输出的feature map提取大小固定的feature map(ROI Pooling使用自适应(根据输入feature的大小自调整)池化区域，不再固定池化区域大小，而固定池化区域个数，这样就确保了输入什么大小的feature，输出的feature大小完全相等，等于池化区域个数)。简单讲可以看做是SPPNet的简化版本，因为全连接层的输入需要尺寸大小一样，所以不能直接将不同大小的region proposal映射到feature map作为输出，需要做尺寸变换。在文章中，VGG16网络使用H = W = 7的参数，即将一个h \\times w的region proposal分割成 H \\times W大小的网格，然后将这个region proposal映射到最后一个卷积层输出的feature map，最后计算每个网格里的最大值作为该网格的输出，所以不管ROI pooling之前的feature map大小是多少，ROI pooling后得到的feature map大小都是 H \\times W 简单说ROI pooling就是： 把图片上selective search选出的候选框映射到特征图上对应的位置，这个映射是根据输入图片缩小的尺寸来的； 将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同） 对每个sections进行max pooling操作； 这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps 两个loss：第一个优化目标是分类，使用softmax（就不用像前面的R-CNN和SPP再用SVM了），第二个优化目标是bbox regression，使用了一个平滑的L1-loss ROI Pooling 与 SPP 的区别： 通过上面的介绍，可以看到两者起到的作用是相同的，把不同尺寸的特征输入转化为相同尺寸的特征输出。SPP针对同一个输入使用了多个不同尺寸的池化操作，把不同尺度的结果拼接作为输出；而ROI Pooling可看作单尺度的SPP，对于一个输入只进行一次池化操作 可以看出Fast RCNN主要有3个改进： 卷积不再是对每个region proposal进行，而是直接对整张图像，这样减少了很多重复计算。原来RCNN是对每个region proposal分别做卷积，因为一张图像中有2000左右的region proposal，肯定相互之间的重叠率很高，因此产生重复计算 用ROI pooling进行特征的尺寸变换，因为全连接层的输入要求尺寸大小一样，因此不能直接把region proposal作为输入 将regressor放进网络一起训练，每个类别对应一个regressor，同时用softmax代替原来的SVM分类器 在实际训练中，每个mini-batch包含2张图像和128个region proposal（或者叫ROI），也就是每张图像有64个ROI。然后从这些ROI中挑选约25%的ROI，这些ROI和ground truth的IOU值都大于0.5。另外只采用随机水平翻转的方式增加数据集 测试的时候则每张图像大约2000个ROI 总结： Fast RCNN将RCNN众多步骤整合在一起，不仅大大提高了检测速度，也提高了检测准确率。其中，对整张图像卷积而不是对每个region proposal卷积，ROI Pooling，分类和回归都放在网络一起训练的multi-task loss是算法的三个核心。另外还有SVD分解等是加速的小贡献，数据集的增加时mAP提高的小贡献 当然Fast RCNN的主要缺点在于region proposal的提取使用selective search，目标检测时间大多消耗在这上面（提region proposal 2~3s，而提特征分类只需0.32s），这也是后续Faster RCNN的改进方向之一 缺点： 依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒） 无法满足实时应用，没有真正实现端到端训练测试 利用了GPU，但是region proposal方法是在CPU上实现的 总结 RCNN: 给定一张输入图片，通过 Selective Search从图片中提取 2000 个类别独立的候选区域 对于每个区域利用 CNN 抽取一个固定长度的特征向量 对每个特征向量利用 SVM 进行目标分类 对于SVM分好类的Region Proposal做边框回归，用Bounding box回归值校正原来的建议窗口，生成预测窗口坐标 缺点： 重复计算，每个region proposal，都需要经过一个AlexNet特征提取 selective search方法生成region proposal，对一帧图像，需要花费2秒 三个模块(提取、分类、回归)是分别训练的，并且在训练时候，对于存储空间消耗较大 训练分为多个阶段，步骤繁琐：微调网络+训练SVM+训练边框回归器 SVM和回归是事后操作，在SVM和回归过程中CNN特征没有被学习更新 SPPNet: 金字塔池化层 当网络输入的是一张任意大小的图片，这个时候我们可以一直进行卷积、池化，直到网络的倒数几层的时候，也就是我们即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（多尺度特征提取出固定大小的特征向量） 首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样 特征提取阶段,把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量，只需要一次对整张图片进行特征提取，速度会大大提升 最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别 Fast-RCNN: 给定一张输入图片，通过 Selective Search从图片中提取 2000 个类别独立的候选区域 对每个类别都训练一个回归器，且只有非背景的region proposal才需要进行回归。 ROI pooling：ROI Pooling的作用是对不同大小的region proposal，从最后卷积层输出的feature map提取大小固定的feature map。简单讲可以看做是SPPNet的简化版本(把图片上selective search选出的候选框映射到特征图上对应的位置，这个映射是根据输入图片缩小的尺寸来的)，因为全连接层的输入需要尺寸大小一样，所以不能直接将不同大小的region proposal映射到feature map作为输出，需要做尺寸变换 两个loss：第一个优化目标是分类，使用softmax（就不用像前面的R-CNN和SPP再用SVM了），第二个优化目标是bbox regression，使用了一个平滑的L1-loss 优点 卷积不再是对每个region proposal进行，而是直接对整张图像，这样减少了很多重复计算 用ROI pooling进行特征的尺寸变换，因为全连接层的输入要求尺寸大小一样 将regressor放进网络一起训练，每个类别对应一个regressor，同时用softmax代替原来的SVM分类器 三个核心：对整张图像卷积、ROI Pooling、分类和回归一起训练的multi-task loss 主要缺点在于region proposal的提取使用selective search，目标检测时间大多消耗在这上面 相比R-CNN，主要两处不同： 最后一层卷积层后加了一个ROI pooling layer； 损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练 faster-rcnn 推荐这篇文章-Object Detection and Classification using R-CNNs Faster R-CNN：提出了RPN(region proposal network) 主要就是多了一个RPN(region proposal network)，就是在卷积提取特征之后，多出一条路来进行候选框的提取 推荐有关RPN层的文章：RPN层解析 RPN只是将框内认为是目标，框外认为是背景，做了个二分类，至于框内目标具体是啥，最终是交给分类网络去做 下图来自Faster-RCNN（二）之RPN层 Faster-RCNN: RPN(region proposal network)，就是在卷积提取特征之后，多出一条路来进行候选框的提取 将整张图片输入CNN，进行特征提取 用RPN生成建议窗口(proposals)，每张图片生成300个建议窗口 通过RoI pooling层使每个RoI生成固定尺寸的feature map 利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练 相比Fast R-CNN，主要两处不同： 使用RPN(Region Proposal Network)代替原来的Selective Search方法产生建议窗口； 产生建议窗口的CNN和目标检测的CNN共享 如何高效快速产生建议框？ Faster R-CNN创造性地采用卷积网络自行产生建议框，并且和目标检测网络共享卷积网络，使得建议框数目从原有的约2000个减少为300个，且建议框的质量也有本质的提高 Yolo系列 Yolov1 网络结构 YOLO_v1讲解 浅谈不同版本YOLO的区别（V1-V3） 输出层7 \\times 7 \\times 30的含义 7 \\times 7把原图平均分成49个网格，每个网格允许预测出2个边框，总共 49*2=98 个bounding box。可以理解为98个候选区，它们很粗略的覆盖了图片的整个区域 30代表20个类别的条件概率和两个boundingbox的五个参数（中心坐标x，y，框的宽和高，置信度） 每个cell只能预测一个类别，并且无法解决物体重叠的问题，此外，YOLOV1对小物体的检测效果也不理想 loss 最核心的loss详解 可以认为Yolo系列里最核心的就是loss的设计，理解YOLOv1的损失函数可以对后续的YOLO系列有一定的帮助 因为后续的YOLO版本在损失函数设计上有所改进，但仍然保留了一些基本的思想和原则 先看来以下loss的计算公式 \\begin{array}{l} loss = \\lambda_{\\text {coord }} \\sum_{i=0}^{S^{2}} \\sum_{j=0}^{B} \\mathbb{1}_{i j}^{\\mathrm{obj}}\\left(x_{i}-\\hat{x}_{i}\\right)^{2}+\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\\\ \\qquad + \\lambda_{\\text {coord }} \\sum_{i=0}^{S^{2}} \\sum_{j=0}^{B} \\mathbb{1}_{i j}^{\\mathrm{obj}}\\left(\\sqrt{w_{i}}-\\sqrt{\\hat{w}_{i}}\\right)^{2}+\\left(\\sqrt{h_{i}}-\\sqrt{\\hat{h}_{i}}\\right)^{2} \\\\ \\qquad +\\sum_{i=0}^{S^{2}} \\sum_{j=0}^{B} \\mathbb{1}_{i j}^{\\mathrm{obj}}\\left(C_{i}-\\hat{C}_{i}\\right)^{2} \\\\ \\qquad + \\lambda_{\\text {noobj }} \\sum_{i=0}^{S^{2}} \\sum_{j=0}^{B} \\mathbb{1}_{i j}^{\\mathrm{noobj}}\\left(C_{i}-\\hat{C}_{i}\\right)^{2} \\\\ \\qquad +\\sum_{i=0}^{S^{2}} \\mathbb{1}_{i}^{\\mathrm{obj}} \\sum_{c \\in \\text { classes }} \\left(p_{i}(c)-\\hat{p}_{i}(c)\\right)^{2} \\end{array} 这里的loss分成了五个部分： 含目标的xy: 含有物体中心点的cell里，负责预测的bbox预测出的xy的MSE 含目标的wh: 含有物体中心点的cell里，负责预测的bbox预测出的wh的MSE 含目标的置信度: 含有物体中心点的cell里，负责预测的bbox预测出的存在物体的置信度的误差 不含目标的置信度: 不含有物体中心点的cell里，每一个bbox都要参与到loss的计算，C_i=0；\\hat{C}_{i}=bbox的预测值，然后累加MSE 含目标的分类损失: 含有物体中心点的cell里，预测出的物体类别向量和GT对应的向量的MSE 不含目标的loss 当cell中不包含物体的中心点，那这个cell的bbox预测出物体的置信度就要趋近于0 A步骤：Yolo对输入图像前向传播的输出张量，下面是该图的目标张量 B步骤：乘以不含物体中心点的cell的掩码，相当于只留下无目标的cell C步骤：挖去了三个洞(去除了3个含有物体中心点的cell的数据) D步骤：一共49个格子，挖去3个剩46，B=2，因此拉直的向量长度为92 E步骤：计算MSE，这里对应公式中的第4部分 含目标的loss 如果有物体，预测正确类别误差就等于0 A步骤：Yolo对输入图像前向传播的输出张量，下面是该图的目标张量 B步骤：乘以含物体中心点的cell的掩码，，相当于只留下含目标的cell C步骤：只留下三个向量(3个含有物体中心点的cell的数据) D步骤：向量合并 E步骤：提取出类别的向量 F步骤：计算MSE，这里对应公式中的第5部分 G步骤：提取出xywh和置信度的向量，向量合并 H步骤：每一个cell会有2个bbox，计算每个bbox与ground truth的IOU，然后得到索引矩阵(可以用来确定哪个bbox来处理这个cell) I步骤：提取出有责任的bbox的向量 J步骤：bbox预测出的xy与GT算MSE，对应公式中的第1部分；bbox预测出的wh与GT算MSE，对应公式中的第2部分 K步骤：bbox预测出的置信度与GT算MSE，对应公式中的第3部分 测试阶段 输入图像，经过YOLO网络后，输出 7 \\times 7 \\times (2 \\times 5 + 20 )的张量 计算每个bbox的类别得分 从每一个cell中找出20个类别score中最大的类别，作为cell中两box预测出的类别 针对每一个cell中的两个bbox，选择置信度大的box来代表这个cell，并计算出box的类别得分 剩下49个bbox，最后进行NMS Score_{i j} = P ( C_{i} | Object ) * Confidence _{j} Yolov2(Yolo9000) YOLO算法之YOLOv2精讲 相比较于Yolov1主要改进点： 网络改进：使用DarckNet19代替了YOLOv1的GoogLeNet网络，这里主要改进是去掉了全连接层，用卷积和softmax进行代替 加入BN：使用Batch Normalization对网络进行优化，让网络提高了收敛性，同时还消除了对其他形式的正则化(regularization)的依赖 HighResolution Classifier：新的YOLO网络把分辨率直接提升到了448 \\times 448，这也意味着原有的网络模型必须进行某种调整以适应新的分辨率输入，mAP获得了4%的提升 Multi-ScaleTraining：让网络在不同的输入尺寸上都能达到一个很好的预测效果，同一网络能在不同分辨率上进行检测。当输入图片尺寸比较小的时候跑的比较快，输入图片尺寸比较大的时候精度高 Anchor机制：Anchor首先要预设好几个虚拟框，在用回归的方法确定最终的预测框，在YOLOv2中，使用K-means算法来生成Anchor bbox，如图1所示，当k=5时，模型的复杂度与召回率达到了一个比较好的平衡，所以YOLOv2使用了5个Anchor bbox 分类、检测训练集联合训练：联合训练方法思路简单清晰，Yolo v2中物体矩形框生成，不依赖于物理类别预测，二者同时独立进行 当输入是检测数据集时，标注信息有类别、有位置，那么对整个loss函数计算loss，进行反向传播 当输入图片只包含分类信息时，loss函数只计算分类loss，其余部分loss为零 当然，一般的训练策略为，先在检测数据集上训练一定的epoch，待预测框的loss基本稳定后，再联合分类数据集、检测数据集进行交替训练，同时为了分类、检测数据量平衡，作者对coco数据集进行了上采样，使得coco数据总数和ImageNet大致相同 输出和Yolov1比较如下 YOLOv1是的输出7 \\times 7 \\times 30的多维向量，其中7 \\times 7是分辨率，对原图进行了7 \\times 7的分割，每个网格对应一个包含30个参数的向量，每个向量中包含两个bbox，每个bbox中包含5个向量，分别是bbox的质心坐标（x, y）和bbox的长和宽，还有一个bbox的置信度，剩下20个则是类别概率 YOLOv2输出的是13 \\times 13 \\times 5 \\times 25的一个多维向量，其中13 \\times 13是分辨率，也就是说网络将输入图片分成了13 \\times 13的网格，每一个网格对应一个包含5 \\times 25 =125 个参数的一维向量，其中5代表5个Anchor bbox，每个Anchor bbox中包含25个参数，分别是bbox的质心坐标(x,y)和bbox的长和宽，还有一个bbox的置信度，剩下20个则是类别概率。这样的好处是YOLOv2可以对一个区域进行多个标签的预测，最主要的改变是：bbox的四个位置参数的损失函数计算方法发生了改变 训练主要包括以下三个阶段： 先在ImageNet分类数据集上预训练Darknet-19，此时模型输入224 \\times 224，共训练160个epochs。 将网络的输入调整为448 \\times 448，继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。 修改Darknet-19分类模型为检测模型：即移除最后一个卷积层、global avg pooling层以及softmax层，并且新增了三个3 \\times 3 \\times 2014卷积层，同时增加了一个passthrough层，最后使用1 \\times 1卷积层输出预测结果，并在检测数据集上继续finetune网络 yolov3 睿智的目标检测26——Pytorch搭建yolo3目标检测平台 从特征获取预测结果的过程可以分为两个部分，分别是： 构建FPN特征金字塔进行加强特征提取 利用Yolo Head对三个有效特征层进行预测 在特征利用部分，YoloV3提取多特征层进行目标检测，一共提取三个特征层，三个特征层位于主干部分Darknet53的不同位置，分别位于中间层，中下层，底层 三个特征层的shape分别为(52,52,256)、(26,26,512)、(13,13,1024) 输出维度解析 网络有三个输出，维度分别为bs \\times 13 \\times 13 \\times 75、bs \\times 26 \\times 26 \\times 75和bs \\times 52 \\times 52 \\times 75，其中75可以分解成3 \\times (4+1+20) 3表示三个候选框，4表示的是xywh的位置信息，1表示是否为背景，20为物体的类别数(这里用的是voc数据集) 因为有三个输出，因此该网络有个13 \\times 13 \\times 3 + 26\\times 26\\times 3 + 52 \\times 52\\times 3 = 10647预测框 对于这些预测框，如果其与任何真实框的IoU大于一定的阈值（如0.5），则将其分配给对应的输出层作为正样本。否则，将其视为负样本 YOLOv3 使用的损失函数是组合了多个部分的综合损失函数，其中包括定位损失（Localization Loss）、分类损失（Classification Loss）和置信度损失（Confidence Loss） L=L_{c l s}+L_{c o n f}+L_{l o c} 只有正样本才有这三类损失，而负样本只有置信度损失 以下是 YOLOv3 的损失函数的伪代码实现，包含详细注释： def yolo_loss(pred_boxes, pred_cls, target_boxes, target_cls): # pred_boxes: 预测的边界框坐标，shape 为 [batch_size, num_anchors, grid_size, grid_size, 4] # pred_cls: 预测的类别概率，shape 为 [batch_size, num_anchors, grid_size, grid_size, num_classes] # target_boxes: 真实的边界框坐标，shape 为 [batch_size, num_anchors, grid_size, grid_size, 4] # target_cls: 真实的类别标签，shape 为 [batch_size, num_anchors, grid_size, grid_size, num_classes] # 计算预测框和真实框的IoU iou_scores = calculate_iou(pred_boxes, target_boxes) # shape: [batch_size, num_anchors, grid_size, grid_size] # 计算类别损失，使用交叉熵损失函数 cls_loss = cross_entropy_loss(pred_cls, target_cls) # shape: [batch_size, num_anchors, grid_size, grid_size] # 计算边界框损失，使用平滑L1损失函数 box_loss = smooth_l1_loss(pred_boxes, target_boxes) # shape: [batch_size, num_anchors, grid_size, grid_size] # 计算正样本的掩码，即与真实框IoU大于阈值的位置为1，其余为0 pos_mask = (iou_scores > positive_iou_threshold).float() # shape: [batch_size, num_anchors, grid_size, grid_size] # 计算负样本的掩码，即与真实框IoU小于阈值的位置为1，其余为0 neg_mask = (iou_scores 正负样本数 在 YOLOv3 中，通常会为每个目标选择一个正样本，即与真实目标框具有最高 IoU 的预测框。这确保了每个目标都有至少一个正样本来参与损失函数的计算和网络的训练。 至于负样本的选择，一般会设置一个阈值来确定预测框与真实框之间的 IoU 阈值。如果预测框的 IoU 低于该阈值，则被视为负样本。对于每个负样本，可以选择保留一定数量的负样本，以确保正负样本的平衡性。 具体来说，关于正负样本的选择数量并没有一个固定的标准，它可以根据具体的数据集和应用场景来确定。在实践中，可以根据数据集的统计信息和训练效果进行调整，以找到一个适合的正负样本比例，从而平衡目标检测的准确性和效率。 yolov5 Yolov8 Yolov8 github地址和文档 目标检测 Model size (pixels) mAPval 50-95 Speed CPU ONNX (ms) Speed A100 TensorRT (ms) params (M) FLOPs (B) YOLOv8n 640 37.3 80.4 0.99 3.2 8.7 YOLOv8s 640 44.9 128.4 1.20 11.2 28.6 YOLOv8m 640 50.2 234.7 1.83 25.9 78.9 YOLOv8l 640 52.9 375.2 2.39 43.7 165.2 YOLOv8x 640 53.9 479.1 3.53 68.2 257.8 tensorRt加速 可以用tensorRt加速，环境教程可以参考这个文档 进行tensort加速，cmake编译失败，缺少zlibwapi.dll文件，解决办法，去cudnn官网下载zlib123dllx64 lib文件放到C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\lib dll文件放到C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin YoloV8提供了导出工具，详见文档，python代码导出可以这样子写 def run(): model = YOLO(\"../../weights/yolov8s.pt\") # load an official detection model model.export(format='engine', device='0', workspace=8, batch=4, dynamic=True) 或者直接命令行执行 yolo mode=export model=yolov8s.pt format=engine device=0 workspace=8 batch=4 dynamic=True 关键参数如下： device: 其中device必须是GPU，可以是多张显卡 workspace: TensorRT: workspace size (GB) batch: 最大批次大小，当设置参数时必须有dynamic=True，可以运行最大不超过batch的数量 dynamic: 动态参数，ONNX/TF/TensorRT: dynamic axes imgsz: image size as scalar or (h, w) list, i.e. (640, 480) 注意事项： TensorRT发布的模型(engine)不能跨平台使用 例如linux发布的模型不能在windows下用 TensorRT发布的模型需要在相同GPU算力(compute capability)的情况下使用 否则会导致compute capability不匹配问题，例如算力6.1发布的模型不能在7.5上用 动态batch和动态宽高的处理方式 动态batch：源自tensorRT编译时对batch的处理，若静态batch则意味着无论你多少图，都按照固定大小batch推理，耗时是固定的 导出模型时，注意view操作不能固定batch维度数值，通常写-1 导出模型时，通常可以指定dynamic_axes，实际上不指定也没关系 动态宽高：源自onnx导出时指定的宽高是固定的，trt编译时也得到固定大小引擎，此时若你想得到一个不同大小的trt引擎时，就需要动态宽高的存在。而使用trt的动态宽高会带来太多不必要的复杂度，这里使用中间方案，编译时修改onnx输入实现相对动态，避免重回pytorch再做导出 不建议使用dynamic_axes指定0以外的维度为动态，复杂度太高，并且存在有的layer不支持，这种需求也不常用，性能也很差 真正需要的，是onnx文件已经导出，但是输入shape固定了，此时希望修改这个onnx的输入shape 步骤一: 使用TRT::compile函数的inputsDimsSetup参数重定义输入的shape 步骤二: 使用TRT:set_layer_hook_reshape钩子动态修改reshape的参数实现适配 其他检测系列 SSD FPN Feature Pyramid Networks for Object Detection 概述 FPN(feature pyramid networks) 是何凯明等作者提出的适用于多尺度目标检测算法 原来多数的object detection算法(比如 faster rcnn)都是只采用顶层特征做预测，但我们知道低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略 另外虽然也有些算法采用多尺度特征融合的方式，但是一般是采用融合后的特征做预测，而本文不一样的地方在于预测是在不同特征层独立进行的，这里主要借鉴了 ResNet的残差: 结合了浅层特征和深层特征 SSD的检测策略: 在不同分辨率的特征图上分别做预测 特征金字塔图示，越粗表示特征语义更强 薰风读论文：Feature Pyramid Network 详解特征金字塔网络FPN的来龙去脉 图中将特征金字塔和其他的金字塔做了比较 (a)是传统中的图片金字塔，图片缩放到不同的大小，分别预测，每个特征提取/预测都是独立进行的，同一张图片的不同分辨率，也很难共享它们中间提取的特征，让模型预测的过程费时费力 (b)是原生的CNN提取的特征，由于后续存在池化和降采样，浅层网络的特征图可以保留更多的分辨率，但是特征语义较为低级 (c)是SSD中的特征，把不同分辨率特征，在不同分辨率的特征上直接预测，那么大物体小物体都能预测到，但仍存在底层特征语义不够和最高分辨率不高的问题 (d)是FPN中用的特征金字塔，结合深浅特征，兼顾分辨率与特征语义 RetinaNet Focal Loss for Dense Object Detection 2018 ppt: Focal Loss for Dense Object Detection RetinaNet是使用FPN和Focal Loss(详情看本站深度学习核心之损失函数部分)的目标检测模型，能够有效解决类别不平衡问题 它通过特征金字塔网络生成多尺度的特征图，并使用Focal Loss重点关注难以分类的样本，从而提高了检测性能 DETR End-to-End Object Detection with Transformers DETR 2020 深度学习之目标检测（十一）--DETR详解 继Transformer应用于图像分类后，Transformer应用于图像目标检测的开山之作–DEtection TRansformer，其大大简化了目标检测的框架，更直观 DETR是Facebook团队于2020年提出的基于Transformer的端到端目标检测，没有非极大值抑制NMS后处理步骤、没有anchor等先验知识和约束，整个由网络实现端到端的目标检测实现，大大简化了目标检测的pipeline。结果在COCO数据集上效果与Faster RCNN相当，在大目标上效果比Faster RCNN好，且可以很容易地将DETR迁移到其他任务例如全景分割 目标跟踪 万字长文 | 多目标跟踪最新综述(基于Transformer/图模型/检测和关联/孪生网络) 【yolov4目标检测】(2) 多目标跟踪，案例：车辆行人的跟踪和计数，附python完整代码和数据集 yolov4-deepsort tensorflow代码 转载：一线算法工程师整理！超实用的3大多目标跟踪算法 Deep-Sort 多目标跟踪算法原理和代码解析 Darklabel多目标跟踪标注工具 万字综述：目标检测模型YOLOv1-v7深度解析 目标跟踪 = 目标检测+目标跟踪算法 目标追踪算法分为单目标追踪SOT(Single-Object Track)和多目标追踪MOT(Multi-Object Track)[1][2] 在单目标跟踪中，使用给定的初始目标位置，在后续视频帧中对给定的物体进行位置预测 多目标跟踪算法，大部分都是不考虑初始目标位置的，目标可自行消失与产生 目标跟踪分类 目标跟踪通常可分为单目标跟踪和多目标跟踪两类 单目标跟踪 多目标跟踪 SDE(separate detecting and embeding) 每部分独立优化能够取得比较高的精度，缺点就是计算量会增加 JDE(joint detecting and embeding) JDE将目标检测与REID特征提取放在一个网络，这样能有效减少计算量，但是多任务学习的精度会低些 解决的任务和视频目标检测相同的点在于都需要对每帧图像中的目标精准定位，不同点在于目标跟踪不考虑目标的识别问题 SDE将REID特征提取和目标检测分为两个独立网络来实现，这样做的优点是每部分独立优化能够取得比较高的精度，缺点就是计算量会增加；JDE将目标检测与REID特征提取放在一个网络，这样能有效减少计算量，但是多任务学习的精度目前来说还没有SDE高。在工程应用上我更偏向于JDE，毕竟跟踪要保证实时性，在能够提取一个不太差的REID特征基础上，加强检测器性能和优化数据关联部分也能一定程度上弥补REID特征不够好带来的性能损失 Sort DeepSort 目标跟踪基础——DeepSORT 【MOT】详解DeepSORT多目标追踪模型 StrongSort StrongSort相比于DeepSort的区别： 使用BoT替代CNN做外表特征的提取 使用EMA(exponential moving average：指数移动平均)策略更新新帧中的目标外观特征 EMA更新策略不仅提高了匹配质量，而且减少了时间消耗 在做Kalman filter之前，使用ECC（enhanced correlation coefficient：增强相关系数）进行相机运动补偿；并使用NSA Kalman代替Kalman进行运动特征获取 将运动信息和外观信息结合来进行匹配 使用Vanilla全局线性赋值代替了匹配级联 BotSort BoT-SORT: Robust Associations Multi-Pedestrian Tracking BoT-SORT ｜超越 DeepSORT、StrongSORT++ 和 ByteTrack ByteTrack ByteTrack: Multi-Object Tracking by Associating Every Detection Box 依赖的算法 卡尔曼滤波 卡尔曼滤波被广泛应用于无人机、自动驾驶、卫星导航等领域 简单来说，其作用就是基于传感器的测量值来更新预测值，以达到更精确的估计 假设我们要跟踪小车的位置变化，如下图所示，蓝色的分布是卡尔曼滤波预测值，棕色的分布是传感器的测量值，灰色的分布就是预测值基于测量值更新后的最优估计 在目标跟踪中，需要估计track的以下两个状态： 均值(Mean)：表示目标的位置信息，由bbox的中心坐标 (cx, cy)，宽高比r，高h，以及各自的速度变化值组成 由8维向量表示为 x = [cx, cy, r, h, vx, vy, vr, vh]，各个速度值初始化为0 协方差(Covariance )：表示目标位置信息的不确定性，由8x8的对角矩阵表示，矩阵中数字越大则表明不确定性越大，可以以任意值初始化 卡尔曼滤波分为两个阶段：(1) 预测track在下一时刻的位置，(2) 基于detection来更新预测的位置。 匈牙利匹配算法 目标跟踪初探(DeepSORT) 先介绍一下什么是分配问题（Assignment Problem）：假设有N个人和N个任务，每个任务可以任意分配给不同的人，已知每个人完成每个任务要花费的代价不尽相同，那么如何分配可以使得总的代价最小 举个例子，假设现在有3个任务，要分别分配给3个人，每个人完成各个任务所需代价矩阵(cost matrix)如下所示(这个代价可以是金钱、时间等等)： Task_1 Task_2 Task_3 Person_1 15 40 45 Person_2 20 60 35 Person_3 20 40 25 怎样才能找到一个最优分配，使得完成所有任务花费的代价最小呢？ 匈牙利算法(又叫KM算法)就是用来解决分配问题的一种方法，它基于定理： 如果代价矩阵的某一行或某一列同时加上或减去某个数，则这个新的代价矩阵的最优分配仍然是原代价矩阵的最优分配 算法步骤(假设矩阵为N阶方阵)： 对于矩阵的每一行，减去其中最小的元素 对于矩阵的每一列，减去其中最小的元素 用最少的水平线或垂直线覆盖矩阵中所有的0 如果线的数量等于N，则找到了最优分配，算法结束，否则进入步骤5 找到没有被任何线覆盖的最小元素，每个没被线覆盖的行减去这个元素，每个被线覆盖的列加上这个元素，返回步骤3 继续拿上面的例子做演示： step1 每一行最小的元素分别为15、20、20，减去得到： Task_1 Task_2 Task_3 Person_1 0 25 30 Person_2 0 40 15 Person_3 0 20 5 step2 每一列最小的元素分别为0、20、5，减去得到： Task_1 Task_2 Task_3 Person_1 0 5 25 Person_2 0 20 10 Person_3 0 0 0 step3 用最少的水平线或垂直线覆盖所有的0，得到： Task_1 Task_2 Task_3 Person_1 0 5 25 Person_2 0 20 10 Person_3 0 0 0 step4 线的数量为2，小于3，进入下一步； step5 现在没被覆盖的最小元素是5，没被覆盖的行(第一和第二行)减去5，得到： Task_1 Task_2 Task_3 Person_1 -5 0 20 Person_2 -5 15 5 Person_3 0 0 0 被覆盖的列(第一列)加上5，得到： Task_1 Task_2 Task_3 Person_1 0 0 20 Person_2 0 15 5 Person_3 5 0 0 跳转到step3，用最少的水平线或垂直线覆盖所有的0，得到： Task_1 Task_2 Task_3 Person_1 0 0 20 Person_2 0 15 5 Person_3 5 0 0 step4：线的数量为3，满足条件，算法结束 显然，将任务2分配给第1个人、任务1分配给第2个人、任务3分配给第3个人时，总的代价最小(0+0+0=0)： 所以原矩阵的最小总代价为40+20+25=85 Task_1 Task_2 Task_3 Person_1 15 40 45 Person_2 20 60 35 Person_3 20 40 25 sklearn里的linear_assignment()函数以及scipy里的linear_sum_assignment()函数都实现了匈牙利算法 import numpy as np from scipy.optimize import linear_sum_assignment cost_matrix = np.array([ [15,40,45], [20,60,35], [20,40,25] ]) matches = linear_sum_assignment(cost_matrix) print('scipy API result:\\n', matches) >>> scipy API result: (array([0, 1, 2], dtype=int64), array([1, 0, 2], dtype=int64)) 追踪指标 多目标跟踪评价指标总结——MOTA、IDF1、HOTA等 MOTA(Multiple Object Tracking Accuracy): 指标体现多目标跟踪的准确度 MOTA=1-\\left(\\sum_{t}\\left(m_{t}+f p_{t}+m m e_{t}\\right)\\right) /\\left(\\sum_{t} g_{t}\\right) MOTA指标是衡量多目标跟踪算法精确性方面最重要的指标，以1为最佳情况，数值越高代表跟踪精确度越好 IDF1: 指标代表被检测和跟踪的目标中获取正确的ID的检测目标的比例，综合考虑ID准确率和ID召回率，代表两者的调和均值 IDF1=2 /(1 / IDP+1 / IDR) 其中，IDP代表ID跟踪的准确率，IDR代表ID跟踪的召回率，IDF1指标更聚焦于跟踪算法跟踪某个目标的时间长短，考察跟踪的连续性和重识别的准确性，IDF1以1为最佳情况，数值越高代表跟踪特定目标的精度越好 HOTA 道路监控管理 Roboflow数据集标注 目标检测算法_模型训练 目标跟踪算法_ByteTrack(实时性) 物体检测分类 主动学习和强化学习 主动学习：是一种通过主动选择最有价值的样本进行标注的机器学习或人工智能方法。其目的是使用尽可能少的、高质量的样本标注使模型达到尽可能好的性能。也就是说，主动学习方法能够提高样本及标注的增益，在有限标注预算的前提下，最大化模型的性能，是一种从样本的角度，提高数据效率的方案，因而被应用在标注成本高、标注难度大等任务中，例如医疗图像、无人驾驶、异常检测、基于互联网大数据的相关问题 强化学习介绍及应用 强化学习：强化学习是一个非常吸引人的人工智能领域，2016年 Alpha Go在围棋领域挑战李世石，以几乎碾压的结果夺冠，引起了人们对于人工智能的广泛讨论。2019年Alpha Star横空出世，在复杂的星际争霸2游戏中达到能和人类顶级玩家PK的水平，登上Nature。这两次与人类顶级玩家的抗衡之战，背后的技术都是强化学习。强化学习是机器学习领域的一个分支，强调基于环境而行动，以取得最大化的长期利益。与监督学习、非监督学习不同，监督学习解决如分类、回归等感知和认知类的任务，而强化学习处理决策问题，着重于环境的交互、序列决策、和长期收益。强化学习与环境的交互模式可以抽象为：智能体Agent在环境Environment中学习，根绝环境的状态State，执行动作Action，并根据环境反馈的奖励Reward来指导输出更好的动作 Copyright © narutohyc.com 2021 all right reserved，powered by Gitbook该文件修订时间： 2024-02-20 08:28:33 new Valine({el: \"#vcomments\",appId: 'evyLlP61gQN3G3OM2GQq1rzH-gzGzoHsz',appKey: 'utUrzoiqNaDEGlgr09JL1pXB',placeholder: '欢迎留下评论交流~',avatar: 'wavatar',meta: undefined,pageSize: 15,lang: 'zh-CN',recordIP: false}) "}}